// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package operations

import (
	"encoding/json"
	"errors"
	"fmt"
	"github.com/criblio/cribl-control-plane-sdk-go/internal/utils"
	"github.com/criblio/cribl-control-plane-sdk-go/models/components"
)

type TypeCloudflareR2 string

const (
	TypeCloudflareR2CloudflareR2 TypeCloudflareR2 = "cloudflare_r2"
)

func (e TypeCloudflareR2) ToPointer() *TypeCloudflareR2 {
	return &e
}
func (e *TypeCloudflareR2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cloudflare_r2":
		*e = TypeCloudflareR2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCloudflareR2: %v", v)
	}
}

// AuthenticationMethodCloudflareR2 - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodCloudflareR2 string

const (
	AuthenticationMethodCloudflareR2Auto   AuthenticationMethodCloudflareR2 = "auto"
	AuthenticationMethodCloudflareR2Secret AuthenticationMethodCloudflareR2 = "secret"
	AuthenticationMethodCloudflareR2Manual AuthenticationMethodCloudflareR2 = "manual"
)

func (e AuthenticationMethodCloudflareR2) ToPointer() *AuthenticationMethodCloudflareR2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodCloudflareR2) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "secret", "manual":
			return true
		}
	}
	return false
}

// SignatureVersionCloudflareR2 - Signature version to use for signing MinIO requests
type SignatureVersionCloudflareR2 string

const (
	SignatureVersionCloudflareR2V2 SignatureVersionCloudflareR2 = "v2"
	SignatureVersionCloudflareR2V4 SignatureVersionCloudflareR2 = "v4"
)

func (e SignatureVersionCloudflareR2) ToPointer() *SignatureVersionCloudflareR2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SignatureVersionCloudflareR2) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

// StorageClassCloudflareR2 - Storage class to select for uploaded objects
type StorageClassCloudflareR2 string

const (
	// StorageClassCloudflareR2Standard Standard
	StorageClassCloudflareR2Standard StorageClassCloudflareR2 = "STANDARD"
	// StorageClassCloudflareR2ReducedRedundancy Reduced Redundancy Storage
	StorageClassCloudflareR2ReducedRedundancy StorageClassCloudflareR2 = "REDUCED_REDUNDANCY"
)

func (e StorageClassCloudflareR2) ToPointer() *StorageClassCloudflareR2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *StorageClassCloudflareR2) IsExact() bool {
	if e != nil {
		switch *e {
		case "STANDARD", "REDUCED_REDUNDANCY":
			return true
		}
	}
	return false
}

// ServerSideEncryptionCloudflareR2 - Server-side encryption for uploaded objects
type ServerSideEncryptionCloudflareR2 string

const (
	// ServerSideEncryptionCloudflareR2Aes256 Amazon S3 Managed Key
	ServerSideEncryptionCloudflareR2Aes256 ServerSideEncryptionCloudflareR2 = "AES256"
)

func (e ServerSideEncryptionCloudflareR2) ToPointer() *ServerSideEncryptionCloudflareR2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ServerSideEncryptionCloudflareR2) IsExact() bool {
	if e != nil {
		switch *e {
		case "AES256":
			return true
		}
	}
	return false
}

// DataFormatCloudflareR2 - Format of the output data
type DataFormatCloudflareR2 string

const (
	// DataFormatCloudflareR2JSON JSON
	DataFormatCloudflareR2JSON DataFormatCloudflareR2 = "json"
	// DataFormatCloudflareR2Raw Raw
	DataFormatCloudflareR2Raw DataFormatCloudflareR2 = "raw"
	// DataFormatCloudflareR2Parquet Parquet
	DataFormatCloudflareR2Parquet DataFormatCloudflareR2 = "parquet"
)

func (e DataFormatCloudflareR2) ToPointer() *DataFormatCloudflareR2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataFormatCloudflareR2) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "parquet":
			return true
		}
	}
	return false
}

// BackpressureBehaviorCloudflareR2 - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCloudflareR2 string

const (
	// BackpressureBehaviorCloudflareR2Block Block
	BackpressureBehaviorCloudflareR2Block BackpressureBehaviorCloudflareR2 = "block"
	// BackpressureBehaviorCloudflareR2Drop Drop
	BackpressureBehaviorCloudflareR2Drop BackpressureBehaviorCloudflareR2 = "drop"
)

func (e BackpressureBehaviorCloudflareR2) ToPointer() *BackpressureBehaviorCloudflareR2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorCloudflareR2) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionCloudflareR2 - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionCloudflareR2 string

const (
	// DiskSpaceProtectionCloudflareR2Block Block
	DiskSpaceProtectionCloudflareR2Block DiskSpaceProtectionCloudflareR2 = "block"
	// DiskSpaceProtectionCloudflareR2Drop Drop
	DiskSpaceProtectionCloudflareR2Drop DiskSpaceProtectionCloudflareR2 = "drop"
)

func (e DiskSpaceProtectionCloudflareR2) ToPointer() *DiskSpaceProtectionCloudflareR2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionCloudflareR2) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// CompressionCloudflareR2 - Data compression format to apply to HTTP content before it is delivered
type CompressionCloudflareR2 string

const (
	CompressionCloudflareR2None CompressionCloudflareR2 = "none"
	CompressionCloudflareR2Gzip CompressionCloudflareR2 = "gzip"
)

func (e CompressionCloudflareR2) ToPointer() *CompressionCloudflareR2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionCloudflareR2) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// CompressionLevelCloudflareR2 - Compression level to apply before moving files to final destination
type CompressionLevelCloudflareR2 string

const (
	// CompressionLevelCloudflareR2BestSpeed Best Speed
	CompressionLevelCloudflareR2BestSpeed CompressionLevelCloudflareR2 = "best_speed"
	// CompressionLevelCloudflareR2Normal Normal
	CompressionLevelCloudflareR2Normal CompressionLevelCloudflareR2 = "normal"
	// CompressionLevelCloudflareR2BestCompression Best Compression
	CompressionLevelCloudflareR2BestCompression CompressionLevelCloudflareR2 = "best_compression"
)

func (e CompressionLevelCloudflareR2) ToPointer() *CompressionLevelCloudflareR2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionLevelCloudflareR2) IsExact() bool {
	if e != nil {
		switch *e {
		case "best_speed", "normal", "best_compression":
			return true
		}
	}
	return false
}

// ParquetVersionCloudflareR2 - Determines which data types are supported and how they are represented
type ParquetVersionCloudflareR2 string

const (
	// ParquetVersionCloudflareR2Parquet10 1.0
	ParquetVersionCloudflareR2Parquet10 ParquetVersionCloudflareR2 = "PARQUET_1_0"
	// ParquetVersionCloudflareR2Parquet24 2.4
	ParquetVersionCloudflareR2Parquet24 ParquetVersionCloudflareR2 = "PARQUET_2_4"
	// ParquetVersionCloudflareR2Parquet26 2.6
	ParquetVersionCloudflareR2Parquet26 ParquetVersionCloudflareR2 = "PARQUET_2_6"
)

func (e ParquetVersionCloudflareR2) ToPointer() *ParquetVersionCloudflareR2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ParquetVersionCloudflareR2) IsExact() bool {
	if e != nil {
		switch *e {
		case "PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6":
			return true
		}
	}
	return false
}

// DataPageVersionCloudflareR2 - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionCloudflareR2 string

const (
	// DataPageVersionCloudflareR2DataPageV1 V1
	DataPageVersionCloudflareR2DataPageV1 DataPageVersionCloudflareR2 = "DATA_PAGE_V1"
	// DataPageVersionCloudflareR2DataPageV2 V2
	DataPageVersionCloudflareR2DataPageV2 DataPageVersionCloudflareR2 = "DATA_PAGE_V2"
)

func (e DataPageVersionCloudflareR2) ToPointer() *DataPageVersionCloudflareR2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataPageVersionCloudflareR2) IsExact() bool {
	if e != nil {
		switch *e {
		case "DATA_PAGE_V1", "DATA_PAGE_V2":
			return true
		}
	}
	return false
}

type KeyValueMetadatumCloudflareR2 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumCloudflareR2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumCloudflareR2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumCloudflareR2) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumCloudflareR2) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputCloudflareR2 struct {
	// Unique ID for this output
	ID   string           `json:"id"`
	Type TypeCloudflareR2 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Cloudflare R2 service URL (example: https://<ACCOUNT_ID>.r2.cloudflarestorage.com)
	Endpoint string `json:"endpoint"`
	// Name of the destination R2 bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodCloudflareR2 `default:"auto" json:"awsAuthenticationMethod"`
	// Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	Region       any     `json:"region,omitempty"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.
	DestPath *string `json:"destPath,omitempty"`
	// Signature version to use for signing MinIO requests
	SignatureVersion *SignatureVersionCloudflareR2 `default:"v4" json:"signatureVersion"`
	ObjectACL        any                           `json:"objectACL,omitempty"`
	// Storage class to select for uploaded objects
	StorageClass *StorageClassCloudflareR2 `json:"storageClass,omitempty"`
	// Server-side encryption for uploaded objects
	ServerSideEncryption *ServerSideEncryptionCloudflareR2 `json:"serverSideEncryption,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatCloudflareR2 `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCloudflareR2 `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionCloudflareR2 `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	Description            *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionCloudflareR2 `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelCloudflareR2 `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionCloudflareR2 `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionCloudflareR2 `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumCloudflareR2 `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputCloudflareR2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCloudflareR2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "endpoint", "bucket"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCloudflareR2) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputCloudflareR2) GetType() TypeCloudflareR2 {
	if o == nil {
		return TypeCloudflareR2("")
	}
	return o.Type
}

func (o *OutputCloudflareR2) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCloudflareR2) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCloudflareR2) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCloudflareR2) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCloudflareR2) GetEndpoint() string {
	if o == nil {
		return ""
	}
	return o.Endpoint
}

func (o *OutputCloudflareR2) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputCloudflareR2) GetAwsAuthenticationMethod() *AuthenticationMethodCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputCloudflareR2) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputCloudflareR2) GetRegion() any {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputCloudflareR2) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputCloudflareR2) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputCloudflareR2) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputCloudflareR2) GetSignatureVersion() *SignatureVersionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputCloudflareR2) GetObjectACL() any {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputCloudflareR2) GetStorageClass() *StorageClassCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputCloudflareR2) GetServerSideEncryption() *ServerSideEncryptionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputCloudflareR2) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputCloudflareR2) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCloudflareR2) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputCloudflareR2) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputCloudflareR2) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputCloudflareR2) GetFormat() *DataFormatCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputCloudflareR2) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputCloudflareR2) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputCloudflareR2) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputCloudflareR2) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputCloudflareR2) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputCloudflareR2) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputCloudflareR2) GetOnBackpressure() *BackpressureBehaviorCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCloudflareR2) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputCloudflareR2) GetOnDiskFullBackpressure() *DiskSpaceProtectionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputCloudflareR2) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputCloudflareR2) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputCloudflareR2) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputCloudflareR2) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputCloudflareR2) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCloudflareR2) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputCloudflareR2) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputCloudflareR2) GetCompress() *CompressionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputCloudflareR2) GetCompressionLevel() *CompressionLevelCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputCloudflareR2) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputCloudflareR2) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputCloudflareR2) GetParquetVersion() *ParquetVersionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputCloudflareR2) GetParquetDataPageVersion() *DataPageVersionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputCloudflareR2) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputCloudflareR2) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputCloudflareR2) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputCloudflareR2) GetKeyValueMetadata() []KeyValueMetadatumCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputCloudflareR2) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputCloudflareR2) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputCloudflareR2) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputCloudflareR2) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputCloudflareR2) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputCloudflareR2) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputCloudflareR2) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeMicrosoftFabric string

const (
	TypeMicrosoftFabricMicrosoftFabric TypeMicrosoftFabric = "microsoft_fabric"
)

func (e TypeMicrosoftFabric) ToPointer() *TypeMicrosoftFabric {
	return &e
}
func (e *TypeMicrosoftFabric) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "microsoft_fabric":
		*e = TypeMicrosoftFabric(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeMicrosoftFabric: %v", v)
	}
}

// AcknowledgmentsMicrosoftFabric - Control the number of required acknowledgments
type AcknowledgmentsMicrosoftFabric int64

const (
	// AcknowledgmentsMicrosoftFabricOne Leader
	AcknowledgmentsMicrosoftFabricOne AcknowledgmentsMicrosoftFabric = 1
	// AcknowledgmentsMicrosoftFabricZero None
	AcknowledgmentsMicrosoftFabricZero AcknowledgmentsMicrosoftFabric = 0
	// AcknowledgmentsMicrosoftFabricMinus1 All
	AcknowledgmentsMicrosoftFabricMinus1 AcknowledgmentsMicrosoftFabric = -1
)

func (e AcknowledgmentsMicrosoftFabric) ToPointer() *AcknowledgmentsMicrosoftFabric {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AcknowledgmentsMicrosoftFabric) IsExact() bool {
	if e != nil {
		switch *e {
		case 1, 0, -1:
			return true
		}
	}
	return false
}

// RecordDataFormatMicrosoftFabric - Format to use to serialize events before writing to the Event Hubs Kafka brokers
type RecordDataFormatMicrosoftFabric string

const (
	// RecordDataFormatMicrosoftFabricJSON JSON
	RecordDataFormatMicrosoftFabricJSON RecordDataFormatMicrosoftFabric = "json"
	// RecordDataFormatMicrosoftFabricRaw Field _raw
	RecordDataFormatMicrosoftFabricRaw RecordDataFormatMicrosoftFabric = "raw"
)

func (e RecordDataFormatMicrosoftFabric) ToPointer() *RecordDataFormatMicrosoftFabric {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *RecordDataFormatMicrosoftFabric) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw":
			return true
		}
	}
	return false
}

type SASLMechanismMicrosoftFabric string

const (
	// SASLMechanismMicrosoftFabricPlain PLAIN
	SASLMechanismMicrosoftFabricPlain SASLMechanismMicrosoftFabric = "plain"
	// SASLMechanismMicrosoftFabricOauthbearer OAUTHBEARER
	SASLMechanismMicrosoftFabricOauthbearer SASLMechanismMicrosoftFabric = "oauthbearer"
)

func (e SASLMechanismMicrosoftFabric) ToPointer() *SASLMechanismMicrosoftFabric {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SASLMechanismMicrosoftFabric) IsExact() bool {
	if e != nil {
		switch *e {
		case "plain", "oauthbearer":
			return true
		}
	}
	return false
}

type AuthenticationMethodMicrosoftFabric string

const (
	AuthenticationMethodMicrosoftFabricSecret      AuthenticationMethodMicrosoftFabric = "secret"
	AuthenticationMethodMicrosoftFabricCertificate AuthenticationMethodMicrosoftFabric = "certificate"
)

func (e AuthenticationMethodMicrosoftFabric) ToPointer() *AuthenticationMethodMicrosoftFabric {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodMicrosoftFabric) IsExact() bool {
	if e != nil {
		switch *e {
		case "secret", "certificate":
			return true
		}
	}
	return false
}

// MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric - Endpoint used to acquire authentication tokens from Azure
type MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric string

const (
	MicrosoftEntraIDAuthenticationEndpointMicrosoftFabricHTTPSLoginMicrosoftonlineCom       MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric = "https://login.microsoftonline.com"
	MicrosoftEntraIDAuthenticationEndpointMicrosoftFabricHTTPSLoginMicrosoftonlineUs        MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric = "https://login.microsoftonline.us"
	MicrosoftEntraIDAuthenticationEndpointMicrosoftFabricHTTPSLoginPartnerMicrosoftonlineCn MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric = "https://login.partner.microsoftonline.cn"
)

func (e MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric) ToPointer() *MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric) IsExact() bool {
	if e != nil {
		switch *e {
		case "https://login.microsoftonline.com", "https://login.microsoftonline.us", "https://login.partner.microsoftonline.cn":
			return true
		}
	}
	return false
}

// AuthenticationMicrosoftFabric - Authentication parameters to use when connecting to bootstrap server. Using TLS is highly recommended.
type AuthenticationMicrosoftFabric struct {
	Disabled  *bool                         `default:"false" json:"disabled"`
	Mechanism *SASLMechanismMicrosoftFabric `default:"plain" json:"mechanism"`
	// The username for authentication. This should always be $ConnectionString.
	Username *string `default:"$ConnectionString" json:"username"`
	// Select or create a stored text secret corresponding to the SASL JASS Password Primary or Password Secondary
	TextSecret           *string                              `json:"textSecret,omitempty"`
	ClientSecretAuthType *AuthenticationMethodMicrosoftFabric `default:"secret" json:"clientSecretAuthType"`
	// Select or create a stored text secret
	ClientTextSecret *string `json:"clientTextSecret,omitempty"`
	// Select or create a stored certificate
	CertificateName *string `json:"certificateName,omitempty"`
	CertPath        *string `json:"certPath,omitempty"`
	PrivKeyPath     *string `json:"privKeyPath,omitempty"`
	Passphrase      *string `json:"passphrase,omitempty"`
	// Endpoint used to acquire authentication tokens from Azure
	OauthEndpoint *MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric `default:"https://login.microsoftonline.com" json:"oauthEndpoint"`
	// client_id to pass in the OAuth request parameter
	ClientID *string `json:"clientId,omitempty"`
	// Directory ID (tenant identifier) in Azure Active Directory
	TenantID *string `json:"tenantId,omitempty"`
	// Scope to pass in the OAuth request parameter
	Scope *string `json:"scope,omitempty"`
}

func (a AuthenticationMicrosoftFabric) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthenticationMicrosoftFabric) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (a *AuthenticationMicrosoftFabric) GetDisabled() *bool {
	if a == nil {
		return nil
	}
	return a.Disabled
}

func (a *AuthenticationMicrosoftFabric) GetMechanism() *SASLMechanismMicrosoftFabric {
	if a == nil {
		return nil
	}
	return a.Mechanism
}

func (a *AuthenticationMicrosoftFabric) GetUsername() *string {
	if a == nil {
		return nil
	}
	return a.Username
}

func (a *AuthenticationMicrosoftFabric) GetTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.TextSecret
}

func (a *AuthenticationMicrosoftFabric) GetClientSecretAuthType() *AuthenticationMethodMicrosoftFabric {
	if a == nil {
		return nil
	}
	return a.ClientSecretAuthType
}

func (a *AuthenticationMicrosoftFabric) GetClientTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.ClientTextSecret
}

func (a *AuthenticationMicrosoftFabric) GetCertificateName() *string {
	if a == nil {
		return nil
	}
	return a.CertificateName
}

func (a *AuthenticationMicrosoftFabric) GetCertPath() *string {
	if a == nil {
		return nil
	}
	return a.CertPath
}

func (a *AuthenticationMicrosoftFabric) GetPrivKeyPath() *string {
	if a == nil {
		return nil
	}
	return a.PrivKeyPath
}

func (a *AuthenticationMicrosoftFabric) GetPassphrase() *string {
	if a == nil {
		return nil
	}
	return a.Passphrase
}

func (a *AuthenticationMicrosoftFabric) GetOauthEndpoint() *MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric {
	if a == nil {
		return nil
	}
	return a.OauthEndpoint
}

func (a *AuthenticationMicrosoftFabric) GetClientID() *string {
	if a == nil {
		return nil
	}
	return a.ClientID
}

func (a *AuthenticationMicrosoftFabric) GetTenantID() *string {
	if a == nil {
		return nil
	}
	return a.TenantID
}

func (a *AuthenticationMicrosoftFabric) GetScope() *string {
	if a == nil {
		return nil
	}
	return a.Scope
}

type TLSSettingsClientSideMicrosoftFabric struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
}

func (t TLSSettingsClientSideMicrosoftFabric) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideMicrosoftFabric) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideMicrosoftFabric) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideMicrosoftFabric) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

// BackpressureBehaviorMicrosoftFabric - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorMicrosoftFabric string

const (
	// BackpressureBehaviorMicrosoftFabricBlock Block
	BackpressureBehaviorMicrosoftFabricBlock BackpressureBehaviorMicrosoftFabric = "block"
	// BackpressureBehaviorMicrosoftFabricDrop Drop
	BackpressureBehaviorMicrosoftFabricDrop BackpressureBehaviorMicrosoftFabric = "drop"
	// BackpressureBehaviorMicrosoftFabricQueue Persistent Queue
	BackpressureBehaviorMicrosoftFabricQueue BackpressureBehaviorMicrosoftFabric = "queue"
)

func (e BackpressureBehaviorMicrosoftFabric) ToPointer() *BackpressureBehaviorMicrosoftFabric {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorMicrosoftFabric) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeMicrosoftFabric - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeMicrosoftFabric string

const (
	// ModeMicrosoftFabricError Error
	ModeMicrosoftFabricError ModeMicrosoftFabric = "error"
	// ModeMicrosoftFabricAlways Backpressure
	ModeMicrosoftFabricAlways ModeMicrosoftFabric = "always"
	// ModeMicrosoftFabricBackpressure Always On
	ModeMicrosoftFabricBackpressure ModeMicrosoftFabric = "backpressure"
)

func (e ModeMicrosoftFabric) ToPointer() *ModeMicrosoftFabric {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeMicrosoftFabric) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionMicrosoftFabric - Codec to use to compress the persisted data
type CompressionMicrosoftFabric string

const (
	// CompressionMicrosoftFabricNone None
	CompressionMicrosoftFabricNone CompressionMicrosoftFabric = "none"
	// CompressionMicrosoftFabricGzip Gzip
	CompressionMicrosoftFabricGzip CompressionMicrosoftFabric = "gzip"
)

func (e CompressionMicrosoftFabric) ToPointer() *CompressionMicrosoftFabric {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionMicrosoftFabric) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorMicrosoftFabric - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorMicrosoftFabric string

const (
	// QueueFullBehaviorMicrosoftFabricBlock Block
	QueueFullBehaviorMicrosoftFabricBlock QueueFullBehaviorMicrosoftFabric = "block"
	// QueueFullBehaviorMicrosoftFabricDrop Drop new data
	QueueFullBehaviorMicrosoftFabricDrop QueueFullBehaviorMicrosoftFabric = "drop"
)

func (e QueueFullBehaviorMicrosoftFabric) ToPointer() *QueueFullBehaviorMicrosoftFabric {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorMicrosoftFabric) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsMicrosoftFabric struct {
}

func (p PqControlsMicrosoftFabric) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsMicrosoftFabric) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputMicrosoftFabric struct {
	// Unique ID for this output
	ID   string              `json:"id"`
	Type TypeMicrosoftFabric `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Topic name from Fabric Eventstream's endpoint
	Topic string `json:"topic"`
	// Control the number of required acknowledgments
	Ack *AcknowledgmentsMicrosoftFabric `default:"1" json:"ack"`
	// Format to use to serialize events before writing to the Event Hubs Kafka brokers
	Format *RecordDataFormatMicrosoftFabric `default:"json" json:"format"`
	// Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// Maximum number of events in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to bootstrap server. Using TLS is highly recommended.
	Sasl *AuthenticationMicrosoftFabric        `json:"sasl,omitempty"`
	TLS  *TLSSettingsClientSideMicrosoftFabric `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorMicrosoftFabric `default:"block" json:"onBackpressure"`
	// Bootstrap server from Fabric Eventstream's endpoint
	BootstrapServer string  `json:"bootstrap_server"`
	Description     *string `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeMicrosoftFabric `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionMicrosoftFabric `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorMicrosoftFabric `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsMicrosoftFabric        `json:"pqControls,omitempty"`
}

func (o OutputMicrosoftFabric) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMicrosoftFabric) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "topic", "bootstrap_server"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputMicrosoftFabric) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputMicrosoftFabric) GetType() TypeMicrosoftFabric {
	if o == nil {
		return TypeMicrosoftFabric("")
	}
	return o.Type
}

func (o *OutputMicrosoftFabric) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputMicrosoftFabric) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputMicrosoftFabric) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputMicrosoftFabric) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputMicrosoftFabric) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputMicrosoftFabric) GetAck() *AcknowledgmentsMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputMicrosoftFabric) GetFormat() *RecordDataFormatMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputMicrosoftFabric) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputMicrosoftFabric) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputMicrosoftFabric) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputMicrosoftFabric) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputMicrosoftFabric) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputMicrosoftFabric) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputMicrosoftFabric) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputMicrosoftFabric) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputMicrosoftFabric) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputMicrosoftFabric) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputMicrosoftFabric) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputMicrosoftFabric) GetSasl() *AuthenticationMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputMicrosoftFabric) GetTLS() *TLSSettingsClientSideMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputMicrosoftFabric) GetOnBackpressure() *BackpressureBehaviorMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputMicrosoftFabric) GetBootstrapServer() string {
	if o == nil {
		return ""
	}
	return o.BootstrapServer
}

func (o *OutputMicrosoftFabric) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputMicrosoftFabric) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputMicrosoftFabric) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputMicrosoftFabric) GetPqMode() *ModeMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputMicrosoftFabric) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputMicrosoftFabric) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputMicrosoftFabric) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputMicrosoftFabric) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputMicrosoftFabric) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputMicrosoftFabric) GetPqCompress() *CompressionMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputMicrosoftFabric) GetPqOnBackpressure() *QueueFullBehaviorMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputMicrosoftFabric) GetPqControls() *PqControlsMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeDatabricks string

const (
	TypeDatabricksDatabricks TypeDatabricks = "databricks"
)

func (e TypeDatabricks) ToPointer() *TypeDatabricks {
	return &e
}
func (e *TypeDatabricks) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "databricks":
		*e = TypeDatabricks(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDatabricks: %v", v)
	}
}

// DataFormatDatabricks - Format of the output data
type DataFormatDatabricks string

const (
	// DataFormatDatabricksJSON JSON
	DataFormatDatabricksJSON DataFormatDatabricks = "json"
	// DataFormatDatabricksRaw Raw
	DataFormatDatabricksRaw DataFormatDatabricks = "raw"
	// DataFormatDatabricksParquet Parquet
	DataFormatDatabricksParquet DataFormatDatabricks = "parquet"
)

func (e DataFormatDatabricks) ToPointer() *DataFormatDatabricks {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataFormatDatabricks) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "parquet":
			return true
		}
	}
	return false
}

// BackpressureBehaviorDatabricks - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDatabricks string

const (
	// BackpressureBehaviorDatabricksBlock Block
	BackpressureBehaviorDatabricksBlock BackpressureBehaviorDatabricks = "block"
	// BackpressureBehaviorDatabricksDrop Drop
	BackpressureBehaviorDatabricksDrop BackpressureBehaviorDatabricks = "drop"
)

func (e BackpressureBehaviorDatabricks) ToPointer() *BackpressureBehaviorDatabricks {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorDatabricks) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionDatabricks - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionDatabricks string

const (
	// DiskSpaceProtectionDatabricksBlock Block
	DiskSpaceProtectionDatabricksBlock DiskSpaceProtectionDatabricks = "block"
	// DiskSpaceProtectionDatabricksDrop Drop
	DiskSpaceProtectionDatabricksDrop DiskSpaceProtectionDatabricks = "drop"
)

func (e DiskSpaceProtectionDatabricks) ToPointer() *DiskSpaceProtectionDatabricks {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionDatabricks) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// CompressionDatabricks - Data compression format to apply to HTTP content before it is delivered
type CompressionDatabricks string

const (
	CompressionDatabricksNone CompressionDatabricks = "none"
	CompressionDatabricksGzip CompressionDatabricks = "gzip"
)

func (e CompressionDatabricks) ToPointer() *CompressionDatabricks {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionDatabricks) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// CompressionLevelDatabricks - Compression level to apply before moving files to final destination
type CompressionLevelDatabricks string

const (
	// CompressionLevelDatabricksBestSpeed Best Speed
	CompressionLevelDatabricksBestSpeed CompressionLevelDatabricks = "best_speed"
	// CompressionLevelDatabricksNormal Normal
	CompressionLevelDatabricksNormal CompressionLevelDatabricks = "normal"
	// CompressionLevelDatabricksBestCompression Best Compression
	CompressionLevelDatabricksBestCompression CompressionLevelDatabricks = "best_compression"
)

func (e CompressionLevelDatabricks) ToPointer() *CompressionLevelDatabricks {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionLevelDatabricks) IsExact() bool {
	if e != nil {
		switch *e {
		case "best_speed", "normal", "best_compression":
			return true
		}
	}
	return false
}

// ParquetVersionDatabricks - Determines which data types are supported and how they are represented
type ParquetVersionDatabricks string

const (
	// ParquetVersionDatabricksParquet10 1.0
	ParquetVersionDatabricksParquet10 ParquetVersionDatabricks = "PARQUET_1_0"
	// ParquetVersionDatabricksParquet24 2.4
	ParquetVersionDatabricksParquet24 ParquetVersionDatabricks = "PARQUET_2_4"
	// ParquetVersionDatabricksParquet26 2.6
	ParquetVersionDatabricksParquet26 ParquetVersionDatabricks = "PARQUET_2_6"
)

func (e ParquetVersionDatabricks) ToPointer() *ParquetVersionDatabricks {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ParquetVersionDatabricks) IsExact() bool {
	if e != nil {
		switch *e {
		case "PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6":
			return true
		}
	}
	return false
}

// DataPageVersionDatabricks - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionDatabricks string

const (
	// DataPageVersionDatabricksDataPageV1 V1
	DataPageVersionDatabricksDataPageV1 DataPageVersionDatabricks = "DATA_PAGE_V1"
	// DataPageVersionDatabricksDataPageV2 V2
	DataPageVersionDatabricksDataPageV2 DataPageVersionDatabricks = "DATA_PAGE_V2"
)

func (e DataPageVersionDatabricks) ToPointer() *DataPageVersionDatabricks {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataPageVersionDatabricks) IsExact() bool {
	if e != nil {
		switch *e {
		case "DATA_PAGE_V1", "DATA_PAGE_V2":
			return true
		}
	}
	return false
}

type KeyValueMetadatumDatabricks struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumDatabricks) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumDatabricks) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumDatabricks) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumDatabricks) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputDatabricks struct {
	// Unique ID for this output
	ID   string         `json:"id"`
	Type TypeDatabricks `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Optional path to prepend to files before uploading.
	DestPath *string `default:"" json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatDatabricks `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDatabricks `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionDatabricks `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Databricks workspace ID
	WorkspaceID string `json:"workspaceId"`
	// OAuth scope for Unity Catalog authentication
	Scope *string `default:"all-apis" json:"scope"`
	// OAuth client ID for Unity Catalog authentication
	ClientID string `json:"clientId"`
	// Name of the catalog to use for the output
	Catalog *string `default:"main" json:"catalog"`
	// Name of the catalog schema to use for the output
	Schema *string `default:"external" json:"schema"`
	// Name of the events volume in Databricks
	EventsVolumeName *string `default:"events" json:"eventsVolumeName"`
	// OAuth client secret for Unity Catalog authentication
	ClientTextSecret string `json:"clientTextSecret"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec  *float64 `default:"60" json:"timeoutSec"`
	Description *string  `json:"description,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionDatabricks `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelDatabricks `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionDatabricks `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionDatabricks `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumDatabricks `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputDatabricks) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDatabricks) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "workspaceId", "clientId", "clientTextSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDatabricks) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDatabricks) GetType() TypeDatabricks {
	if o == nil {
		return TypeDatabricks("")
	}
	return o.Type
}

func (o *OutputDatabricks) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDatabricks) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDatabricks) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDatabricks) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDatabricks) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDatabricks) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDatabricks) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDatabricks) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDatabricks) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputDatabricks) GetFormat() *DataFormatDatabricks {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDatabricks) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDatabricks) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDatabricks) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDatabricks) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDatabricks) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDatabricks) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDatabricks) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDatabricks) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDatabricks) GetOnBackpressure() *BackpressureBehaviorDatabricks {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDatabricks) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDatabricks) GetOnDiskFullBackpressure() *DiskSpaceProtectionDatabricks {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDatabricks) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputDatabricks) GetWorkspaceID() string {
	if o == nil {
		return ""
	}
	return o.WorkspaceID
}

func (o *OutputDatabricks) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputDatabricks) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputDatabricks) GetCatalog() *string {
	if o == nil {
		return nil
	}
	return o.Catalog
}

func (o *OutputDatabricks) GetSchema() *string {
	if o == nil {
		return nil
	}
	return o.Schema
}

func (o *OutputDatabricks) GetEventsVolumeName() *string {
	if o == nil {
		return nil
	}
	return o.EventsVolumeName
}

func (o *OutputDatabricks) GetClientTextSecret() string {
	if o == nil {
		return ""
	}
	return o.ClientTextSecret
}

func (o *OutputDatabricks) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDatabricks) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDatabricks) GetCompress() *CompressionDatabricks {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDatabricks) GetCompressionLevel() *CompressionLevelDatabricks {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDatabricks) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDatabricks) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputDatabricks) GetParquetVersion() *ParquetVersionDatabricks {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDatabricks) GetParquetDataPageVersion() *DataPageVersionDatabricks {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDatabricks) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDatabricks) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDatabricks) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDatabricks) GetKeyValueMetadata() []KeyValueMetadatumDatabricks {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputDatabricks) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDatabricks) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDatabricks) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDatabricks) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDatabricks) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputDatabricks) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDatabricks) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeChronicle string

const (
	TypeChronicleChronicle TypeChronicle = "chronicle"
)

func (e TypeChronicle) ToPointer() *TypeChronicle {
	return &e
}
func (e *TypeChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "chronicle":
		*e = TypeChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeChronicle: %v", v)
	}
}

type AuthenticationMethodChronicle string

const (
	AuthenticationMethodChronicleServiceAccount       AuthenticationMethodChronicle = "serviceAccount"
	AuthenticationMethodChronicleServiceAccountSecret AuthenticationMethodChronicle = "serviceAccountSecret"
)

func (e AuthenticationMethodChronicle) ToPointer() *AuthenticationMethodChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "serviceAccount", "serviceAccountSecret":
			return true
		}
	}
	return false
}

type ResponseRetrySettingChronicle struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingChronicle) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingChronicle) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingChronicle) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingChronicle) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsChronicle struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsChronicle) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsChronicle) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsChronicle) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsChronicle) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type ExtraHTTPHeaderChronicle struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderChronicle) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderChronicle) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeChronicle - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeChronicle string

const (
	// FailedRequestLoggingModeChroniclePayload Payload
	FailedRequestLoggingModeChroniclePayload FailedRequestLoggingModeChronicle = "payload"
	// FailedRequestLoggingModeChroniclePayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeChroniclePayloadAndHeaders FailedRequestLoggingModeChronicle = "payloadAndHeaders"
	// FailedRequestLoggingModeChronicleNone None
	FailedRequestLoggingModeChronicleNone FailedRequestLoggingModeChronicle = "none"
)

func (e FailedRequestLoggingModeChronicle) ToPointer() *FailedRequestLoggingModeChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

// BackpressureBehaviorChronicle - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorChronicle string

const (
	// BackpressureBehaviorChronicleBlock Block
	BackpressureBehaviorChronicleBlock BackpressureBehaviorChronicle = "block"
	// BackpressureBehaviorChronicleDrop Drop
	BackpressureBehaviorChronicleDrop BackpressureBehaviorChronicle = "drop"
	// BackpressureBehaviorChronicleQueue Persistent Queue
	BackpressureBehaviorChronicleQueue BackpressureBehaviorChronicle = "queue"
)

func (e BackpressureBehaviorChronicle) ToPointer() *BackpressureBehaviorChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type CustomLabelChronicle struct {
	Key   string `json:"key"`
	Value string `json:"value"`
	// Designate this label for role-based access control and filtering
	RbacEnabled *bool `default:"false" json:"rbacEnabled"`
}

func (c CustomLabelChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CustomLabelChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"key", "value"}); err != nil {
		return err
	}
	return nil
}

func (c *CustomLabelChronicle) GetKey() string {
	if c == nil {
		return ""
	}
	return c.Key
}

func (c *CustomLabelChronicle) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

func (c *CustomLabelChronicle) GetRbacEnabled() *bool {
	if c == nil {
		return nil
	}
	return c.RbacEnabled
}

// ModeChronicle - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeChronicle string

const (
	// ModeChronicleError Error
	ModeChronicleError ModeChronicle = "error"
	// ModeChronicleAlways Backpressure
	ModeChronicleAlways ModeChronicle = "always"
	// ModeChronicleBackpressure Always On
	ModeChronicleBackpressure ModeChronicle = "backpressure"
)

func (e ModeChronicle) ToPointer() *ModeChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionChronicle - Codec to use to compress the persisted data
type CompressionChronicle string

const (
	// CompressionChronicleNone None
	CompressionChronicleNone CompressionChronicle = "none"
	// CompressionChronicleGzip Gzip
	CompressionChronicleGzip CompressionChronicle = "gzip"
)

func (e CompressionChronicle) ToPointer() *CompressionChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorChronicle - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorChronicle string

const (
	// QueueFullBehaviorChronicleBlock Block
	QueueFullBehaviorChronicleBlock QueueFullBehaviorChronicle = "block"
	// QueueFullBehaviorChronicleDrop Drop new data
	QueueFullBehaviorChronicleDrop QueueFullBehaviorChronicle = "drop"
)

func (e QueueFullBehaviorChronicle) ToPointer() *QueueFullBehaviorChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsChronicle struct {
}

func (p PqControlsChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputChronicle struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeChronicle `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags           []string                       `json:"streamtags,omitempty"`
	APIVersion           *string                        `default:"v1alpha" json:"apiVersion"`
	AuthenticationMethod *AuthenticationMethodChronicle `default:"serviceAccount" json:"authenticationMethod"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingChronicle `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsChronicle  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Regional endpoint to send events to
	Region string `json:"region"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"90" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderChronicle `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeChronicle `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorChronicle `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	IngestionMethod    *string  `default:"ImportLogs" json:"ingestionMethod"`
	// User-configured environment namespace to identify the data domain the logs originated from. This namespace is used as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.
	Namespace *string `json:"namespace,omitempty"`
	// Default log type value to send to SecOps. Can be overwritten by event field __logType.
	LogType string `json:"logType"`
	// Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.
	LogTextField *string `json:"logTextField,omitempty"`
	// The Google Cloud Platform (GCP) project ID to send events to
	GcpProjectID string `json:"gcpProjectId"`
	// The Google Cloud Platform (GCP) instance to send events to. This is the Chronicle customer uuid.
	GcpInstance string `json:"gcpInstance"`
	// Custom labels to be added to every event
	CustomLabels []CustomLabelChronicle `json:"customLabels,omitempty"`
	Description  *string                `json:"description,omitempty"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	ServiceAccountCredentialsSecret *string `json:"serviceAccountCredentialsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeChronicle `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionChronicle `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorChronicle `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsChronicle        `json:"pqControls,omitempty"`
}

func (o OutputChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "region", "logType", "gcpProjectId", "gcpInstance"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputChronicle) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputChronicle) GetType() TypeChronicle {
	if o == nil {
		return TypeChronicle("")
	}
	return o.Type
}

func (o *OutputChronicle) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputChronicle) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputChronicle) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputChronicle) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputChronicle) GetAPIVersion() *string {
	if o == nil {
		return nil
	}
	return o.APIVersion
}

func (o *OutputChronicle) GetAuthenticationMethod() *AuthenticationMethodChronicle {
	if o == nil {
		return nil
	}
	return o.AuthenticationMethod
}

func (o *OutputChronicle) GetResponseRetrySettings() []ResponseRetrySettingChronicle {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputChronicle) GetTimeoutRetrySettings() *TimeoutRetrySettingsChronicle {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputChronicle) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputChronicle) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputChronicle) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputChronicle) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputChronicle) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputChronicle) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputChronicle) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputChronicle) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputChronicle) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputChronicle) GetExtraHTTPHeaders() []ExtraHTTPHeaderChronicle {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputChronicle) GetFailedRequestLoggingMode() *FailedRequestLoggingModeChronicle {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputChronicle) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputChronicle) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputChronicle) GetOnBackpressure() *BackpressureBehaviorChronicle {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputChronicle) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputChronicle) GetIngestionMethod() *string {
	if o == nil {
		return nil
	}
	return o.IngestionMethod
}

func (o *OutputChronicle) GetNamespace() *string {
	if o == nil {
		return nil
	}
	return o.Namespace
}

func (o *OutputChronicle) GetLogType() string {
	if o == nil {
		return ""
	}
	return o.LogType
}

func (o *OutputChronicle) GetLogTextField() *string {
	if o == nil {
		return nil
	}
	return o.LogTextField
}

func (o *OutputChronicle) GetGcpProjectID() string {
	if o == nil {
		return ""
	}
	return o.GcpProjectID
}

func (o *OutputChronicle) GetGcpInstance() string {
	if o == nil {
		return ""
	}
	return o.GcpInstance
}

func (o *OutputChronicle) GetCustomLabels() []CustomLabelChronicle {
	if o == nil {
		return nil
	}
	return o.CustomLabels
}

func (o *OutputChronicle) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputChronicle) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputChronicle) GetServiceAccountCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentialsSecret
}

func (o *OutputChronicle) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputChronicle) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputChronicle) GetPqMode() *ModeChronicle {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputChronicle) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputChronicle) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputChronicle) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputChronicle) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputChronicle) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputChronicle) GetPqCompress() *CompressionChronicle {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputChronicle) GetPqOnBackpressure() *QueueFullBehaviorChronicle {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputChronicle) GetPqControls() *PqControlsChronicle {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeSentinelOneAiSiem string

const (
	TypeSentinelOneAiSiemSentinelOneAiSiem TypeSentinelOneAiSiem = "sentinel_one_ai_siem"
)

func (e TypeSentinelOneAiSiem) ToPointer() *TypeSentinelOneAiSiem {
	return &e
}
func (e *TypeSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sentinel_one_ai_siem":
		*e = TypeSentinelOneAiSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSentinelOneAiSiem: %v", v)
	}
}

// RegionSentinelOneAiSiem - The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.
type RegionSentinelOneAiSiem string

const (
	RegionSentinelOneAiSiemUs     RegionSentinelOneAiSiem = "US"
	RegionSentinelOneAiSiemCa     RegionSentinelOneAiSiem = "CA"
	RegionSentinelOneAiSiemEmea   RegionSentinelOneAiSiem = "EMEA"
	RegionSentinelOneAiSiemAp     RegionSentinelOneAiSiem = "AP"
	RegionSentinelOneAiSiemAps    RegionSentinelOneAiSiem = "APS"
	RegionSentinelOneAiSiemAu     RegionSentinelOneAiSiem = "AU"
	RegionSentinelOneAiSiemCustom RegionSentinelOneAiSiem = "Custom"
)

func (e RegionSentinelOneAiSiem) ToPointer() *RegionSentinelOneAiSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *RegionSentinelOneAiSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "US", "CA", "EMEA", "AP", "APS", "AU", "Custom":
			return true
		}
	}
	return false
}

// AISIEMEndpointPath - Endpoint to send events to. Use /services/collector/event for structured JSON payloads with standard HEC top-level fields. Use /services/collector/raw for unstructured log lines (plain text).
type AISIEMEndpointPath string

const (
	AISIEMEndpointPathRootServicesCollectorEvent AISIEMEndpointPath = "/services/collector/event"
	AISIEMEndpointPathRootServicesCollectorRaw   AISIEMEndpointPath = "/services/collector/raw"
)

func (e AISIEMEndpointPath) ToPointer() *AISIEMEndpointPath {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AISIEMEndpointPath) IsExact() bool {
	if e != nil {
		switch *e {
		case "/services/collector/event", "/services/collector/raw":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderSentinelOneAiSiem struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderSentinelOneAiSiem) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderSentinelOneAiSiem) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeSentinelOneAiSiem - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSentinelOneAiSiem string

const (
	// FailedRequestLoggingModeSentinelOneAiSiemPayload Payload
	FailedRequestLoggingModeSentinelOneAiSiemPayload FailedRequestLoggingModeSentinelOneAiSiem = "payload"
	// FailedRequestLoggingModeSentinelOneAiSiemPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeSentinelOneAiSiemPayloadAndHeaders FailedRequestLoggingModeSentinelOneAiSiem = "payloadAndHeaders"
	// FailedRequestLoggingModeSentinelOneAiSiemNone None
	FailedRequestLoggingModeSentinelOneAiSiemNone FailedRequestLoggingModeSentinelOneAiSiem = "none"
)

func (e FailedRequestLoggingModeSentinelOneAiSiem) ToPointer() *FailedRequestLoggingModeSentinelOneAiSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeSentinelOneAiSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

// AuthenticationMethodSentinelOneAiSiem - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSentinelOneAiSiem string

const (
	AuthenticationMethodSentinelOneAiSiemManual AuthenticationMethodSentinelOneAiSiem = "manual"
	AuthenticationMethodSentinelOneAiSiemSecret AuthenticationMethodSentinelOneAiSiem = "secret"
)

func (e AuthenticationMethodSentinelOneAiSiem) ToPointer() *AuthenticationMethodSentinelOneAiSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodSentinelOneAiSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

type ResponseRetrySettingSentinelOneAiSiem struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingSentinelOneAiSiem) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingSentinelOneAiSiem) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingSentinelOneAiSiem) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingSentinelOneAiSiem) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsSentinelOneAiSiem struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsSentinelOneAiSiem) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsSentinelOneAiSiem) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsSentinelOneAiSiem) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsSentinelOneAiSiem) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorSentinelOneAiSiem - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSentinelOneAiSiem string

const (
	// BackpressureBehaviorSentinelOneAiSiemBlock Block
	BackpressureBehaviorSentinelOneAiSiemBlock BackpressureBehaviorSentinelOneAiSiem = "block"
	// BackpressureBehaviorSentinelOneAiSiemDrop Drop
	BackpressureBehaviorSentinelOneAiSiemDrop BackpressureBehaviorSentinelOneAiSiem = "drop"
	// BackpressureBehaviorSentinelOneAiSiemQueue Persistent Queue
	BackpressureBehaviorSentinelOneAiSiemQueue BackpressureBehaviorSentinelOneAiSiem = "queue"
)

func (e BackpressureBehaviorSentinelOneAiSiem) ToPointer() *BackpressureBehaviorSentinelOneAiSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorSentinelOneAiSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeSentinelOneAiSiem - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSentinelOneAiSiem string

const (
	// ModeSentinelOneAiSiemError Error
	ModeSentinelOneAiSiemError ModeSentinelOneAiSiem = "error"
	// ModeSentinelOneAiSiemAlways Backpressure
	ModeSentinelOneAiSiemAlways ModeSentinelOneAiSiem = "always"
	// ModeSentinelOneAiSiemBackpressure Always On
	ModeSentinelOneAiSiemBackpressure ModeSentinelOneAiSiem = "backpressure"
)

func (e ModeSentinelOneAiSiem) ToPointer() *ModeSentinelOneAiSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeSentinelOneAiSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionSentinelOneAiSiem - Codec to use to compress the persisted data
type CompressionSentinelOneAiSiem string

const (
	// CompressionSentinelOneAiSiemNone None
	CompressionSentinelOneAiSiemNone CompressionSentinelOneAiSiem = "none"
	// CompressionSentinelOneAiSiemGzip Gzip
	CompressionSentinelOneAiSiemGzip CompressionSentinelOneAiSiem = "gzip"
)

func (e CompressionSentinelOneAiSiem) ToPointer() *CompressionSentinelOneAiSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionSentinelOneAiSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorSentinelOneAiSiem - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSentinelOneAiSiem string

const (
	// QueueFullBehaviorSentinelOneAiSiemBlock Block
	QueueFullBehaviorSentinelOneAiSiemBlock QueueFullBehaviorSentinelOneAiSiem = "block"
	// QueueFullBehaviorSentinelOneAiSiemDrop Drop new data
	QueueFullBehaviorSentinelOneAiSiemDrop QueueFullBehaviorSentinelOneAiSiem = "drop"
)

func (e QueueFullBehaviorSentinelOneAiSiem) ToPointer() *QueueFullBehaviorSentinelOneAiSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorSentinelOneAiSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsSentinelOneAiSiem struct {
}

func (p PqControlsSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSentinelOneAiSiem struct {
	// Unique ID for this output
	ID   string                `json:"id"`
	Type TypeSentinelOneAiSiem `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.
	Region *RegionSentinelOneAiSiem `default:"US" json:"region"`
	// Endpoint to send events to. Use /services/collector/event for structured JSON payloads with standard HEC top-level fields. Use /services/collector/raw for unstructured log lines (plain text).
	Endpoint *AISIEMEndpointPath `default:"/services/collector/event" json:"endpoint"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"5120" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"5" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderSentinelOneAiSiem `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSentinelOneAiSiem `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodSentinelOneAiSiem `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSentinelOneAiSiem `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSentinelOneAiSiem  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSentinelOneAiSiem `default:"block" json:"onBackpressure"`
	Description    *string                                `json:"description,omitempty"`
	// In the SentinelOne Console select Policy & Settings then select the Singularity AI SIEM section, API Keys will be at the bottom. Under Log Access Keys select a Write token and copy it here
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Base URL of the endpoint used to send events to, such as https://<Your-S1-Tenant>.sentinelone.net. Must begin with http:// or https://, can include a port number, and no trailing slashes. Matches pattern: ^https?://[a-zA-Z0-9.-]+(:[0-9]+)?$.
	BaseURL *string `default:"https://<Your-S1-Tenant>.sentinelone.net" json:"baseUrl"`
	// Define serverHost for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myServer').
	HostExpression *string `default:"__e.host || C.os.hostname()" json:"hostExpression"`
	// Define logFile for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myLogFile.txt').
	SourceExpression *string `default:"__e.source || (__e.__criblMetrics ? 'metrics' : 'cribl')" json:"sourceExpression"`
	// Define the parser for events using a JavaScript expression. This value helps parse data into AI SIEM. You must enclose text constants in quotes (such as, 'dottedJson'). For custom parsers, substitute 'dottedJson' with your parser's name.
	SourceTypeExpression *string `default:"__e.sourcetype || 'dottedJson'" json:"sourceTypeExpression"`
	// Define the dataSource.category for events using a JavaScript expression. This value helps categorize data and helps enable extra features in SentinelOne AI SIEM. You must enclose text constants in quotes. The default value is 'security'.
	DataSourceCategoryExpression *string `default:"'security'" json:"dataSourceCategoryExpression"`
	// Define the dataSource.name for events using a JavaScript expression. This value should reflect the type of data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'networkActivity' or 'authLogs').
	DataSourceNameExpression *string `default:"__e.__dataSourceName || 'cribl'" json:"dataSourceNameExpression"`
	// Define the dataSource.vendor for events using a JavaScript expression. This value should reflect the vendor of the data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'Cisco' or 'Microsoft').
	DataSourceVendorExpression *string `default:"__e.__dataSourceVendor || 'cribl'" json:"dataSourceVendorExpression"`
	// Optionally, define the event.type for events using a JavaScript expression. This value acts as a label, grouping events into meaningful categories. You must enclose text constants in quotes (such as, 'Process Creation' or 'Network Connection').
	EventTypeExpression *string `default:"" json:"eventTypeExpression"`
	// Define the serverHost for events using a JavaScript expression. This value will be passed to AI SIEM. You must enclose text constants in quotes (such as, 'myServerName').
	Host *string `default:"C.os.hostname()" json:"host"`
	// Specify the logFile value to pass as a parameter to SentinelOne AI SIEM. Don't quote this value. The default is cribl.
	Source *string `default:"cribl" json:"source"`
	// Specify the sourcetype parameter for SentinelOne AI SIEM, which determines the parser. Don't quote this value. For custom parsers, substitute hecRawParser with your parser's name. The default is hecRawParser.
	SourceType *string `default:"hecRawParser" json:"sourceType"`
	// Specify the dataSource.category value to pass as a parameter to SentinelOne AI SIEM. This value helps categorize data and enables additional features. Don't quote this value. The default is security.
	DataSourceCategory *string `default:"security" json:"dataSourceCategory"`
	// Specify the dataSource.name value to pass as a parameter to AI SIEM. This value should reflect the type of data being inserted. Don't quote this value. The default is cribl.
	DataSourceName *string `default:"cribl" json:"dataSourceName"`
	// Specify the dataSource.vendorvalue to pass as a parameter to AI SIEM. This value should reflect the vendor of the data being inserted. Don't quote this value. The default is cribl.
	DataSourceVendor *string `default:"cribl" json:"dataSourceVendor"`
	// Specify the event.type value to pass as an optional parameter to AI SIEM. This value acts as a label, grouping events into meaningful categories like Process Creation, File Modification, or Network Connection. Don't quote this value. By default, this field is empty.
	EventType *string `default:"" json:"eventType"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSentinelOneAiSiem `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSentinelOneAiSiem `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSentinelOneAiSiem `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsSentinelOneAiSiem        `json:"pqControls,omitempty"`
}

func (o OutputSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSentinelOneAiSiem) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSentinelOneAiSiem) GetType() TypeSentinelOneAiSiem {
	if o == nil {
		return TypeSentinelOneAiSiem("")
	}
	return o.Type
}

func (o *OutputSentinelOneAiSiem) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSentinelOneAiSiem) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSentinelOneAiSiem) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSentinelOneAiSiem) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSentinelOneAiSiem) GetRegion() *RegionSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputSentinelOneAiSiem) GetEndpoint() *AISIEMEndpointPath {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSentinelOneAiSiem) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSentinelOneAiSiem) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSentinelOneAiSiem) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSentinelOneAiSiem) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSentinelOneAiSiem) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSentinelOneAiSiem) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSentinelOneAiSiem) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSentinelOneAiSiem) GetExtraHTTPHeaders() []ExtraHTTPHeaderSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSentinelOneAiSiem) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSentinelOneAiSiem) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSentinelOneAiSiem) GetAuthType() *AuthenticationMethodSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSentinelOneAiSiem) GetResponseRetrySettings() []ResponseRetrySettingSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSentinelOneAiSiem) GetTimeoutRetrySettings() *TimeoutRetrySettingsSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSentinelOneAiSiem) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSentinelOneAiSiem) GetOnBackpressure() *BackpressureBehaviorSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSentinelOneAiSiem) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSentinelOneAiSiem) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputSentinelOneAiSiem) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSentinelOneAiSiem) GetBaseURL() *string {
	if o == nil {
		return nil
	}
	return o.BaseURL
}

func (o *OutputSentinelOneAiSiem) GetHostExpression() *string {
	if o == nil {
		return nil
	}
	return o.HostExpression
}

func (o *OutputSentinelOneAiSiem) GetSourceExpression() *string {
	if o == nil {
		return nil
	}
	return o.SourceExpression
}

func (o *OutputSentinelOneAiSiem) GetSourceTypeExpression() *string {
	if o == nil {
		return nil
	}
	return o.SourceTypeExpression
}

func (o *OutputSentinelOneAiSiem) GetDataSourceCategoryExpression() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceCategoryExpression
}

func (o *OutputSentinelOneAiSiem) GetDataSourceNameExpression() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceNameExpression
}

func (o *OutputSentinelOneAiSiem) GetDataSourceVendorExpression() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceVendorExpression
}

func (o *OutputSentinelOneAiSiem) GetEventTypeExpression() *string {
	if o == nil {
		return nil
	}
	return o.EventTypeExpression
}

func (o *OutputSentinelOneAiSiem) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputSentinelOneAiSiem) GetSource() *string {
	if o == nil {
		return nil
	}
	return o.Source
}

func (o *OutputSentinelOneAiSiem) GetSourceType() *string {
	if o == nil {
		return nil
	}
	return o.SourceType
}

func (o *OutputSentinelOneAiSiem) GetDataSourceCategory() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceCategory
}

func (o *OutputSentinelOneAiSiem) GetDataSourceName() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceName
}

func (o *OutputSentinelOneAiSiem) GetDataSourceVendor() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceVendor
}

func (o *OutputSentinelOneAiSiem) GetEventType() *string {
	if o == nil {
		return nil
	}
	return o.EventType
}

func (o *OutputSentinelOneAiSiem) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSentinelOneAiSiem) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSentinelOneAiSiem) GetPqMode() *ModeSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSentinelOneAiSiem) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSentinelOneAiSiem) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSentinelOneAiSiem) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSentinelOneAiSiem) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSentinelOneAiSiem) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSentinelOneAiSiem) GetPqCompress() *CompressionSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSentinelOneAiSiem) GetPqOnBackpressure() *QueueFullBehaviorSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSentinelOneAiSiem) GetPqControls() *PqControlsSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeDynatraceOtlp string

const (
	TypeDynatraceOtlpDynatraceOtlp TypeDynatraceOtlp = "dynatrace_otlp"
)

func (e TypeDynatraceOtlp) ToPointer() *TypeDynatraceOtlp {
	return &e
}
func (e *TypeDynatraceOtlp) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dynatrace_otlp":
		*e = TypeDynatraceOtlp(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDynatraceOtlp: %v", v)
	}
}

// ProtocolDynatraceOtlp - Select a transport option for Dynatrace
type ProtocolDynatraceOtlp string

const (
	// ProtocolDynatraceOtlpHTTP HTTP
	ProtocolDynatraceOtlpHTTP ProtocolDynatraceOtlp = "http"
)

func (e ProtocolDynatraceOtlp) ToPointer() *ProtocolDynatraceOtlp {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ProtocolDynatraceOtlp) IsExact() bool {
	if e != nil {
		switch *e {
		case "http":
			return true
		}
	}
	return false
}

// OTLPVersionDynatraceOTLP - The version of OTLP Protobuf definitions to use when structuring data to send
type OTLPVersionDynatraceOTLP string

const (
	// OTLPVersionDynatraceOTLPOneDot3Dot1 1.3.1
	OTLPVersionDynatraceOTLPOneDot3Dot1 OTLPVersionDynatraceOTLP = "1.3.1"
)

func (e OTLPVersionDynatraceOTLP) ToPointer() *OTLPVersionDynatraceOTLP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OTLPVersionDynatraceOTLP) IsExact() bool {
	if e != nil {
		switch *e {
		case "1.3.1":
			return true
		}
	}
	return false
}

// CompressCompressionDynatraceOtlp - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type CompressCompressionDynatraceOtlp string

const (
	// CompressCompressionDynatraceOtlpNone None
	CompressCompressionDynatraceOtlpNone CompressCompressionDynatraceOtlp = "none"
	// CompressCompressionDynatraceOtlpDeflate Deflate
	CompressCompressionDynatraceOtlpDeflate CompressCompressionDynatraceOtlp = "deflate"
	// CompressCompressionDynatraceOtlpGzip Gzip
	CompressCompressionDynatraceOtlpGzip CompressCompressionDynatraceOtlp = "gzip"
)

func (e CompressCompressionDynatraceOtlp) ToPointer() *CompressCompressionDynatraceOtlp {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressCompressionDynatraceOtlp) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "deflate", "gzip":
			return true
		}
	}
	return false
}

// HTTPCompressCompressionDynatraceOtlp - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompressionDynatraceOtlp string

const (
	// HTTPCompressCompressionDynatraceOtlpNone None
	HTTPCompressCompressionDynatraceOtlpNone HTTPCompressCompressionDynatraceOtlp = "none"
	// HTTPCompressCompressionDynatraceOtlpGzip Gzip
	HTTPCompressCompressionDynatraceOtlpGzip HTTPCompressCompressionDynatraceOtlp = "gzip"
)

func (e HTTPCompressCompressionDynatraceOtlp) ToPointer() *HTTPCompressCompressionDynatraceOtlp {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *HTTPCompressCompressionDynatraceOtlp) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

type MetadatumDynatraceOtlp struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (m MetadatumDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(m, "", false)
}

func (m *MetadatumDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &m, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (m *MetadatumDynatraceOtlp) GetKey() *string {
	if m == nil {
		return nil
	}
	return m.Key
}

func (m *MetadatumDynatraceOtlp) GetValue() string {
	if m == nil {
		return ""
	}
	return m.Value
}

// FailedRequestLoggingModeDynatraceOtlp - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDynatraceOtlp string

const (
	// FailedRequestLoggingModeDynatraceOtlpPayload Payload
	FailedRequestLoggingModeDynatraceOtlpPayload FailedRequestLoggingModeDynatraceOtlp = "payload"
	// FailedRequestLoggingModeDynatraceOtlpPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeDynatraceOtlpPayloadAndHeaders FailedRequestLoggingModeDynatraceOtlp = "payloadAndHeaders"
	// FailedRequestLoggingModeDynatraceOtlpNone None
	FailedRequestLoggingModeDynatraceOtlpNone FailedRequestLoggingModeDynatraceOtlp = "none"
)

func (e FailedRequestLoggingModeDynatraceOtlp) ToPointer() *FailedRequestLoggingModeDynatraceOtlp {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeDynatraceOtlp) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

// EndpointType - Select the type of Dynatrace endpoint configured
type EndpointType string

const (
	// EndpointTypeSaas SaaS
	EndpointTypeSaas EndpointType = "saas"
	// EndpointTypeAg ActiveGate
	EndpointTypeAg EndpointType = "ag"
)

func (e EndpointType) ToPointer() *EndpointType {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *EndpointType) IsExact() bool {
	if e != nil {
		switch *e {
		case "saas", "ag":
			return true
		}
	}
	return false
}

// BackpressureBehaviorDynatraceOtlp - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDynatraceOtlp string

const (
	// BackpressureBehaviorDynatraceOtlpBlock Block
	BackpressureBehaviorDynatraceOtlpBlock BackpressureBehaviorDynatraceOtlp = "block"
	// BackpressureBehaviorDynatraceOtlpDrop Drop
	BackpressureBehaviorDynatraceOtlpDrop BackpressureBehaviorDynatraceOtlp = "drop"
	// BackpressureBehaviorDynatraceOtlpQueue Persistent Queue
	BackpressureBehaviorDynatraceOtlpQueue BackpressureBehaviorDynatraceOtlp = "queue"
)

func (e BackpressureBehaviorDynatraceOtlp) ToPointer() *BackpressureBehaviorDynatraceOtlp {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorDynatraceOtlp) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderDynatraceOtlp struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderDynatraceOtlp) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderDynatraceOtlp) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

type ResponseRetrySettingDynatraceOtlp struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingDynatraceOtlp) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingDynatraceOtlp) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingDynatraceOtlp) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingDynatraceOtlp) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsDynatraceOtlp struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsDynatraceOtlp) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsDynatraceOtlp) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsDynatraceOtlp) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsDynatraceOtlp) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// ModeDynatraceOtlp - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeDynatraceOtlp string

const (
	// ModeDynatraceOtlpError Error
	ModeDynatraceOtlpError ModeDynatraceOtlp = "error"
	// ModeDynatraceOtlpAlways Backpressure
	ModeDynatraceOtlpAlways ModeDynatraceOtlp = "always"
	// ModeDynatraceOtlpBackpressure Always On
	ModeDynatraceOtlpBackpressure ModeDynatraceOtlp = "backpressure"
)

func (e ModeDynatraceOtlp) ToPointer() *ModeDynatraceOtlp {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeDynatraceOtlp) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionDynatraceOtlp - Codec to use to compress the persisted data
type PqCompressCompressionDynatraceOtlp string

const (
	// PqCompressCompressionDynatraceOtlpNone None
	PqCompressCompressionDynatraceOtlpNone PqCompressCompressionDynatraceOtlp = "none"
	// PqCompressCompressionDynatraceOtlpGzip Gzip
	PqCompressCompressionDynatraceOtlpGzip PqCompressCompressionDynatraceOtlp = "gzip"
)

func (e PqCompressCompressionDynatraceOtlp) ToPointer() *PqCompressCompressionDynatraceOtlp {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionDynatraceOtlp) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorDynatraceOtlp - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDynatraceOtlp string

const (
	// QueueFullBehaviorDynatraceOtlpBlock Block
	QueueFullBehaviorDynatraceOtlpBlock QueueFullBehaviorDynatraceOtlp = "block"
	// QueueFullBehaviorDynatraceOtlpDrop Drop new data
	QueueFullBehaviorDynatraceOtlpDrop QueueFullBehaviorDynatraceOtlp = "drop"
)

func (e QueueFullBehaviorDynatraceOtlp) ToPointer() *QueueFullBehaviorDynatraceOtlp {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorDynatraceOtlp) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsDynatraceOtlp struct {
}

func (p PqControlsDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputDynatraceOtlp struct {
	// Unique ID for this output
	ID   string            `json:"id"`
	Type TypeDynatraceOtlp `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select a transport option for Dynatrace
	Protocol *ProtocolDynatraceOtlp `default:"http" json:"protocol"`
	// The endpoint where Dynatrace events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)
	Endpoint *string `default:"https://{your-environment-id}.live.dynatrace.com/api/v2/otlp" json:"endpoint"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *OTLPVersionDynatraceOTLP `default:"1.3.1" json:"otlpVersion"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *CompressCompressionDynatraceOtlp `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompressionDynatraceOtlp `default:"gzip" json:"httpCompress"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []MetadatumDynatraceOtlp `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (in KB) of the request body. The maximum payload size is 4 MB. If this limit is exceeded, the entire OTLP message is dropped
	MaxPayloadSizeKB *float64 `default:"2048" json:"maxPayloadSizeKB"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDynatraceOtlp `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Select the type of Dynatrace endpoint configured
	EndpointType *EndpointType `default:"saas" json:"endpointType"`
	// Select or create a stored text secret
	TokenSecret   string  `json:"tokenSecret"`
	AuthTokenName *string `default:"Authorization" json:"authTokenName"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDynatraceOtlp `default:"block" json:"onBackpressure"`
	Description    *string                            `json:"description,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderDynatraceOtlp `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingDynatraceOtlp `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDynatraceOtlp  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeDynatraceOtlp `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionDynatraceOtlp `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorDynatraceOtlp `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsDynatraceOtlp        `json:"pqControls,omitempty"`
}

func (o OutputDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "tokenSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDynatraceOtlp) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDynatraceOtlp) GetType() TypeDynatraceOtlp {
	if o == nil {
		return TypeDynatraceOtlp("")
	}
	return o.Type
}

func (o *OutputDynatraceOtlp) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDynatraceOtlp) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDynatraceOtlp) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDynatraceOtlp) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDynatraceOtlp) GetProtocol() *ProtocolDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputDynatraceOtlp) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDynatraceOtlp) GetOtlpVersion() *OTLPVersionDynatraceOTLP {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputDynatraceOtlp) GetCompress() *CompressCompressionDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDynatraceOtlp) GetHTTPCompress() *HTTPCompressCompressionDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputDynatraceOtlp) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputDynatraceOtlp) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputDynatraceOtlp) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputDynatraceOtlp) GetMetadata() []MetadatumDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputDynatraceOtlp) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDynatraceOtlp) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDynatraceOtlp) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDynatraceOtlp) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDynatraceOtlp) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDynatraceOtlp) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputDynatraceOtlp) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputDynatraceOtlp) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputDynatraceOtlp) GetEndpointType() *EndpointType {
	if o == nil {
		return nil
	}
	return o.EndpointType
}

func (o *OutputDynatraceOtlp) GetTokenSecret() string {
	if o == nil {
		return ""
	}
	return o.TokenSecret
}

func (o *OutputDynatraceOtlp) GetAuthTokenName() *string {
	if o == nil {
		return nil
	}
	return o.AuthTokenName
}

func (o *OutputDynatraceOtlp) GetOnBackpressure() *BackpressureBehaviorDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDynatraceOtlp) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDynatraceOtlp) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDynatraceOtlp) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDynatraceOtlp) GetExtraHTTPHeaders() []ExtraHTTPHeaderDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDynatraceOtlp) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDynatraceOtlp) GetResponseRetrySettings() []ResponseRetrySettingDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDynatraceOtlp) GetTimeoutRetrySettings() *TimeoutRetrySettingsDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDynatraceOtlp) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDynatraceOtlp) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputDynatraceOtlp) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputDynatraceOtlp) GetPqMode() *ModeDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDynatraceOtlp) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputDynatraceOtlp) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputDynatraceOtlp) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDynatraceOtlp) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDynatraceOtlp) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDynatraceOtlp) GetPqCompress() *PqCompressCompressionDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDynatraceOtlp) GetPqOnBackpressure() *QueueFullBehaviorDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDynatraceOtlp) GetPqControls() *PqControlsDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeDynatraceHTTP string

const (
	TypeDynatraceHTTPDynatraceHTTP TypeDynatraceHTTP = "dynatrace_http"
)

func (e TypeDynatraceHTTP) ToPointer() *TypeDynatraceHTTP {
	return &e
}
func (e *TypeDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dynatrace_http":
		*e = TypeDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDynatraceHTTP: %v", v)
	}
}

// MethodDynatraceHTTP - The method to use when sending events
type MethodDynatraceHTTP string

const (
	MethodDynatraceHTTPPost  MethodDynatraceHTTP = "POST"
	MethodDynatraceHTTPPut   MethodDynatraceHTTP = "PUT"
	MethodDynatraceHTTPPatch MethodDynatraceHTTP = "PATCH"
)

func (e MethodDynatraceHTTP) ToPointer() *MethodDynatraceHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MethodDynatraceHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "POST", "PUT", "PATCH":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderDynatraceHTTP struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderDynatraceHTTP) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderDynatraceHTTP) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeDynatraceHTTP - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDynatraceHTTP string

const (
	// FailedRequestLoggingModeDynatraceHTTPPayload Payload
	FailedRequestLoggingModeDynatraceHTTPPayload FailedRequestLoggingModeDynatraceHTTP = "payload"
	// FailedRequestLoggingModeDynatraceHTTPPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeDynatraceHTTPPayloadAndHeaders FailedRequestLoggingModeDynatraceHTTP = "payloadAndHeaders"
	// FailedRequestLoggingModeDynatraceHTTPNone None
	FailedRequestLoggingModeDynatraceHTTPNone FailedRequestLoggingModeDynatraceHTTP = "none"
)

func (e FailedRequestLoggingModeDynatraceHTTP) ToPointer() *FailedRequestLoggingModeDynatraceHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeDynatraceHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingDynatraceHTTP struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingDynatraceHTTP) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingDynatraceHTTP) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingDynatraceHTTP) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingDynatraceHTTP) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsDynatraceHTTP struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsDynatraceHTTP) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsDynatraceHTTP) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsDynatraceHTTP) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsDynatraceHTTP) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorDynatraceHTTP - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDynatraceHTTP string

const (
	// BackpressureBehaviorDynatraceHTTPBlock Block
	BackpressureBehaviorDynatraceHTTPBlock BackpressureBehaviorDynatraceHTTP = "block"
	// BackpressureBehaviorDynatraceHTTPDrop Drop
	BackpressureBehaviorDynatraceHTTPDrop BackpressureBehaviorDynatraceHTTP = "drop"
	// BackpressureBehaviorDynatraceHTTPQueue Persistent Queue
	BackpressureBehaviorDynatraceHTTPQueue BackpressureBehaviorDynatraceHTTP = "queue"
)

func (e BackpressureBehaviorDynatraceHTTP) ToPointer() *BackpressureBehaviorDynatraceHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorDynatraceHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type AuthenticationTypeDynatraceHTTP string

const (
	// AuthenticationTypeDynatraceHTTPToken Auth token
	AuthenticationTypeDynatraceHTTPToken AuthenticationTypeDynatraceHTTP = "token"
	// AuthenticationTypeDynatraceHTTPTextSecret Token (text secret)
	AuthenticationTypeDynatraceHTTPTextSecret AuthenticationTypeDynatraceHTTP = "textSecret"
)

func (e AuthenticationTypeDynatraceHTTP) ToPointer() *AuthenticationTypeDynatraceHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationTypeDynatraceHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "token", "textSecret":
			return true
		}
	}
	return false
}

// FormatDynatraceHTTP - How to format events before sending. Defaults to JSON. Plaintext is not currently supported.
type FormatDynatraceHTTP string

const (
	// FormatDynatraceHTTPJSONArray JSON
	FormatDynatraceHTTPJSONArray FormatDynatraceHTTP = "json_array"
	// FormatDynatraceHTTPPlaintext Plaintext
	FormatDynatraceHTTPPlaintext FormatDynatraceHTTP = "plaintext"
)

func (e FormatDynatraceHTTP) ToPointer() *FormatDynatraceHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FormatDynatraceHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "json_array", "plaintext":
			return true
		}
	}
	return false
}

type Endpoint string

const (
	// EndpointCloud Cloud
	EndpointCloud Endpoint = "cloud"
	// EndpointActiveGate ActiveGate
	EndpointActiveGate Endpoint = "activeGate"
	// EndpointManual Manual
	EndpointManual Endpoint = "manual"
)

func (e Endpoint) ToPointer() *Endpoint {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *Endpoint) IsExact() bool {
	if e != nil {
		switch *e {
		case "cloud", "activeGate", "manual":
			return true
		}
	}
	return false
}

type TelemetryType string

const (
	// TelemetryTypeLogs Logs
	TelemetryTypeLogs TelemetryType = "logs"
	// TelemetryTypeMetrics Metrics
	TelemetryTypeMetrics TelemetryType = "metrics"
)

func (e TelemetryType) ToPointer() *TelemetryType {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *TelemetryType) IsExact() bool {
	if e != nil {
		switch *e {
		case "logs", "metrics":
			return true
		}
	}
	return false
}

// ModeDynatraceHTTP - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeDynatraceHTTP string

const (
	// ModeDynatraceHTTPError Error
	ModeDynatraceHTTPError ModeDynatraceHTTP = "error"
	// ModeDynatraceHTTPAlways Backpressure
	ModeDynatraceHTTPAlways ModeDynatraceHTTP = "always"
	// ModeDynatraceHTTPBackpressure Always On
	ModeDynatraceHTTPBackpressure ModeDynatraceHTTP = "backpressure"
)

func (e ModeDynatraceHTTP) ToPointer() *ModeDynatraceHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeDynatraceHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionDynatraceHTTP - Codec to use to compress the persisted data
type CompressionDynatraceHTTP string

const (
	// CompressionDynatraceHTTPNone None
	CompressionDynatraceHTTPNone CompressionDynatraceHTTP = "none"
	// CompressionDynatraceHTTPGzip Gzip
	CompressionDynatraceHTTPGzip CompressionDynatraceHTTP = "gzip"
)

func (e CompressionDynatraceHTTP) ToPointer() *CompressionDynatraceHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionDynatraceHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorDynatraceHTTP - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDynatraceHTTP string

const (
	// QueueFullBehaviorDynatraceHTTPBlock Block
	QueueFullBehaviorDynatraceHTTPBlock QueueFullBehaviorDynatraceHTTP = "block"
	// QueueFullBehaviorDynatraceHTTPDrop Drop new data
	QueueFullBehaviorDynatraceHTTPDrop QueueFullBehaviorDynatraceHTTP = "drop"
)

func (e QueueFullBehaviorDynatraceHTTP) ToPointer() *QueueFullBehaviorDynatraceHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorDynatraceHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsDynatraceHTTP struct {
}

func (p PqControlsDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputDynatraceHTTP struct {
	// Unique ID for this output
	ID   string            `json:"id"`
	Type TypeDynatraceHTTP `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The method to use when sending events
	Method *MethodDynatraceHTTP `default:"POST" json:"method"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
	ExtraHTTPHeaders []ExtraHTTPHeaderDynatraceHTTP `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDynatraceHTTP `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingDynatraceHTTP `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDynatraceHTTP  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDynatraceHTTP `default:"block" json:"onBackpressure"`
	AuthType       *AuthenticationTypeDynatraceHTTP   `default:"token" json:"authType"`
	// How to format events before sending. Defaults to JSON. Plaintext is not currently supported.
	Format        *FormatDynatraceHTTP `default:"json_array" json:"format"`
	Endpoint      *Endpoint            `default:"cloud" json:"endpoint"`
	TelemetryType *TelemetryType       `default:"logs" json:"telemetryType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeDynatraceHTTP `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionDynatraceHTTP `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorDynatraceHTTP `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsDynatraceHTTP        `json:"pqControls,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// ID of the environment to send to
	EnvironmentID *string `json:"environmentId,omitempty"`
	// ActiveGate domain with Log analytics collector module enabled. For example https://{activeGate-domain}:9999/e/{environment-id}/api/v2/logs/ingest.
	ActiveGateDomain *string `json:"activeGateDomain,omitempty"`
	// URL to send events to. Can be overwritten by an event's __url field.
	URL *string `json:"url,omitempty"`
}

func (o OutputDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDynatraceHTTP) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDynatraceHTTP) GetType() TypeDynatraceHTTP {
	if o == nil {
		return TypeDynatraceHTTP("")
	}
	return o.Type
}

func (o *OutputDynatraceHTTP) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDynatraceHTTP) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDynatraceHTTP) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDynatraceHTTP) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDynatraceHTTP) GetMethod() *MethodDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.Method
}

func (o *OutputDynatraceHTTP) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputDynatraceHTTP) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDynatraceHTTP) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDynatraceHTTP) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputDynatraceHTTP) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDynatraceHTTP) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDynatraceHTTP) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDynatraceHTTP) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDynatraceHTTP) GetExtraHTTPHeaders() []ExtraHTTPHeaderDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDynatraceHTTP) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDynatraceHTTP) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDynatraceHTTP) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDynatraceHTTP) GetResponseRetrySettings() []ResponseRetrySettingDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDynatraceHTTP) GetTimeoutRetrySettings() *TimeoutRetrySettingsDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDynatraceHTTP) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDynatraceHTTP) GetOnBackpressure() *BackpressureBehaviorDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDynatraceHTTP) GetAuthType() *AuthenticationTypeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputDynatraceHTTP) GetFormat() *FormatDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDynatraceHTTP) GetEndpoint() *Endpoint {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDynatraceHTTP) GetTelemetryType() *TelemetryType {
	if o == nil {
		return nil
	}
	return o.TelemetryType
}

func (o *OutputDynatraceHTTP) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputDynatraceHTTP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDynatraceHTTP) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputDynatraceHTTP) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputDynatraceHTTP) GetPqMode() *ModeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDynatraceHTTP) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputDynatraceHTTP) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputDynatraceHTTP) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDynatraceHTTP) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDynatraceHTTP) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDynatraceHTTP) GetPqCompress() *CompressionDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDynatraceHTTP) GetPqOnBackpressure() *QueueFullBehaviorDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDynatraceHTTP) GetPqControls() *PqControlsDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDynatraceHTTP) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputDynatraceHTTP) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputDynatraceHTTP) GetEnvironmentID() *string {
	if o == nil {
		return nil
	}
	return o.EnvironmentID
}

func (o *OutputDynatraceHTTP) GetActiveGateDomain() *string {
	if o == nil {
		return nil
	}
	return o.ActiveGateDomain
}

func (o *OutputDynatraceHTTP) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

type CreateOutputTypeNetflow string

const (
	CreateOutputTypeNetflowNetflow CreateOutputTypeNetflow = "netflow"
)

func (e CreateOutputTypeNetflow) ToPointer() *CreateOutputTypeNetflow {
	return &e
}
func (e *CreateOutputTypeNetflow) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "netflow":
		*e = CreateOutputTypeNetflow(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeNetflow: %v", v)
	}
}

type HostNetflow struct {
	// Destination host
	Host string `json:"host"`
	// Destination port, default is 2055
	Port *float64 `default:"2055" json:"port"`
}

func (h HostNetflow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostNetflow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host"}); err != nil {
		return err
	}
	return nil
}

func (h *HostNetflow) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostNetflow) GetPort() *float64 {
	if h == nil {
		return nil
	}
	return h.Port
}

type OutputNetflow struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type CreateOutputTypeNetflow `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// One or more NetFlow Destinations to forward events to
	Hosts []HostNetflow `json:"hosts"`
	// How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every datagram sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	// Send NetFlow traffic using the original event's Source IP and port. To enable this, you must install the external `udp-sender` helper binary at `/usr/bin/udp-sender` on all Worker Nodes and grant it the `CAP_NET_RAW` capability.
	EnableIPSpoofing *bool   `default:"false" json:"enableIpSpoofing"`
	Description      *string `json:"description,omitempty"`
	// MTU in bytes. The actual maximum NetFlow payload size will be MTU minus IP and UDP headers (28 bytes for IPv4, 48 bytes for IPv6). For example, with the default MTU of 1500, the max payload is 1472 bytes for IPv4. Payloads exceeding this limit will be dropped.
	MaxRecordSize *float64 `default:"1500" json:"maxRecordSize"`
}

func (o OutputNetflow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputNetflow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "hosts"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputNetflow) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputNetflow) GetType() CreateOutputTypeNetflow {
	if o == nil {
		return CreateOutputTypeNetflow("")
	}
	return o.Type
}

func (o *OutputNetflow) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputNetflow) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputNetflow) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputNetflow) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputNetflow) GetHosts() []HostNetflow {
	if o == nil {
		return []HostNetflow{}
	}
	return o.Hosts
}

func (o *OutputNetflow) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputNetflow) GetEnableIPSpoofing() *bool {
	if o == nil {
		return nil
	}
	return o.EnableIPSpoofing
}

func (o *OutputNetflow) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputNetflow) GetMaxRecordSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSize
}

type TypeXsiam string

const (
	TypeXsiamXsiam TypeXsiam = "xsiam"
)

func (e TypeXsiam) ToPointer() *TypeXsiam {
	return &e
}
func (e *TypeXsiam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "xsiam":
		*e = TypeXsiam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeXsiam: %v", v)
	}
}

type ExtraHTTPHeaderXsiam struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderXsiam) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderXsiam) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeXsiam - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeXsiam string

const (
	// FailedRequestLoggingModeXsiamPayload Payload
	FailedRequestLoggingModeXsiamPayload FailedRequestLoggingModeXsiam = "payload"
	// FailedRequestLoggingModeXsiamPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeXsiamPayloadAndHeaders FailedRequestLoggingModeXsiam = "payloadAndHeaders"
	// FailedRequestLoggingModeXsiamNone None
	FailedRequestLoggingModeXsiamNone FailedRequestLoggingModeXsiam = "none"
)

func (e FailedRequestLoggingModeXsiam) ToPointer() *FailedRequestLoggingModeXsiam {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeXsiam) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

// AuthenticationMethodXsiam - Enter a token directly, or provide a secret referencing a token
type AuthenticationMethodXsiam string

const (
	AuthenticationMethodXsiamToken  AuthenticationMethodXsiam = "token"
	AuthenticationMethodXsiamSecret AuthenticationMethodXsiam = "secret"
)

func (e AuthenticationMethodXsiam) ToPointer() *AuthenticationMethodXsiam {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodXsiam) IsExact() bool {
	if e != nil {
		switch *e {
		case "token", "secret":
			return true
		}
	}
	return false
}

type ResponseRetrySettingXsiam struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingXsiam) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingXsiam) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingXsiam) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingXsiam) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsXsiam struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsXsiam) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsXsiam) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsXsiam) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsXsiam) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorXsiam - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorXsiam string

const (
	// BackpressureBehaviorXsiamBlock Block
	BackpressureBehaviorXsiamBlock BackpressureBehaviorXsiam = "block"
	// BackpressureBehaviorXsiamDrop Drop
	BackpressureBehaviorXsiamDrop BackpressureBehaviorXsiam = "drop"
	// BackpressureBehaviorXsiamQueue Persistent Queue
	BackpressureBehaviorXsiamQueue BackpressureBehaviorXsiam = "queue"
)

func (e BackpressureBehaviorXsiam) ToPointer() *BackpressureBehaviorXsiam {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorXsiam) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type URLXsiam struct {
	URL any `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, []string{"url"}); err != nil {
		return err
	}
	return nil
}

func (u *URLXsiam) GetURL() any {
	if u == nil {
		return nil
	}
	return u.URL
}

func (u *URLXsiam) GetWeight() *float64 {
	if u == nil {
		return nil
	}
	return u.Weight
}

// ModeXsiam - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeXsiam string

const (
	// ModeXsiamError Error
	ModeXsiamError ModeXsiam = "error"
	// ModeXsiamAlways Backpressure
	ModeXsiamAlways ModeXsiam = "always"
	// ModeXsiamBackpressure Always On
	ModeXsiamBackpressure ModeXsiam = "backpressure"
)

func (e ModeXsiam) ToPointer() *ModeXsiam {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeXsiam) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionXsiam - Codec to use to compress the persisted data
type CompressionXsiam string

const (
	// CompressionXsiamNone None
	CompressionXsiamNone CompressionXsiam = "none"
	// CompressionXsiamGzip Gzip
	CompressionXsiamGzip CompressionXsiam = "gzip"
)

func (e CompressionXsiam) ToPointer() *CompressionXsiam {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionXsiam) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorXsiam - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorXsiam string

const (
	// QueueFullBehaviorXsiamBlock Block
	QueueFullBehaviorXsiamBlock QueueFullBehaviorXsiam = "block"
	// QueueFullBehaviorXsiamDrop Drop new data
	QueueFullBehaviorXsiamDrop QueueFullBehaviorXsiam = "drop"
)

func (e QueueFullBehaviorXsiam) ToPointer() *QueueFullBehaviorXsiam {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorXsiam) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsXsiam struct {
}

func (p PqControlsXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputXsiam struct {
	// Unique ID for this output
	ID   string    `json:"id"`
	Type TypeXsiam `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"false" json:"loadBalanced"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"10000" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderXsiam `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeXsiam `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Enter a token directly, or provide a secret referencing a token
	AuthType *AuthenticationMethodXsiam `default:"token" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingXsiam `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsXsiam  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Maximum number of requests to limit to per second
	ThrottleRateReqPerSec *int64 `default:"400" json:"throttleRateReqPerSec"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorXsiam `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// XSIAM endpoint URL to send events to, such as https://api-{tenant external URL}/logs/v1/event
	URL *string `default:"http://localhost:8088/logs/v1/event" json:"url"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool      `default:"false" json:"excludeSelf"`
	Urls        []URLXsiam `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// XSIAM authentication token
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeXsiam `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionXsiam `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorXsiam `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsXsiam        `json:"pqControls,omitempty"`
}

func (o OutputXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputXsiam) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputXsiam) GetType() TypeXsiam {
	if o == nil {
		return TypeXsiam("")
	}
	return o.Type
}

func (o *OutputXsiam) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputXsiam) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputXsiam) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputXsiam) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputXsiam) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputXsiam) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputXsiam) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputXsiam) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputXsiam) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputXsiam) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputXsiam) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputXsiam) GetExtraHTTPHeaders() []ExtraHTTPHeaderXsiam {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputXsiam) GetFailedRequestLoggingMode() *FailedRequestLoggingModeXsiam {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputXsiam) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputXsiam) GetAuthType() *AuthenticationMethodXsiam {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputXsiam) GetResponseRetrySettings() []ResponseRetrySettingXsiam {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputXsiam) GetTimeoutRetrySettings() *TimeoutRetrySettingsXsiam {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputXsiam) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputXsiam) GetThrottleRateReqPerSec() *int64 {
	if o == nil {
		return nil
	}
	return o.ThrottleRateReqPerSec
}

func (o *OutputXsiam) GetOnBackpressure() *BackpressureBehaviorXsiam {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputXsiam) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputXsiam) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputXsiam) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputXsiam) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputXsiam) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputXsiam) GetUrls() []URLXsiam {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputXsiam) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputXsiam) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputXsiam) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputXsiam) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputXsiam) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputXsiam) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputXsiam) GetPqMode() *ModeXsiam {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputXsiam) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputXsiam) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputXsiam) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputXsiam) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputXsiam) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputXsiam) GetPqCompress() *CompressionXsiam {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputXsiam) GetPqOnBackpressure() *QueueFullBehaviorXsiam {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputXsiam) GetPqControls() *PqControlsXsiam {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeClickHouse string

const (
	TypeClickHouseClickHouse TypeClickHouse = "click_house"
)

func (e TypeClickHouse) ToPointer() *TypeClickHouse {
	return &e
}
func (e *TypeClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "click_house":
		*e = TypeClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeClickHouse: %v", v)
	}
}

type AuthenticationTypeClickHouse string

const (
	AuthenticationTypeClickHouseNone               AuthenticationTypeClickHouse = "none"
	AuthenticationTypeClickHouseBasic              AuthenticationTypeClickHouse = "basic"
	AuthenticationTypeClickHouseCredentialsSecret  AuthenticationTypeClickHouse = "credentialsSecret"
	AuthenticationTypeClickHouseSslUserCertificate AuthenticationTypeClickHouse = "sslUserCertificate"
	AuthenticationTypeClickHouseToken              AuthenticationTypeClickHouse = "token"
	AuthenticationTypeClickHouseTextSecret         AuthenticationTypeClickHouse = "textSecret"
	AuthenticationTypeClickHouseOauth              AuthenticationTypeClickHouse = "oauth"
)

func (e AuthenticationTypeClickHouse) ToPointer() *AuthenticationTypeClickHouse {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationTypeClickHouse) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "basic", "credentialsSecret", "sslUserCertificate", "token", "textSecret", "oauth":
			return true
		}
	}
	return false
}

// FormatClickHouse - Data format to use when sending data to ClickHouse. Defaults to JSON Compact.
type FormatClickHouse string

const (
	// FormatClickHouseJSONCompactEachRowWithNames JSONCompactEachRowWithNames
	FormatClickHouseJSONCompactEachRowWithNames FormatClickHouse = "json-compact-each-row-with-names"
	// FormatClickHouseJSONEachRow JSONEachRow
	FormatClickHouseJSONEachRow FormatClickHouse = "json-each-row"
)

func (e FormatClickHouse) ToPointer() *FormatClickHouse {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FormatClickHouse) IsExact() bool {
	if e != nil {
		switch *e {
		case "json-compact-each-row-with-names", "json-each-row":
			return true
		}
	}
	return false
}

// MappingType - How event fields are mapped to ClickHouse columns.
type MappingType string

const (
	// MappingTypeAutomatic Automatic
	MappingTypeAutomatic MappingType = "automatic"
	// MappingTypeCustom Custom
	MappingTypeCustom MappingType = "custom"
)

func (e MappingType) ToPointer() *MappingType {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MappingType) IsExact() bool {
	if e != nil {
		switch *e {
		case "automatic", "custom":
			return true
		}
	}
	return false
}

type MinimumTLSVersionClickHouse string

const (
	MinimumTLSVersionClickHouseTlSv1  MinimumTLSVersionClickHouse = "TLSv1"
	MinimumTLSVersionClickHouseTlSv11 MinimumTLSVersionClickHouse = "TLSv1.1"
	MinimumTLSVersionClickHouseTlSv12 MinimumTLSVersionClickHouse = "TLSv1.2"
	MinimumTLSVersionClickHouseTlSv13 MinimumTLSVersionClickHouse = "TLSv1.3"
)

func (e MinimumTLSVersionClickHouse) ToPointer() *MinimumTLSVersionClickHouse {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MinimumTLSVersionClickHouse) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type MaximumTLSVersionClickHouse string

const (
	MaximumTLSVersionClickHouseTlSv1  MaximumTLSVersionClickHouse = "TLSv1"
	MaximumTLSVersionClickHouseTlSv11 MaximumTLSVersionClickHouse = "TLSv1.1"
	MaximumTLSVersionClickHouseTlSv12 MaximumTLSVersionClickHouse = "TLSv1.2"
	MaximumTLSVersionClickHouseTlSv13 MaximumTLSVersionClickHouse = "TLSv1.3"
)

func (e MaximumTLSVersionClickHouse) ToPointer() *MaximumTLSVersionClickHouse {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MaximumTLSVersionClickHouse) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideClickHouse struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                      `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionClickHouse `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionClickHouse `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideClickHouse) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideClickHouse) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideClickHouse) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideClickHouse) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideClickHouse) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideClickHouse) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideClickHouse) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideClickHouse) GetMinVersion() *MinimumTLSVersionClickHouse {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideClickHouse) GetMaxVersion() *MaximumTLSVersionClickHouse {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

type ExtraHTTPHeaderClickHouse struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderClickHouse) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderClickHouse) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeClickHouse - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeClickHouse string

const (
	// FailedRequestLoggingModeClickHousePayload Payload
	FailedRequestLoggingModeClickHousePayload FailedRequestLoggingModeClickHouse = "payload"
	// FailedRequestLoggingModeClickHousePayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeClickHousePayloadAndHeaders FailedRequestLoggingModeClickHouse = "payloadAndHeaders"
	// FailedRequestLoggingModeClickHouseNone None
	FailedRequestLoggingModeClickHouseNone FailedRequestLoggingModeClickHouse = "none"
)

func (e FailedRequestLoggingModeClickHouse) ToPointer() *FailedRequestLoggingModeClickHouse {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeClickHouse) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingClickHouse struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingClickHouse) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingClickHouse) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingClickHouse) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingClickHouse) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsClickHouse struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsClickHouse) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsClickHouse) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsClickHouse) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsClickHouse) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type StatsDestination struct {
	URL         *string `json:"url,omitempty"`
	Database    *string `json:"database,omitempty"`
	TableName   *string `json:"tableName,omitempty"`
	AuthType    *string `json:"authType,omitempty"`
	Username    *string `json:"username,omitempty"`
	SQLUsername *string `json:"sqlUsername,omitempty"`
	Password    *string `json:"password,omitempty"`
}

func (s StatsDestination) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *StatsDestination) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *StatsDestination) GetURL() *string {
	if s == nil {
		return nil
	}
	return s.URL
}

func (s *StatsDestination) GetDatabase() *string {
	if s == nil {
		return nil
	}
	return s.Database
}

func (s *StatsDestination) GetTableName() *string {
	if s == nil {
		return nil
	}
	return s.TableName
}

func (s *StatsDestination) GetAuthType() *string {
	if s == nil {
		return nil
	}
	return s.AuthType
}

func (s *StatsDestination) GetUsername() *string {
	if s == nil {
		return nil
	}
	return s.Username
}

func (s *StatsDestination) GetSQLUsername() *string {
	if s == nil {
		return nil
	}
	return s.SQLUsername
}

func (s *StatsDestination) GetPassword() *string {
	if s == nil {
		return nil
	}
	return s.Password
}

// BackpressureBehaviorClickHouse - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorClickHouse string

const (
	// BackpressureBehaviorClickHouseBlock Block
	BackpressureBehaviorClickHouseBlock BackpressureBehaviorClickHouse = "block"
	// BackpressureBehaviorClickHouseDrop Drop
	BackpressureBehaviorClickHouseDrop BackpressureBehaviorClickHouse = "drop"
	// BackpressureBehaviorClickHouseQueue Persistent Queue
	BackpressureBehaviorClickHouseQueue BackpressureBehaviorClickHouse = "queue"
)

func (e BackpressureBehaviorClickHouse) ToPointer() *BackpressureBehaviorClickHouse {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorClickHouse) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type OauthParamClickHouse struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o OauthParamClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthParamClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthParamClickHouse) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamClickHouse) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderClickHouse struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o OauthHeaderClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthHeaderClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthHeaderClickHouse) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderClickHouse) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ColumnMapping struct {
	// Name of the column in ClickHouse that will store field value
	ColumnName string `json:"columnName"`
	// Type of the column in the ClickHouse database
	ColumnType *string `json:"columnType,omitempty"`
	// JavaScript expression to compute value to be inserted into ClickHouse table
	ColumnValueExpression string `json:"columnValueExpression"`
}

func (c ColumnMapping) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *ColumnMapping) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"columnName", "columnValueExpression"}); err != nil {
		return err
	}
	return nil
}

func (c *ColumnMapping) GetColumnName() string {
	if c == nil {
		return ""
	}
	return c.ColumnName
}

func (c *ColumnMapping) GetColumnType() *string {
	if c == nil {
		return nil
	}
	return c.ColumnType
}

func (c *ColumnMapping) GetColumnValueExpression() string {
	if c == nil {
		return ""
	}
	return c.ColumnValueExpression
}

// ModeClickHouse - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeClickHouse string

const (
	// ModeClickHouseError Error
	ModeClickHouseError ModeClickHouse = "error"
	// ModeClickHouseAlways Backpressure
	ModeClickHouseAlways ModeClickHouse = "always"
	// ModeClickHouseBackpressure Always On
	ModeClickHouseBackpressure ModeClickHouse = "backpressure"
)

func (e ModeClickHouse) ToPointer() *ModeClickHouse {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeClickHouse) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionClickHouse - Codec to use to compress the persisted data
type CompressionClickHouse string

const (
	// CompressionClickHouseNone None
	CompressionClickHouseNone CompressionClickHouse = "none"
	// CompressionClickHouseGzip Gzip
	CompressionClickHouseGzip CompressionClickHouse = "gzip"
)

func (e CompressionClickHouse) ToPointer() *CompressionClickHouse {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionClickHouse) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorClickHouse - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorClickHouse string

const (
	// QueueFullBehaviorClickHouseBlock Block
	QueueFullBehaviorClickHouseBlock QueueFullBehaviorClickHouse = "block"
	// QueueFullBehaviorClickHouseDrop Drop new data
	QueueFullBehaviorClickHouseDrop QueueFullBehaviorClickHouse = "drop"
)

func (e QueueFullBehaviorClickHouse) ToPointer() *QueueFullBehaviorClickHouse {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorClickHouse) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsClickHouse struct {
}

func (p PqControlsClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputClickHouse struct {
	// Unique ID for this output
	ID   string         `json:"id"`
	Type TypeClickHouse `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL of the ClickHouse instance. Example: http://localhost:8123/
	URL      string                        `json:"url"`
	AuthType *AuthenticationTypeClickHouse `default:"none" json:"authType"`
	Database string                        `json:"database"`
	// Name of the ClickHouse table where data will be inserted. Name can contain letters (A-Z, a-z), numbers (0-9), and the character "_", and must start with either a letter or the character "_".
	TableName string `json:"tableName"`
	// Data format to use when sending data to ClickHouse. Defaults to JSON Compact.
	Format *FormatClickHouse `default:"json-compact-each-row-with-names" json:"format"`
	// How event fields are mapped to ClickHouse columns.
	MappingType *MappingType `default:"automatic" json:"mappingType"`
	// Collect data into batches for later processing. Disable to write to a ClickHouse table immediately.
	AsyncInserts *bool                            `default:"false" json:"asyncInserts"`
	TLS          *TLSSettingsClientSideClickHouse `json:"tls,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderClickHouse `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeClickHouse `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingClickHouse `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsClickHouse  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Log the most recent event that fails to match the table schema
	DumpFormatErrorsToDisk *bool             `default:"false" json:"dumpFormatErrorsToDisk"`
	StatsDestination       *StatsDestination `json:"statsDestination,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorClickHouse `default:"block" json:"onBackpressure"`
	Description    *string                         `json:"description,omitempty"`
	Username       *string                         `json:"username,omitempty"`
	Password       *string                         `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamClickHouse `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderClickHouse `json:"oauthHeaders,omitempty"`
	// Username for certificate authentication
	SQLUsername *string `json:"sqlUsername,omitempty"`
	// Cribl will wait for confirmation that data has been fully inserted into the ClickHouse database before proceeding. Disabling this option can increase throughput, but Cribl won’t be able to verify data has been completely inserted.
	WaitForAsyncInserts *bool `default:"true" json:"waitForAsyncInserts"`
	// Fields to exclude from sending to ClickHouse
	ExcludeMappingFields []string `json:"excludeMappingFields,omitempty"`
	// Retrieves the table schema from ClickHouse and populates the Column Mapping table
	DescribeTable  *string         `json:"describeTable,omitempty"`
	ColumnMappings []ColumnMapping `json:"columnMappings,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeClickHouse `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionClickHouse `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorClickHouse `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsClickHouse        `json:"pqControls,omitempty"`
}

func (o OutputClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "url", "database", "tableName"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputClickHouse) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputClickHouse) GetType() TypeClickHouse {
	if o == nil {
		return TypeClickHouse("")
	}
	return o.Type
}

func (o *OutputClickHouse) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputClickHouse) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputClickHouse) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputClickHouse) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputClickHouse) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputClickHouse) GetAuthType() *AuthenticationTypeClickHouse {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputClickHouse) GetDatabase() string {
	if o == nil {
		return ""
	}
	return o.Database
}

func (o *OutputClickHouse) GetTableName() string {
	if o == nil {
		return ""
	}
	return o.TableName
}

func (o *OutputClickHouse) GetFormat() *FormatClickHouse {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputClickHouse) GetMappingType() *MappingType {
	if o == nil {
		return nil
	}
	return o.MappingType
}

func (o *OutputClickHouse) GetAsyncInserts() *bool {
	if o == nil {
		return nil
	}
	return o.AsyncInserts
}

func (o *OutputClickHouse) GetTLS() *TLSSettingsClientSideClickHouse {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputClickHouse) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputClickHouse) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputClickHouse) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputClickHouse) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputClickHouse) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputClickHouse) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputClickHouse) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputClickHouse) GetExtraHTTPHeaders() []ExtraHTTPHeaderClickHouse {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputClickHouse) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputClickHouse) GetFailedRequestLoggingMode() *FailedRequestLoggingModeClickHouse {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputClickHouse) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputClickHouse) GetResponseRetrySettings() []ResponseRetrySettingClickHouse {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputClickHouse) GetTimeoutRetrySettings() *TimeoutRetrySettingsClickHouse {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputClickHouse) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputClickHouse) GetDumpFormatErrorsToDisk() *bool {
	if o == nil {
		return nil
	}
	return o.DumpFormatErrorsToDisk
}

func (o *OutputClickHouse) GetStatsDestination() *StatsDestination {
	if o == nil {
		return nil
	}
	return o.StatsDestination
}

func (o *OutputClickHouse) GetOnBackpressure() *BackpressureBehaviorClickHouse {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputClickHouse) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputClickHouse) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputClickHouse) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputClickHouse) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputClickHouse) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputClickHouse) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputClickHouse) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputClickHouse) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputClickHouse) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputClickHouse) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputClickHouse) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputClickHouse) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputClickHouse) GetOauthParams() []OauthParamClickHouse {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputClickHouse) GetOauthHeaders() []OauthHeaderClickHouse {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputClickHouse) GetSQLUsername() *string {
	if o == nil {
		return nil
	}
	return o.SQLUsername
}

func (o *OutputClickHouse) GetWaitForAsyncInserts() *bool {
	if o == nil {
		return nil
	}
	return o.WaitForAsyncInserts
}

func (o *OutputClickHouse) GetExcludeMappingFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeMappingFields
}

func (o *OutputClickHouse) GetDescribeTable() *string {
	if o == nil {
		return nil
	}
	return o.DescribeTable
}

func (o *OutputClickHouse) GetColumnMappings() []ColumnMapping {
	if o == nil {
		return nil
	}
	return o.ColumnMappings
}

func (o *OutputClickHouse) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputClickHouse) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputClickHouse) GetPqMode() *ModeClickHouse {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputClickHouse) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputClickHouse) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputClickHouse) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputClickHouse) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputClickHouse) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputClickHouse) GetPqCompress() *CompressionClickHouse {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputClickHouse) GetPqOnBackpressure() *QueueFullBehaviorClickHouse {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputClickHouse) GetPqControls() *PqControlsClickHouse {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeDiskSpool string

const (
	TypeDiskSpoolDiskSpool TypeDiskSpool = "disk_spool"
)

func (e TypeDiskSpool) ToPointer() *TypeDiskSpool {
	return &e
}
func (e *TypeDiskSpool) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "disk_spool":
		*e = TypeDiskSpool(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDiskSpool: %v", v)
	}
}

// CompressionDiskSpool - Data compression format. Default is gzip.
type CompressionDiskSpool string

const (
	CompressionDiskSpoolNone CompressionDiskSpool = "none"
	CompressionDiskSpoolGzip CompressionDiskSpool = "gzip"
)

func (e CompressionDiskSpool) ToPointer() *CompressionDiskSpool {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionDiskSpool) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

type OutputDiskSpool struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeDiskSpool `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Time period for grouping spooled events. Default is 10m.
	TimeWindow *string `default:"10m" json:"timeWindow"`
	// Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.
	MaxDataSize *string `default:"1GB" json:"maxDataSize"`
	// Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.
	MaxDataTime *string `default:"24h" json:"maxDataTime"`
	// Data compression format. Default is gzip.
	Compress *CompressionDiskSpool `default:"gzip" json:"compress"`
	// JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory.
	PartitionExpr *string `json:"partitionExpr,omitempty"`
	Description   *string `json:"description,omitempty"`
}

func (o OutputDiskSpool) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDiskSpool) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDiskSpool) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDiskSpool) GetType() TypeDiskSpool {
	if o == nil {
		return TypeDiskSpool("")
	}
	return o.Type
}

func (o *OutputDiskSpool) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDiskSpool) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDiskSpool) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDiskSpool) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDiskSpool) GetTimeWindow() *string {
	if o == nil {
		return nil
	}
	return o.TimeWindow
}

func (o *OutputDiskSpool) GetMaxDataSize() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataSize
}

func (o *OutputDiskSpool) GetMaxDataTime() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataTime
}

func (o *OutputDiskSpool) GetCompress() *CompressionDiskSpool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDiskSpool) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputDiskSpool) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

type TypeCriblLake string

const (
	TypeCriblLakeCriblLake TypeCriblLake = "cribl_lake"
)

func (e TypeCriblLake) ToPointer() *TypeCriblLake {
	return &e
}
func (e *TypeCriblLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cribl_lake":
		*e = TypeCriblLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCriblLake: %v", v)
	}
}

// SignatureVersionCriblLake - Signature version to use for signing S3 requests
type SignatureVersionCriblLake string

const (
	SignatureVersionCriblLakeV2 SignatureVersionCriblLake = "v2"
	SignatureVersionCriblLakeV4 SignatureVersionCriblLake = "v4"
)

func (e SignatureVersionCriblLake) ToPointer() *SignatureVersionCriblLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SignatureVersionCriblLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

// ObjectACLCriblLake - Object ACL to assign to uploaded objects
type ObjectACLCriblLake string

const (
	// ObjectACLCriblLakePrivate Private
	ObjectACLCriblLakePrivate ObjectACLCriblLake = "private"
	// ObjectACLCriblLakePublicRead Public Read Only
	ObjectACLCriblLakePublicRead ObjectACLCriblLake = "public-read"
	// ObjectACLCriblLakePublicReadWrite Public Read/Write
	ObjectACLCriblLakePublicReadWrite ObjectACLCriblLake = "public-read-write"
	// ObjectACLCriblLakeAuthenticatedRead Authenticated Read Only
	ObjectACLCriblLakeAuthenticatedRead ObjectACLCriblLake = "authenticated-read"
	// ObjectACLCriblLakeAwsExecRead AWS EC2 AMI Read Only
	ObjectACLCriblLakeAwsExecRead ObjectACLCriblLake = "aws-exec-read"
	// ObjectACLCriblLakeBucketOwnerRead Bucket Owner Read Only
	ObjectACLCriblLakeBucketOwnerRead ObjectACLCriblLake = "bucket-owner-read"
	// ObjectACLCriblLakeBucketOwnerFullControl Bucket Owner Full Control
	ObjectACLCriblLakeBucketOwnerFullControl ObjectACLCriblLake = "bucket-owner-full-control"
)

func (e ObjectACLCriblLake) ToPointer() *ObjectACLCriblLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ObjectACLCriblLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "private", "public-read", "public-read-write", "authenticated-read", "aws-exec-read", "bucket-owner-read", "bucket-owner-full-control":
			return true
		}
	}
	return false
}

// StorageClassCriblLake - Storage class to select for uploaded objects
type StorageClassCriblLake string

const (
	// StorageClassCriblLakeStandard Standard
	StorageClassCriblLakeStandard StorageClassCriblLake = "STANDARD"
	// StorageClassCriblLakeReducedRedundancy Reduced Redundancy Storage
	StorageClassCriblLakeReducedRedundancy StorageClassCriblLake = "REDUCED_REDUNDANCY"
	// StorageClassCriblLakeStandardIa Standard, Infrequent Access
	StorageClassCriblLakeStandardIa StorageClassCriblLake = "STANDARD_IA"
	// StorageClassCriblLakeOnezoneIa One Zone, Infrequent Access
	StorageClassCriblLakeOnezoneIa StorageClassCriblLake = "ONEZONE_IA"
	// StorageClassCriblLakeIntelligentTiering Intelligent Tiering
	StorageClassCriblLakeIntelligentTiering StorageClassCriblLake = "INTELLIGENT_TIERING"
	// StorageClassCriblLakeGlacier Glacier Flexible Retrieval
	StorageClassCriblLakeGlacier StorageClassCriblLake = "GLACIER"
	// StorageClassCriblLakeGlacierIr Glacier Instant Retrieval
	StorageClassCriblLakeGlacierIr StorageClassCriblLake = "GLACIER_IR"
	// StorageClassCriblLakeDeepArchive Glacier Deep Archive
	StorageClassCriblLakeDeepArchive StorageClassCriblLake = "DEEP_ARCHIVE"
)

func (e StorageClassCriblLake) ToPointer() *StorageClassCriblLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *StorageClassCriblLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "STANDARD", "REDUCED_REDUNDANCY", "STANDARD_IA", "ONEZONE_IA", "INTELLIGENT_TIERING", "GLACIER", "GLACIER_IR", "DEEP_ARCHIVE":
			return true
		}
	}
	return false
}

type ServerSideEncryptionForUploadedObjectsCriblLake string

const (
	// ServerSideEncryptionForUploadedObjectsCriblLakeAes256 Amazon S3 Managed Key
	ServerSideEncryptionForUploadedObjectsCriblLakeAes256 ServerSideEncryptionForUploadedObjectsCriblLake = "AES256"
	// ServerSideEncryptionForUploadedObjectsCriblLakeAwsKms AWS KMS Managed Key
	ServerSideEncryptionForUploadedObjectsCriblLakeAwsKms ServerSideEncryptionForUploadedObjectsCriblLake = "aws:kms"
)

func (e ServerSideEncryptionForUploadedObjectsCriblLake) ToPointer() *ServerSideEncryptionForUploadedObjectsCriblLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ServerSideEncryptionForUploadedObjectsCriblLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "AES256", "aws:kms":
			return true
		}
	}
	return false
}

// BackpressureBehaviorCriblLake - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCriblLake string

const (
	// BackpressureBehaviorCriblLakeBlock Block
	BackpressureBehaviorCriblLakeBlock BackpressureBehaviorCriblLake = "block"
	// BackpressureBehaviorCriblLakeDrop Drop
	BackpressureBehaviorCriblLakeDrop BackpressureBehaviorCriblLake = "drop"
)

func (e BackpressureBehaviorCriblLake) ToPointer() *BackpressureBehaviorCriblLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorCriblLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionCriblLake - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionCriblLake string

const (
	// DiskSpaceProtectionCriblLakeBlock Block
	DiskSpaceProtectionCriblLakeBlock DiskSpaceProtectionCriblLake = "block"
	// DiskSpaceProtectionCriblLakeDrop Drop
	DiskSpaceProtectionCriblLakeDrop DiskSpaceProtectionCriblLake = "drop"
)

func (e DiskSpaceProtectionCriblLake) ToPointer() *DiskSpaceProtectionCriblLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionCriblLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type AwsAuthenticationMethod string

const (
	AwsAuthenticationMethodAuto    AwsAuthenticationMethod = "auto"
	AwsAuthenticationMethodAutoRPC AwsAuthenticationMethod = "auto_rpc"
	AwsAuthenticationMethodManual  AwsAuthenticationMethod = "manual"
)

func (e AwsAuthenticationMethod) ToPointer() *AwsAuthenticationMethod {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AwsAuthenticationMethod) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "auto_rpc", "manual":
			return true
		}
	}
	return false
}

type FormatCriblLake string

const (
	FormatCriblLakeJSON    FormatCriblLake = "json"
	FormatCriblLakeParquet FormatCriblLake = "parquet"
	FormatCriblLakeDdss    FormatCriblLake = "ddss"
)

func (e FormatCriblLake) ToPointer() *FormatCriblLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FormatCriblLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "parquet", "ddss":
			return true
		}
	}
	return false
}

type OutputCriblLake struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeCriblLake `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket *string `json:"bucket,omitempty"`
	// Region where the S3 bucket is located
	Region *string `json:"region,omitempty"`
	// Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing S3 requests
	SignatureVersion *SignatureVersionCriblLake `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Lake dataset to send the data to.
	DestPath *string `json:"destPath,omitempty"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLCriblLake `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass         *StorageClassCriblLake                           `json:"storageClass,omitempty"`
	ServerSideEncryption *ServerSideEncryptionForUploadedObjectsCriblLake `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"64" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCriblLake `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionCriblLake `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"300" json:"maxFileIdleTimeSec"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64                 `default:"100" json:"maxClosingFilesToBackpressure"`
	AwsAuthenticationMethod       *AwsAuthenticationMethod `default:"auto" json:"awsAuthenticationMethod"`
	Format                        *FormatCriblLake         `json:"format,omitempty"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	Description            *string  `json:"description,omitempty"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputCriblLake) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCriblLake) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCriblLake) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputCriblLake) GetType() TypeCriblLake {
	if o == nil {
		return TypeCriblLake("")
	}
	return o.Type
}

func (o *OutputCriblLake) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCriblLake) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCriblLake) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCriblLake) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCriblLake) GetBucket() *string {
	if o == nil {
		return nil
	}
	return o.Bucket
}

func (o *OutputCriblLake) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputCriblLake) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputCriblLake) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputCriblLake) GetSignatureVersion() *SignatureVersionCriblLake {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputCriblLake) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputCriblLake) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCriblLake) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputCriblLake) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputCriblLake) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputCriblLake) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputCriblLake) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputCriblLake) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputCriblLake) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputCriblLake) GetObjectACL() *ObjectACLCriblLake {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputCriblLake) GetStorageClass() *StorageClassCriblLake {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputCriblLake) GetServerSideEncryption() *ServerSideEncryptionForUploadedObjectsCriblLake {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputCriblLake) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputCriblLake) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputCriblLake) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputCriblLake) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputCriblLake) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputCriblLake) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputCriblLake) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputCriblLake) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputCriblLake) GetOnBackpressure() *BackpressureBehaviorCriblLake {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCriblLake) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputCriblLake) GetOnDiskFullBackpressure() *DiskSpaceProtectionCriblLake {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputCriblLake) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputCriblLake) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputCriblLake) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputCriblLake) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputCriblLake) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputCriblLake) GetAwsAuthenticationMethod() *AwsAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputCriblLake) GetFormat() *FormatCriblLake {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputCriblLake) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputCriblLake) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCriblLake) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputCriblLake) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputCriblLake) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputCriblLake) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type CreateOutputTypeSecurityLake string

const (
	CreateOutputTypeSecurityLakeSecurityLake CreateOutputTypeSecurityLake = "security_lake"
)

func (e CreateOutputTypeSecurityLake) ToPointer() *CreateOutputTypeSecurityLake {
	return &e
}
func (e *CreateOutputTypeSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "security_lake":
		*e = CreateOutputTypeSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeSecurityLake: %v", v)
	}
}

// CreateOutputAuthenticationMethodSecurityLake - AWS authentication method. Choose Auto to use IAM roles.
type CreateOutputAuthenticationMethodSecurityLake string

const (
	// CreateOutputAuthenticationMethodSecurityLakeAuto Auto
	CreateOutputAuthenticationMethodSecurityLakeAuto CreateOutputAuthenticationMethodSecurityLake = "auto"
	// CreateOutputAuthenticationMethodSecurityLakeManual Manual
	CreateOutputAuthenticationMethodSecurityLakeManual CreateOutputAuthenticationMethodSecurityLake = "manual"
	// CreateOutputAuthenticationMethodSecurityLakeSecret Secret Key pair
	CreateOutputAuthenticationMethodSecurityLakeSecret CreateOutputAuthenticationMethodSecurityLake = "secret"
)

func (e CreateOutputAuthenticationMethodSecurityLake) ToPointer() *CreateOutputAuthenticationMethodSecurityLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationMethodSecurityLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// CreateOutputSignatureVersionSecurityLake - Signature version to use for signing Amazon Security Lake requests
type CreateOutputSignatureVersionSecurityLake string

const (
	CreateOutputSignatureVersionSecurityLakeV2 CreateOutputSignatureVersionSecurityLake = "v2"
	CreateOutputSignatureVersionSecurityLakeV4 CreateOutputSignatureVersionSecurityLake = "v4"
)

func (e CreateOutputSignatureVersionSecurityLake) ToPointer() *CreateOutputSignatureVersionSecurityLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputSignatureVersionSecurityLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

// ObjectACLSecurityLake - Object ACL to assign to uploaded objects
type ObjectACLSecurityLake string

const (
	// ObjectACLSecurityLakePrivate Private
	ObjectACLSecurityLakePrivate ObjectACLSecurityLake = "private"
	// ObjectACLSecurityLakePublicRead Public Read Only
	ObjectACLSecurityLakePublicRead ObjectACLSecurityLake = "public-read"
	// ObjectACLSecurityLakePublicReadWrite Public Read/Write
	ObjectACLSecurityLakePublicReadWrite ObjectACLSecurityLake = "public-read-write"
	// ObjectACLSecurityLakeAuthenticatedRead Authenticated Read Only
	ObjectACLSecurityLakeAuthenticatedRead ObjectACLSecurityLake = "authenticated-read"
	// ObjectACLSecurityLakeAwsExecRead AWS EC2 AMI Read Only
	ObjectACLSecurityLakeAwsExecRead ObjectACLSecurityLake = "aws-exec-read"
	// ObjectACLSecurityLakeBucketOwnerRead Bucket Owner Read Only
	ObjectACLSecurityLakeBucketOwnerRead ObjectACLSecurityLake = "bucket-owner-read"
	// ObjectACLSecurityLakeBucketOwnerFullControl Bucket Owner Full Control
	ObjectACLSecurityLakeBucketOwnerFullControl ObjectACLSecurityLake = "bucket-owner-full-control"
)

func (e ObjectACLSecurityLake) ToPointer() *ObjectACLSecurityLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ObjectACLSecurityLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "private", "public-read", "public-read-write", "authenticated-read", "aws-exec-read", "bucket-owner-read", "bucket-owner-full-control":
			return true
		}
	}
	return false
}

// StorageClassSecurityLake - Storage class to select for uploaded objects
type StorageClassSecurityLake string

const (
	// StorageClassSecurityLakeStandard Standard
	StorageClassSecurityLakeStandard StorageClassSecurityLake = "STANDARD"
	// StorageClassSecurityLakeReducedRedundancy Reduced Redundancy Storage
	StorageClassSecurityLakeReducedRedundancy StorageClassSecurityLake = "REDUCED_REDUNDANCY"
	// StorageClassSecurityLakeStandardIa Standard, Infrequent Access
	StorageClassSecurityLakeStandardIa StorageClassSecurityLake = "STANDARD_IA"
	// StorageClassSecurityLakeOnezoneIa One Zone, Infrequent Access
	StorageClassSecurityLakeOnezoneIa StorageClassSecurityLake = "ONEZONE_IA"
	// StorageClassSecurityLakeIntelligentTiering Intelligent Tiering
	StorageClassSecurityLakeIntelligentTiering StorageClassSecurityLake = "INTELLIGENT_TIERING"
	// StorageClassSecurityLakeGlacier Glacier Flexible Retrieval
	StorageClassSecurityLakeGlacier StorageClassSecurityLake = "GLACIER"
	// StorageClassSecurityLakeGlacierIr Glacier Instant Retrieval
	StorageClassSecurityLakeGlacierIr StorageClassSecurityLake = "GLACIER_IR"
	// StorageClassSecurityLakeDeepArchive Glacier Deep Archive
	StorageClassSecurityLakeDeepArchive StorageClassSecurityLake = "DEEP_ARCHIVE"
)

func (e StorageClassSecurityLake) ToPointer() *StorageClassSecurityLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *StorageClassSecurityLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "STANDARD", "REDUCED_REDUNDANCY", "STANDARD_IA", "ONEZONE_IA", "INTELLIGENT_TIERING", "GLACIER", "GLACIER_IR", "DEEP_ARCHIVE":
			return true
		}
	}
	return false
}

type ServerSideEncryptionForUploadedObjectsSecurityLake string

const (
	// ServerSideEncryptionForUploadedObjectsSecurityLakeAes256 Amazon S3 Managed Key
	ServerSideEncryptionForUploadedObjectsSecurityLakeAes256 ServerSideEncryptionForUploadedObjectsSecurityLake = "AES256"
	// ServerSideEncryptionForUploadedObjectsSecurityLakeAwsKms AWS KMS Managed Key
	ServerSideEncryptionForUploadedObjectsSecurityLakeAwsKms ServerSideEncryptionForUploadedObjectsSecurityLake = "aws:kms"
)

func (e ServerSideEncryptionForUploadedObjectsSecurityLake) ToPointer() *ServerSideEncryptionForUploadedObjectsSecurityLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ServerSideEncryptionForUploadedObjectsSecurityLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "AES256", "aws:kms":
			return true
		}
	}
	return false
}

// BackpressureBehaviorSecurityLake - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSecurityLake string

const (
	// BackpressureBehaviorSecurityLakeBlock Block
	BackpressureBehaviorSecurityLakeBlock BackpressureBehaviorSecurityLake = "block"
	// BackpressureBehaviorSecurityLakeDrop Drop
	BackpressureBehaviorSecurityLakeDrop BackpressureBehaviorSecurityLake = "drop"
)

func (e BackpressureBehaviorSecurityLake) ToPointer() *BackpressureBehaviorSecurityLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorSecurityLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionSecurityLake - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionSecurityLake string

const (
	// DiskSpaceProtectionSecurityLakeBlock Block
	DiskSpaceProtectionSecurityLakeBlock DiskSpaceProtectionSecurityLake = "block"
	// DiskSpaceProtectionSecurityLakeDrop Drop
	DiskSpaceProtectionSecurityLakeDrop DiskSpaceProtectionSecurityLake = "drop"
)

func (e DiskSpaceProtectionSecurityLake) ToPointer() *DiskSpaceProtectionSecurityLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionSecurityLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// ParquetVersionSecurityLake - Determines which data types are supported and how they are represented
type ParquetVersionSecurityLake string

const (
	// ParquetVersionSecurityLakeParquet10 1.0
	ParquetVersionSecurityLakeParquet10 ParquetVersionSecurityLake = "PARQUET_1_0"
	// ParquetVersionSecurityLakeParquet24 2.4
	ParquetVersionSecurityLakeParquet24 ParquetVersionSecurityLake = "PARQUET_2_4"
	// ParquetVersionSecurityLakeParquet26 2.6
	ParquetVersionSecurityLakeParquet26 ParquetVersionSecurityLake = "PARQUET_2_6"
)

func (e ParquetVersionSecurityLake) ToPointer() *ParquetVersionSecurityLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ParquetVersionSecurityLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6":
			return true
		}
	}
	return false
}

// DataPageVersionSecurityLake - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionSecurityLake string

const (
	// DataPageVersionSecurityLakeDataPageV1 V1
	DataPageVersionSecurityLakeDataPageV1 DataPageVersionSecurityLake = "DATA_PAGE_V1"
	// DataPageVersionSecurityLakeDataPageV2 V2
	DataPageVersionSecurityLakeDataPageV2 DataPageVersionSecurityLake = "DATA_PAGE_V2"
)

func (e DataPageVersionSecurityLake) ToPointer() *DataPageVersionSecurityLake {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataPageVersionSecurityLake) IsExact() bool {
	if e != nil {
		switch *e {
		case "DATA_PAGE_V1", "DATA_PAGE_V2":
			return true
		}
	}
	return false
}

type KeyValueMetadatumSecurityLake struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumSecurityLake) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumSecurityLake) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumSecurityLake) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumSecurityLake) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputSecurityLake struct {
	// Unique ID for this output
	ID   string                       `json:"id"`
	Type CreateOutputTypeSecurityLake `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// Region where the Amazon Security Lake is located.
	Region       string  `json:"region"`
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *CreateOutputAuthenticationMethodSecurityLake `default:"auto" json:"awsAuthenticationMethod"`
	// Amazon Security Lake service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Amazon Security Lake-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing Amazon Security Lake requests
	SignatureVersion *CreateOutputSignatureVersionSecurityLake `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn string `json:"assumeRoleArn"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLSecurityLake `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass         *StorageClassSecurityLake                           `json:"storageClass,omitempty"`
	ServerSideEncryption *ServerSideEncryptionForUploadedObjectsSecurityLake `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSecurityLake `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionSecurityLake `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	// ID of the AWS account whose data the Destination will write to Security Lake. This should have been configured when creating the Amazon Security Lake custom source.
	AccountID string `json:"accountId"`
	// Name of the custom source configured in Amazon Security Lake
	CustomSource string `json:"customSource"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionSecurityLake `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionSecurityLake `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumSecurityLake `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool   `default:"false" json:"enablePageChecksum"`
	Description        *string `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputSecurityLake) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSecurityLake) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "bucket", "region", "assumeRoleArn", "accountId", "customSource"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSecurityLake) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSecurityLake) GetType() CreateOutputTypeSecurityLake {
	if o == nil {
		return CreateOutputTypeSecurityLake("")
	}
	return o.Type
}

func (o *OutputSecurityLake) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSecurityLake) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSecurityLake) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSecurityLake) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSecurityLake) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputSecurityLake) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputSecurityLake) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSecurityLake) GetAwsAuthenticationMethod() *CreateOutputAuthenticationMethodSecurityLake {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSecurityLake) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSecurityLake) GetSignatureVersion() *CreateOutputSignatureVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSecurityLake) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSecurityLake) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSecurityLake) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSecurityLake) GetAssumeRoleArn() string {
	if o == nil {
		return ""
	}
	return o.AssumeRoleArn
}

func (o *OutputSecurityLake) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSecurityLake) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSecurityLake) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputSecurityLake) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputSecurityLake) GetObjectACL() *ObjectACLSecurityLake {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputSecurityLake) GetStorageClass() *StorageClassSecurityLake {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputSecurityLake) GetServerSideEncryption() *ServerSideEncryptionForUploadedObjectsSecurityLake {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputSecurityLake) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputSecurityLake) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputSecurityLake) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputSecurityLake) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputSecurityLake) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputSecurityLake) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputSecurityLake) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputSecurityLake) GetOnBackpressure() *BackpressureBehaviorSecurityLake {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSecurityLake) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputSecurityLake) GetOnDiskFullBackpressure() *DiskSpaceProtectionSecurityLake {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputSecurityLake) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputSecurityLake) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputSecurityLake) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputSecurityLake) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputSecurityLake) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputSecurityLake) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputSecurityLake) GetAccountID() string {
	if o == nil {
		return ""
	}
	return o.AccountID
}

func (o *OutputSecurityLake) GetCustomSource() string {
	if o == nil {
		return ""
	}
	return o.CustomSource
}

func (o *OutputSecurityLake) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputSecurityLake) GetParquetVersion() *ParquetVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputSecurityLake) GetParquetDataPageVersion() *DataPageVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputSecurityLake) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputSecurityLake) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputSecurityLake) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputSecurityLake) GetKeyValueMetadata() []KeyValueMetadatumSecurityLake {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputSecurityLake) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputSecurityLake) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputSecurityLake) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputSecurityLake) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSecurityLake) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSecurityLake) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSecurityLake) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputSecurityLake) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputSecurityLake) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputSecurityLake) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputSecurityLake) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeDlS3 string

const (
	TypeDlS3DlS3 TypeDlS3 = "dl_s3"
)

func (e TypeDlS3) ToPointer() *TypeDlS3 {
	return &e
}
func (e *TypeDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dl_s3":
		*e = TypeDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDlS3: %v", v)
	}
}

// AuthenticationMethodDlS3 - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodDlS3 string

const (
	// AuthenticationMethodDlS3Auto Auto
	AuthenticationMethodDlS3Auto AuthenticationMethodDlS3 = "auto"
	// AuthenticationMethodDlS3Manual Manual
	AuthenticationMethodDlS3Manual AuthenticationMethodDlS3 = "manual"
	// AuthenticationMethodDlS3Secret Secret Key pair
	AuthenticationMethodDlS3Secret AuthenticationMethodDlS3 = "secret"
)

func (e AuthenticationMethodDlS3) ToPointer() *AuthenticationMethodDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// SignatureVersionDlS3 - Signature version to use for signing S3 requests
type SignatureVersionDlS3 string

const (
	SignatureVersionDlS3V2 SignatureVersionDlS3 = "v2"
	SignatureVersionDlS3V4 SignatureVersionDlS3 = "v4"
)

func (e SignatureVersionDlS3) ToPointer() *SignatureVersionDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SignatureVersionDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

// ObjectACLDlS3 - Object ACL to assign to uploaded objects
type ObjectACLDlS3 string

const (
	// ObjectACLDlS3Private Private
	ObjectACLDlS3Private ObjectACLDlS3 = "private"
	// ObjectACLDlS3PublicRead Public Read Only
	ObjectACLDlS3PublicRead ObjectACLDlS3 = "public-read"
	// ObjectACLDlS3PublicReadWrite Public Read/Write
	ObjectACLDlS3PublicReadWrite ObjectACLDlS3 = "public-read-write"
	// ObjectACLDlS3AuthenticatedRead Authenticated Read Only
	ObjectACLDlS3AuthenticatedRead ObjectACLDlS3 = "authenticated-read"
	// ObjectACLDlS3AwsExecRead AWS EC2 AMI Read Only
	ObjectACLDlS3AwsExecRead ObjectACLDlS3 = "aws-exec-read"
	// ObjectACLDlS3BucketOwnerRead Bucket Owner Read Only
	ObjectACLDlS3BucketOwnerRead ObjectACLDlS3 = "bucket-owner-read"
	// ObjectACLDlS3BucketOwnerFullControl Bucket Owner Full Control
	ObjectACLDlS3BucketOwnerFullControl ObjectACLDlS3 = "bucket-owner-full-control"
)

func (e ObjectACLDlS3) ToPointer() *ObjectACLDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ObjectACLDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "private", "public-read", "public-read-write", "authenticated-read", "aws-exec-read", "bucket-owner-read", "bucket-owner-full-control":
			return true
		}
	}
	return false
}

// StorageClassDlS3 - Storage class to select for uploaded objects
type StorageClassDlS3 string

const (
	// StorageClassDlS3Standard Standard
	StorageClassDlS3Standard StorageClassDlS3 = "STANDARD"
	// StorageClassDlS3ReducedRedundancy Reduced Redundancy Storage
	StorageClassDlS3ReducedRedundancy StorageClassDlS3 = "REDUCED_REDUNDANCY"
	// StorageClassDlS3StandardIa Standard, Infrequent Access
	StorageClassDlS3StandardIa StorageClassDlS3 = "STANDARD_IA"
	// StorageClassDlS3OnezoneIa One Zone, Infrequent Access
	StorageClassDlS3OnezoneIa StorageClassDlS3 = "ONEZONE_IA"
	// StorageClassDlS3IntelligentTiering Intelligent Tiering
	StorageClassDlS3IntelligentTiering StorageClassDlS3 = "INTELLIGENT_TIERING"
	// StorageClassDlS3Glacier Glacier Flexible Retrieval
	StorageClassDlS3Glacier StorageClassDlS3 = "GLACIER"
	// StorageClassDlS3GlacierIr Glacier Instant Retrieval
	StorageClassDlS3GlacierIr StorageClassDlS3 = "GLACIER_IR"
	// StorageClassDlS3DeepArchive Glacier Deep Archive
	StorageClassDlS3DeepArchive StorageClassDlS3 = "DEEP_ARCHIVE"
)

func (e StorageClassDlS3) ToPointer() *StorageClassDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *StorageClassDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "STANDARD", "REDUCED_REDUNDANCY", "STANDARD_IA", "ONEZONE_IA", "INTELLIGENT_TIERING", "GLACIER", "GLACIER_IR", "DEEP_ARCHIVE":
			return true
		}
	}
	return false
}

type ServerSideEncryptionForUploadedObjectsDlS3 string

const (
	// ServerSideEncryptionForUploadedObjectsDlS3Aes256 Amazon S3 Managed Key
	ServerSideEncryptionForUploadedObjectsDlS3Aes256 ServerSideEncryptionForUploadedObjectsDlS3 = "AES256"
	// ServerSideEncryptionForUploadedObjectsDlS3AwsKms AWS KMS Managed Key
	ServerSideEncryptionForUploadedObjectsDlS3AwsKms ServerSideEncryptionForUploadedObjectsDlS3 = "aws:kms"
)

func (e ServerSideEncryptionForUploadedObjectsDlS3) ToPointer() *ServerSideEncryptionForUploadedObjectsDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ServerSideEncryptionForUploadedObjectsDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "AES256", "aws:kms":
			return true
		}
	}
	return false
}

// DataFormatDlS3 - Format of the output data
type DataFormatDlS3 string

const (
	// DataFormatDlS3JSON JSON
	DataFormatDlS3JSON DataFormatDlS3 = "json"
	// DataFormatDlS3Raw Raw
	DataFormatDlS3Raw DataFormatDlS3 = "raw"
	// DataFormatDlS3Parquet Parquet
	DataFormatDlS3Parquet DataFormatDlS3 = "parquet"
)

func (e DataFormatDlS3) ToPointer() *DataFormatDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataFormatDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "parquet":
			return true
		}
	}
	return false
}

// BackpressureBehaviorDlS3 - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDlS3 string

const (
	// BackpressureBehaviorDlS3Block Block
	BackpressureBehaviorDlS3Block BackpressureBehaviorDlS3 = "block"
	// BackpressureBehaviorDlS3Drop Drop
	BackpressureBehaviorDlS3Drop BackpressureBehaviorDlS3 = "drop"
)

func (e BackpressureBehaviorDlS3) ToPointer() *BackpressureBehaviorDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionDlS3 - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionDlS3 string

const (
	// DiskSpaceProtectionDlS3Block Block
	DiskSpaceProtectionDlS3Block DiskSpaceProtectionDlS3 = "block"
	// DiskSpaceProtectionDlS3Drop Drop
	DiskSpaceProtectionDlS3Drop DiskSpaceProtectionDlS3 = "drop"
)

func (e DiskSpaceProtectionDlS3) ToPointer() *DiskSpaceProtectionDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// CompressionDlS3 - Data compression format to apply to HTTP content before it is delivered
type CompressionDlS3 string

const (
	CompressionDlS3None CompressionDlS3 = "none"
	CompressionDlS3Gzip CompressionDlS3 = "gzip"
)

func (e CompressionDlS3) ToPointer() *CompressionDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// CompressionLevelDlS3 - Compression level to apply before moving files to final destination
type CompressionLevelDlS3 string

const (
	// CompressionLevelDlS3BestSpeed Best Speed
	CompressionLevelDlS3BestSpeed CompressionLevelDlS3 = "best_speed"
	// CompressionLevelDlS3Normal Normal
	CompressionLevelDlS3Normal CompressionLevelDlS3 = "normal"
	// CompressionLevelDlS3BestCompression Best Compression
	CompressionLevelDlS3BestCompression CompressionLevelDlS3 = "best_compression"
)

func (e CompressionLevelDlS3) ToPointer() *CompressionLevelDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionLevelDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "best_speed", "normal", "best_compression":
			return true
		}
	}
	return false
}

// ParquetVersionDlS3 - Determines which data types are supported and how they are represented
type ParquetVersionDlS3 string

const (
	// ParquetVersionDlS3Parquet10 1.0
	ParquetVersionDlS3Parquet10 ParquetVersionDlS3 = "PARQUET_1_0"
	// ParquetVersionDlS3Parquet24 2.4
	ParquetVersionDlS3Parquet24 ParquetVersionDlS3 = "PARQUET_2_4"
	// ParquetVersionDlS3Parquet26 2.6
	ParquetVersionDlS3Parquet26 ParquetVersionDlS3 = "PARQUET_2_6"
)

func (e ParquetVersionDlS3) ToPointer() *ParquetVersionDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ParquetVersionDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6":
			return true
		}
	}
	return false
}

// DataPageVersionDlS3 - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionDlS3 string

const (
	// DataPageVersionDlS3DataPageV1 V1
	DataPageVersionDlS3DataPageV1 DataPageVersionDlS3 = "DATA_PAGE_V1"
	// DataPageVersionDlS3DataPageV2 V2
	DataPageVersionDlS3DataPageV2 DataPageVersionDlS3 = "DATA_PAGE_V2"
)

func (e DataPageVersionDlS3) ToPointer() *DataPageVersionDlS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataPageVersionDlS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "DATA_PAGE_V1", "DATA_PAGE_V2":
			return true
		}
	}
	return false
}

type KeyValueMetadatumDlS3 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumDlS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumDlS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumDlS3) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumDlS3) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputDlS3 struct {
	// Unique ID for this output
	ID   string   `json:"id"`
	Type TypeDlS3 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// Region where the S3 bucket is located
	Region *string `json:"region,omitempty"`
	// Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodDlS3 `default:"auto" json:"awsAuthenticationMethod"`
	// S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing S3 requests
	SignatureVersion *SignatureVersionDlS3 `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`
	DestPath *string `default:"" json:"destPath"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLDlS3 `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass         *StorageClassDlS3                           `json:"storageClass,omitempty"`
	ServerSideEncryption *ServerSideEncryptionForUploadedObjectsDlS3 `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// Format of the output data
	Format *DataFormatDlS3 `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDlS3 `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionDlS3 `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	// List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>.
	PartitioningFields []string `json:"partitioningFields,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionDlS3 `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelDlS3 `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionDlS3 `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionDlS3 `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumDlS3 `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputDlS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDlS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "bucket"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDlS3) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDlS3) GetType() TypeDlS3 {
	if o == nil {
		return TypeDlS3("")
	}
	return o.Type
}

func (o *OutputDlS3) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDlS3) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDlS3) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDlS3) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDlS3) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputDlS3) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputDlS3) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputDlS3) GetAwsAuthenticationMethod() *AuthenticationMethodDlS3 {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputDlS3) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDlS3) GetSignatureVersion() *SignatureVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputDlS3) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputDlS3) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDlS3) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputDlS3) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputDlS3) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputDlS3) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputDlS3) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDlS3) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDlS3) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDlS3) GetObjectACL() *ObjectACLDlS3 {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputDlS3) GetStorageClass() *StorageClassDlS3 {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputDlS3) GetServerSideEncryption() *ServerSideEncryptionForUploadedObjectsDlS3 {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputDlS3) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputDlS3) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDlS3) GetFormat() *DataFormatDlS3 {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDlS3) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDlS3) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDlS3) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDlS3) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDlS3) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDlS3) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDlS3) GetOnBackpressure() *BackpressureBehaviorDlS3 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDlS3) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDlS3) GetOnDiskFullBackpressure() *DiskSpaceProtectionDlS3 {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDlS3) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputDlS3) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDlS3) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDlS3) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputDlS3) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputDlS3) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputDlS3) GetPartitioningFields() []string {
	if o == nil {
		return nil
	}
	return o.PartitioningFields
}

func (o *OutputDlS3) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDlS3) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputDlS3) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputDlS3) GetCompress() *CompressionDlS3 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDlS3) GetCompressionLevel() *CompressionLevelDlS3 {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDlS3) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDlS3) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputDlS3) GetParquetVersion() *ParquetVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDlS3) GetParquetDataPageVersion() *DataPageVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDlS3) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDlS3) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDlS3) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDlS3) GetKeyValueMetadata() []KeyValueMetadatumDlS3 {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputDlS3) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDlS3) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDlS3) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDlS3) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDlS3) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputDlS3) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDlS3) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeCrowdstrikeNextGenSiem string

const (
	TypeCrowdstrikeNextGenSiemCrowdstrikeNextGenSiem TypeCrowdstrikeNextGenSiem = "crowdstrike_next_gen_siem"
)

func (e TypeCrowdstrikeNextGenSiem) ToPointer() *TypeCrowdstrikeNextGenSiem {
	return &e
}
func (e *TypeCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "crowdstrike_next_gen_siem":
		*e = TypeCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCrowdstrikeNextGenSiem: %v", v)
	}
}

type ExtraHTTPHeaderCrowdstrikeNextGenSiem struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderCrowdstrikeNextGenSiem) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderCrowdstrikeNextGenSiem) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeCrowdstrikeNextGenSiem - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeCrowdstrikeNextGenSiem string

const (
	// FailedRequestLoggingModeCrowdstrikeNextGenSiemPayload Payload
	FailedRequestLoggingModeCrowdstrikeNextGenSiemPayload FailedRequestLoggingModeCrowdstrikeNextGenSiem = "payload"
	// FailedRequestLoggingModeCrowdstrikeNextGenSiemPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeCrowdstrikeNextGenSiemPayloadAndHeaders FailedRequestLoggingModeCrowdstrikeNextGenSiem = "payloadAndHeaders"
	// FailedRequestLoggingModeCrowdstrikeNextGenSiemNone None
	FailedRequestLoggingModeCrowdstrikeNextGenSiemNone FailedRequestLoggingModeCrowdstrikeNextGenSiem = "none"
)

func (e FailedRequestLoggingModeCrowdstrikeNextGenSiem) ToPointer() *FailedRequestLoggingModeCrowdstrikeNextGenSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeCrowdstrikeNextGenSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

// RequestFormatCrowdstrikeNextGenSiem - When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
type RequestFormatCrowdstrikeNextGenSiem string

const (
	// RequestFormatCrowdstrikeNextGenSiemJSON JSON
	RequestFormatCrowdstrikeNextGenSiemJSON RequestFormatCrowdstrikeNextGenSiem = "JSON"
	// RequestFormatCrowdstrikeNextGenSiemRaw Raw
	RequestFormatCrowdstrikeNextGenSiemRaw RequestFormatCrowdstrikeNextGenSiem = "raw"
)

func (e RequestFormatCrowdstrikeNextGenSiem) ToPointer() *RequestFormatCrowdstrikeNextGenSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *RequestFormatCrowdstrikeNextGenSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "JSON", "raw":
			return true
		}
	}
	return false
}

// AuthenticationMethodCrowdstrikeNextGenSiem - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodCrowdstrikeNextGenSiem string

const (
	AuthenticationMethodCrowdstrikeNextGenSiemManual AuthenticationMethodCrowdstrikeNextGenSiem = "manual"
	AuthenticationMethodCrowdstrikeNextGenSiemSecret AuthenticationMethodCrowdstrikeNextGenSiem = "secret"
)

func (e AuthenticationMethodCrowdstrikeNextGenSiem) ToPointer() *AuthenticationMethodCrowdstrikeNextGenSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodCrowdstrikeNextGenSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

type ResponseRetrySettingCrowdstrikeNextGenSiem struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsCrowdstrikeNextGenSiem struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorCrowdstrikeNextGenSiem - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCrowdstrikeNextGenSiem string

const (
	// BackpressureBehaviorCrowdstrikeNextGenSiemBlock Block
	BackpressureBehaviorCrowdstrikeNextGenSiemBlock BackpressureBehaviorCrowdstrikeNextGenSiem = "block"
	// BackpressureBehaviorCrowdstrikeNextGenSiemDrop Drop
	BackpressureBehaviorCrowdstrikeNextGenSiemDrop BackpressureBehaviorCrowdstrikeNextGenSiem = "drop"
	// BackpressureBehaviorCrowdstrikeNextGenSiemQueue Persistent Queue
	BackpressureBehaviorCrowdstrikeNextGenSiemQueue BackpressureBehaviorCrowdstrikeNextGenSiem = "queue"
)

func (e BackpressureBehaviorCrowdstrikeNextGenSiem) ToPointer() *BackpressureBehaviorCrowdstrikeNextGenSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorCrowdstrikeNextGenSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeCrowdstrikeNextGenSiem - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeCrowdstrikeNextGenSiem string

const (
	// ModeCrowdstrikeNextGenSiemError Error
	ModeCrowdstrikeNextGenSiemError ModeCrowdstrikeNextGenSiem = "error"
	// ModeCrowdstrikeNextGenSiemAlways Backpressure
	ModeCrowdstrikeNextGenSiemAlways ModeCrowdstrikeNextGenSiem = "always"
	// ModeCrowdstrikeNextGenSiemBackpressure Always On
	ModeCrowdstrikeNextGenSiemBackpressure ModeCrowdstrikeNextGenSiem = "backpressure"
)

func (e ModeCrowdstrikeNextGenSiem) ToPointer() *ModeCrowdstrikeNextGenSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeCrowdstrikeNextGenSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionCrowdstrikeNextGenSiem - Codec to use to compress the persisted data
type CompressionCrowdstrikeNextGenSiem string

const (
	// CompressionCrowdstrikeNextGenSiemNone None
	CompressionCrowdstrikeNextGenSiemNone CompressionCrowdstrikeNextGenSiem = "none"
	// CompressionCrowdstrikeNextGenSiemGzip Gzip
	CompressionCrowdstrikeNextGenSiemGzip CompressionCrowdstrikeNextGenSiem = "gzip"
)

func (e CompressionCrowdstrikeNextGenSiem) ToPointer() *CompressionCrowdstrikeNextGenSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionCrowdstrikeNextGenSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorCrowdstrikeNextGenSiem - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCrowdstrikeNextGenSiem string

const (
	// QueueFullBehaviorCrowdstrikeNextGenSiemBlock Block
	QueueFullBehaviorCrowdstrikeNextGenSiemBlock QueueFullBehaviorCrowdstrikeNextGenSiem = "block"
	// QueueFullBehaviorCrowdstrikeNextGenSiemDrop Drop new data
	QueueFullBehaviorCrowdstrikeNextGenSiemDrop QueueFullBehaviorCrowdstrikeNextGenSiem = "drop"
)

func (e QueueFullBehaviorCrowdstrikeNextGenSiem) ToPointer() *QueueFullBehaviorCrowdstrikeNextGenSiem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorCrowdstrikeNextGenSiem) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsCrowdstrikeNextGenSiem struct {
}

func (p PqControlsCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputCrowdstrikeNextGenSiem struct {
	// Unique ID for this output
	ID   string                     `json:"id"`
	Type TypeCrowdstrikeNextGenSiem `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL provided from a CrowdStrike data connector.
	// Example: https://ingest.<region>.crowdstrike.com/api/ingest/hec/<connection-id>/v1/services/collector
	URL string `json:"url"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderCrowdstrikeNextGenSiem `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"true" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeCrowdstrikeNextGenSiem `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
	Format *RequestFormatCrowdstrikeNextGenSiem `default:"JSON" json:"format"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodCrowdstrikeNextGenSiem `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingCrowdstrikeNextGenSiem `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsCrowdstrikeNextGenSiem  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCrowdstrikeNextGenSiem `default:"block" json:"onBackpressure"`
	Description    *string                                     `json:"description,omitempty"`
	Token          *string                                     `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeCrowdstrikeNextGenSiem `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionCrowdstrikeNextGenSiem `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorCrowdstrikeNextGenSiem `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsCrowdstrikeNextGenSiem        `json:"pqControls,omitempty"`
}

func (o OutputCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "url"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCrowdstrikeNextGenSiem) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputCrowdstrikeNextGenSiem) GetType() TypeCrowdstrikeNextGenSiem {
	if o == nil {
		return TypeCrowdstrikeNextGenSiem("")
	}
	return o.Type
}

func (o *OutputCrowdstrikeNextGenSiem) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCrowdstrikeNextGenSiem) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCrowdstrikeNextGenSiem) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCrowdstrikeNextGenSiem) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCrowdstrikeNextGenSiem) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputCrowdstrikeNextGenSiem) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputCrowdstrikeNextGenSiem) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputCrowdstrikeNextGenSiem) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputCrowdstrikeNextGenSiem) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputCrowdstrikeNextGenSiem) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCrowdstrikeNextGenSiem) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetExtraHTTPHeaders() []ExtraHTTPHeaderCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputCrowdstrikeNextGenSiem) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputCrowdstrikeNextGenSiem) GetFailedRequestLoggingMode() *FailedRequestLoggingModeCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputCrowdstrikeNextGenSiem) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputCrowdstrikeNextGenSiem) GetFormat() *RequestFormatCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputCrowdstrikeNextGenSiem) GetAuthType() *AuthenticationMethodCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputCrowdstrikeNextGenSiem) GetResponseRetrySettings() []ResponseRetrySettingCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputCrowdstrikeNextGenSiem) GetTimeoutRetrySettings() *TimeoutRetrySettingsCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputCrowdstrikeNextGenSiem) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputCrowdstrikeNextGenSiem) GetOnBackpressure() *BackpressureBehaviorCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCrowdstrikeNextGenSiem) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCrowdstrikeNextGenSiem) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputCrowdstrikeNextGenSiem) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMode() *ModeCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqCompress() *CompressionCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqOnBackpressure() *QueueFullBehaviorCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqControls() *PqControlsCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeHumioHec string

const (
	TypeHumioHecHumioHec TypeHumioHec = "humio_hec"
)

func (e TypeHumioHec) ToPointer() *TypeHumioHec {
	return &e
}
func (e *TypeHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "humio_hec":
		*e = TypeHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeHumioHec: %v", v)
	}
}

type ExtraHTTPHeaderHumioHec struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderHumioHec) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderHumioHec) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeHumioHec - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeHumioHec string

const (
	// FailedRequestLoggingModeHumioHecPayload Payload
	FailedRequestLoggingModeHumioHecPayload FailedRequestLoggingModeHumioHec = "payload"
	// FailedRequestLoggingModeHumioHecPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeHumioHecPayloadAndHeaders FailedRequestLoggingModeHumioHec = "payloadAndHeaders"
	// FailedRequestLoggingModeHumioHecNone None
	FailedRequestLoggingModeHumioHecNone FailedRequestLoggingModeHumioHec = "none"
)

func (e FailedRequestLoggingModeHumioHec) ToPointer() *FailedRequestLoggingModeHumioHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeHumioHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

// RequestFormatHumioHec - When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
type RequestFormatHumioHec string

const (
	// RequestFormatHumioHecJSON JSON
	RequestFormatHumioHecJSON RequestFormatHumioHec = "JSON"
	// RequestFormatHumioHecRaw Raw
	RequestFormatHumioHecRaw RequestFormatHumioHec = "raw"
)

func (e RequestFormatHumioHec) ToPointer() *RequestFormatHumioHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *RequestFormatHumioHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "JSON", "raw":
			return true
		}
	}
	return false
}

// AuthenticationMethodHumioHec - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodHumioHec string

const (
	AuthenticationMethodHumioHecManual AuthenticationMethodHumioHec = "manual"
	AuthenticationMethodHumioHecSecret AuthenticationMethodHumioHec = "secret"
)

func (e AuthenticationMethodHumioHec) ToPointer() *AuthenticationMethodHumioHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodHumioHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

type ResponseRetrySettingHumioHec struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingHumioHec) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingHumioHec) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingHumioHec) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingHumioHec) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsHumioHec struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsHumioHec) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsHumioHec) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsHumioHec) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsHumioHec) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorHumioHec - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorHumioHec string

const (
	// BackpressureBehaviorHumioHecBlock Block
	BackpressureBehaviorHumioHecBlock BackpressureBehaviorHumioHec = "block"
	// BackpressureBehaviorHumioHecDrop Drop
	BackpressureBehaviorHumioHecDrop BackpressureBehaviorHumioHec = "drop"
	// BackpressureBehaviorHumioHecQueue Persistent Queue
	BackpressureBehaviorHumioHecQueue BackpressureBehaviorHumioHec = "queue"
)

func (e BackpressureBehaviorHumioHec) ToPointer() *BackpressureBehaviorHumioHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorHumioHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeHumioHec - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeHumioHec string

const (
	// ModeHumioHecError Error
	ModeHumioHecError ModeHumioHec = "error"
	// ModeHumioHecAlways Backpressure
	ModeHumioHecAlways ModeHumioHec = "always"
	// ModeHumioHecBackpressure Always On
	ModeHumioHecBackpressure ModeHumioHec = "backpressure"
)

func (e ModeHumioHec) ToPointer() *ModeHumioHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeHumioHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionHumioHec - Codec to use to compress the persisted data
type CompressionHumioHec string

const (
	// CompressionHumioHecNone None
	CompressionHumioHecNone CompressionHumioHec = "none"
	// CompressionHumioHecGzip Gzip
	CompressionHumioHecGzip CompressionHumioHec = "gzip"
)

func (e CompressionHumioHec) ToPointer() *CompressionHumioHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionHumioHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorHumioHec - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorHumioHec string

const (
	// QueueFullBehaviorHumioHecBlock Block
	QueueFullBehaviorHumioHecBlock QueueFullBehaviorHumioHec = "block"
	// QueueFullBehaviorHumioHecDrop Drop new data
	QueueFullBehaviorHumioHecDrop QueueFullBehaviorHumioHec = "drop"
)

func (e QueueFullBehaviorHumioHec) ToPointer() *QueueFullBehaviorHumioHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorHumioHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsHumioHec struct {
}

func (p PqControlsHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputHumioHec struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type TypeHumioHec `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL to a CrowdStrike Falcon LogScale endpoint to send events to. Examples: https://cloud.us.humio.com/api/v1/ingest/hec for JSON and https://cloud.us.humio.com/api/v1/ingest/hec/raw for raw
	URL *string `default:"https://cloud.us.humio.com/api/v1/ingest/hec" json:"url"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderHumioHec `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"true" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeHumioHec `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
	Format *RequestFormatHumioHec `default:"JSON" json:"format"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodHumioHec `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingHumioHec `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsHumioHec  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorHumioHec `default:"block" json:"onBackpressure"`
	Description    *string                       `json:"description,omitempty"`
	// CrowdStrike Falcon LogScale authentication token
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeHumioHec `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionHumioHec `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorHumioHec `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsHumioHec        `json:"pqControls,omitempty"`
}

func (o OutputHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputHumioHec) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputHumioHec) GetType() TypeHumioHec {
	if o == nil {
		return TypeHumioHec("")
	}
	return o.Type
}

func (o *OutputHumioHec) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputHumioHec) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputHumioHec) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputHumioHec) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputHumioHec) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputHumioHec) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputHumioHec) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputHumioHec) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputHumioHec) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputHumioHec) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputHumioHec) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputHumioHec) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputHumioHec) GetExtraHTTPHeaders() []ExtraHTTPHeaderHumioHec {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputHumioHec) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputHumioHec) GetFailedRequestLoggingMode() *FailedRequestLoggingModeHumioHec {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputHumioHec) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputHumioHec) GetFormat() *RequestFormatHumioHec {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputHumioHec) GetAuthType() *AuthenticationMethodHumioHec {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputHumioHec) GetResponseRetrySettings() []ResponseRetrySettingHumioHec {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputHumioHec) GetTimeoutRetrySettings() *TimeoutRetrySettingsHumioHec {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputHumioHec) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputHumioHec) GetOnBackpressure() *BackpressureBehaviorHumioHec {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputHumioHec) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputHumioHec) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputHumioHec) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputHumioHec) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputHumioHec) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputHumioHec) GetPqMode() *ModeHumioHec {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputHumioHec) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputHumioHec) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputHumioHec) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputHumioHec) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputHumioHec) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputHumioHec) GetPqCompress() *CompressionHumioHec {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputHumioHec) GetPqOnBackpressure() *QueueFullBehaviorHumioHec {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputHumioHec) GetPqControls() *PqControlsHumioHec {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeCriblSearchEngine string

const (
	TypeCriblSearchEngineCriblSearchEngine TypeCriblSearchEngine = "cribl_search_engine"
)

func (e TypeCriblSearchEngine) ToPointer() *TypeCriblSearchEngine {
	return &e
}
func (e *TypeCriblSearchEngine) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cribl_search_engine":
		*e = TypeCriblSearchEngine(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCriblSearchEngine: %v", v)
	}
}

type MinimumTLSVersionCriblSearchEngine string

const (
	MinimumTLSVersionCriblSearchEngineTlSv1  MinimumTLSVersionCriblSearchEngine = "TLSv1"
	MinimumTLSVersionCriblSearchEngineTlSv11 MinimumTLSVersionCriblSearchEngine = "TLSv1.1"
	MinimumTLSVersionCriblSearchEngineTlSv12 MinimumTLSVersionCriblSearchEngine = "TLSv1.2"
	MinimumTLSVersionCriblSearchEngineTlSv13 MinimumTLSVersionCriblSearchEngine = "TLSv1.3"
)

func (e MinimumTLSVersionCriblSearchEngine) ToPointer() *MinimumTLSVersionCriblSearchEngine {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MinimumTLSVersionCriblSearchEngine) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type MaximumTLSVersionCriblSearchEngine string

const (
	MaximumTLSVersionCriblSearchEngineTlSv1  MaximumTLSVersionCriblSearchEngine = "TLSv1"
	MaximumTLSVersionCriblSearchEngineTlSv11 MaximumTLSVersionCriblSearchEngine = "TLSv1.1"
	MaximumTLSVersionCriblSearchEngineTlSv12 MaximumTLSVersionCriblSearchEngine = "TLSv1.2"
	MaximumTLSVersionCriblSearchEngineTlSv13 MaximumTLSVersionCriblSearchEngine = "TLSv1.3"
)

func (e MaximumTLSVersionCriblSearchEngine) ToPointer() *MaximumTLSVersionCriblSearchEngine {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MaximumTLSVersionCriblSearchEngine) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideCriblSearchEngine struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                             `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionCriblSearchEngine `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionCriblSearchEngine `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideCriblSearchEngine) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideCriblSearchEngine) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideCriblSearchEngine) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideCriblSearchEngine) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideCriblSearchEngine) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideCriblSearchEngine) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideCriblSearchEngine) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideCriblSearchEngine) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideCriblSearchEngine) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideCriblSearchEngine) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideCriblSearchEngine) GetMinVersion() *MinimumTLSVersionCriblSearchEngine {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideCriblSearchEngine) GetMaxVersion() *MaximumTLSVersionCriblSearchEngine {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// CompressionCriblSearchEngine - Codec to use to compress the data before sending
type CompressionCriblSearchEngine string

const (
	// CompressionCriblSearchEngineNone None
	CompressionCriblSearchEngineNone CompressionCriblSearchEngine = "none"
	// CompressionCriblSearchEngineGzip Gzip
	CompressionCriblSearchEngineGzip CompressionCriblSearchEngine = "gzip"
)

func (e CompressionCriblSearchEngine) ToPointer() *CompressionCriblSearchEngine {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionCriblSearchEngine) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderCriblSearchEngine struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderCriblSearchEngine) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderCriblSearchEngine) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderCriblSearchEngine) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderCriblSearchEngine) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeCriblSearchEngine - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeCriblSearchEngine string

const (
	// FailedRequestLoggingModeCriblSearchEnginePayload Payload
	FailedRequestLoggingModeCriblSearchEnginePayload FailedRequestLoggingModeCriblSearchEngine = "payload"
	// FailedRequestLoggingModeCriblSearchEnginePayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeCriblSearchEnginePayloadAndHeaders FailedRequestLoggingModeCriblSearchEngine = "payloadAndHeaders"
	// FailedRequestLoggingModeCriblSearchEngineNone None
	FailedRequestLoggingModeCriblSearchEngineNone FailedRequestLoggingModeCriblSearchEngine = "none"
)

func (e FailedRequestLoggingModeCriblSearchEngine) ToPointer() *FailedRequestLoggingModeCriblSearchEngine {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeCriblSearchEngine) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingCriblSearchEngine struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingCriblSearchEngine) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingCriblSearchEngine) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingCriblSearchEngine) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingCriblSearchEngine) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingCriblSearchEngine) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingCriblSearchEngine) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsCriblSearchEngine struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsCriblSearchEngine) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsCriblSearchEngine) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsCriblSearchEngine) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsCriblSearchEngine) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsCriblSearchEngine) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsCriblSearchEngine) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type AuthTokenCriblSearchEngine struct {
	// Select or create a stored text secret
	TokenSecret string  `json:"tokenSecret"`
	Enabled     *bool   `default:"true" json:"enabled"`
	Description *string `json:"description,omitempty"`
}

func (a AuthTokenCriblSearchEngine) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthTokenCriblSearchEngine) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, []string{"tokenSecret"}); err != nil {
		return err
	}
	return nil
}

func (a *AuthTokenCriblSearchEngine) GetTokenSecret() string {
	if a == nil {
		return ""
	}
	return a.TokenSecret
}

func (a *AuthTokenCriblSearchEngine) GetEnabled() *bool {
	if a == nil {
		return nil
	}
	return a.Enabled
}

func (a *AuthTokenCriblSearchEngine) GetDescription() *string {
	if a == nil {
		return nil
	}
	return a.Description
}

// BackpressureBehaviorCriblSearchEngine - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCriblSearchEngine string

const (
	// BackpressureBehaviorCriblSearchEngineBlock Block
	BackpressureBehaviorCriblSearchEngineBlock BackpressureBehaviorCriblSearchEngine = "block"
	// BackpressureBehaviorCriblSearchEngineDrop Drop
	BackpressureBehaviorCriblSearchEngineDrop BackpressureBehaviorCriblSearchEngine = "drop"
	// BackpressureBehaviorCriblSearchEngineQueue Persistent Queue
	BackpressureBehaviorCriblSearchEngineQueue BackpressureBehaviorCriblSearchEngine = "queue"
)

func (e BackpressureBehaviorCriblSearchEngine) ToPointer() *BackpressureBehaviorCriblSearchEngine {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorCriblSearchEngine) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type URLCriblSearchEngine struct {
	// URL of a Cribl Worker to send events to, such as http://localhost:10200
	URL string `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLCriblSearchEngine) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLCriblSearchEngine) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, []string{"url"}); err != nil {
		return err
	}
	return nil
}

func (u *URLCriblSearchEngine) GetURL() string {
	if u == nil {
		return ""
	}
	return u.URL
}

func (u *URLCriblSearchEngine) GetWeight() *float64 {
	if u == nil {
		return nil
	}
	return u.Weight
}

// ModeCriblSearchEngine - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeCriblSearchEngine string

const (
	// ModeCriblSearchEngineError Error
	ModeCriblSearchEngineError ModeCriblSearchEngine = "error"
	// ModeCriblSearchEngineAlways Backpressure
	ModeCriblSearchEngineAlways ModeCriblSearchEngine = "always"
	// ModeCriblSearchEngineBackpressure Always On
	ModeCriblSearchEngineBackpressure ModeCriblSearchEngine = "backpressure"
)

func (e ModeCriblSearchEngine) ToPointer() *ModeCriblSearchEngine {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeCriblSearchEngine) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionCriblSearchEngine - Codec to use to compress the persisted data
type PqCompressCompressionCriblSearchEngine string

const (
	// PqCompressCompressionCriblSearchEngineNone None
	PqCompressCompressionCriblSearchEngineNone PqCompressCompressionCriblSearchEngine = "none"
	// PqCompressCompressionCriblSearchEngineGzip Gzip
	PqCompressCompressionCriblSearchEngineGzip PqCompressCompressionCriblSearchEngine = "gzip"
)

func (e PqCompressCompressionCriblSearchEngine) ToPointer() *PqCompressCompressionCriblSearchEngine {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionCriblSearchEngine) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorCriblSearchEngine - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCriblSearchEngine string

const (
	// QueueFullBehaviorCriblSearchEngineBlock Block
	QueueFullBehaviorCriblSearchEngineBlock QueueFullBehaviorCriblSearchEngine = "block"
	// QueueFullBehaviorCriblSearchEngineDrop Drop new data
	QueueFullBehaviorCriblSearchEngineDrop QueueFullBehaviorCriblSearchEngine = "drop"
)

func (e QueueFullBehaviorCriblSearchEngine) ToPointer() *QueueFullBehaviorCriblSearchEngine {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorCriblSearchEngine) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsCriblSearchEngine struct {
}

func (p PqControlsCriblSearchEngine) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsCriblSearchEngine) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputCriblSearchEngine struct {
	// Unique ID for this output
	ID   string                `json:"id"`
	Type TypeCriblSearchEngine `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs. If this setting is disabled, consider enabling round-robin DNS.
	LoadBalanced *bool                                   `default:"false" json:"loadBalanced"`
	TLS          *TLSSettingsClientSideCriblSearchEngine `json:"tls,omitempty"`
	// The number of minutes before the internally generated authentication token expires. Valid values are between 1 and 60.
	TokenTTLMinutes *float64 `default:"60" json:"tokenTTLMinutes"`
	// Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.
	ExcludeFields []string `json:"excludeFields,omitempty"`
	// Codec to use to compress the data before sending
	Compression *CompressionCriblSearchEngine `default:"gzip" json:"compression"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderCriblSearchEngine `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeCriblSearchEngine `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingCriblSearchEngine `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsCriblSearchEngine  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl HTTP Source in Cribl.Cloud.
	AuthTokens []AuthTokenCriblSearchEngine `json:"authTokens,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCriblSearchEngine `default:"block" json:"onBackpressure"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool   `default:"false" json:"useRoundRobinDns"`
	Description      *string `json:"description,omitempty"`
	// URL of a Cribl Worker to send events to, such as http://localhost:10200
	URL *string `json:"url,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool                  `default:"false" json:"excludeSelf"`
	Urls        []URLCriblSearchEngine `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeCriblSearchEngine `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionCriblSearchEngine `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorCriblSearchEngine `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsCriblSearchEngine        `json:"pqControls,omitempty"`
}

func (o OutputCriblSearchEngine) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCriblSearchEngine) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCriblSearchEngine) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputCriblSearchEngine) GetType() TypeCriblSearchEngine {
	if o == nil {
		return TypeCriblSearchEngine("")
	}
	return o.Type
}

func (o *OutputCriblSearchEngine) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCriblSearchEngine) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCriblSearchEngine) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCriblSearchEngine) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCriblSearchEngine) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputCriblSearchEngine) GetTLS() *TLSSettingsClientSideCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputCriblSearchEngine) GetTokenTTLMinutes() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTTLMinutes
}

func (o *OutputCriblSearchEngine) GetExcludeFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeFields
}

func (o *OutputCriblSearchEngine) GetCompression() *CompressionCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputCriblSearchEngine) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputCriblSearchEngine) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputCriblSearchEngine) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputCriblSearchEngine) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCriblSearchEngine) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputCriblSearchEngine) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCriblSearchEngine) GetExtraHTTPHeaders() []ExtraHTTPHeaderCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputCriblSearchEngine) GetFailedRequestLoggingMode() *FailedRequestLoggingModeCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputCriblSearchEngine) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputCriblSearchEngine) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputCriblSearchEngine) GetResponseRetrySettings() []ResponseRetrySettingCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputCriblSearchEngine) GetTimeoutRetrySettings() *TimeoutRetrySettingsCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputCriblSearchEngine) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputCriblSearchEngine) GetAuthTokens() []AuthTokenCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.AuthTokens
}

func (o *OutputCriblSearchEngine) GetOnBackpressure() *BackpressureBehaviorCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCriblSearchEngine) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputCriblSearchEngine) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCriblSearchEngine) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputCriblSearchEngine) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputCriblSearchEngine) GetUrls() []URLCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputCriblSearchEngine) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputCriblSearchEngine) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputCriblSearchEngine) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputCriblSearchEngine) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputCriblSearchEngine) GetPqMode() *ModeCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCriblSearchEngine) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputCriblSearchEngine) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputCriblSearchEngine) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCriblSearchEngine) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCriblSearchEngine) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCriblSearchEngine) GetPqCompress() *PqCompressCompressionCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCriblSearchEngine) GetPqOnBackpressure() *QueueFullBehaviorCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCriblSearchEngine) GetPqControls() *PqControlsCriblSearchEngine {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeCriblHTTP string

const (
	CreateOutputTypeCriblHTTPCriblHTTP CreateOutputTypeCriblHTTP = "cribl_http"
)

func (e CreateOutputTypeCriblHTTP) ToPointer() *CreateOutputTypeCriblHTTP {
	return &e
}
func (e *CreateOutputTypeCriblHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cribl_http":
		*e = CreateOutputTypeCriblHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeCriblHTTP: %v", v)
	}
}

type CreateOutputMinimumTLSVersionCriblHTTP string

const (
	CreateOutputMinimumTLSVersionCriblHTTPTlSv1  CreateOutputMinimumTLSVersionCriblHTTP = "TLSv1"
	CreateOutputMinimumTLSVersionCriblHTTPTlSv11 CreateOutputMinimumTLSVersionCriblHTTP = "TLSv1.1"
	CreateOutputMinimumTLSVersionCriblHTTPTlSv12 CreateOutputMinimumTLSVersionCriblHTTP = "TLSv1.2"
	CreateOutputMinimumTLSVersionCriblHTTPTlSv13 CreateOutputMinimumTLSVersionCriblHTTP = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionCriblHTTP) ToPointer() *CreateOutputMinimumTLSVersionCriblHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMinimumTLSVersionCriblHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputMaximumTLSVersionCriblHTTP string

const (
	CreateOutputMaximumTLSVersionCriblHTTPTlSv1  CreateOutputMaximumTLSVersionCriblHTTP = "TLSv1"
	CreateOutputMaximumTLSVersionCriblHTTPTlSv11 CreateOutputMaximumTLSVersionCriblHTTP = "TLSv1.1"
	CreateOutputMaximumTLSVersionCriblHTTPTlSv12 CreateOutputMaximumTLSVersionCriblHTTP = "TLSv1.2"
	CreateOutputMaximumTLSVersionCriblHTTPTlSv13 CreateOutputMaximumTLSVersionCriblHTTP = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionCriblHTTP) ToPointer() *CreateOutputMaximumTLSVersionCriblHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMaximumTLSVersionCriblHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideCriblHTTP struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                 `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionCriblHTTP `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionCriblHTTP `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideCriblHTTP) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideCriblHTTP) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideCriblHTTP) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideCriblHTTP) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideCriblHTTP) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideCriblHTTP) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideCriblHTTP) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideCriblHTTP) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideCriblHTTP) GetMinVersion() *CreateOutputMinimumTLSVersionCriblHTTP {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideCriblHTTP) GetMaxVersion() *CreateOutputMaximumTLSVersionCriblHTTP {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// CreateOutputCompressionCriblHTTP - Codec to use to compress the data before sending
type CreateOutputCompressionCriblHTTP string

const (
	// CreateOutputCompressionCriblHTTPNone None
	CreateOutputCompressionCriblHTTPNone CreateOutputCompressionCriblHTTP = "none"
	// CreateOutputCompressionCriblHTTPGzip Gzip
	CreateOutputCompressionCriblHTTPGzip CreateOutputCompressionCriblHTTP = "gzip"
)

func (e CreateOutputCompressionCriblHTTP) ToPointer() *CreateOutputCompressionCriblHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputCompressionCriblHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderCriblHTTP struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderCriblHTTP) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderCriblHTTP) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeCriblHTTP - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeCriblHTTP string

const (
	// FailedRequestLoggingModeCriblHTTPPayload Payload
	FailedRequestLoggingModeCriblHTTPPayload FailedRequestLoggingModeCriblHTTP = "payload"
	// FailedRequestLoggingModeCriblHTTPPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeCriblHTTPPayloadAndHeaders FailedRequestLoggingModeCriblHTTP = "payloadAndHeaders"
	// FailedRequestLoggingModeCriblHTTPNone None
	FailedRequestLoggingModeCriblHTTPNone FailedRequestLoggingModeCriblHTTP = "none"
)

func (e FailedRequestLoggingModeCriblHTTP) ToPointer() *FailedRequestLoggingModeCriblHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeCriblHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingCriblHTTP struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingCriblHTTP) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingCriblHTTP) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingCriblHTTP) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingCriblHTTP) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsCriblHTTP struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsCriblHTTP) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsCriblHTTP) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsCriblHTTP) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsCriblHTTP) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type CreateOutputAuthTokenCriblHTTP struct {
	// Select or create a stored text secret
	TokenSecret string  `json:"tokenSecret"`
	Enabled     *bool   `default:"true" json:"enabled"`
	Description *string `json:"description,omitempty"`
}

func (c CreateOutputAuthTokenCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthTokenCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"tokenSecret"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputAuthTokenCriblHTTP) GetTokenSecret() string {
	if c == nil {
		return ""
	}
	return c.TokenSecret
}

func (c *CreateOutputAuthTokenCriblHTTP) GetEnabled() *bool {
	if c == nil {
		return nil
	}
	return c.Enabled
}

func (c *CreateOutputAuthTokenCriblHTTP) GetDescription() *string {
	if c == nil {
		return nil
	}
	return c.Description
}

// BackpressureBehaviorCriblHTTP - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCriblHTTP string

const (
	// BackpressureBehaviorCriblHTTPBlock Block
	BackpressureBehaviorCriblHTTPBlock BackpressureBehaviorCriblHTTP = "block"
	// BackpressureBehaviorCriblHTTPDrop Drop
	BackpressureBehaviorCriblHTTPDrop BackpressureBehaviorCriblHTTP = "drop"
	// BackpressureBehaviorCriblHTTPQueue Persistent Queue
	BackpressureBehaviorCriblHTTPQueue BackpressureBehaviorCriblHTTP = "queue"
)

func (e BackpressureBehaviorCriblHTTP) ToPointer() *BackpressureBehaviorCriblHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorCriblHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type URLCriblHTTP struct {
	// URL of a Cribl Worker to send events to, such as http://localhost:10200
	URL string `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, []string{"url"}); err != nil {
		return err
	}
	return nil
}

func (u *URLCriblHTTP) GetURL() string {
	if u == nil {
		return ""
	}
	return u.URL
}

func (u *URLCriblHTTP) GetWeight() *float64 {
	if u == nil {
		return nil
	}
	return u.Weight
}

// CreateOutputModeCriblHTTP - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeCriblHTTP string

const (
	// CreateOutputModeCriblHTTPError Error
	CreateOutputModeCriblHTTPError CreateOutputModeCriblHTTP = "error"
	// CreateOutputModeCriblHTTPAlways Backpressure
	CreateOutputModeCriblHTTPAlways CreateOutputModeCriblHTTP = "always"
	// CreateOutputModeCriblHTTPBackpressure Always On
	CreateOutputModeCriblHTTPBackpressure CreateOutputModeCriblHTTP = "backpressure"
)

func (e CreateOutputModeCriblHTTP) ToPointer() *CreateOutputModeCriblHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeCriblHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionCriblHTTP - Codec to use to compress the persisted data
type PqCompressCompressionCriblHTTP string

const (
	// PqCompressCompressionCriblHTTPNone None
	PqCompressCompressionCriblHTTPNone PqCompressCompressionCriblHTTP = "none"
	// PqCompressCompressionCriblHTTPGzip Gzip
	PqCompressCompressionCriblHTTPGzip PqCompressCompressionCriblHTTP = "gzip"
)

func (e PqCompressCompressionCriblHTTP) ToPointer() *PqCompressCompressionCriblHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionCriblHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorCriblHTTP - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCriblHTTP string

const (
	// QueueFullBehaviorCriblHTTPBlock Block
	QueueFullBehaviorCriblHTTPBlock QueueFullBehaviorCriblHTTP = "block"
	// QueueFullBehaviorCriblHTTPDrop Drop new data
	QueueFullBehaviorCriblHTTPDrop QueueFullBehaviorCriblHTTP = "drop"
)

func (e QueueFullBehaviorCriblHTTP) ToPointer() *QueueFullBehaviorCriblHTTP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorCriblHTTP) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsCriblHTTP struct {
}

func (c CreateOutputPqControlsCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputCriblHTTP struct {
	// Unique ID for this output
	ID   string                    `json:"id"`
	Type CreateOutputTypeCriblHTTP `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs. If this setting is disabled, consider enabling round-robin DNS.
	LoadBalanced *bool                           `default:"true" json:"loadBalanced"`
	TLS          *TLSSettingsClientSideCriblHTTP `json:"tls,omitempty"`
	// The number of minutes before the internally generated authentication token expires. Valid values are between 1 and 60.
	TokenTTLMinutes *float64 `default:"60" json:"tokenTTLMinutes"`
	// Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.
	ExcludeFields []string `json:"excludeFields,omitempty"`
	// Codec to use to compress the data before sending
	Compression *CreateOutputCompressionCriblHTTP `default:"gzip" json:"compression"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderCriblHTTP `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeCriblHTTP `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingCriblHTTP `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsCriblHTTP  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl HTTP Source in Cribl.Cloud.
	AuthTokens []CreateOutputAuthTokenCriblHTTP `json:"authTokens,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCriblHTTP `default:"block" json:"onBackpressure"`
	Description    *string                        `json:"description,omitempty"`
	// URL of a Cribl Worker to send events to, such as http://localhost:10200
	URL *string `json:"url,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool          `default:"false" json:"excludeSelf"`
	Urls        []URLCriblHTTP `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeCriblHTTP `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionCriblHTTP `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorCriblHTTP      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsCriblHTTP `json:"pqControls,omitempty"`
}

func (o OutputCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCriblHTTP) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputCriblHTTP) GetType() CreateOutputTypeCriblHTTP {
	if o == nil {
		return CreateOutputTypeCriblHTTP("")
	}
	return o.Type
}

func (o *OutputCriblHTTP) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCriblHTTP) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCriblHTTP) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCriblHTTP) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCriblHTTP) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputCriblHTTP) GetTLS() *TLSSettingsClientSideCriblHTTP {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputCriblHTTP) GetTokenTTLMinutes() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTTLMinutes
}

func (o *OutputCriblHTTP) GetExcludeFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeFields
}

func (o *OutputCriblHTTP) GetCompression() *CreateOutputCompressionCriblHTTP {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputCriblHTTP) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputCriblHTTP) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputCriblHTTP) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputCriblHTTP) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCriblHTTP) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputCriblHTTP) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCriblHTTP) GetExtraHTTPHeaders() []ExtraHTTPHeaderCriblHTTP {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputCriblHTTP) GetFailedRequestLoggingMode() *FailedRequestLoggingModeCriblHTTP {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputCriblHTTP) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputCriblHTTP) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputCriblHTTP) GetResponseRetrySettings() []ResponseRetrySettingCriblHTTP {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputCriblHTTP) GetTimeoutRetrySettings() *TimeoutRetrySettingsCriblHTTP {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputCriblHTTP) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputCriblHTTP) GetAuthTokens() []CreateOutputAuthTokenCriblHTTP {
	if o == nil {
		return nil
	}
	return o.AuthTokens
}

func (o *OutputCriblHTTP) GetOnBackpressure() *BackpressureBehaviorCriblHTTP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCriblHTTP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCriblHTTP) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputCriblHTTP) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputCriblHTTP) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputCriblHTTP) GetUrls() []URLCriblHTTP {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputCriblHTTP) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputCriblHTTP) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputCriblHTTP) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputCriblHTTP) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputCriblHTTP) GetPqMode() *CreateOutputModeCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCriblHTTP) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputCriblHTTP) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputCriblHTTP) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCriblHTTP) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCriblHTTP) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCriblHTTP) GetPqCompress() *PqCompressCompressionCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCriblHTTP) GetPqOnBackpressure() *QueueFullBehaviorCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCriblHTTP) GetPqControls() *CreateOutputPqControlsCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeCriblTCP string

const (
	CreateOutputTypeCriblTCPCriblTCP CreateOutputTypeCriblTCP = "cribl_tcp"
)

func (e CreateOutputTypeCriblTCP) ToPointer() *CreateOutputTypeCriblTCP {
	return &e
}
func (e *CreateOutputTypeCriblTCP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cribl_tcp":
		*e = CreateOutputTypeCriblTCP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeCriblTCP: %v", v)
	}
}

// CreateOutputCompressionCriblTCP - Codec to use to compress the data before sending
type CreateOutputCompressionCriblTCP string

const (
	// CreateOutputCompressionCriblTCPNone None
	CreateOutputCompressionCriblTCPNone CreateOutputCompressionCriblTCP = "none"
	// CreateOutputCompressionCriblTCPGzip Gzip
	CreateOutputCompressionCriblTCPGzip CreateOutputCompressionCriblTCP = "gzip"
)

func (e CreateOutputCompressionCriblTCP) ToPointer() *CreateOutputCompressionCriblTCP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputCompressionCriblTCP) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

type CreateOutputMinimumTLSVersionCriblTCP string

const (
	CreateOutputMinimumTLSVersionCriblTCPTlSv1  CreateOutputMinimumTLSVersionCriblTCP = "TLSv1"
	CreateOutputMinimumTLSVersionCriblTCPTlSv11 CreateOutputMinimumTLSVersionCriblTCP = "TLSv1.1"
	CreateOutputMinimumTLSVersionCriblTCPTlSv12 CreateOutputMinimumTLSVersionCriblTCP = "TLSv1.2"
	CreateOutputMinimumTLSVersionCriblTCPTlSv13 CreateOutputMinimumTLSVersionCriblTCP = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionCriblTCP) ToPointer() *CreateOutputMinimumTLSVersionCriblTCP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMinimumTLSVersionCriblTCP) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputMaximumTLSVersionCriblTCP string

const (
	CreateOutputMaximumTLSVersionCriblTCPTlSv1  CreateOutputMaximumTLSVersionCriblTCP = "TLSv1"
	CreateOutputMaximumTLSVersionCriblTCPTlSv11 CreateOutputMaximumTLSVersionCriblTCP = "TLSv1.1"
	CreateOutputMaximumTLSVersionCriblTCPTlSv12 CreateOutputMaximumTLSVersionCriblTCP = "TLSv1.2"
	CreateOutputMaximumTLSVersionCriblTCPTlSv13 CreateOutputMaximumTLSVersionCriblTCP = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionCriblTCP) ToPointer() *CreateOutputMaximumTLSVersionCriblTCP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMaximumTLSVersionCriblTCP) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideCriblTCP struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionCriblTCP `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionCriblTCP `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideCriblTCP) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideCriblTCP) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideCriblTCP) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideCriblTCP) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideCriblTCP) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideCriblTCP) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideCriblTCP) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideCriblTCP) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideCriblTCP) GetMinVersion() *CreateOutputMinimumTLSVersionCriblTCP {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideCriblTCP) GetMaxVersion() *CreateOutputMaximumTLSVersionCriblTCP {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

type CreateOutputAuthTokenCriblTCP struct {
	// Select or create a stored text secret
	TokenSecret string `json:"tokenSecret"`
	Enabled     *bool  `default:"true" json:"enabled"`
	// Optional token description
	Description *string `json:"description,omitempty"`
}

func (c CreateOutputAuthTokenCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthTokenCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"tokenSecret"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputAuthTokenCriblTCP) GetTokenSecret() string {
	if c == nil {
		return ""
	}
	return c.TokenSecret
}

func (c *CreateOutputAuthTokenCriblTCP) GetEnabled() *bool {
	if c == nil {
		return nil
	}
	return c.Enabled
}

func (c *CreateOutputAuthTokenCriblTCP) GetDescription() *string {
	if c == nil {
		return nil
	}
	return c.Description
}

// BackpressureBehaviorCriblTCP - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCriblTCP string

const (
	// BackpressureBehaviorCriblTCPBlock Block
	BackpressureBehaviorCriblTCPBlock BackpressureBehaviorCriblTCP = "block"
	// BackpressureBehaviorCriblTCPDrop Drop
	BackpressureBehaviorCriblTCPDrop BackpressureBehaviorCriblTCP = "drop"
	// BackpressureBehaviorCriblTCPQueue Persistent Queue
	BackpressureBehaviorCriblTCPQueue BackpressureBehaviorCriblTCP = "queue"
)

func (e BackpressureBehaviorCriblTCP) ToPointer() *BackpressureBehaviorCriblTCP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorCriblTCP) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// TLSCriblTCP - Whether to inherit TLS configs from group setting or disable TLS
type TLSCriblTCP string

const (
	TLSCriblTCPInherit TLSCriblTCP = "inherit"
	TLSCriblTCPOff     TLSCriblTCP = "off"
)

func (e TLSCriblTCP) ToPointer() *TLSCriblTCP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *TLSCriblTCP) IsExact() bool {
	if e != nil {
		switch *e {
		case "inherit", "off":
			return true
		}
	}
	return false
}

type HostCriblTCP struct {
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port *float64 `default:"10300" json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS
	TLS *TLSCriblTCP `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host"}); err != nil {
		return err
	}
	return nil
}

func (h *HostCriblTCP) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostCriblTCP) GetPort() *float64 {
	if h == nil {
		return nil
	}
	return h.Port
}

func (h *HostCriblTCP) GetTLS() *TLSCriblTCP {
	if h == nil {
		return nil
	}
	return h.TLS
}

func (h *HostCriblTCP) GetServername() *string {
	if h == nil {
		return nil
	}
	return h.Servername
}

func (h *HostCriblTCP) GetWeight() *float64 {
	if h == nil {
		return nil
	}
	return h.Weight
}

// CreateOutputModeCriblTCP - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeCriblTCP string

const (
	// CreateOutputModeCriblTCPError Error
	CreateOutputModeCriblTCPError CreateOutputModeCriblTCP = "error"
	// CreateOutputModeCriblTCPAlways Backpressure
	CreateOutputModeCriblTCPAlways CreateOutputModeCriblTCP = "always"
	// CreateOutputModeCriblTCPBackpressure Always On
	CreateOutputModeCriblTCPBackpressure CreateOutputModeCriblTCP = "backpressure"
)

func (e CreateOutputModeCriblTCP) ToPointer() *CreateOutputModeCriblTCP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeCriblTCP) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionCriblTCP - Codec to use to compress the persisted data
type PqCompressCompressionCriblTCP string

const (
	// PqCompressCompressionCriblTCPNone None
	PqCompressCompressionCriblTCPNone PqCompressCompressionCriblTCP = "none"
	// PqCompressCompressionCriblTCPGzip Gzip
	PqCompressCompressionCriblTCPGzip PqCompressCompressionCriblTCP = "gzip"
)

func (e PqCompressCompressionCriblTCP) ToPointer() *PqCompressCompressionCriblTCP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionCriblTCP) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorCriblTCP - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCriblTCP string

const (
	// QueueFullBehaviorCriblTCPBlock Block
	QueueFullBehaviorCriblTCPBlock QueueFullBehaviorCriblTCP = "block"
	// QueueFullBehaviorCriblTCPDrop Drop new data
	QueueFullBehaviorCriblTCPDrop QueueFullBehaviorCriblTCP = "drop"
)

func (e QueueFullBehaviorCriblTCP) ToPointer() *QueueFullBehaviorCriblTCP {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorCriblTCP) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsCriblTCP struct {
}

func (c CreateOutputPqControlsCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputCriblTCP struct {
	// Unique ID for this output
	ID   string                   `json:"id"`
	Type CreateOutputTypeCriblTCP `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Use load-balanced destinations
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// Codec to use to compress the data before sending
	Compression *CreateOutputCompressionCriblTCP `default:"gzip" json:"compression"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string                        `default:"0" json:"throttleRatePerSec"`
	TLS                *TLSSettingsClientSideCriblTCP `json:"tls,omitempty"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// The number of minutes before the internally generated authentication token expires, valid values between 1 and 60
	TokenTTLMinutes *float64 `default:"60" json:"tokenTTLMinutes"`
	// Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl TCP Source in Cribl.Cloud.
	AuthTokens []CreateOutputAuthTokenCriblTCP `json:"authTokens,omitempty"`
	// Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.
	ExcludeFields []string `json:"excludeFields,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCriblTCP `default:"block" json:"onBackpressure"`
	Description    *string                       `json:"description,omitempty"`
	// The hostname of the receiver
	Host *string `json:"host,omitempty"`
	// The port to connect to on the provided host
	Port *float64 `default:"10300" json:"port"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of hosts to load-balance data to
	Hosts []HostCriblTCP `json:"hosts,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeCriblTCP `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionCriblTCP `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorCriblTCP      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsCriblTCP `json:"pqControls,omitempty"`
}

func (o OutputCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCriblTCP) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputCriblTCP) GetType() CreateOutputTypeCriblTCP {
	if o == nil {
		return CreateOutputTypeCriblTCP("")
	}
	return o.Type
}

func (o *OutputCriblTCP) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCriblTCP) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCriblTCP) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCriblTCP) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCriblTCP) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputCriblTCP) GetCompression() *CreateOutputCompressionCriblTCP {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputCriblTCP) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputCriblTCP) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputCriblTCP) GetTLS() *TLSSettingsClientSideCriblTCP {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputCriblTCP) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputCriblTCP) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputCriblTCP) GetTokenTTLMinutes() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTTLMinutes
}

func (o *OutputCriblTCP) GetAuthTokens() []CreateOutputAuthTokenCriblTCP {
	if o == nil {
		return nil
	}
	return o.AuthTokens
}

func (o *OutputCriblTCP) GetExcludeFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeFields
}

func (o *OutputCriblTCP) GetOnBackpressure() *BackpressureBehaviorCriblTCP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCriblTCP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCriblTCP) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputCriblTCP) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputCriblTCP) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputCriblTCP) GetHosts() []HostCriblTCP {
	if o == nil {
		return nil
	}
	return o.Hosts
}

func (o *OutputCriblTCP) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputCriblTCP) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputCriblTCP) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputCriblTCP) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputCriblTCP) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputCriblTCP) GetPqMode() *CreateOutputModeCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCriblTCP) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputCriblTCP) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputCriblTCP) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCriblTCP) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCriblTCP) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCriblTCP) GetPqCompress() *PqCompressCompressionCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCriblTCP) GetPqOnBackpressure() *QueueFullBehaviorCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCriblTCP) GetPqControls() *CreateOutputPqControlsCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeDataset string

const (
	TypeDatasetDataset TypeDataset = "dataset"
)

func (e TypeDataset) ToPointer() *TypeDataset {
	return &e
}
func (e *TypeDataset) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dataset":
		*e = TypeDataset(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDataset: %v", v)
	}
}

// DefaultSeveritySeverity - Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.
type DefaultSeveritySeverity string

const (
	// DefaultSeveritySeverityFinest 0 - finest
	DefaultSeveritySeverityFinest DefaultSeveritySeverity = "finest"
	// DefaultSeveritySeverityFiner 1 - finer
	DefaultSeveritySeverityFiner DefaultSeveritySeverity = "finer"
	// DefaultSeveritySeverityFine 2 - fine
	DefaultSeveritySeverityFine DefaultSeveritySeverity = "fine"
	// DefaultSeveritySeverityInfo 3 - info
	DefaultSeveritySeverityInfo DefaultSeveritySeverity = "info"
	// DefaultSeveritySeverityWarning 4 - warning
	DefaultSeveritySeverityWarning DefaultSeveritySeverity = "warning"
	// DefaultSeveritySeverityError 5 - error
	DefaultSeveritySeverityError DefaultSeveritySeverity = "error"
	// DefaultSeveritySeverityFatal 6 - fatal
	DefaultSeveritySeverityFatal DefaultSeveritySeverity = "fatal"
)

func (e DefaultSeveritySeverity) ToPointer() *DefaultSeveritySeverity {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DefaultSeveritySeverity) IsExact() bool {
	if e != nil {
		switch *e {
		case "finest", "finer", "fine", "info", "warning", "error", "fatal":
			return true
		}
	}
	return false
}

type ResponseRetrySettingDataset struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingDataset) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingDataset) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingDataset) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingDataset) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsDataset struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsDataset) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsDataset) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsDataset) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsDataset) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// DataSetSite - DataSet site to which events should be sent
type DataSetSite string

const (
	// DataSetSiteUs US
	DataSetSiteUs DataSetSite = "us"
	// DataSetSiteEu Europe
	DataSetSiteEu DataSetSite = "eu"
	// DataSetSiteCustom Custom
	DataSetSiteCustom DataSetSite = "custom"
)

func (e DataSetSite) ToPointer() *DataSetSite {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataSetSite) IsExact() bool {
	if e != nil {
		switch *e {
		case "us", "eu", "custom":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderDataset struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderDataset) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderDataset) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeDataset - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDataset string

const (
	// FailedRequestLoggingModeDatasetPayload Payload
	FailedRequestLoggingModeDatasetPayload FailedRequestLoggingModeDataset = "payload"
	// FailedRequestLoggingModeDatasetPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeDatasetPayloadAndHeaders FailedRequestLoggingModeDataset = "payloadAndHeaders"
	// FailedRequestLoggingModeDatasetNone None
	FailedRequestLoggingModeDatasetNone FailedRequestLoggingModeDataset = "none"
)

func (e FailedRequestLoggingModeDataset) ToPointer() *FailedRequestLoggingModeDataset {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeDataset) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

// BackpressureBehaviorDataset - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDataset string

const (
	// BackpressureBehaviorDatasetBlock Block
	BackpressureBehaviorDatasetBlock BackpressureBehaviorDataset = "block"
	// BackpressureBehaviorDatasetDrop Drop
	BackpressureBehaviorDatasetDrop BackpressureBehaviorDataset = "drop"
	// BackpressureBehaviorDatasetQueue Persistent Queue
	BackpressureBehaviorDatasetQueue BackpressureBehaviorDataset = "queue"
)

func (e BackpressureBehaviorDataset) ToPointer() *BackpressureBehaviorDataset {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorDataset) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// AuthenticationMethodDataset - Enter API key directly, or select a stored secret
type AuthenticationMethodDataset string

const (
	AuthenticationMethodDatasetManual AuthenticationMethodDataset = "manual"
	AuthenticationMethodDatasetSecret AuthenticationMethodDataset = "secret"
)

func (e AuthenticationMethodDataset) ToPointer() *AuthenticationMethodDataset {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodDataset) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

// ModeDataset - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeDataset string

const (
	// ModeDatasetError Error
	ModeDatasetError ModeDataset = "error"
	// ModeDatasetAlways Backpressure
	ModeDatasetAlways ModeDataset = "always"
	// ModeDatasetBackpressure Always On
	ModeDatasetBackpressure ModeDataset = "backpressure"
)

func (e ModeDataset) ToPointer() *ModeDataset {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeDataset) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionDataset - Codec to use to compress the persisted data
type CompressionDataset string

const (
	// CompressionDatasetNone None
	CompressionDatasetNone CompressionDataset = "none"
	// CompressionDatasetGzip Gzip
	CompressionDatasetGzip CompressionDataset = "gzip"
)

func (e CompressionDataset) ToPointer() *CompressionDataset {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionDataset) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorDataset - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDataset string

const (
	// QueueFullBehaviorDatasetBlock Block
	QueueFullBehaviorDatasetBlock QueueFullBehaviorDataset = "block"
	// QueueFullBehaviorDatasetDrop Drop new data
	QueueFullBehaviorDatasetDrop QueueFullBehaviorDataset = "drop"
)

func (e QueueFullBehaviorDataset) ToPointer() *QueueFullBehaviorDataset {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorDataset) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsDataset struct {
}

func (p PqControlsDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputDataset struct {
	// Unique ID for this output
	ID   string      `json:"id"`
	Type TypeDataset `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the event field that contains the message or attributes to send. If not specified, all of the event's non-internal fields will be sent as attributes.
	MessageField *string `json:"messageField,omitempty"`
	// Fields to exclude from the event if the Message field is either unspecified or refers to an object. Ignored if the Message field is a string. If empty, we send all non-internal fields.
	ExcludeFields []string `json:"excludeFields,omitempty"`
	// Name of the event field that contains the `serverHost` identifier. If not specified, defaults to `cribl_<outputId>`.
	ServerHostField *string `json:"serverHostField,omitempty"`
	// Name of the event field that contains the timestamp. If not specified, defaults to `ts`, `_time`, or `Date.now()`, in that order.
	TimestampField *string `json:"timestampField,omitempty"`
	// Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.
	DefaultSeverity *DefaultSeveritySeverity `default:"info" json:"defaultSeverity"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingDataset `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDataset  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// DataSet site to which events should be sent
	Site *DataSetSite `default:"us" json:"site"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderDataset `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDataset `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDataset `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType *AuthenticationMethodDataset `default:"manual" json:"authType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	CustomURL          *string  `json:"customUrl,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeDataset `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionDataset `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorDataset `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsDataset        `json:"pqControls,omitempty"`
	// A 'Log Write Access' API key for the DataSet account
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDataset) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDataset) GetType() TypeDataset {
	if o == nil {
		return TypeDataset("")
	}
	return o.Type
}

func (o *OutputDataset) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDataset) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDataset) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDataset) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDataset) GetMessageField() *string {
	if o == nil {
		return nil
	}
	return o.MessageField
}

func (o *OutputDataset) GetExcludeFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeFields
}

func (o *OutputDataset) GetServerHostField() *string {
	if o == nil {
		return nil
	}
	return o.ServerHostField
}

func (o *OutputDataset) GetTimestampField() *string {
	if o == nil {
		return nil
	}
	return o.TimestampField
}

func (o *OutputDataset) GetDefaultSeverity() *DefaultSeveritySeverity {
	if o == nil {
		return nil
	}
	return o.DefaultSeverity
}

func (o *OutputDataset) GetResponseRetrySettings() []ResponseRetrySettingDataset {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDataset) GetTimeoutRetrySettings() *TimeoutRetrySettingsDataset {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDataset) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDataset) GetSite() *DataSetSite {
	if o == nil {
		return nil
	}
	return o.Site
}

func (o *OutputDataset) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDataset) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDataset) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputDataset) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDataset) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDataset) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDataset) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDataset) GetExtraHTTPHeaders() []ExtraHTTPHeaderDataset {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDataset) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDataset) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDataset {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDataset) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDataset) GetOnBackpressure() *BackpressureBehaviorDataset {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDataset) GetAuthType() *AuthenticationMethodDataset {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputDataset) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputDataset) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDataset) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputDataset) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputDataset) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputDataset) GetPqMode() *ModeDataset {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDataset) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputDataset) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputDataset) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDataset) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDataset) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDataset) GetPqCompress() *CompressionDataset {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDataset) GetPqOnBackpressure() *QueueFullBehaviorDataset {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDataset) GetPqControls() *PqControlsDataset {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDataset) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputDataset) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type TypeServiceNow string

const (
	TypeServiceNowServiceNow TypeServiceNow = "service_now"
)

func (e TypeServiceNow) ToPointer() *TypeServiceNow {
	return &e
}
func (e *TypeServiceNow) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "service_now":
		*e = TypeServiceNow(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeServiceNow: %v", v)
	}
}

// OTLPVersionServiceNow - The version of OTLP Protobuf definitions to use when structuring data to send
type OTLPVersionServiceNow string

const (
	// OTLPVersionServiceNowOneDot3Dot1 1.3.1
	OTLPVersionServiceNowOneDot3Dot1 OTLPVersionServiceNow = "1.3.1"
)

func (e OTLPVersionServiceNow) ToPointer() *OTLPVersionServiceNow {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OTLPVersionServiceNow) IsExact() bool {
	if e != nil {
		switch *e {
		case "1.3.1":
			return true
		}
	}
	return false
}

// ProtocolServiceNow - Select a transport option for OpenTelemetry
type ProtocolServiceNow string

const (
	// ProtocolServiceNowGrpc gRPC
	ProtocolServiceNowGrpc ProtocolServiceNow = "grpc"
	// ProtocolServiceNowHTTP HTTP
	ProtocolServiceNowHTTP ProtocolServiceNow = "http"
)

func (e ProtocolServiceNow) ToPointer() *ProtocolServiceNow {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ProtocolServiceNow) IsExact() bool {
	if e != nil {
		switch *e {
		case "grpc", "http":
			return true
		}
	}
	return false
}

// CompressCompressionServiceNow - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type CompressCompressionServiceNow string

const (
	// CompressCompressionServiceNowNone None
	CompressCompressionServiceNowNone CompressCompressionServiceNow = "none"
	// CompressCompressionServiceNowDeflate Deflate
	CompressCompressionServiceNowDeflate CompressCompressionServiceNow = "deflate"
	// CompressCompressionServiceNowGzip Gzip
	CompressCompressionServiceNowGzip CompressCompressionServiceNow = "gzip"
)

func (e CompressCompressionServiceNow) ToPointer() *CompressCompressionServiceNow {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressCompressionServiceNow) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "deflate", "gzip":
			return true
		}
	}
	return false
}

// HTTPCompressCompressionServiceNow - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompressionServiceNow string

const (
	// HTTPCompressCompressionServiceNowNone None
	HTTPCompressCompressionServiceNowNone HTTPCompressCompressionServiceNow = "none"
	// HTTPCompressCompressionServiceNowGzip Gzip
	HTTPCompressCompressionServiceNowGzip HTTPCompressCompressionServiceNow = "gzip"
)

func (e HTTPCompressCompressionServiceNow) ToPointer() *HTTPCompressCompressionServiceNow {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *HTTPCompressCompressionServiceNow) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

type MetadatumServiceNow struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (m MetadatumServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(m, "", false)
}

func (m *MetadatumServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &m, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (m *MetadatumServiceNow) GetKey() *string {
	if m == nil {
		return nil
	}
	return m.Key
}

func (m *MetadatumServiceNow) GetValue() string {
	if m == nil {
		return ""
	}
	return m.Value
}

// FailedRequestLoggingModeServiceNow - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeServiceNow string

const (
	// FailedRequestLoggingModeServiceNowPayload Payload
	FailedRequestLoggingModeServiceNowPayload FailedRequestLoggingModeServiceNow = "payload"
	// FailedRequestLoggingModeServiceNowPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeServiceNowPayloadAndHeaders FailedRequestLoggingModeServiceNow = "payloadAndHeaders"
	// FailedRequestLoggingModeServiceNowNone None
	FailedRequestLoggingModeServiceNowNone FailedRequestLoggingModeServiceNow = "none"
)

func (e FailedRequestLoggingModeServiceNow) ToPointer() *FailedRequestLoggingModeServiceNow {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeServiceNow) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

// BackpressureBehaviorServiceNow - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorServiceNow string

const (
	// BackpressureBehaviorServiceNowBlock Block
	BackpressureBehaviorServiceNowBlock BackpressureBehaviorServiceNow = "block"
	// BackpressureBehaviorServiceNowDrop Drop
	BackpressureBehaviorServiceNowDrop BackpressureBehaviorServiceNow = "drop"
	// BackpressureBehaviorServiceNowQueue Persistent Queue
	BackpressureBehaviorServiceNowQueue BackpressureBehaviorServiceNow = "queue"
)

func (e BackpressureBehaviorServiceNow) ToPointer() *BackpressureBehaviorServiceNow {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorServiceNow) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderServiceNow struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderServiceNow) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderServiceNow) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

type ResponseRetrySettingServiceNow struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingServiceNow) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingServiceNow) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingServiceNow) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingServiceNow) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsServiceNow struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsServiceNow) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsServiceNow) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsServiceNow) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsServiceNow) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type MinimumTLSVersionServiceNow string

const (
	MinimumTLSVersionServiceNowTlSv1  MinimumTLSVersionServiceNow = "TLSv1"
	MinimumTLSVersionServiceNowTlSv11 MinimumTLSVersionServiceNow = "TLSv1.1"
	MinimumTLSVersionServiceNowTlSv12 MinimumTLSVersionServiceNow = "TLSv1.2"
	MinimumTLSVersionServiceNowTlSv13 MinimumTLSVersionServiceNow = "TLSv1.3"
)

func (e MinimumTLSVersionServiceNow) ToPointer() *MinimumTLSVersionServiceNow {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MinimumTLSVersionServiceNow) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type MaximumTLSVersionServiceNow string

const (
	MaximumTLSVersionServiceNowTlSv1  MaximumTLSVersionServiceNow = "TLSv1"
	MaximumTLSVersionServiceNowTlSv11 MaximumTLSVersionServiceNow = "TLSv1.1"
	MaximumTLSVersionServiceNowTlSv12 MaximumTLSVersionServiceNow = "TLSv1.2"
	MaximumTLSVersionServiceNowTlSv13 MaximumTLSVersionServiceNow = "TLSv1.3"
)

func (e MaximumTLSVersionServiceNow) ToPointer() *MaximumTLSVersionServiceNow {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MaximumTLSVersionServiceNow) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideServiceNow struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                      `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionServiceNow `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionServiceNow `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideServiceNow) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideServiceNow) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideServiceNow) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideServiceNow) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideServiceNow) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideServiceNow) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideServiceNow) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideServiceNow) GetMinVersion() *MinimumTLSVersionServiceNow {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideServiceNow) GetMaxVersion() *MaximumTLSVersionServiceNow {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// ModeServiceNow - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeServiceNow string

const (
	// ModeServiceNowError Error
	ModeServiceNowError ModeServiceNow = "error"
	// ModeServiceNowAlways Backpressure
	ModeServiceNowAlways ModeServiceNow = "always"
	// ModeServiceNowBackpressure Always On
	ModeServiceNowBackpressure ModeServiceNow = "backpressure"
)

func (e ModeServiceNow) ToPointer() *ModeServiceNow {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeServiceNow) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionServiceNow - Codec to use to compress the persisted data
type PqCompressCompressionServiceNow string

const (
	// PqCompressCompressionServiceNowNone None
	PqCompressCompressionServiceNowNone PqCompressCompressionServiceNow = "none"
	// PqCompressCompressionServiceNowGzip Gzip
	PqCompressCompressionServiceNowGzip PqCompressCompressionServiceNow = "gzip"
)

func (e PqCompressCompressionServiceNow) ToPointer() *PqCompressCompressionServiceNow {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionServiceNow) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorServiceNow - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorServiceNow string

const (
	// QueueFullBehaviorServiceNowBlock Block
	QueueFullBehaviorServiceNowBlock QueueFullBehaviorServiceNow = "block"
	// QueueFullBehaviorServiceNowDrop Drop new data
	QueueFullBehaviorServiceNowDrop QueueFullBehaviorServiceNow = "drop"
)

func (e QueueFullBehaviorServiceNow) ToPointer() *QueueFullBehaviorServiceNow {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorServiceNow) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsServiceNow struct {
}

func (p PqControlsServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputServiceNow struct {
	// Unique ID for this output
	ID   string         `json:"id"`
	Type TypeServiceNow `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint where ServiceNow events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)
	Endpoint *string `default:"ingest.lightstep.com:443" json:"endpoint"`
	// Select or create a stored text secret
	TokenSecret   string  `json:"tokenSecret"`
	AuthTokenName *string `default:"lightstep-access-token" json:"authTokenName"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *OTLPVersionServiceNow `default:"1.3.1" json:"otlpVersion"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"2048" json:"maxPayloadSizeKB"`
	// Select a transport option for OpenTelemetry
	Protocol *ProtocolServiceNow `default:"grpc" json:"protocol"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *CompressCompressionServiceNow `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompressionServiceNow `default:"gzip" json:"httpCompress"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []MetadatumServiceNow `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeServiceNow `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorServiceNow `default:"block" json:"onBackpressure"`
	Description    *string                         `json:"description,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderServiceNow `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingServiceNow `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsServiceNow  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool                            `default:"true" json:"responseHonorRetryAfterHeader"`
	TLS                           *TLSSettingsClientSideServiceNow `json:"tls,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeServiceNow `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionServiceNow `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorServiceNow `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsServiceNow        `json:"pqControls,omitempty"`
}

func (o OutputServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "tokenSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputServiceNow) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputServiceNow) GetType() TypeServiceNow {
	if o == nil {
		return TypeServiceNow("")
	}
	return o.Type
}

func (o *OutputServiceNow) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputServiceNow) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputServiceNow) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputServiceNow) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputServiceNow) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputServiceNow) GetTokenSecret() string {
	if o == nil {
		return ""
	}
	return o.TokenSecret
}

func (o *OutputServiceNow) GetAuthTokenName() *string {
	if o == nil {
		return nil
	}
	return o.AuthTokenName
}

func (o *OutputServiceNow) GetOtlpVersion() *OTLPVersionServiceNow {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputServiceNow) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputServiceNow) GetProtocol() *ProtocolServiceNow {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputServiceNow) GetCompress() *CompressCompressionServiceNow {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputServiceNow) GetHTTPCompress() *HTTPCompressCompressionServiceNow {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputServiceNow) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputServiceNow) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputServiceNow) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputServiceNow) GetMetadata() []MetadatumServiceNow {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputServiceNow) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputServiceNow) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputServiceNow) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputServiceNow) GetFailedRequestLoggingMode() *FailedRequestLoggingModeServiceNow {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputServiceNow) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputServiceNow) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputServiceNow) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputServiceNow) GetOnBackpressure() *BackpressureBehaviorServiceNow {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputServiceNow) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputServiceNow) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputServiceNow) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputServiceNow) GetExtraHTTPHeaders() []ExtraHTTPHeaderServiceNow {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputServiceNow) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputServiceNow) GetResponseRetrySettings() []ResponseRetrySettingServiceNow {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputServiceNow) GetTimeoutRetrySettings() *TimeoutRetrySettingsServiceNow {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputServiceNow) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputServiceNow) GetTLS() *TLSSettingsClientSideServiceNow {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputServiceNow) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputServiceNow) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputServiceNow) GetPqMode() *ModeServiceNow {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputServiceNow) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputServiceNow) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputServiceNow) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputServiceNow) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputServiceNow) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputServiceNow) GetPqCompress() *PqCompressCompressionServiceNow {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputServiceNow) GetPqOnBackpressure() *QueueFullBehaviorServiceNow {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputServiceNow) GetPqControls() *PqControlsServiceNow {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeOpenTelemetry string

const (
	CreateOutputTypeOpenTelemetryOpenTelemetry CreateOutputTypeOpenTelemetry = "open_telemetry"
)

func (e CreateOutputTypeOpenTelemetry) ToPointer() *CreateOutputTypeOpenTelemetry {
	return &e
}
func (e *CreateOutputTypeOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "open_telemetry":
		*e = CreateOutputTypeOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeOpenTelemetry: %v", v)
	}
}

// CreateOutputProtocolOpenTelemetry - Select a transport option for OpenTelemetry
type CreateOutputProtocolOpenTelemetry string

const (
	// CreateOutputProtocolOpenTelemetryGrpc gRPC
	CreateOutputProtocolOpenTelemetryGrpc CreateOutputProtocolOpenTelemetry = "grpc"
	// CreateOutputProtocolOpenTelemetryHTTP HTTP
	CreateOutputProtocolOpenTelemetryHTTP CreateOutputProtocolOpenTelemetry = "http"
)

func (e CreateOutputProtocolOpenTelemetry) ToPointer() *CreateOutputProtocolOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputProtocolOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "grpc", "http":
			return true
		}
	}
	return false
}

// CreateOutputOTLPVersionOpenTelemetry - The version of OTLP Protobuf definitions to use when structuring data to send
type CreateOutputOTLPVersionOpenTelemetry string

const (
	// CreateOutputOTLPVersionOpenTelemetryZeroDot10Dot0 0.10.0
	CreateOutputOTLPVersionOpenTelemetryZeroDot10Dot0 CreateOutputOTLPVersionOpenTelemetry = "0.10.0"
	// CreateOutputOTLPVersionOpenTelemetryOneDot3Dot1 1.3.1
	CreateOutputOTLPVersionOpenTelemetryOneDot3Dot1 CreateOutputOTLPVersionOpenTelemetry = "1.3.1"
)

func (e CreateOutputOTLPVersionOpenTelemetry) ToPointer() *CreateOutputOTLPVersionOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputOTLPVersionOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "0.10.0", "1.3.1":
			return true
		}
	}
	return false
}

// CreateOutputCompressCompressionOpenTelemetry - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type CreateOutputCompressCompressionOpenTelemetry string

const (
	// CreateOutputCompressCompressionOpenTelemetryNone None
	CreateOutputCompressCompressionOpenTelemetryNone CreateOutputCompressCompressionOpenTelemetry = "none"
	// CreateOutputCompressCompressionOpenTelemetryDeflate Deflate
	CreateOutputCompressCompressionOpenTelemetryDeflate CreateOutputCompressCompressionOpenTelemetry = "deflate"
	// CreateOutputCompressCompressionOpenTelemetryGzip Gzip
	CreateOutputCompressCompressionOpenTelemetryGzip CreateOutputCompressCompressionOpenTelemetry = "gzip"
)

func (e CreateOutputCompressCompressionOpenTelemetry) ToPointer() *CreateOutputCompressCompressionOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputCompressCompressionOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "deflate", "gzip":
			return true
		}
	}
	return false
}

// HTTPCompressCompressionOpenTelemetry - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompressionOpenTelemetry string

const (
	// HTTPCompressCompressionOpenTelemetryNone None
	HTTPCompressCompressionOpenTelemetryNone HTTPCompressCompressionOpenTelemetry = "none"
	// HTTPCompressCompressionOpenTelemetryGzip Gzip
	HTTPCompressCompressionOpenTelemetryGzip HTTPCompressCompressionOpenTelemetry = "gzip"
)

func (e HTTPCompressCompressionOpenTelemetry) ToPointer() *HTTPCompressCompressionOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *HTTPCompressCompressionOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// CreateOutputAuthenticationTypeOpenTelemetry - OpenTelemetry authentication type
type CreateOutputAuthenticationTypeOpenTelemetry string

const (
	CreateOutputAuthenticationTypeOpenTelemetryNone              CreateOutputAuthenticationTypeOpenTelemetry = "none"
	CreateOutputAuthenticationTypeOpenTelemetryBasic             CreateOutputAuthenticationTypeOpenTelemetry = "basic"
	CreateOutputAuthenticationTypeOpenTelemetryCredentialsSecret CreateOutputAuthenticationTypeOpenTelemetry = "credentialsSecret"
	CreateOutputAuthenticationTypeOpenTelemetryToken             CreateOutputAuthenticationTypeOpenTelemetry = "token"
	CreateOutputAuthenticationTypeOpenTelemetryTextSecret        CreateOutputAuthenticationTypeOpenTelemetry = "textSecret"
	CreateOutputAuthenticationTypeOpenTelemetryOauth             CreateOutputAuthenticationTypeOpenTelemetry = "oauth"
)

func (e CreateOutputAuthenticationTypeOpenTelemetry) ToPointer() *CreateOutputAuthenticationTypeOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationTypeOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "basic", "credentialsSecret", "token", "textSecret", "oauth":
			return true
		}
	}
	return false
}

type CreateOutputMetadatumOpenTelemetry struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (c CreateOutputMetadatumOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputMetadatumOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputMetadatumOpenTelemetry) GetKey() *string {
	if c == nil {
		return nil
	}
	return c.Key
}

func (c *CreateOutputMetadatumOpenTelemetry) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

// FailedRequestLoggingModeOpenTelemetry - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeOpenTelemetry string

const (
	// FailedRequestLoggingModeOpenTelemetryPayload Payload
	FailedRequestLoggingModeOpenTelemetryPayload FailedRequestLoggingModeOpenTelemetry = "payload"
	// FailedRequestLoggingModeOpenTelemetryPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeOpenTelemetryPayloadAndHeaders FailedRequestLoggingModeOpenTelemetry = "payloadAndHeaders"
	// FailedRequestLoggingModeOpenTelemetryNone None
	FailedRequestLoggingModeOpenTelemetryNone FailedRequestLoggingModeOpenTelemetry = "none"
)

func (e FailedRequestLoggingModeOpenTelemetry) ToPointer() *FailedRequestLoggingModeOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

// BackpressureBehaviorOpenTelemetry - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorOpenTelemetry string

const (
	// BackpressureBehaviorOpenTelemetryBlock Block
	BackpressureBehaviorOpenTelemetryBlock BackpressureBehaviorOpenTelemetry = "block"
	// BackpressureBehaviorOpenTelemetryDrop Drop
	BackpressureBehaviorOpenTelemetryDrop BackpressureBehaviorOpenTelemetry = "drop"
	// BackpressureBehaviorOpenTelemetryQueue Persistent Queue
	BackpressureBehaviorOpenTelemetryQueue BackpressureBehaviorOpenTelemetry = "queue"
)

func (e BackpressureBehaviorOpenTelemetry) ToPointer() *BackpressureBehaviorOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type CreateOutputOauthParamOpenTelemetry struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (c CreateOutputOauthParamOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputOauthParamOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputOauthParamOpenTelemetry) GetName() string {
	if c == nil {
		return ""
	}
	return c.Name
}

func (c *CreateOutputOauthParamOpenTelemetry) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

type CreateOutputOauthHeaderOpenTelemetry struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (c CreateOutputOauthHeaderOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputOauthHeaderOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputOauthHeaderOpenTelemetry) GetName() string {
	if c == nil {
		return ""
	}
	return c.Name
}

func (c *CreateOutputOauthHeaderOpenTelemetry) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

type ExtraHTTPHeaderOpenTelemetry struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderOpenTelemetry) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderOpenTelemetry) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

type ResponseRetrySettingOpenTelemetry struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingOpenTelemetry) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingOpenTelemetry) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingOpenTelemetry) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingOpenTelemetry) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsOpenTelemetry struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsOpenTelemetry) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsOpenTelemetry) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsOpenTelemetry) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsOpenTelemetry) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type CreateOutputMinimumTLSVersionOpenTelemetry string

const (
	CreateOutputMinimumTLSVersionOpenTelemetryTlSv1  CreateOutputMinimumTLSVersionOpenTelemetry = "TLSv1"
	CreateOutputMinimumTLSVersionOpenTelemetryTlSv11 CreateOutputMinimumTLSVersionOpenTelemetry = "TLSv1.1"
	CreateOutputMinimumTLSVersionOpenTelemetryTlSv12 CreateOutputMinimumTLSVersionOpenTelemetry = "TLSv1.2"
	CreateOutputMinimumTLSVersionOpenTelemetryTlSv13 CreateOutputMinimumTLSVersionOpenTelemetry = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionOpenTelemetry) ToPointer() *CreateOutputMinimumTLSVersionOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMinimumTLSVersionOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputMaximumTLSVersionOpenTelemetry string

const (
	CreateOutputMaximumTLSVersionOpenTelemetryTlSv1  CreateOutputMaximumTLSVersionOpenTelemetry = "TLSv1"
	CreateOutputMaximumTLSVersionOpenTelemetryTlSv11 CreateOutputMaximumTLSVersionOpenTelemetry = "TLSv1.1"
	CreateOutputMaximumTLSVersionOpenTelemetryTlSv12 CreateOutputMaximumTLSVersionOpenTelemetry = "TLSv1.2"
	CreateOutputMaximumTLSVersionOpenTelemetryTlSv13 CreateOutputMaximumTLSVersionOpenTelemetry = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionOpenTelemetry) ToPointer() *CreateOutputMaximumTLSVersionOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMaximumTLSVersionOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideOpenTelemetry struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                     `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionOpenTelemetry `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionOpenTelemetry `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideOpenTelemetry) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideOpenTelemetry) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideOpenTelemetry) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideOpenTelemetry) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideOpenTelemetry) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideOpenTelemetry) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideOpenTelemetry) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideOpenTelemetry) GetMinVersion() *CreateOutputMinimumTLSVersionOpenTelemetry {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideOpenTelemetry) GetMaxVersion() *CreateOutputMaximumTLSVersionOpenTelemetry {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// CreateOutputModeOpenTelemetry - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeOpenTelemetry string

const (
	// CreateOutputModeOpenTelemetryError Error
	CreateOutputModeOpenTelemetryError CreateOutputModeOpenTelemetry = "error"
	// CreateOutputModeOpenTelemetryAlways Backpressure
	CreateOutputModeOpenTelemetryAlways CreateOutputModeOpenTelemetry = "always"
	// CreateOutputModeOpenTelemetryBackpressure Always On
	CreateOutputModeOpenTelemetryBackpressure CreateOutputModeOpenTelemetry = "backpressure"
)

func (e CreateOutputModeOpenTelemetry) ToPointer() *CreateOutputModeOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionOpenTelemetry - Codec to use to compress the persisted data
type PqCompressCompressionOpenTelemetry string

const (
	// PqCompressCompressionOpenTelemetryNone None
	PqCompressCompressionOpenTelemetryNone PqCompressCompressionOpenTelemetry = "none"
	// PqCompressCompressionOpenTelemetryGzip Gzip
	PqCompressCompressionOpenTelemetryGzip PqCompressCompressionOpenTelemetry = "gzip"
)

func (e PqCompressCompressionOpenTelemetry) ToPointer() *PqCompressCompressionOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorOpenTelemetry - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorOpenTelemetry string

const (
	// QueueFullBehaviorOpenTelemetryBlock Block
	QueueFullBehaviorOpenTelemetryBlock QueueFullBehaviorOpenTelemetry = "block"
	// QueueFullBehaviorOpenTelemetryDrop Drop new data
	QueueFullBehaviorOpenTelemetryDrop QueueFullBehaviorOpenTelemetry = "drop"
)

func (e QueueFullBehaviorOpenTelemetry) ToPointer() *QueueFullBehaviorOpenTelemetry {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorOpenTelemetry) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsOpenTelemetry struct {
}

func (c CreateOutputPqControlsOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputOpenTelemetry struct {
	// Unique ID for this output
	ID   string                        `json:"id"`
	Type CreateOutputTypeOpenTelemetry `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select a transport option for OpenTelemetry
	Protocol *CreateOutputProtocolOpenTelemetry `default:"grpc" json:"protocol"`
	// The endpoint where OTel events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Unspecified ports will default to 4317, unless the endpoint is an HTTPS-based URL or TLS is enabled, in which case 443 will be used.
	Endpoint string `json:"endpoint"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *CreateOutputOTLPVersionOpenTelemetry `default:"0.10.0" json:"otlpVersion"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *CreateOutputCompressCompressionOpenTelemetry `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompressionOpenTelemetry `default:"gzip" json:"httpCompress"`
	// OpenTelemetry authentication type
	AuthType *CreateOutputAuthenticationTypeOpenTelemetry `default:"none" json:"authType"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []CreateOutputMetadatumOpenTelemetry `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeOpenTelemetry `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorOpenTelemetry `default:"block" json:"onBackpressure"`
	Description    *string                            `json:"description,omitempty"`
	Username       *string                            `json:"username,omitempty"`
	Password       *string                            `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []CreateOutputOauthParamOpenTelemetry `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []CreateOutputOauthHeaderOpenTelemetry `json:"oauthHeaders,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderOpenTelemetry `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingOpenTelemetry `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsOpenTelemetry  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool                               `default:"true" json:"responseHonorRetryAfterHeader"`
	TLS                           *TLSSettingsClientSideOpenTelemetry `json:"tls,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeOpenTelemetry `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionOpenTelemetry `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorOpenTelemetry      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsOpenTelemetry `json:"pqControls,omitempty"`
}

func (o OutputOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "endpoint"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputOpenTelemetry) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputOpenTelemetry) GetType() CreateOutputTypeOpenTelemetry {
	if o == nil {
		return CreateOutputTypeOpenTelemetry("")
	}
	return o.Type
}

func (o *OutputOpenTelemetry) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputOpenTelemetry) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputOpenTelemetry) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputOpenTelemetry) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputOpenTelemetry) GetProtocol() *CreateOutputProtocolOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputOpenTelemetry) GetEndpoint() string {
	if o == nil {
		return ""
	}
	return o.Endpoint
}

func (o *OutputOpenTelemetry) GetOtlpVersion() *CreateOutputOTLPVersionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputOpenTelemetry) GetCompress() *CreateOutputCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputOpenTelemetry) GetHTTPCompress() *HTTPCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputOpenTelemetry) GetAuthType() *CreateOutputAuthenticationTypeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputOpenTelemetry) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputOpenTelemetry) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputOpenTelemetry) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputOpenTelemetry) GetMetadata() []CreateOutputMetadatumOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputOpenTelemetry) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputOpenTelemetry) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputOpenTelemetry) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputOpenTelemetry) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputOpenTelemetry) GetFailedRequestLoggingMode() *FailedRequestLoggingModeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputOpenTelemetry) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputOpenTelemetry) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputOpenTelemetry) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputOpenTelemetry) GetOnBackpressure() *BackpressureBehaviorOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputOpenTelemetry) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputOpenTelemetry) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputOpenTelemetry) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputOpenTelemetry) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputOpenTelemetry) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputOpenTelemetry) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputOpenTelemetry) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputOpenTelemetry) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputOpenTelemetry) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputOpenTelemetry) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputOpenTelemetry) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputOpenTelemetry) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputOpenTelemetry) GetOauthParams() []CreateOutputOauthParamOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputOpenTelemetry) GetOauthHeaders() []CreateOutputOauthHeaderOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputOpenTelemetry) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputOpenTelemetry) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputOpenTelemetry) GetExtraHTTPHeaders() []ExtraHTTPHeaderOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputOpenTelemetry) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputOpenTelemetry) GetResponseRetrySettings() []ResponseRetrySettingOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputOpenTelemetry) GetTimeoutRetrySettings() *TimeoutRetrySettingsOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputOpenTelemetry) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputOpenTelemetry) GetTLS() *TLSSettingsClientSideOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputOpenTelemetry) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputOpenTelemetry) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputOpenTelemetry) GetPqMode() *CreateOutputModeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputOpenTelemetry) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputOpenTelemetry) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputOpenTelemetry) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputOpenTelemetry) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputOpenTelemetry) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputOpenTelemetry) GetPqCompress() *PqCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputOpenTelemetry) GetPqOnBackpressure() *QueueFullBehaviorOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputOpenTelemetry) GetPqControls() *CreateOutputPqControlsOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeRing string

const (
	TypeRingRing TypeRing = "ring"
)

func (e TypeRing) ToPointer() *TypeRing {
	return &e
}
func (e *TypeRing) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "ring":
		*e = TypeRing(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeRing: %v", v)
	}
}

// DataFormatRing - Format of the output data.
type DataFormatRing string

const (
	DataFormatRingJSON DataFormatRing = "json"
	DataFormatRingRaw  DataFormatRing = "raw"
)

func (e DataFormatRing) ToPointer() *DataFormatRing {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataFormatRing) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw":
			return true
		}
	}
	return false
}

type CreateOutputDataCompressionFormat string

const (
	CreateOutputDataCompressionFormatNone CreateOutputDataCompressionFormat = "none"
	CreateOutputDataCompressionFormatGzip CreateOutputDataCompressionFormat = "gzip"
)

func (e CreateOutputDataCompressionFormat) ToPointer() *CreateOutputDataCompressionFormat {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputDataCompressionFormat) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// BackpressureBehaviorRing - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorRing string

const (
	// BackpressureBehaviorRingBlock Block
	BackpressureBehaviorRingBlock BackpressureBehaviorRing = "block"
	// BackpressureBehaviorRingDrop Drop
	BackpressureBehaviorRingDrop BackpressureBehaviorRing = "drop"
)

func (e BackpressureBehaviorRing) ToPointer() *BackpressureBehaviorRing {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorRing) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type OutputRing struct {
	// Unique ID for this output
	ID   string   `json:"id"`
	Type TypeRing `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Format of the output data.
	Format *DataFormatRing `default:"json" json:"format"`
	// JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition.
	PartitionExpr *string `json:"partitionExpr,omitempty"`
	// Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.
	MaxDataSize *string `default:"1GB" json:"maxDataSize"`
	// Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.
	MaxDataTime *string                            `default:"24h" json:"maxDataTime"`
	Compress    *CreateOutputDataCompressionFormat `default:"gzip" json:"compress"`
	// Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>
	DestPath *string `json:"destPath,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorRing `default:"block" json:"onBackpressure"`
	Description    *string                   `json:"description,omitempty"`
}

func (o OutputRing) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputRing) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputRing) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputRing) GetType() TypeRing {
	if o == nil {
		return TypeRing("")
	}
	return o.Type
}

func (o *OutputRing) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputRing) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputRing) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputRing) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputRing) GetFormat() *DataFormatRing {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputRing) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputRing) GetMaxDataSize() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataSize
}

func (o *OutputRing) GetMaxDataTime() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataTime
}

func (o *OutputRing) GetCompress() *CreateOutputDataCompressionFormat {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputRing) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputRing) GetOnBackpressure() *BackpressureBehaviorRing {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputRing) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

type CreateOutputTypePrometheus string

const (
	CreateOutputTypePrometheusPrometheus CreateOutputTypePrometheus = "prometheus"
)

func (e CreateOutputTypePrometheus) ToPointer() *CreateOutputTypePrometheus {
	return &e
}
func (e *CreateOutputTypePrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "prometheus":
		*e = CreateOutputTypePrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypePrometheus: %v", v)
	}
}

type ExtraHTTPHeaderPrometheus struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderPrometheus) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderPrometheus) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModePrometheus - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModePrometheus string

const (
	// FailedRequestLoggingModePrometheusPayload Payload
	FailedRequestLoggingModePrometheusPayload FailedRequestLoggingModePrometheus = "payload"
	// FailedRequestLoggingModePrometheusPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModePrometheusPayloadAndHeaders FailedRequestLoggingModePrometheus = "payloadAndHeaders"
	// FailedRequestLoggingModePrometheusNone None
	FailedRequestLoggingModePrometheusNone FailedRequestLoggingModePrometheus = "none"
)

func (e FailedRequestLoggingModePrometheus) ToPointer() *FailedRequestLoggingModePrometheus {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModePrometheus) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingPrometheus struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingPrometheus) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingPrometheus) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingPrometheus) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingPrometheus) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsPrometheus struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsPrometheus) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsPrometheus) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsPrometheus) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsPrometheus) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorPrometheus - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorPrometheus string

const (
	// BackpressureBehaviorPrometheusBlock Block
	BackpressureBehaviorPrometheusBlock BackpressureBehaviorPrometheus = "block"
	// BackpressureBehaviorPrometheusDrop Drop
	BackpressureBehaviorPrometheusDrop BackpressureBehaviorPrometheus = "drop"
	// BackpressureBehaviorPrometheusQueue Persistent Queue
	BackpressureBehaviorPrometheusQueue BackpressureBehaviorPrometheus = "queue"
)

func (e BackpressureBehaviorPrometheus) ToPointer() *BackpressureBehaviorPrometheus {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorPrometheus) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// AuthenticationTypePrometheus - Remote Write authentication type
type AuthenticationTypePrometheus string

const (
	AuthenticationTypePrometheusNone              AuthenticationTypePrometheus = "none"
	AuthenticationTypePrometheusBasic             AuthenticationTypePrometheus = "basic"
	AuthenticationTypePrometheusCredentialsSecret AuthenticationTypePrometheus = "credentialsSecret"
	AuthenticationTypePrometheusToken             AuthenticationTypePrometheus = "token"
	AuthenticationTypePrometheusTextSecret        AuthenticationTypePrometheus = "textSecret"
	AuthenticationTypePrometheusOauth             AuthenticationTypePrometheus = "oauth"
)

func (e AuthenticationTypePrometheus) ToPointer() *AuthenticationTypePrometheus {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationTypePrometheus) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "basic", "credentialsSecret", "token", "textSecret", "oauth":
			return true
		}
	}
	return false
}

// CreateOutputModePrometheus - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModePrometheus string

const (
	// CreateOutputModePrometheusError Error
	CreateOutputModePrometheusError CreateOutputModePrometheus = "error"
	// CreateOutputModePrometheusAlways Backpressure
	CreateOutputModePrometheusAlways CreateOutputModePrometheus = "always"
	// CreateOutputModePrometheusBackpressure Always On
	CreateOutputModePrometheusBackpressure CreateOutputModePrometheus = "backpressure"
)

func (e CreateOutputModePrometheus) ToPointer() *CreateOutputModePrometheus {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModePrometheus) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionPrometheus - Codec to use to compress the persisted data
type PqCompressCompressionPrometheus string

const (
	// PqCompressCompressionPrometheusNone None
	PqCompressCompressionPrometheusNone PqCompressCompressionPrometheus = "none"
	// PqCompressCompressionPrometheusGzip Gzip
	PqCompressCompressionPrometheusGzip PqCompressCompressionPrometheus = "gzip"
)

func (e PqCompressCompressionPrometheus) ToPointer() *PqCompressCompressionPrometheus {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionPrometheus) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorPrometheus - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorPrometheus string

const (
	// QueueFullBehaviorPrometheusBlock Block
	QueueFullBehaviorPrometheusBlock QueueFullBehaviorPrometheus = "block"
	// QueueFullBehaviorPrometheusDrop Drop new data
	QueueFullBehaviorPrometheusDrop QueueFullBehaviorPrometheus = "drop"
)

func (e QueueFullBehaviorPrometheus) ToPointer() *QueueFullBehaviorPrometheus {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorPrometheus) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsPrometheus struct {
}

func (c CreateOutputPqControlsPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OauthParamPrometheus struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o OauthParamPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthParamPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthParamPrometheus) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamPrometheus) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderPrometheus struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o OauthHeaderPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthHeaderPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthHeaderPrometheus) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderPrometheus) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputPrometheus struct {
	// Unique ID for this output
	ID   string                     `json:"id"`
	Type CreateOutputTypePrometheus `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions to generated metrics.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send metrics to
	URL string `json:"url"`
	// JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.
	MetricRenameExpr *string `default:"name.replace(/[^a-zA-Z0-9_]/g, '_')" json:"metricRenameExpr"`
	// Generate and send metadata (`type` and `metricFamilyName`) requests
	SendMetadata *bool `default:"true" json:"sendMetadata"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderPrometheus `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModePrometheus `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingPrometheus `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsPrometheus  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorPrometheus `default:"block" json:"onBackpressure"`
	// Remote Write authentication type
	AuthType    *AuthenticationTypePrometheus `default:"none" json:"authType"`
	Description *string                       `json:"description,omitempty"`
	// How frequently metrics metadata is sent out. Value cannot be smaller than the base Flush period set above.
	MetricsFlushPeriodSec *float64 `default:"60" json:"metricsFlushPeriodSec"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModePrometheus `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionPrometheus `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorPrometheus      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsPrometheus `json:"pqControls,omitempty"`
	Username         *string                           `json:"username,omitempty"`
	Password         *string                           `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamPrometheus `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderPrometheus `json:"oauthHeaders,omitempty"`
}

func (o OutputPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "url"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputPrometheus) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputPrometheus) GetType() CreateOutputTypePrometheus {
	if o == nil {
		return CreateOutputTypePrometheus("")
	}
	return o.Type
}

func (o *OutputPrometheus) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputPrometheus) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputPrometheus) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputPrometheus) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputPrometheus) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputPrometheus) GetMetricRenameExpr() *string {
	if o == nil {
		return nil
	}
	return o.MetricRenameExpr
}

func (o *OutputPrometheus) GetSendMetadata() *bool {
	if o == nil {
		return nil
	}
	return o.SendMetadata
}

func (o *OutputPrometheus) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputPrometheus) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputPrometheus) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputPrometheus) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputPrometheus) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputPrometheus) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputPrometheus) GetExtraHTTPHeaders() []ExtraHTTPHeaderPrometheus {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputPrometheus) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputPrometheus) GetFailedRequestLoggingMode() *FailedRequestLoggingModePrometheus {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputPrometheus) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputPrometheus) GetResponseRetrySettings() []ResponseRetrySettingPrometheus {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputPrometheus) GetTimeoutRetrySettings() *TimeoutRetrySettingsPrometheus {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputPrometheus) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputPrometheus) GetOnBackpressure() *BackpressureBehaviorPrometheus {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputPrometheus) GetAuthType() *AuthenticationTypePrometheus {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputPrometheus) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputPrometheus) GetMetricsFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MetricsFlushPeriodSec
}

func (o *OutputPrometheus) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputPrometheus) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputPrometheus) GetPqMode() *CreateOutputModePrometheus {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputPrometheus) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputPrometheus) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputPrometheus) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputPrometheus) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputPrometheus) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputPrometheus) GetPqCompress() *PqCompressCompressionPrometheus {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputPrometheus) GetPqOnBackpressure() *QueueFullBehaviorPrometheus {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputPrometheus) GetPqControls() *CreateOutputPqControlsPrometheus {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputPrometheus) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputPrometheus) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputPrometheus) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputPrometheus) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputPrometheus) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputPrometheus) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputPrometheus) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputPrometheus) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputPrometheus) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputPrometheus) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputPrometheus) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputPrometheus) GetOauthParams() []OauthParamPrometheus {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputPrometheus) GetOauthHeaders() []OauthHeaderPrometheus {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

type CreateOutputTypeLoki string

const (
	CreateOutputTypeLokiLoki CreateOutputTypeLoki = "loki"
)

func (e CreateOutputTypeLoki) ToPointer() *CreateOutputTypeLoki {
	return &e
}
func (e *CreateOutputTypeLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "loki":
		*e = CreateOutputTypeLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeLoki: %v", v)
	}
}

// MessageFormatLoki - Format to use when sending logs to Loki (Protobuf or JSON)
type MessageFormatLoki string

const (
	// MessageFormatLokiProtobuf Protobuf
	MessageFormatLokiProtobuf MessageFormatLoki = "protobuf"
	// MessageFormatLokiJSON JSON
	MessageFormatLokiJSON MessageFormatLoki = "json"
)

func (e MessageFormatLoki) ToPointer() *MessageFormatLoki {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MessageFormatLoki) IsExact() bool {
	if e != nil {
		switch *e {
		case "protobuf", "json":
			return true
		}
	}
	return false
}

type LabelLoki struct {
	Name  *string `default:"" json:"name"`
	Value string  `json:"value"`
}

func (l LabelLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(l, "", false)
}

func (l *LabelLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &l, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (l *LabelLoki) GetName() *string {
	if l == nil {
		return nil
	}
	return l.Name
}

func (l *LabelLoki) GetValue() string {
	if l == nil {
		return ""
	}
	return l.Value
}

type CreateOutputAuthenticationTypeLoki string

const (
	// CreateOutputAuthenticationTypeLokiNone None
	CreateOutputAuthenticationTypeLokiNone CreateOutputAuthenticationTypeLoki = "none"
	// CreateOutputAuthenticationTypeLokiToken Auth token
	CreateOutputAuthenticationTypeLokiToken CreateOutputAuthenticationTypeLoki = "token"
	// CreateOutputAuthenticationTypeLokiTextSecret Auth token (text secret)
	CreateOutputAuthenticationTypeLokiTextSecret CreateOutputAuthenticationTypeLoki = "textSecret"
	// CreateOutputAuthenticationTypeLokiBasic Basic
	CreateOutputAuthenticationTypeLokiBasic CreateOutputAuthenticationTypeLoki = "basic"
	// CreateOutputAuthenticationTypeLokiCredentialsSecret Basic (credentials secret)
	CreateOutputAuthenticationTypeLokiCredentialsSecret CreateOutputAuthenticationTypeLoki = "credentialsSecret"
)

func (e CreateOutputAuthenticationTypeLoki) ToPointer() *CreateOutputAuthenticationTypeLoki {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationTypeLoki) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "token", "textSecret", "basic", "credentialsSecret":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderLoki struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderLoki) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderLoki) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeLoki - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeLoki string

const (
	// FailedRequestLoggingModeLokiPayload Payload
	FailedRequestLoggingModeLokiPayload FailedRequestLoggingModeLoki = "payload"
	// FailedRequestLoggingModeLokiPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeLokiPayloadAndHeaders FailedRequestLoggingModeLoki = "payloadAndHeaders"
	// FailedRequestLoggingModeLokiNone None
	FailedRequestLoggingModeLokiNone FailedRequestLoggingModeLoki = "none"
)

func (e FailedRequestLoggingModeLoki) ToPointer() *FailedRequestLoggingModeLoki {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeLoki) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingLoki struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingLoki) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingLoki) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingLoki) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingLoki) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsLoki struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsLoki) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsLoki) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsLoki) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsLoki) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorLoki - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorLoki string

const (
	// BackpressureBehaviorLokiBlock Block
	BackpressureBehaviorLokiBlock BackpressureBehaviorLoki = "block"
	// BackpressureBehaviorLokiDrop Drop
	BackpressureBehaviorLokiDrop BackpressureBehaviorLoki = "drop"
	// BackpressureBehaviorLokiQueue Persistent Queue
	BackpressureBehaviorLokiQueue BackpressureBehaviorLoki = "queue"
)

func (e BackpressureBehaviorLoki) ToPointer() *BackpressureBehaviorLoki {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorLoki) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// CreateOutputModeLoki - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeLoki string

const (
	// CreateOutputModeLokiError Error
	CreateOutputModeLokiError CreateOutputModeLoki = "error"
	// CreateOutputModeLokiAlways Backpressure
	CreateOutputModeLokiAlways CreateOutputModeLoki = "always"
	// CreateOutputModeLokiBackpressure Always On
	CreateOutputModeLokiBackpressure CreateOutputModeLoki = "backpressure"
)

func (e CreateOutputModeLoki) ToPointer() *CreateOutputModeLoki {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeLoki) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionLoki - Codec to use to compress the persisted data
type PqCompressCompressionLoki string

const (
	// PqCompressCompressionLokiNone None
	PqCompressCompressionLokiNone PqCompressCompressionLoki = "none"
	// PqCompressCompressionLokiGzip Gzip
	PqCompressCompressionLokiGzip PqCompressCompressionLoki = "gzip"
)

func (e PqCompressCompressionLoki) ToPointer() *PqCompressCompressionLoki {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionLoki) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorLoki - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorLoki string

const (
	// QueueFullBehaviorLokiBlock Block
	QueueFullBehaviorLokiBlock QueueFullBehaviorLoki = "block"
	// QueueFullBehaviorLokiDrop Drop new data
	QueueFullBehaviorLokiDrop QueueFullBehaviorLoki = "drop"
)

func (e QueueFullBehaviorLoki) ToPointer() *QueueFullBehaviorLoki {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorLoki) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsLoki struct {
}

func (c CreateOutputPqControlsLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputLoki struct {
	// Unique ID for this output
	ID   string               `json:"id"`
	Type CreateOutputTypeLoki `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as labels to generated logs.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send logs to
	URL string `json:"url"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Format to use when sending logs to Loki (Protobuf or JSON)
	MessageFormat *MessageFormatLoki `default:"protobuf" json:"messageFormat"`
	// List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'
	Labels   []LabelLoki                         `json:"labels,omitempty"`
	AuthType *CreateOutputAuthenticationTypeLoki `default:"none" json:"authType"`
	// Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki to complain about entries being delivered out of order.
	Concurrency *float64 `default:"1" json:"concurrency"`
	// Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Defaults to 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	FlushPeriodSec *float64 `default:"15" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderLoki `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeLoki `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingLoki `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsLoki  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Add per-event HTTP headers from the __headers field to outgoing requests. Events with different headers are batched and sent separately.
	EnableDynamicHeaders *bool `default:"false" json:"enableDynamicHeaders"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorLoki `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeLoki `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionLoki `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorLoki      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsLoki `json:"pqControls,omitempty"`
}

func (o OutputLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "url"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputLoki) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputLoki) GetType() CreateOutputTypeLoki {
	if o == nil {
		return CreateOutputTypeLoki("")
	}
	return o.Type
}

func (o *OutputLoki) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputLoki) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputLoki) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputLoki) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputLoki) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputLoki) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputLoki) GetMessageFormat() *MessageFormatLoki {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputLoki) GetLabels() []LabelLoki {
	if o == nil {
		return nil
	}
	return o.Labels
}

func (o *OutputLoki) GetAuthType() *CreateOutputAuthenticationTypeLoki {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputLoki) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputLoki) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputLoki) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputLoki) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputLoki) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputLoki) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputLoki) GetExtraHTTPHeaders() []ExtraHTTPHeaderLoki {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputLoki) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputLoki) GetFailedRequestLoggingMode() *FailedRequestLoggingModeLoki {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputLoki) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputLoki) GetResponseRetrySettings() []ResponseRetrySettingLoki {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputLoki) GetTimeoutRetrySettings() *TimeoutRetrySettingsLoki {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputLoki) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputLoki) GetEnableDynamicHeaders() *bool {
	if o == nil {
		return nil
	}
	return o.EnableDynamicHeaders
}

func (o *OutputLoki) GetOnBackpressure() *BackpressureBehaviorLoki {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputLoki) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputLoki) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputLoki) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputLoki) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputLoki) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputLoki) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputLoki) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputLoki) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputLoki) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputLoki) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputLoki) GetPqMode() *CreateOutputModeLoki {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputLoki) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputLoki) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputLoki) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputLoki) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputLoki) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputLoki) GetPqCompress() *PqCompressCompressionLoki {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputLoki) GetPqOnBackpressure() *QueueFullBehaviorLoki {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputLoki) GetPqControls() *CreateOutputPqControlsLoki {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type OutputGrafanaCloudType2 string

const (
	OutputGrafanaCloudType2GrafanaCloud OutputGrafanaCloudType2 = "grafana_cloud"
)

func (e OutputGrafanaCloudType2) ToPointer() *OutputGrafanaCloudType2 {
	return &e
}
func (e *OutputGrafanaCloudType2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grafana_cloud":
		*e = OutputGrafanaCloudType2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudType2: %v", v)
	}
}

// OutputGrafanaCloudMessageFormat2 - Format to use when sending logs to Loki (Protobuf or JSON)
type OutputGrafanaCloudMessageFormat2 string

const (
	// OutputGrafanaCloudMessageFormat2Protobuf Protobuf
	OutputGrafanaCloudMessageFormat2Protobuf OutputGrafanaCloudMessageFormat2 = "protobuf"
	// OutputGrafanaCloudMessageFormat2JSON JSON
	OutputGrafanaCloudMessageFormat2JSON OutputGrafanaCloudMessageFormat2 = "json"
)

func (e OutputGrafanaCloudMessageFormat2) ToPointer() *OutputGrafanaCloudMessageFormat2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudMessageFormat2) IsExact() bool {
	if e != nil {
		switch *e {
		case "protobuf", "json":
			return true
		}
	}
	return false
}

type OutputGrafanaCloudLabel2 struct {
	Name  *string `default:"" json:"name"`
	Value string  `json:"value"`
}

func (o OutputGrafanaCloudLabel2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudLabel2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudLabel2) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudLabel2) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputGrafanaCloudPrometheusAuthAuthenticationType2 string

const (
	// OutputGrafanaCloudPrometheusAuthAuthenticationType2None None
	OutputGrafanaCloudPrometheusAuthAuthenticationType2None OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "none"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType2Token Auth token
	OutputGrafanaCloudPrometheusAuthAuthenticationType2Token OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "token"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType2TextSecret Auth token (text secret)
	OutputGrafanaCloudPrometheusAuthAuthenticationType2TextSecret OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "textSecret"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType2Basic Basic
	OutputGrafanaCloudPrometheusAuthAuthenticationType2Basic OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "basic"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType2CredentialsSecret Basic (credentials secret)
	OutputGrafanaCloudPrometheusAuthAuthenticationType2CredentialsSecret OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "credentialsSecret"
)

func (e OutputGrafanaCloudPrometheusAuthAuthenticationType2) ToPointer() *OutputGrafanaCloudPrometheusAuthAuthenticationType2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudPrometheusAuthAuthenticationType2) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "token", "textSecret", "basic", "credentialsSecret":
			return true
		}
	}
	return false
}

type CreateOutputPrometheusAuth2 struct {
	AuthType *OutputGrafanaCloudPrometheusAuthAuthenticationType2 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (c CreateOutputPrometheusAuth2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPrometheusAuth2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputPrometheusAuth2) GetAuthType() *OutputGrafanaCloudPrometheusAuthAuthenticationType2 {
	if c == nil {
		return nil
	}
	return c.AuthType
}

func (c *CreateOutputPrometheusAuth2) GetToken() *string {
	if c == nil {
		return nil
	}
	return c.Token
}

func (c *CreateOutputPrometheusAuth2) GetTextSecret() *string {
	if c == nil {
		return nil
	}
	return c.TextSecret
}

func (c *CreateOutputPrometheusAuth2) GetUsername() *string {
	if c == nil {
		return nil
	}
	return c.Username
}

func (c *CreateOutputPrometheusAuth2) GetPassword() *string {
	if c == nil {
		return nil
	}
	return c.Password
}

func (c *CreateOutputPrometheusAuth2) GetCredentialsSecret() *string {
	if c == nil {
		return nil
	}
	return c.CredentialsSecret
}

type OutputGrafanaCloudLokiAuthAuthenticationType2 string

const (
	// OutputGrafanaCloudLokiAuthAuthenticationType2None None
	OutputGrafanaCloudLokiAuthAuthenticationType2None OutputGrafanaCloudLokiAuthAuthenticationType2 = "none"
	// OutputGrafanaCloudLokiAuthAuthenticationType2Token Auth token
	OutputGrafanaCloudLokiAuthAuthenticationType2Token OutputGrafanaCloudLokiAuthAuthenticationType2 = "token"
	// OutputGrafanaCloudLokiAuthAuthenticationType2TextSecret Auth token (text secret)
	OutputGrafanaCloudLokiAuthAuthenticationType2TextSecret OutputGrafanaCloudLokiAuthAuthenticationType2 = "textSecret"
	// OutputGrafanaCloudLokiAuthAuthenticationType2Basic Basic
	OutputGrafanaCloudLokiAuthAuthenticationType2Basic OutputGrafanaCloudLokiAuthAuthenticationType2 = "basic"
	// OutputGrafanaCloudLokiAuthAuthenticationType2CredentialsSecret Basic (credentials secret)
	OutputGrafanaCloudLokiAuthAuthenticationType2CredentialsSecret OutputGrafanaCloudLokiAuthAuthenticationType2 = "credentialsSecret"
)

func (e OutputGrafanaCloudLokiAuthAuthenticationType2) ToPointer() *OutputGrafanaCloudLokiAuthAuthenticationType2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudLokiAuthAuthenticationType2) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "token", "textSecret", "basic", "credentialsSecret":
			return true
		}
	}
	return false
}

type CreateOutputLokiAuth2 struct {
	AuthType *OutputGrafanaCloudLokiAuthAuthenticationType2 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (c CreateOutputLokiAuth2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputLokiAuth2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputLokiAuth2) GetAuthType() *OutputGrafanaCloudLokiAuthAuthenticationType2 {
	if c == nil {
		return nil
	}
	return c.AuthType
}

func (c *CreateOutputLokiAuth2) GetToken() *string {
	if c == nil {
		return nil
	}
	return c.Token
}

func (c *CreateOutputLokiAuth2) GetTextSecret() *string {
	if c == nil {
		return nil
	}
	return c.TextSecret
}

func (c *CreateOutputLokiAuth2) GetUsername() *string {
	if c == nil {
		return nil
	}
	return c.Username
}

func (c *CreateOutputLokiAuth2) GetPassword() *string {
	if c == nil {
		return nil
	}
	return c.Password
}

func (c *CreateOutputLokiAuth2) GetCredentialsSecret() *string {
	if c == nil {
		return nil
	}
	return c.CredentialsSecret
}

type OutputGrafanaCloudExtraHTTPHeader2 struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o OutputGrafanaCloudExtraHTTPHeader2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudExtraHTTPHeader2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudExtraHTTPHeader2) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudExtraHTTPHeader2) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// OutputGrafanaCloudFailedRequestLoggingMode2 - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type OutputGrafanaCloudFailedRequestLoggingMode2 string

const (
	// OutputGrafanaCloudFailedRequestLoggingMode2Payload Payload
	OutputGrafanaCloudFailedRequestLoggingMode2Payload OutputGrafanaCloudFailedRequestLoggingMode2 = "payload"
	// OutputGrafanaCloudFailedRequestLoggingMode2PayloadAndHeaders Payload + Headers
	OutputGrafanaCloudFailedRequestLoggingMode2PayloadAndHeaders OutputGrafanaCloudFailedRequestLoggingMode2 = "payloadAndHeaders"
	// OutputGrafanaCloudFailedRequestLoggingMode2None None
	OutputGrafanaCloudFailedRequestLoggingMode2None OutputGrafanaCloudFailedRequestLoggingMode2 = "none"
)

func (e OutputGrafanaCloudFailedRequestLoggingMode2) ToPointer() *OutputGrafanaCloudFailedRequestLoggingMode2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudFailedRequestLoggingMode2) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type OutputGrafanaCloudResponseRetrySetting2 struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudResponseRetrySetting2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudResponseRetrySetting2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type OutputGrafanaCloudTimeoutRetrySettings2 struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudTimeoutRetrySettings2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// OutputGrafanaCloudBackpressureBehavior2 - How to handle events when all receivers are exerting backpressure
type OutputGrafanaCloudBackpressureBehavior2 string

const (
	// OutputGrafanaCloudBackpressureBehavior2Block Block
	OutputGrafanaCloudBackpressureBehavior2Block OutputGrafanaCloudBackpressureBehavior2 = "block"
	// OutputGrafanaCloudBackpressureBehavior2Drop Drop
	OutputGrafanaCloudBackpressureBehavior2Drop OutputGrafanaCloudBackpressureBehavior2 = "drop"
	// OutputGrafanaCloudBackpressureBehavior2Queue Persistent Queue
	OutputGrafanaCloudBackpressureBehavior2Queue OutputGrafanaCloudBackpressureBehavior2 = "queue"
)

func (e OutputGrafanaCloudBackpressureBehavior2) ToPointer() *OutputGrafanaCloudBackpressureBehavior2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudBackpressureBehavior2) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// OutputGrafanaCloudMode2 - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputGrafanaCloudMode2 string

const (
	// OutputGrafanaCloudMode2Error Error
	OutputGrafanaCloudMode2Error OutputGrafanaCloudMode2 = "error"
	// OutputGrafanaCloudMode2Always Backpressure
	OutputGrafanaCloudMode2Always OutputGrafanaCloudMode2 = "always"
	// OutputGrafanaCloudMode2Backpressure Always On
	OutputGrafanaCloudMode2Backpressure OutputGrafanaCloudMode2 = "backpressure"
)

func (e OutputGrafanaCloudMode2) ToPointer() *OutputGrafanaCloudMode2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudMode2) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// OutputGrafanaCloudCompression2 - Codec to use to compress the persisted data
type OutputGrafanaCloudCompression2 string

const (
	// OutputGrafanaCloudCompression2None None
	OutputGrafanaCloudCompression2None OutputGrafanaCloudCompression2 = "none"
	// OutputGrafanaCloudCompression2Gzip Gzip
	OutputGrafanaCloudCompression2Gzip OutputGrafanaCloudCompression2 = "gzip"
)

func (e OutputGrafanaCloudCompression2) ToPointer() *OutputGrafanaCloudCompression2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudCompression2) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// OutputGrafanaCloudQueueFullBehavior2 - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type OutputGrafanaCloudQueueFullBehavior2 string

const (
	// OutputGrafanaCloudQueueFullBehavior2Block Block
	OutputGrafanaCloudQueueFullBehavior2Block OutputGrafanaCloudQueueFullBehavior2 = "block"
	// OutputGrafanaCloudQueueFullBehavior2Drop Drop new data
	OutputGrafanaCloudQueueFullBehavior2Drop OutputGrafanaCloudQueueFullBehavior2 = "drop"
)

func (e OutputGrafanaCloudQueueFullBehavior2) ToPointer() *OutputGrafanaCloudQueueFullBehavior2 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudQueueFullBehavior2) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type OutputGrafanaCloudPqControls2 struct {
}

func (o OutputGrafanaCloudPqControls2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudPqControls2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGrafanaCloudGrafanaCloud2 struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type OutputGrafanaCloudType2 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net
	LokiURL *string `json:"lokiUrl,omitempty"`
	// The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push
	PrometheusURL string `json:"prometheusUrl"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Format to use when sending logs to Loki (Protobuf or JSON)
	MessageFormat *OutputGrafanaCloudMessageFormat2 `default:"protobuf" json:"messageFormat"`
	// List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'
	Labels []OutputGrafanaCloudLabel2 `json:"labels,omitempty"`
	// JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.
	MetricRenameExpr *string                      `default:"name.replace(/[^a-zA-Z0-9_]/g, '_')" json:"metricRenameExpr"`
	PrometheusAuth   *CreateOutputPrometheusAuth2 `json:"prometheusAuth,omitempty"`
	LokiAuth         *CreateOutputLokiAuth2       `json:"lokiAuth,omitempty"`
	// Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.
	Concurrency *float64 `default:"1" json:"concurrency"`
	// Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	FlushPeriodSec *float64 `default:"15" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []OutputGrafanaCloudExtraHTTPHeader2 `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *OutputGrafanaCloudFailedRequestLoggingMode2 `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []OutputGrafanaCloudResponseRetrySetting2 `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *OutputGrafanaCloudTimeoutRetrySettings2  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *OutputGrafanaCloudBackpressureBehavior2 `default:"block" json:"onBackpressure"`
	Description    *string                                  `json:"description,omitempty"`
	// Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default.
	Compress *bool `default:"true" json:"compress"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputGrafanaCloudMode2 `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *OutputGrafanaCloudCompression2 `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *OutputGrafanaCloudQueueFullBehavior2 `default:"block" json:"pqOnBackpressure"`
	PqControls       *OutputGrafanaCloudPqControls2        `json:"pqControls,omitempty"`
}

func (o OutputGrafanaCloudGrafanaCloud2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudGrafanaCloud2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "prometheusUrl"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetType() OutputGrafanaCloudType2 {
	if o == nil {
		return OutputGrafanaCloudType2("")
	}
	return o.Type
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetLokiURL() *string {
	if o == nil {
		return nil
	}
	return o.LokiURL
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPrometheusURL() string {
	if o == nil {
		return ""
	}
	return o.PrometheusURL
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMessageFormat() *OutputGrafanaCloudMessageFormat2 {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetLabels() []OutputGrafanaCloudLabel2 {
	if o == nil {
		return nil
	}
	return o.Labels
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMetricRenameExpr() *string {
	if o == nil {
		return nil
	}
	return o.MetricRenameExpr
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPrometheusAuth() *CreateOutputPrometheusAuth2 {
	if o == nil {
		return nil
	}
	return o.PrometheusAuth
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetLokiAuth() *CreateOutputLokiAuth2 {
	if o == nil {
		return nil
	}
	return o.LokiAuth
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetExtraHTTPHeaders() []OutputGrafanaCloudExtraHTTPHeader2 {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetFailedRequestLoggingMode() *OutputGrafanaCloudFailedRequestLoggingMode2 {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetResponseRetrySettings() []OutputGrafanaCloudResponseRetrySetting2 {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetTimeoutRetrySettings() *OutputGrafanaCloudTimeoutRetrySettings2 {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetOnBackpressure() *OutputGrafanaCloudBackpressureBehavior2 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMode() *OutputGrafanaCloudMode2 {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqCompress() *OutputGrafanaCloudCompression2 {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqOnBackpressure() *OutputGrafanaCloudQueueFullBehavior2 {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqControls() *OutputGrafanaCloudPqControls2 {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type OutputGrafanaCloudType1 string

const (
	OutputGrafanaCloudType1GrafanaCloud OutputGrafanaCloudType1 = "grafana_cloud"
)

func (e OutputGrafanaCloudType1) ToPointer() *OutputGrafanaCloudType1 {
	return &e
}
func (e *OutputGrafanaCloudType1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grafana_cloud":
		*e = OutputGrafanaCloudType1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudType1: %v", v)
	}
}

// OutputGrafanaCloudMessageFormat1 - Format to use when sending logs to Loki (Protobuf or JSON)
type OutputGrafanaCloudMessageFormat1 string

const (
	// OutputGrafanaCloudMessageFormat1Protobuf Protobuf
	OutputGrafanaCloudMessageFormat1Protobuf OutputGrafanaCloudMessageFormat1 = "protobuf"
	// OutputGrafanaCloudMessageFormat1JSON JSON
	OutputGrafanaCloudMessageFormat1JSON OutputGrafanaCloudMessageFormat1 = "json"
)

func (e OutputGrafanaCloudMessageFormat1) ToPointer() *OutputGrafanaCloudMessageFormat1 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudMessageFormat1) IsExact() bool {
	if e != nil {
		switch *e {
		case "protobuf", "json":
			return true
		}
	}
	return false
}

type OutputGrafanaCloudLabel1 struct {
	Name  *string `default:"" json:"name"`
	Value string  `json:"value"`
}

func (o OutputGrafanaCloudLabel1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudLabel1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudLabel1) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudLabel1) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputGrafanaCloudPrometheusAuthAuthenticationType1 string

const (
	// OutputGrafanaCloudPrometheusAuthAuthenticationType1None None
	OutputGrafanaCloudPrometheusAuthAuthenticationType1None OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "none"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType1Token Auth token
	OutputGrafanaCloudPrometheusAuthAuthenticationType1Token OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "token"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType1TextSecret Auth token (text secret)
	OutputGrafanaCloudPrometheusAuthAuthenticationType1TextSecret OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "textSecret"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType1Basic Basic
	OutputGrafanaCloudPrometheusAuthAuthenticationType1Basic OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "basic"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType1CredentialsSecret Basic (credentials secret)
	OutputGrafanaCloudPrometheusAuthAuthenticationType1CredentialsSecret OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "credentialsSecret"
)

func (e OutputGrafanaCloudPrometheusAuthAuthenticationType1) ToPointer() *OutputGrafanaCloudPrometheusAuthAuthenticationType1 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudPrometheusAuthAuthenticationType1) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "token", "textSecret", "basic", "credentialsSecret":
			return true
		}
	}
	return false
}

type CreateOutputPrometheusAuth1 struct {
	AuthType *OutputGrafanaCloudPrometheusAuthAuthenticationType1 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (c CreateOutputPrometheusAuth1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPrometheusAuth1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputPrometheusAuth1) GetAuthType() *OutputGrafanaCloudPrometheusAuthAuthenticationType1 {
	if c == nil {
		return nil
	}
	return c.AuthType
}

func (c *CreateOutputPrometheusAuth1) GetToken() *string {
	if c == nil {
		return nil
	}
	return c.Token
}

func (c *CreateOutputPrometheusAuth1) GetTextSecret() *string {
	if c == nil {
		return nil
	}
	return c.TextSecret
}

func (c *CreateOutputPrometheusAuth1) GetUsername() *string {
	if c == nil {
		return nil
	}
	return c.Username
}

func (c *CreateOutputPrometheusAuth1) GetPassword() *string {
	if c == nil {
		return nil
	}
	return c.Password
}

func (c *CreateOutputPrometheusAuth1) GetCredentialsSecret() *string {
	if c == nil {
		return nil
	}
	return c.CredentialsSecret
}

type OutputGrafanaCloudLokiAuthAuthenticationType1 string

const (
	// OutputGrafanaCloudLokiAuthAuthenticationType1None None
	OutputGrafanaCloudLokiAuthAuthenticationType1None OutputGrafanaCloudLokiAuthAuthenticationType1 = "none"
	// OutputGrafanaCloudLokiAuthAuthenticationType1Token Auth token
	OutputGrafanaCloudLokiAuthAuthenticationType1Token OutputGrafanaCloudLokiAuthAuthenticationType1 = "token"
	// OutputGrafanaCloudLokiAuthAuthenticationType1TextSecret Auth token (text secret)
	OutputGrafanaCloudLokiAuthAuthenticationType1TextSecret OutputGrafanaCloudLokiAuthAuthenticationType1 = "textSecret"
	// OutputGrafanaCloudLokiAuthAuthenticationType1Basic Basic
	OutputGrafanaCloudLokiAuthAuthenticationType1Basic OutputGrafanaCloudLokiAuthAuthenticationType1 = "basic"
	// OutputGrafanaCloudLokiAuthAuthenticationType1CredentialsSecret Basic (credentials secret)
	OutputGrafanaCloudLokiAuthAuthenticationType1CredentialsSecret OutputGrafanaCloudLokiAuthAuthenticationType1 = "credentialsSecret"
)

func (e OutputGrafanaCloudLokiAuthAuthenticationType1) ToPointer() *OutputGrafanaCloudLokiAuthAuthenticationType1 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudLokiAuthAuthenticationType1) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "token", "textSecret", "basic", "credentialsSecret":
			return true
		}
	}
	return false
}

type CreateOutputLokiAuth1 struct {
	AuthType *OutputGrafanaCloudLokiAuthAuthenticationType1 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (c CreateOutputLokiAuth1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputLokiAuth1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputLokiAuth1) GetAuthType() *OutputGrafanaCloudLokiAuthAuthenticationType1 {
	if c == nil {
		return nil
	}
	return c.AuthType
}

func (c *CreateOutputLokiAuth1) GetToken() *string {
	if c == nil {
		return nil
	}
	return c.Token
}

func (c *CreateOutputLokiAuth1) GetTextSecret() *string {
	if c == nil {
		return nil
	}
	return c.TextSecret
}

func (c *CreateOutputLokiAuth1) GetUsername() *string {
	if c == nil {
		return nil
	}
	return c.Username
}

func (c *CreateOutputLokiAuth1) GetPassword() *string {
	if c == nil {
		return nil
	}
	return c.Password
}

func (c *CreateOutputLokiAuth1) GetCredentialsSecret() *string {
	if c == nil {
		return nil
	}
	return c.CredentialsSecret
}

type OutputGrafanaCloudExtraHTTPHeader1 struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o OutputGrafanaCloudExtraHTTPHeader1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudExtraHTTPHeader1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudExtraHTTPHeader1) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudExtraHTTPHeader1) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// OutputGrafanaCloudFailedRequestLoggingMode1 - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type OutputGrafanaCloudFailedRequestLoggingMode1 string

const (
	// OutputGrafanaCloudFailedRequestLoggingMode1Payload Payload
	OutputGrafanaCloudFailedRequestLoggingMode1Payload OutputGrafanaCloudFailedRequestLoggingMode1 = "payload"
	// OutputGrafanaCloudFailedRequestLoggingMode1PayloadAndHeaders Payload + Headers
	OutputGrafanaCloudFailedRequestLoggingMode1PayloadAndHeaders OutputGrafanaCloudFailedRequestLoggingMode1 = "payloadAndHeaders"
	// OutputGrafanaCloudFailedRequestLoggingMode1None None
	OutputGrafanaCloudFailedRequestLoggingMode1None OutputGrafanaCloudFailedRequestLoggingMode1 = "none"
)

func (e OutputGrafanaCloudFailedRequestLoggingMode1) ToPointer() *OutputGrafanaCloudFailedRequestLoggingMode1 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudFailedRequestLoggingMode1) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type OutputGrafanaCloudResponseRetrySetting1 struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudResponseRetrySetting1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudResponseRetrySetting1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type OutputGrafanaCloudTimeoutRetrySettings1 struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudTimeoutRetrySettings1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// OutputGrafanaCloudBackpressureBehavior1 - How to handle events when all receivers are exerting backpressure
type OutputGrafanaCloudBackpressureBehavior1 string

const (
	// OutputGrafanaCloudBackpressureBehavior1Block Block
	OutputGrafanaCloudBackpressureBehavior1Block OutputGrafanaCloudBackpressureBehavior1 = "block"
	// OutputGrafanaCloudBackpressureBehavior1Drop Drop
	OutputGrafanaCloudBackpressureBehavior1Drop OutputGrafanaCloudBackpressureBehavior1 = "drop"
	// OutputGrafanaCloudBackpressureBehavior1Queue Persistent Queue
	OutputGrafanaCloudBackpressureBehavior1Queue OutputGrafanaCloudBackpressureBehavior1 = "queue"
)

func (e OutputGrafanaCloudBackpressureBehavior1) ToPointer() *OutputGrafanaCloudBackpressureBehavior1 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudBackpressureBehavior1) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// OutputGrafanaCloudMode1 - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputGrafanaCloudMode1 string

const (
	// OutputGrafanaCloudMode1Error Error
	OutputGrafanaCloudMode1Error OutputGrafanaCloudMode1 = "error"
	// OutputGrafanaCloudMode1Always Backpressure
	OutputGrafanaCloudMode1Always OutputGrafanaCloudMode1 = "always"
	// OutputGrafanaCloudMode1Backpressure Always On
	OutputGrafanaCloudMode1Backpressure OutputGrafanaCloudMode1 = "backpressure"
)

func (e OutputGrafanaCloudMode1) ToPointer() *OutputGrafanaCloudMode1 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudMode1) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// OutputGrafanaCloudCompression1 - Codec to use to compress the persisted data
type OutputGrafanaCloudCompression1 string

const (
	// OutputGrafanaCloudCompression1None None
	OutputGrafanaCloudCompression1None OutputGrafanaCloudCompression1 = "none"
	// OutputGrafanaCloudCompression1Gzip Gzip
	OutputGrafanaCloudCompression1Gzip OutputGrafanaCloudCompression1 = "gzip"
)

func (e OutputGrafanaCloudCompression1) ToPointer() *OutputGrafanaCloudCompression1 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudCompression1) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// OutputGrafanaCloudQueueFullBehavior1 - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type OutputGrafanaCloudQueueFullBehavior1 string

const (
	// OutputGrafanaCloudQueueFullBehavior1Block Block
	OutputGrafanaCloudQueueFullBehavior1Block OutputGrafanaCloudQueueFullBehavior1 = "block"
	// OutputGrafanaCloudQueueFullBehavior1Drop Drop new data
	OutputGrafanaCloudQueueFullBehavior1Drop OutputGrafanaCloudQueueFullBehavior1 = "drop"
)

func (e OutputGrafanaCloudQueueFullBehavior1) ToPointer() *OutputGrafanaCloudQueueFullBehavior1 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OutputGrafanaCloudQueueFullBehavior1) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type OutputGrafanaCloudPqControls1 struct {
}

func (o OutputGrafanaCloudPqControls1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudPqControls1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGrafanaCloudGrafanaCloud1 struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type OutputGrafanaCloudType1 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net
	LokiURL string `json:"lokiUrl"`
	// The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push
	PrometheusURL *string `json:"prometheusUrl,omitempty"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Format to use when sending logs to Loki (Protobuf or JSON)
	MessageFormat *OutputGrafanaCloudMessageFormat1 `default:"protobuf" json:"messageFormat"`
	// List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'
	Labels []OutputGrafanaCloudLabel1 `json:"labels,omitempty"`
	// JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.
	MetricRenameExpr *string                      `default:"name.replace(/[^a-zA-Z0-9_]/g, '_')" json:"metricRenameExpr"`
	PrometheusAuth   *CreateOutputPrometheusAuth1 `json:"prometheusAuth,omitempty"`
	LokiAuth         *CreateOutputLokiAuth1       `json:"lokiAuth,omitempty"`
	// Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.
	Concurrency *float64 `default:"1" json:"concurrency"`
	// Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	FlushPeriodSec *float64 `default:"15" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []OutputGrafanaCloudExtraHTTPHeader1 `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *OutputGrafanaCloudFailedRequestLoggingMode1 `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []OutputGrafanaCloudResponseRetrySetting1 `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *OutputGrafanaCloudTimeoutRetrySettings1  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *OutputGrafanaCloudBackpressureBehavior1 `default:"block" json:"onBackpressure"`
	Description    *string                                  `json:"description,omitempty"`
	// Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default.
	Compress *bool `default:"true" json:"compress"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputGrafanaCloudMode1 `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *OutputGrafanaCloudCompression1 `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *OutputGrafanaCloudQueueFullBehavior1 `default:"block" json:"pqOnBackpressure"`
	PqControls       *OutputGrafanaCloudPqControls1        `json:"pqControls,omitempty"`
}

func (o OutputGrafanaCloudGrafanaCloud1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudGrafanaCloud1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "lokiUrl"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetType() OutputGrafanaCloudType1 {
	if o == nil {
		return OutputGrafanaCloudType1("")
	}
	return o.Type
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetLokiURL() string {
	if o == nil {
		return ""
	}
	return o.LokiURL
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPrometheusURL() *string {
	if o == nil {
		return nil
	}
	return o.PrometheusURL
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMessageFormat() *OutputGrafanaCloudMessageFormat1 {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetLabels() []OutputGrafanaCloudLabel1 {
	if o == nil {
		return nil
	}
	return o.Labels
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMetricRenameExpr() *string {
	if o == nil {
		return nil
	}
	return o.MetricRenameExpr
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPrometheusAuth() *CreateOutputPrometheusAuth1 {
	if o == nil {
		return nil
	}
	return o.PrometheusAuth
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetLokiAuth() *CreateOutputLokiAuth1 {
	if o == nil {
		return nil
	}
	return o.LokiAuth
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetExtraHTTPHeaders() []OutputGrafanaCloudExtraHTTPHeader1 {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetFailedRequestLoggingMode() *OutputGrafanaCloudFailedRequestLoggingMode1 {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetResponseRetrySettings() []OutputGrafanaCloudResponseRetrySetting1 {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetTimeoutRetrySettings() *OutputGrafanaCloudTimeoutRetrySettings1 {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetOnBackpressure() *OutputGrafanaCloudBackpressureBehavior1 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMode() *OutputGrafanaCloudMode1 {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqCompress() *OutputGrafanaCloudCompression1 {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqOnBackpressure() *OutputGrafanaCloudQueueFullBehavior1 {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqControls() *OutputGrafanaCloudPqControls1 {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type OutputGrafanaCloudType string

const (
	OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud1 OutputGrafanaCloudType = "OutputGrafanaCloud_GrafanaCloud_1"
	OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud2 OutputGrafanaCloudType = "OutputGrafanaCloud_GrafanaCloud_2"
)

type OutputGrafanaCloud struct {
	OutputGrafanaCloudGrafanaCloud1 *OutputGrafanaCloudGrafanaCloud1 `queryParam:"inline,name=OutputGrafanaCloud" union:"member"`
	OutputGrafanaCloudGrafanaCloud2 *OutputGrafanaCloudGrafanaCloud2 `queryParam:"inline,name=OutputGrafanaCloud" union:"member"`

	Type OutputGrafanaCloudType
}

func CreateOutputGrafanaCloudOutputGrafanaCloudGrafanaCloud1(outputGrafanaCloudGrafanaCloud1 OutputGrafanaCloudGrafanaCloud1) OutputGrafanaCloud {
	typ := OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud1

	return OutputGrafanaCloud{
		OutputGrafanaCloudGrafanaCloud1: &outputGrafanaCloudGrafanaCloud1,
		Type:                            typ,
	}
}

func CreateOutputGrafanaCloudOutputGrafanaCloudGrafanaCloud2(outputGrafanaCloudGrafanaCloud2 OutputGrafanaCloudGrafanaCloud2) OutputGrafanaCloud {
	typ := OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud2

	return OutputGrafanaCloud{
		OutputGrafanaCloudGrafanaCloud2: &outputGrafanaCloudGrafanaCloud2,
		Type:                            typ,
	}
}

func (u *OutputGrafanaCloud) UnmarshalJSON(data []byte) error {

	var outputGrafanaCloudGrafanaCloud1 OutputGrafanaCloudGrafanaCloud1 = OutputGrafanaCloudGrafanaCloud1{}
	if err := utils.UnmarshalJSON(data, &outputGrafanaCloudGrafanaCloud1, "", true, nil); err == nil {
		u.OutputGrafanaCloudGrafanaCloud1 = &outputGrafanaCloudGrafanaCloud1
		u.Type = OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud1
		return nil
	}

	var outputGrafanaCloudGrafanaCloud2 OutputGrafanaCloudGrafanaCloud2 = OutputGrafanaCloudGrafanaCloud2{}
	if err := utils.UnmarshalJSON(data, &outputGrafanaCloudGrafanaCloud2, "", true, nil); err == nil {
		u.OutputGrafanaCloudGrafanaCloud2 = &outputGrafanaCloudGrafanaCloud2
		u.Type = OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud2
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for OutputGrafanaCloud", string(data))
}

func (u OutputGrafanaCloud) MarshalJSON() ([]byte, error) {
	if u.OutputGrafanaCloudGrafanaCloud1 != nil {
		return utils.MarshalJSON(u.OutputGrafanaCloudGrafanaCloud1, "", true)
	}

	if u.OutputGrafanaCloudGrafanaCloud2 != nil {
		return utils.MarshalJSON(u.OutputGrafanaCloudGrafanaCloud2, "", true)
	}

	return nil, errors.New("could not marshal union type OutputGrafanaCloud: all fields are null")
}

type TypeDatadog string

const (
	TypeDatadogDatadog TypeDatadog = "datadog"
)

func (e TypeDatadog) ToPointer() *TypeDatadog {
	return &e
}
func (e *TypeDatadog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "datadog":
		*e = TypeDatadog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDatadog: %v", v)
	}
}

// SendLogsAs - The content type to use when sending logs
type SendLogsAs string

const (
	// SendLogsAsText text/plain
	SendLogsAsText SendLogsAs = "text"
	// SendLogsAsJSON application/json
	SendLogsAsJSON SendLogsAs = "json"
)

func (e SendLogsAs) ToPointer() *SendLogsAs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SendLogsAs) IsExact() bool {
	if e != nil {
		switch *e {
		case "text", "json":
			return true
		}
	}
	return false
}

// SeverityDatadog - Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.
type SeverityDatadog string

const (
	// SeverityDatadogEmergency emergency
	SeverityDatadogEmergency SeverityDatadog = "emergency"
	// SeverityDatadogAlert alert
	SeverityDatadogAlert SeverityDatadog = "alert"
	// SeverityDatadogCritical critical
	SeverityDatadogCritical SeverityDatadog = "critical"
	// SeverityDatadogError error
	SeverityDatadogError SeverityDatadog = "error"
	// SeverityDatadogWarning warning
	SeverityDatadogWarning SeverityDatadog = "warning"
	// SeverityDatadogNotice notice
	SeverityDatadogNotice SeverityDatadog = "notice"
	// SeverityDatadogInfo info
	SeverityDatadogInfo SeverityDatadog = "info"
	// SeverityDatadogDebug debug
	SeverityDatadogDebug SeverityDatadog = "debug"
)

func (e SeverityDatadog) ToPointer() *SeverityDatadog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SeverityDatadog) IsExact() bool {
	if e != nil {
		switch *e {
		case "emergency", "alert", "critical", "error", "warning", "notice", "info", "debug":
			return true
		}
	}
	return false
}

// DatadogSite - Datadog site to which events should be sent
type DatadogSite string

const (
	// DatadogSiteUs US
	DatadogSiteUs DatadogSite = "us"
	// DatadogSiteUs3 US3
	DatadogSiteUs3 DatadogSite = "us3"
	// DatadogSiteUs5 US5
	DatadogSiteUs5 DatadogSite = "us5"
	// DatadogSiteEu Europe
	DatadogSiteEu DatadogSite = "eu"
	// DatadogSiteFed1 US1-FED
	DatadogSiteFed1 DatadogSite = "fed1"
	// DatadogSiteAp1 AP1
	DatadogSiteAp1 DatadogSite = "ap1"
	// DatadogSiteCustom Custom
	DatadogSiteCustom DatadogSite = "custom"
)

func (e DatadogSite) ToPointer() *DatadogSite {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DatadogSite) IsExact() bool {
	if e != nil {
		switch *e {
		case "us", "us3", "us5", "eu", "fed1", "ap1", "custom":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderDatadog struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderDatadog) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderDatadog) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeDatadog - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDatadog string

const (
	// FailedRequestLoggingModeDatadogPayload Payload
	FailedRequestLoggingModeDatadogPayload FailedRequestLoggingModeDatadog = "payload"
	// FailedRequestLoggingModeDatadogPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeDatadogPayloadAndHeaders FailedRequestLoggingModeDatadog = "payloadAndHeaders"
	// FailedRequestLoggingModeDatadogNone None
	FailedRequestLoggingModeDatadogNone FailedRequestLoggingModeDatadog = "none"
)

func (e FailedRequestLoggingModeDatadog) ToPointer() *FailedRequestLoggingModeDatadog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeDatadog) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingDatadog struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingDatadog) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingDatadog) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingDatadog) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingDatadog) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsDatadog struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsDatadog) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsDatadog) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsDatadog) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsDatadog) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorDatadog - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDatadog string

const (
	// BackpressureBehaviorDatadogBlock Block
	BackpressureBehaviorDatadogBlock BackpressureBehaviorDatadog = "block"
	// BackpressureBehaviorDatadogDrop Drop
	BackpressureBehaviorDatadogDrop BackpressureBehaviorDatadog = "drop"
	// BackpressureBehaviorDatadogQueue Persistent Queue
	BackpressureBehaviorDatadogQueue BackpressureBehaviorDatadog = "queue"
)

func (e BackpressureBehaviorDatadog) ToPointer() *BackpressureBehaviorDatadog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorDatadog) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// AuthenticationMethodDatadog - Enter API key directly, or select a stored secret
type AuthenticationMethodDatadog string

const (
	AuthenticationMethodDatadogManual AuthenticationMethodDatadog = "manual"
	AuthenticationMethodDatadogSecret AuthenticationMethodDatadog = "secret"
)

func (e AuthenticationMethodDatadog) ToPointer() *AuthenticationMethodDatadog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodDatadog) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

// ModeDatadog - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeDatadog string

const (
	// ModeDatadogError Error
	ModeDatadogError ModeDatadog = "error"
	// ModeDatadogAlways Backpressure
	ModeDatadogAlways ModeDatadog = "always"
	// ModeDatadogBackpressure Always On
	ModeDatadogBackpressure ModeDatadog = "backpressure"
)

func (e ModeDatadog) ToPointer() *ModeDatadog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeDatadog) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionDatadog - Codec to use to compress the persisted data
type CompressionDatadog string

const (
	// CompressionDatadogNone None
	CompressionDatadogNone CompressionDatadog = "none"
	// CompressionDatadogGzip Gzip
	CompressionDatadogGzip CompressionDatadog = "gzip"
)

func (e CompressionDatadog) ToPointer() *CompressionDatadog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionDatadog) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorDatadog - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDatadog string

const (
	// QueueFullBehaviorDatadogBlock Block
	QueueFullBehaviorDatadogBlock QueueFullBehaviorDatadog = "block"
	// QueueFullBehaviorDatadogDrop Drop new data
	QueueFullBehaviorDatadogDrop QueueFullBehaviorDatadog = "drop"
)

func (e QueueFullBehaviorDatadog) ToPointer() *QueueFullBehaviorDatadog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorDatadog) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsDatadog struct {
}

func (p PqControlsDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputDatadog struct {
	// Unique ID for this output
	ID   string      `json:"id"`
	Type TypeDatadog `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The content type to use when sending logs
	ContentType *SendLogsAs `default:"json" json:"contentType"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Name of the source to send with logs. When you send logs as JSON objects, the event's 'source' field (if set) will override this value.
	Source *string `json:"source,omitempty"`
	// Name of the host to send with logs. When you send logs as JSON objects, the event's 'host' field (if set) will override this value.
	Host *string `json:"host,omitempty"`
	// Name of the service to send with logs. When you send logs as JSON objects, the event's '__service' field (if set) will override this value.
	Service *string `json:"service,omitempty"`
	// List of tags to send with logs, such as 'env:prod' and 'env_staging:east'
	Tags []string `json:"tags,omitempty"`
	// Batch events by API key and the ddtags field on the event. When disabled, batches events only by API key. If incoming events have high cardinality in the ddtags field, disabling this setting may improve Destination performance.
	BatchByTags *bool `default:"true" json:"batchByTags"`
	// Allow API key to be set from the event's '__agent_api_key' field
	AllowAPIKeyFromEvents *bool `default:"false" json:"allowApiKeyFromEvents"`
	// Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.
	Severity *SeverityDatadog `json:"severity,omitempty"`
	// Datadog site to which events should be sent
	Site *DatadogSite `default:"us" json:"site"`
	// If not enabled, Datadog will transform 'counter' metrics to 'gauge'. [Learn more about Datadog metrics types.](https://docs.datadoghq.com/metrics/types/?tab=count)
	SendCountersAsCount *bool `default:"false" json:"sendCountersAsCount"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderDatadog `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDatadog `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingDatadog `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDatadog  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDatadog `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType *AuthenticationMethodDatadog `default:"manual" json:"authType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	CustomURL          *string  `json:"customUrl,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeDatadog `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionDatadog `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorDatadog `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsDatadog        `json:"pqControls,omitempty"`
	// Organization's API key in Datadog
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDatadog) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDatadog) GetType() TypeDatadog {
	if o == nil {
		return TypeDatadog("")
	}
	return o.Type
}

func (o *OutputDatadog) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDatadog) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDatadog) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDatadog) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDatadog) GetContentType() *SendLogsAs {
	if o == nil {
		return nil
	}
	return o.ContentType
}

func (o *OutputDatadog) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputDatadog) GetSource() *string {
	if o == nil {
		return nil
	}
	return o.Source
}

func (o *OutputDatadog) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputDatadog) GetService() *string {
	if o == nil {
		return nil
	}
	return o.Service
}

func (o *OutputDatadog) GetTags() []string {
	if o == nil {
		return nil
	}
	return o.Tags
}

func (o *OutputDatadog) GetBatchByTags() *bool {
	if o == nil {
		return nil
	}
	return o.BatchByTags
}

func (o *OutputDatadog) GetAllowAPIKeyFromEvents() *bool {
	if o == nil {
		return nil
	}
	return o.AllowAPIKeyFromEvents
}

func (o *OutputDatadog) GetSeverity() *SeverityDatadog {
	if o == nil {
		return nil
	}
	return o.Severity
}

func (o *OutputDatadog) GetSite() *DatadogSite {
	if o == nil {
		return nil
	}
	return o.Site
}

func (o *OutputDatadog) GetSendCountersAsCount() *bool {
	if o == nil {
		return nil
	}
	return o.SendCountersAsCount
}

func (o *OutputDatadog) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDatadog) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDatadog) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputDatadog) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDatadog) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDatadog) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDatadog) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDatadog) GetExtraHTTPHeaders() []ExtraHTTPHeaderDatadog {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDatadog) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDatadog) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDatadog {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDatadog) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDatadog) GetResponseRetrySettings() []ResponseRetrySettingDatadog {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDatadog) GetTimeoutRetrySettings() *TimeoutRetrySettingsDatadog {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDatadog) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDatadog) GetOnBackpressure() *BackpressureBehaviorDatadog {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDatadog) GetAuthType() *AuthenticationMethodDatadog {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputDatadog) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputDatadog) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDatadog) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputDatadog) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputDatadog) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputDatadog) GetPqMode() *ModeDatadog {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDatadog) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputDatadog) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputDatadog) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDatadog) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDatadog) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDatadog) GetPqCompress() *CompressionDatadog {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDatadog) GetPqOnBackpressure() *QueueFullBehaviorDatadog {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDatadog) GetPqControls() *PqControlsDatadog {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDatadog) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputDatadog) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type TypeSumoLogic string

const (
	TypeSumoLogicSumoLogic TypeSumoLogic = "sumo_logic"
)

func (e TypeSumoLogic) ToPointer() *TypeSumoLogic {
	return &e
}
func (e *TypeSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sumo_logic":
		*e = TypeSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSumoLogic: %v", v)
	}
}

// DataFormatSumoLogic - Preserve the raw event format instead of JSONifying it
type DataFormatSumoLogic string

const (
	// DataFormatSumoLogicJSON JSON
	DataFormatSumoLogicJSON DataFormatSumoLogic = "json"
	// DataFormatSumoLogicRaw Raw
	DataFormatSumoLogicRaw DataFormatSumoLogic = "raw"
)

func (e DataFormatSumoLogic) ToPointer() *DataFormatSumoLogic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataFormatSumoLogic) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderSumoLogic struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderSumoLogic) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderSumoLogic) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeSumoLogic - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSumoLogic string

const (
	// FailedRequestLoggingModeSumoLogicPayload Payload
	FailedRequestLoggingModeSumoLogicPayload FailedRequestLoggingModeSumoLogic = "payload"
	// FailedRequestLoggingModeSumoLogicPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeSumoLogicPayloadAndHeaders FailedRequestLoggingModeSumoLogic = "payloadAndHeaders"
	// FailedRequestLoggingModeSumoLogicNone None
	FailedRequestLoggingModeSumoLogicNone FailedRequestLoggingModeSumoLogic = "none"
)

func (e FailedRequestLoggingModeSumoLogic) ToPointer() *FailedRequestLoggingModeSumoLogic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeSumoLogic) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingSumoLogic struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingSumoLogic) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingSumoLogic) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingSumoLogic) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingSumoLogic) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsSumoLogic struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsSumoLogic) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsSumoLogic) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsSumoLogic) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsSumoLogic) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorSumoLogic - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSumoLogic string

const (
	// BackpressureBehaviorSumoLogicBlock Block
	BackpressureBehaviorSumoLogicBlock BackpressureBehaviorSumoLogic = "block"
	// BackpressureBehaviorSumoLogicDrop Drop
	BackpressureBehaviorSumoLogicDrop BackpressureBehaviorSumoLogic = "drop"
	// BackpressureBehaviorSumoLogicQueue Persistent Queue
	BackpressureBehaviorSumoLogicQueue BackpressureBehaviorSumoLogic = "queue"
)

func (e BackpressureBehaviorSumoLogic) ToPointer() *BackpressureBehaviorSumoLogic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorSumoLogic) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeSumoLogic - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSumoLogic string

const (
	// ModeSumoLogicError Error
	ModeSumoLogicError ModeSumoLogic = "error"
	// ModeSumoLogicAlways Backpressure
	ModeSumoLogicAlways ModeSumoLogic = "always"
	// ModeSumoLogicBackpressure Always On
	ModeSumoLogicBackpressure ModeSumoLogic = "backpressure"
)

func (e ModeSumoLogic) ToPointer() *ModeSumoLogic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeSumoLogic) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionSumoLogic - Codec to use to compress the persisted data
type CompressionSumoLogic string

const (
	// CompressionSumoLogicNone None
	CompressionSumoLogicNone CompressionSumoLogic = "none"
	// CompressionSumoLogicGzip Gzip
	CompressionSumoLogicGzip CompressionSumoLogic = "gzip"
)

func (e CompressionSumoLogic) ToPointer() *CompressionSumoLogic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionSumoLogic) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorSumoLogic - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSumoLogic string

const (
	// QueueFullBehaviorSumoLogicBlock Block
	QueueFullBehaviorSumoLogicBlock QueueFullBehaviorSumoLogic = "block"
	// QueueFullBehaviorSumoLogicDrop Drop new data
	QueueFullBehaviorSumoLogicDrop QueueFullBehaviorSumoLogic = "drop"
)

func (e QueueFullBehaviorSumoLogic) ToPointer() *QueueFullBehaviorSumoLogic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorSumoLogic) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsSumoLogic struct {
}

func (p PqControlsSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSumoLogic struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeSumoLogic `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Sumo Logic HTTP collector URL to which events should be sent
	URL string `json:"url"`
	// Override the source name configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceName field.
	CustomSource *string `json:"customSource,omitempty"`
	// Override the source category configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceCategory field.
	CustomCategory *string `json:"customCategory,omitempty"`
	// Preserve the raw event format instead of JSONifying it
	Format *DataFormatSumoLogic `default:"json" json:"format"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderSumoLogic `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSumoLogic `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSumoLogic `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSumoLogic  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSumoLogic `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSumoLogic `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSumoLogic `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSumoLogic `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsSumoLogic        `json:"pqControls,omitempty"`
}

func (o OutputSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "url"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSumoLogic) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSumoLogic) GetType() TypeSumoLogic {
	if o == nil {
		return TypeSumoLogic("")
	}
	return o.Type
}

func (o *OutputSumoLogic) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSumoLogic) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSumoLogic) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSumoLogic) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSumoLogic) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputSumoLogic) GetCustomSource() *string {
	if o == nil {
		return nil
	}
	return o.CustomSource
}

func (o *OutputSumoLogic) GetCustomCategory() *string {
	if o == nil {
		return nil
	}
	return o.CustomCategory
}

func (o *OutputSumoLogic) GetFormat() *DataFormatSumoLogic {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputSumoLogic) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSumoLogic) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSumoLogic) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSumoLogic) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSumoLogic) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSumoLogic) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSumoLogic) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSumoLogic) GetExtraHTTPHeaders() []ExtraHTTPHeaderSumoLogic {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSumoLogic) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSumoLogic) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSumoLogic {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSumoLogic) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSumoLogic) GetResponseRetrySettings() []ResponseRetrySettingSumoLogic {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSumoLogic) GetTimeoutRetrySettings() *TimeoutRetrySettingsSumoLogic {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSumoLogic) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSumoLogic) GetOnBackpressure() *BackpressureBehaviorSumoLogic {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSumoLogic) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputSumoLogic) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSumoLogic) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSumoLogic) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSumoLogic) GetPqMode() *ModeSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSumoLogic) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSumoLogic) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSumoLogic) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSumoLogic) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSumoLogic) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSumoLogic) GetPqCompress() *CompressionSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSumoLogic) GetPqOnBackpressure() *QueueFullBehaviorSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSumoLogic) GetPqControls() *PqControlsSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeSnmp string

const (
	CreateOutputTypeSnmpSnmp CreateOutputTypeSnmp = "snmp"
)

func (e CreateOutputTypeSnmp) ToPointer() *CreateOutputTypeSnmp {
	return &e
}
func (e *CreateOutputTypeSnmp) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "snmp":
		*e = CreateOutputTypeSnmp(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeSnmp: %v", v)
	}
}

type HostSnmp struct {
	// Destination host
	Host string `json:"host"`
	// Destination port, default is 162
	Port *float64 `default:"162" json:"port"`
}

func (h HostSnmp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostSnmp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host"}); err != nil {
		return err
	}
	return nil
}

func (h *HostSnmp) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostSnmp) GetPort() *float64 {
	if h == nil {
		return nil
	}
	return h.Port
}

type OutputSnmp struct {
	// Unique ID for this output
	ID   string               `json:"id"`
	Type CreateOutputTypeSnmp `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// One or more SNMP destinations to forward traps to
	Hosts []HostSnmp `json:"hosts"`
	// How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every trap sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
}

func (o OutputSnmp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSnmp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "hosts"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSnmp) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSnmp) GetType() CreateOutputTypeSnmp {
	if o == nil {
		return CreateOutputTypeSnmp("")
	}
	return o.Type
}

func (o *OutputSnmp) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSnmp) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSnmp) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSnmp) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSnmp) GetHosts() []HostSnmp {
	if o == nil {
		return []HostSnmp{}
	}
	return o.Hosts
}

func (o *OutputSnmp) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSnmp) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

type CreateOutputTypeSqs string

const (
	CreateOutputTypeSqsSqs CreateOutputTypeSqs = "sqs"
)

func (e CreateOutputTypeSqs) ToPointer() *CreateOutputTypeSqs {
	return &e
}
func (e *CreateOutputTypeSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sqs":
		*e = CreateOutputTypeSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeSqs: %v", v)
	}
}

// CreateOutputQueueType - The queue type used (or created). Defaults to Standard.
type CreateOutputQueueType string

const (
	// CreateOutputQueueTypeStandard Standard
	CreateOutputQueueTypeStandard CreateOutputQueueType = "standard"
	// CreateOutputQueueTypeFifo FIFO
	CreateOutputQueueTypeFifo CreateOutputQueueType = "fifo"
)

func (e CreateOutputQueueType) ToPointer() *CreateOutputQueueType {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputQueueType) IsExact() bool {
	if e != nil {
		switch *e {
		case "standard", "fifo":
			return true
		}
	}
	return false
}

// CreateOutputAuthenticationMethodSqs - AWS authentication method. Choose Auto to use IAM roles.
type CreateOutputAuthenticationMethodSqs string

const (
	// CreateOutputAuthenticationMethodSqsAuto Auto
	CreateOutputAuthenticationMethodSqsAuto CreateOutputAuthenticationMethodSqs = "auto"
	// CreateOutputAuthenticationMethodSqsManual Manual
	CreateOutputAuthenticationMethodSqsManual CreateOutputAuthenticationMethodSqs = "manual"
	// CreateOutputAuthenticationMethodSqsSecret Secret Key pair
	CreateOutputAuthenticationMethodSqsSecret CreateOutputAuthenticationMethodSqs = "secret"
)

func (e CreateOutputAuthenticationMethodSqs) ToPointer() *CreateOutputAuthenticationMethodSqs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationMethodSqs) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// CreateOutputSignatureVersionSqs - Signature version to use for signing SQS requests
type CreateOutputSignatureVersionSqs string

const (
	CreateOutputSignatureVersionSqsV2 CreateOutputSignatureVersionSqs = "v2"
	CreateOutputSignatureVersionSqsV4 CreateOutputSignatureVersionSqs = "v4"
)

func (e CreateOutputSignatureVersionSqs) ToPointer() *CreateOutputSignatureVersionSqs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputSignatureVersionSqs) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

// BackpressureBehaviorSqs - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSqs string

const (
	// BackpressureBehaviorSqsBlock Block
	BackpressureBehaviorSqsBlock BackpressureBehaviorSqs = "block"
	// BackpressureBehaviorSqsDrop Drop
	BackpressureBehaviorSqsDrop BackpressureBehaviorSqs = "drop"
	// BackpressureBehaviorSqsQueue Persistent Queue
	BackpressureBehaviorSqsQueue BackpressureBehaviorSqs = "queue"
)

func (e BackpressureBehaviorSqs) ToPointer() *BackpressureBehaviorSqs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorSqs) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// CreateOutputModeSqs - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeSqs string

const (
	// CreateOutputModeSqsError Error
	CreateOutputModeSqsError CreateOutputModeSqs = "error"
	// CreateOutputModeSqsAlways Backpressure
	CreateOutputModeSqsAlways CreateOutputModeSqs = "always"
	// CreateOutputModeSqsBackpressure Always On
	CreateOutputModeSqsBackpressure CreateOutputModeSqs = "backpressure"
)

func (e CreateOutputModeSqs) ToPointer() *CreateOutputModeSqs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeSqs) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionSqs - Codec to use to compress the persisted data
type PqCompressCompressionSqs string

const (
	// PqCompressCompressionSqsNone None
	PqCompressCompressionSqsNone PqCompressCompressionSqs = "none"
	// PqCompressCompressionSqsGzip Gzip
	PqCompressCompressionSqsGzip PqCompressCompressionSqs = "gzip"
)

func (e PqCompressCompressionSqs) ToPointer() *PqCompressCompressionSqs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionSqs) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorSqs - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSqs string

const (
	// QueueFullBehaviorSqsBlock Block
	QueueFullBehaviorSqsBlock QueueFullBehaviorSqs = "block"
	// QueueFullBehaviorSqsDrop Drop new data
	QueueFullBehaviorSqsDrop QueueFullBehaviorSqs = "drop"
)

func (e QueueFullBehaviorSqs) ToPointer() *QueueFullBehaviorSqs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorSqs) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsSqs struct {
}

func (c CreateOutputPqControlsSqs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsSqs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSqs struct {
	// Unique ID for this output
	ID   string              `json:"id"`
	Type CreateOutputTypeSqs `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The name, URL, or ARN of the SQS queue to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
	QueueName string `json:"queueName"`
	// The queue type used (or created). Defaults to Standard.
	QueueType CreateOutputQueueType `json:"queueType"`
	// SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
	AwsAccountID *string `json:"awsAccountId,omitempty"`
	// This parameter applies only to FIFO queues. The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Use event field __messageGroupId to override this value.
	MessageGroupID *string `default:"cribl" json:"messageGroupId"`
	// Create queue if it does not exist.
	CreateQueue *bool `default:"true" json:"createQueue"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *CreateOutputAuthenticationMethodSqs `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                              `json:"awsSecretKey,omitempty"`
	// AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.
	Region *string `json:"region,omitempty"`
	// SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing SQS requests
	SignatureVersion *CreateOutputSignatureVersionSqs `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access SQS
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of queued batches before blocking.
	MaxQueueSize *float64 `default:"100" json:"maxQueueSize"`
	// Maximum size (KB) of batches to send. Per the SQS spec, the max allowed value is 256 KB.
	MaxRecordSizeKB *float64 `default:"256" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// The maximum number of in-progress API requests before backpressure is applied.
	MaxInProgress *float64 `default:"10" json:"maxInProgress"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSqs `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeSqs `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionSqs `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSqs      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsSqs `json:"pqControls,omitempty"`
}

func (o OutputSqs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSqs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "queueName", "queueType"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSqs) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSqs) GetType() CreateOutputTypeSqs {
	if o == nil {
		return CreateOutputTypeSqs("")
	}
	return o.Type
}

func (o *OutputSqs) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSqs) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSqs) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSqs) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSqs) GetQueueName() string {
	if o == nil {
		return ""
	}
	return o.QueueName
}

func (o *OutputSqs) GetQueueType() CreateOutputQueueType {
	if o == nil {
		return CreateOutputQueueType("")
	}
	return o.QueueType
}

func (o *OutputSqs) GetAwsAccountID() *string {
	if o == nil {
		return nil
	}
	return o.AwsAccountID
}

func (o *OutputSqs) GetMessageGroupID() *string {
	if o == nil {
		return nil
	}
	return o.MessageGroupID
}

func (o *OutputSqs) GetCreateQueue() *bool {
	if o == nil {
		return nil
	}
	return o.CreateQueue
}

func (o *OutputSqs) GetAwsAuthenticationMethod() *CreateOutputAuthenticationMethodSqs {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSqs) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSqs) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputSqs) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSqs) GetSignatureVersion() *CreateOutputSignatureVersionSqs {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSqs) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSqs) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSqs) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSqs) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputSqs) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSqs) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSqs) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputSqs) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputSqs) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSqs) GetMaxInProgress() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxInProgress
}

func (o *OutputSqs) GetOnBackpressure() *BackpressureBehaviorSqs {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSqs) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSqs) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSqs) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSqs) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSqs) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSqs) GetPqMode() *CreateOutputModeSqs {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSqs) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSqs) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSqs) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSqs) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSqs) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSqs) GetPqCompress() *PqCompressCompressionSqs {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSqs) GetPqOnBackpressure() *QueueFullBehaviorSqs {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSqs) GetPqControls() *CreateOutputPqControlsSqs {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeSns string

const (
	TypeSnsSns TypeSns = "sns"
)

func (e TypeSns) ToPointer() *TypeSns {
	return &e
}
func (e *TypeSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sns":
		*e = TypeSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSns: %v", v)
	}
}

// AuthenticationMethodSns - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodSns string

const (
	// AuthenticationMethodSnsAuto Auto
	AuthenticationMethodSnsAuto AuthenticationMethodSns = "auto"
	// AuthenticationMethodSnsManual Manual
	AuthenticationMethodSnsManual AuthenticationMethodSns = "manual"
	// AuthenticationMethodSnsSecret Secret Key pair
	AuthenticationMethodSnsSecret AuthenticationMethodSns = "secret"
)

func (e AuthenticationMethodSns) ToPointer() *AuthenticationMethodSns {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodSns) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// SignatureVersionSns - Signature version to use for signing SNS requests
type SignatureVersionSns string

const (
	SignatureVersionSnsV2 SignatureVersionSns = "v2"
	SignatureVersionSnsV4 SignatureVersionSns = "v4"
)

func (e SignatureVersionSns) ToPointer() *SignatureVersionSns {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SignatureVersionSns) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

// BackpressureBehaviorSns - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSns string

const (
	// BackpressureBehaviorSnsBlock Block
	BackpressureBehaviorSnsBlock BackpressureBehaviorSns = "block"
	// BackpressureBehaviorSnsDrop Drop
	BackpressureBehaviorSnsDrop BackpressureBehaviorSns = "drop"
	// BackpressureBehaviorSnsQueue Persistent Queue
	BackpressureBehaviorSnsQueue BackpressureBehaviorSns = "queue"
)

func (e BackpressureBehaviorSns) ToPointer() *BackpressureBehaviorSns {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorSns) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeSns - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSns string

const (
	// ModeSnsError Error
	ModeSnsError ModeSns = "error"
	// ModeSnsAlways Backpressure
	ModeSnsAlways ModeSns = "always"
	// ModeSnsBackpressure Always On
	ModeSnsBackpressure ModeSns = "backpressure"
)

func (e ModeSns) ToPointer() *ModeSns {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeSns) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionSns - Codec to use to compress the persisted data
type CompressionSns string

const (
	// CompressionSnsNone None
	CompressionSnsNone CompressionSns = "none"
	// CompressionSnsGzip Gzip
	CompressionSnsGzip CompressionSns = "gzip"
)

func (e CompressionSns) ToPointer() *CompressionSns {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionSns) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorSns - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSns string

const (
	// QueueFullBehaviorSnsBlock Block
	QueueFullBehaviorSnsBlock QueueFullBehaviorSns = "block"
	// QueueFullBehaviorSnsDrop Drop new data
	QueueFullBehaviorSnsDrop QueueFullBehaviorSns = "drop"
)

func (e QueueFullBehaviorSns) ToPointer() *QueueFullBehaviorSns {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorSns) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsSns struct {
}

func (p PqControlsSns) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSns) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSns struct {
	// Unique ID for this output
	ID   string  `json:"id"`
	Type TypeSns `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The ARN of the SNS topic to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. E.g., 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`
	TopicArn string `json:"topicArn"`
	// Messages in the same group are processed in a FIFO manner. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
	MessageGroupID string `json:"messageGroupId"`
	// Maximum number of retries before the output returns an error. Note that not all errors are retryable. The retries use an exponential backoff policy.
	MaxRetries *float64 `json:"maxRetries,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodSns `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                  `json:"awsSecretKey,omitempty"`
	// Region where the SNS is located
	Region *string `json:"region,omitempty"`
	// SNS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SNS-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing SNS requests
	SignatureVersion *SignatureVersionSns `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access SNS
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSns `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSns `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSns `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSns `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsSns        `json:"pqControls,omitempty"`
}

func (o OutputSns) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSns) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "topicArn", "messageGroupId"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSns) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSns) GetType() TypeSns {
	if o == nil {
		return TypeSns("")
	}
	return o.Type
}

func (o *OutputSns) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSns) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSns) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSns) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSns) GetTopicArn() string {
	if o == nil {
		return ""
	}
	return o.TopicArn
}

func (o *OutputSns) GetMessageGroupID() string {
	if o == nil {
		return ""
	}
	return o.MessageGroupID
}

func (o *OutputSns) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputSns) GetAwsAuthenticationMethod() *AuthenticationMethodSns {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSns) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSns) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputSns) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSns) GetSignatureVersion() *SignatureVersionSns {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSns) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSns) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSns) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSns) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputSns) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSns) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSns) GetOnBackpressure() *BackpressureBehaviorSns {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSns) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSns) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSns) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSns) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSns) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSns) GetPqMode() *ModeSns {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSns) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSns) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSns) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSns) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSns) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSns) GetPqCompress() *CompressionSns {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSns) GetPqOnBackpressure() *QueueFullBehaviorSns {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSns) GetPqControls() *PqControlsSns {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeRouter string

const (
	TypeRouterRouter TypeRouter = "router"
)

func (e TypeRouter) ToPointer() *TypeRouter {
	return &e
}
func (e *TypeRouter) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "router":
		*e = TypeRouter(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeRouter: %v", v)
	}
}

type CreateOutputRule struct {
	// JavaScript expression to select events to send to output
	Filter string `json:"filter"`
	// Output to send matching events to
	Output string `json:"output"`
	// Description of this rule's purpose
	Description *string `json:"description,omitempty"`
	// Flag to control whether to stop the event from being checked against other rules
	Final *bool `default:"true" json:"final"`
}

func (c CreateOutputRule) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputRule) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"filter", "output"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputRule) GetFilter() string {
	if c == nil {
		return ""
	}
	return c.Filter
}

func (c *CreateOutputRule) GetOutput() string {
	if c == nil {
		return ""
	}
	return c.Output
}

func (c *CreateOutputRule) GetDescription() *string {
	if c == nil {
		return nil
	}
	return c.Description
}

func (c *CreateOutputRule) GetFinal() *bool {
	if c == nil {
		return nil
	}
	return c.Final
}

type OutputRouter struct {
	// Unique ID for this output
	ID   string     `json:"id"`
	Type TypeRouter `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Event routing rules
	Rules       []CreateOutputRule `json:"rules"`
	Description *string            `json:"description,omitempty"`
}

func (o OutputRouter) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputRouter) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "rules"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputRouter) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputRouter) GetType() TypeRouter {
	if o == nil {
		return TypeRouter("")
	}
	return o.Type
}

func (o *OutputRouter) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputRouter) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputRouter) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputRouter) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputRouter) GetRules() []CreateOutputRule {
	if o == nil {
		return []CreateOutputRule{}
	}
	return o.Rules
}

func (o *OutputRouter) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

type TypeGraphite string

const (
	TypeGraphiteGraphite TypeGraphite = "graphite"
)

func (e TypeGraphite) ToPointer() *TypeGraphite {
	return &e
}
func (e *TypeGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "graphite":
		*e = TypeGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGraphite: %v", v)
	}
}

// DestinationProtocolGraphite - Protocol to use when communicating with the destination.
type DestinationProtocolGraphite string

const (
	// DestinationProtocolGraphiteUDP UDP
	DestinationProtocolGraphiteUDP DestinationProtocolGraphite = "udp"
	// DestinationProtocolGraphiteTCP TCP
	DestinationProtocolGraphiteTCP DestinationProtocolGraphite = "tcp"
)

func (e DestinationProtocolGraphite) ToPointer() *DestinationProtocolGraphite {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DestinationProtocolGraphite) IsExact() bool {
	if e != nil {
		switch *e {
		case "udp", "tcp":
			return true
		}
	}
	return false
}

// BackpressureBehaviorGraphite - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGraphite string

const (
	// BackpressureBehaviorGraphiteBlock Block
	BackpressureBehaviorGraphiteBlock BackpressureBehaviorGraphite = "block"
	// BackpressureBehaviorGraphiteDrop Drop
	BackpressureBehaviorGraphiteDrop BackpressureBehaviorGraphite = "drop"
	// BackpressureBehaviorGraphiteQueue Persistent Queue
	BackpressureBehaviorGraphiteQueue BackpressureBehaviorGraphite = "queue"
)

func (e BackpressureBehaviorGraphite) ToPointer() *BackpressureBehaviorGraphite {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorGraphite) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeGraphite - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeGraphite string

const (
	// ModeGraphiteError Error
	ModeGraphiteError ModeGraphite = "error"
	// ModeGraphiteAlways Backpressure
	ModeGraphiteAlways ModeGraphite = "always"
	// ModeGraphiteBackpressure Always On
	ModeGraphiteBackpressure ModeGraphite = "backpressure"
)

func (e ModeGraphite) ToPointer() *ModeGraphite {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeGraphite) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionGraphite - Codec to use to compress the persisted data
type CompressionGraphite string

const (
	// CompressionGraphiteNone None
	CompressionGraphiteNone CompressionGraphite = "none"
	// CompressionGraphiteGzip Gzip
	CompressionGraphiteGzip CompressionGraphite = "gzip"
)

func (e CompressionGraphite) ToPointer() *CompressionGraphite {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionGraphite) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorGraphite - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGraphite string

const (
	// QueueFullBehaviorGraphiteBlock Block
	QueueFullBehaviorGraphiteBlock QueueFullBehaviorGraphite = "block"
	// QueueFullBehaviorGraphiteDrop Drop new data
	QueueFullBehaviorGraphiteDrop QueueFullBehaviorGraphite = "drop"
)

func (e QueueFullBehaviorGraphite) ToPointer() *QueueFullBehaviorGraphite {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorGraphite) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsGraphite struct {
}

func (p PqControlsGraphite) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsGraphite) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGraphite struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type TypeGraphite `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolGraphite `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGraphite `default:"block" json:"onBackpressure"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeGraphite `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionGraphite `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGraphite `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsGraphite        `json:"pqControls,omitempty"`
}

func (o OutputGraphite) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGraphite) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "host"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGraphite) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGraphite) GetType() TypeGraphite {
	if o == nil {
		return TypeGraphite("")
	}
	return o.Type
}

func (o *OutputGraphite) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGraphite) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGraphite) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGraphite) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGraphite) GetProtocol() *DestinationProtocolGraphite {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputGraphite) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputGraphite) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputGraphite) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputGraphite) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGraphite) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputGraphite) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGraphite) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputGraphite) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputGraphite) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputGraphite) GetOnBackpressure() *BackpressureBehaviorGraphite {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGraphite) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGraphite) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGraphite) GetPqMode() *ModeGraphite {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGraphite) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGraphite) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGraphite) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGraphite) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGraphite) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGraphite) GetPqCompress() *CompressionGraphite {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGraphite) GetPqOnBackpressure() *QueueFullBehaviorGraphite {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGraphite) GetPqControls() *PqControlsGraphite {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeStatsdExt string

const (
	TypeStatsdExtStatsdExt TypeStatsdExt = "statsd_ext"
)

func (e TypeStatsdExt) ToPointer() *TypeStatsdExt {
	return &e
}
func (e *TypeStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "statsd_ext":
		*e = TypeStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeStatsdExt: %v", v)
	}
}

// DestinationProtocolStatsdExt - Protocol to use when communicating with the destination.
type DestinationProtocolStatsdExt string

const (
	// DestinationProtocolStatsdExtUDP UDP
	DestinationProtocolStatsdExtUDP DestinationProtocolStatsdExt = "udp"
	// DestinationProtocolStatsdExtTCP TCP
	DestinationProtocolStatsdExtTCP DestinationProtocolStatsdExt = "tcp"
)

func (e DestinationProtocolStatsdExt) ToPointer() *DestinationProtocolStatsdExt {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DestinationProtocolStatsdExt) IsExact() bool {
	if e != nil {
		switch *e {
		case "udp", "tcp":
			return true
		}
	}
	return false
}

// BackpressureBehaviorStatsdExt - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorStatsdExt string

const (
	// BackpressureBehaviorStatsdExtBlock Block
	BackpressureBehaviorStatsdExtBlock BackpressureBehaviorStatsdExt = "block"
	// BackpressureBehaviorStatsdExtDrop Drop
	BackpressureBehaviorStatsdExtDrop BackpressureBehaviorStatsdExt = "drop"
	// BackpressureBehaviorStatsdExtQueue Persistent Queue
	BackpressureBehaviorStatsdExtQueue BackpressureBehaviorStatsdExt = "queue"
)

func (e BackpressureBehaviorStatsdExt) ToPointer() *BackpressureBehaviorStatsdExt {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorStatsdExt) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeStatsdExt - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeStatsdExt string

const (
	// ModeStatsdExtError Error
	ModeStatsdExtError ModeStatsdExt = "error"
	// ModeStatsdExtAlways Backpressure
	ModeStatsdExtAlways ModeStatsdExt = "always"
	// ModeStatsdExtBackpressure Always On
	ModeStatsdExtBackpressure ModeStatsdExt = "backpressure"
)

func (e ModeStatsdExt) ToPointer() *ModeStatsdExt {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeStatsdExt) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionStatsdExt - Codec to use to compress the persisted data
type CompressionStatsdExt string

const (
	// CompressionStatsdExtNone None
	CompressionStatsdExtNone CompressionStatsdExt = "none"
	// CompressionStatsdExtGzip Gzip
	CompressionStatsdExtGzip CompressionStatsdExt = "gzip"
)

func (e CompressionStatsdExt) ToPointer() *CompressionStatsdExt {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionStatsdExt) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorStatsdExt - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorStatsdExt string

const (
	// QueueFullBehaviorStatsdExtBlock Block
	QueueFullBehaviorStatsdExtBlock QueueFullBehaviorStatsdExt = "block"
	// QueueFullBehaviorStatsdExtDrop Drop new data
	QueueFullBehaviorStatsdExtDrop QueueFullBehaviorStatsdExt = "drop"
)

func (e QueueFullBehaviorStatsdExt) ToPointer() *QueueFullBehaviorStatsdExt {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorStatsdExt) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsStatsdExt struct {
}

func (p PqControlsStatsdExt) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsStatsdExt) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputStatsdExt struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeStatsdExt `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolStatsdExt `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorStatsdExt `default:"block" json:"onBackpressure"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeStatsdExt `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionStatsdExt `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorStatsdExt `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsStatsdExt        `json:"pqControls,omitempty"`
}

func (o OutputStatsdExt) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputStatsdExt) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "host"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputStatsdExt) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputStatsdExt) GetType() TypeStatsdExt {
	if o == nil {
		return TypeStatsdExt("")
	}
	return o.Type
}

func (o *OutputStatsdExt) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputStatsdExt) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputStatsdExt) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputStatsdExt) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputStatsdExt) GetProtocol() *DestinationProtocolStatsdExt {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputStatsdExt) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputStatsdExt) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputStatsdExt) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputStatsdExt) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputStatsdExt) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputStatsdExt) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputStatsdExt) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputStatsdExt) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputStatsdExt) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputStatsdExt) GetOnBackpressure() *BackpressureBehaviorStatsdExt {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputStatsdExt) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputStatsdExt) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputStatsdExt) GetPqMode() *ModeStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputStatsdExt) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputStatsdExt) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputStatsdExt) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputStatsdExt) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputStatsdExt) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputStatsdExt) GetPqCompress() *CompressionStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputStatsdExt) GetPqOnBackpressure() *QueueFullBehaviorStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputStatsdExt) GetPqControls() *PqControlsStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeStatsd string

const (
	TypeStatsdStatsd TypeStatsd = "statsd"
)

func (e TypeStatsd) ToPointer() *TypeStatsd {
	return &e
}
func (e *TypeStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "statsd":
		*e = TypeStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeStatsd: %v", v)
	}
}

// DestinationProtocolStatsd - Protocol to use when communicating with the destination.
type DestinationProtocolStatsd string

const (
	// DestinationProtocolStatsdUDP UDP
	DestinationProtocolStatsdUDP DestinationProtocolStatsd = "udp"
	// DestinationProtocolStatsdTCP TCP
	DestinationProtocolStatsdTCP DestinationProtocolStatsd = "tcp"
)

func (e DestinationProtocolStatsd) ToPointer() *DestinationProtocolStatsd {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DestinationProtocolStatsd) IsExact() bool {
	if e != nil {
		switch *e {
		case "udp", "tcp":
			return true
		}
	}
	return false
}

// BackpressureBehaviorStatsd - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorStatsd string

const (
	// BackpressureBehaviorStatsdBlock Block
	BackpressureBehaviorStatsdBlock BackpressureBehaviorStatsd = "block"
	// BackpressureBehaviorStatsdDrop Drop
	BackpressureBehaviorStatsdDrop BackpressureBehaviorStatsd = "drop"
	// BackpressureBehaviorStatsdQueue Persistent Queue
	BackpressureBehaviorStatsdQueue BackpressureBehaviorStatsd = "queue"
)

func (e BackpressureBehaviorStatsd) ToPointer() *BackpressureBehaviorStatsd {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorStatsd) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeStatsd - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeStatsd string

const (
	// ModeStatsdError Error
	ModeStatsdError ModeStatsd = "error"
	// ModeStatsdAlways Backpressure
	ModeStatsdAlways ModeStatsd = "always"
	// ModeStatsdBackpressure Always On
	ModeStatsdBackpressure ModeStatsd = "backpressure"
)

func (e ModeStatsd) ToPointer() *ModeStatsd {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeStatsd) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionStatsd - Codec to use to compress the persisted data
type CompressionStatsd string

const (
	// CompressionStatsdNone None
	CompressionStatsdNone CompressionStatsd = "none"
	// CompressionStatsdGzip Gzip
	CompressionStatsdGzip CompressionStatsd = "gzip"
)

func (e CompressionStatsd) ToPointer() *CompressionStatsd {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionStatsd) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorStatsd - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorStatsd string

const (
	// QueueFullBehaviorStatsdBlock Block
	QueueFullBehaviorStatsdBlock QueueFullBehaviorStatsd = "block"
	// QueueFullBehaviorStatsdDrop Drop new data
	QueueFullBehaviorStatsdDrop QueueFullBehaviorStatsd = "drop"
)

func (e QueueFullBehaviorStatsd) ToPointer() *QueueFullBehaviorStatsd {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorStatsd) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsStatsd struct {
}

func (p PqControlsStatsd) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsStatsd) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputStatsd struct {
	// Unique ID for this output
	ID   string     `json:"id"`
	Type TypeStatsd `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolStatsd `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorStatsd `default:"block" json:"onBackpressure"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeStatsd `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionStatsd `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorStatsd `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsStatsd        `json:"pqControls,omitempty"`
}

func (o OutputStatsd) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputStatsd) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "host"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputStatsd) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputStatsd) GetType() TypeStatsd {
	if o == nil {
		return TypeStatsd("")
	}
	return o.Type
}

func (o *OutputStatsd) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputStatsd) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputStatsd) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputStatsd) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputStatsd) GetProtocol() *DestinationProtocolStatsd {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputStatsd) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputStatsd) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputStatsd) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputStatsd) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputStatsd) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputStatsd) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputStatsd) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputStatsd) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputStatsd) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputStatsd) GetOnBackpressure() *BackpressureBehaviorStatsd {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputStatsd) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputStatsd) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputStatsd) GetPqMode() *ModeStatsd {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputStatsd) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputStatsd) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputStatsd) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputStatsd) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputStatsd) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputStatsd) GetPqCompress() *CompressionStatsd {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputStatsd) GetPqOnBackpressure() *QueueFullBehaviorStatsd {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputStatsd) GetPqControls() *PqControlsStatsd {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeMinio string

const (
	TypeMinioMinio TypeMinio = "minio"
)

func (e TypeMinio) ToPointer() *TypeMinio {
	return &e
}
func (e *TypeMinio) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "minio":
		*e = TypeMinio(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeMinio: %v", v)
	}
}

// AuthenticationMethodMinio - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodMinio string

const (
	// AuthenticationMethodMinioAuto Auto
	AuthenticationMethodMinioAuto AuthenticationMethodMinio = "auto"
	// AuthenticationMethodMinioManual Manual
	AuthenticationMethodMinioManual AuthenticationMethodMinio = "manual"
	// AuthenticationMethodMinioSecret Secret Key pair
	AuthenticationMethodMinioSecret AuthenticationMethodMinio = "secret"
)

func (e AuthenticationMethodMinio) ToPointer() *AuthenticationMethodMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// SignatureVersionMinio - Signature version to use for signing MinIO requests
type SignatureVersionMinio string

const (
	SignatureVersionMinioV2 SignatureVersionMinio = "v2"
	SignatureVersionMinioV4 SignatureVersionMinio = "v4"
)

func (e SignatureVersionMinio) ToPointer() *SignatureVersionMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SignatureVersionMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

// ObjectACLMinio - Object ACL to assign to uploaded objects
type ObjectACLMinio string

const (
	// ObjectACLMinioPrivate Private
	ObjectACLMinioPrivate ObjectACLMinio = "private"
	// ObjectACLMinioPublicRead Public Read Only
	ObjectACLMinioPublicRead ObjectACLMinio = "public-read"
	// ObjectACLMinioPublicReadWrite Public Read/Write
	ObjectACLMinioPublicReadWrite ObjectACLMinio = "public-read-write"
	// ObjectACLMinioAuthenticatedRead Authenticated Read Only
	ObjectACLMinioAuthenticatedRead ObjectACLMinio = "authenticated-read"
	// ObjectACLMinioAwsExecRead AWS EC2 AMI Read Only
	ObjectACLMinioAwsExecRead ObjectACLMinio = "aws-exec-read"
	// ObjectACLMinioBucketOwnerRead Bucket Owner Read Only
	ObjectACLMinioBucketOwnerRead ObjectACLMinio = "bucket-owner-read"
	// ObjectACLMinioBucketOwnerFullControl Bucket Owner Full Control
	ObjectACLMinioBucketOwnerFullControl ObjectACLMinio = "bucket-owner-full-control"
)

func (e ObjectACLMinio) ToPointer() *ObjectACLMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ObjectACLMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "private", "public-read", "public-read-write", "authenticated-read", "aws-exec-read", "bucket-owner-read", "bucket-owner-full-control":
			return true
		}
	}
	return false
}

// StorageClassMinio - Storage class to select for uploaded objects
type StorageClassMinio string

const (
	// StorageClassMinioStandard Standard
	StorageClassMinioStandard StorageClassMinio = "STANDARD"
	// StorageClassMinioReducedRedundancy Reduced Redundancy Storage
	StorageClassMinioReducedRedundancy StorageClassMinio = "REDUCED_REDUNDANCY"
)

func (e StorageClassMinio) ToPointer() *StorageClassMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *StorageClassMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "STANDARD", "REDUCED_REDUNDANCY":
			return true
		}
	}
	return false
}

// ServerSideEncryptionMinio - Server-side encryption for uploaded objects
type ServerSideEncryptionMinio string

const (
	// ServerSideEncryptionMinioAes256 Amazon S3 Managed Key
	ServerSideEncryptionMinioAes256 ServerSideEncryptionMinio = "AES256"
)

func (e ServerSideEncryptionMinio) ToPointer() *ServerSideEncryptionMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ServerSideEncryptionMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "AES256":
			return true
		}
	}
	return false
}

// DataFormatMinio - Format of the output data
type DataFormatMinio string

const (
	// DataFormatMinioJSON JSON
	DataFormatMinioJSON DataFormatMinio = "json"
	// DataFormatMinioRaw Raw
	DataFormatMinioRaw DataFormatMinio = "raw"
	// DataFormatMinioParquet Parquet
	DataFormatMinioParquet DataFormatMinio = "parquet"
)

func (e DataFormatMinio) ToPointer() *DataFormatMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataFormatMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "parquet":
			return true
		}
	}
	return false
}

// BackpressureBehaviorMinio - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorMinio string

const (
	// BackpressureBehaviorMinioBlock Block
	BackpressureBehaviorMinioBlock BackpressureBehaviorMinio = "block"
	// BackpressureBehaviorMinioDrop Drop
	BackpressureBehaviorMinioDrop BackpressureBehaviorMinio = "drop"
)

func (e BackpressureBehaviorMinio) ToPointer() *BackpressureBehaviorMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionMinio - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionMinio string

const (
	// DiskSpaceProtectionMinioBlock Block
	DiskSpaceProtectionMinioBlock DiskSpaceProtectionMinio = "block"
	// DiskSpaceProtectionMinioDrop Drop
	DiskSpaceProtectionMinioDrop DiskSpaceProtectionMinio = "drop"
)

func (e DiskSpaceProtectionMinio) ToPointer() *DiskSpaceProtectionMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// CompressionMinio - Data compression format to apply to HTTP content before it is delivered
type CompressionMinio string

const (
	CompressionMinioNone CompressionMinio = "none"
	CompressionMinioGzip CompressionMinio = "gzip"
)

func (e CompressionMinio) ToPointer() *CompressionMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// CompressionLevelMinio - Compression level to apply before moving files to final destination
type CompressionLevelMinio string

const (
	// CompressionLevelMinioBestSpeed Best Speed
	CompressionLevelMinioBestSpeed CompressionLevelMinio = "best_speed"
	// CompressionLevelMinioNormal Normal
	CompressionLevelMinioNormal CompressionLevelMinio = "normal"
	// CompressionLevelMinioBestCompression Best Compression
	CompressionLevelMinioBestCompression CompressionLevelMinio = "best_compression"
)

func (e CompressionLevelMinio) ToPointer() *CompressionLevelMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionLevelMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "best_speed", "normal", "best_compression":
			return true
		}
	}
	return false
}

// ParquetVersionMinio - Determines which data types are supported and how they are represented
type ParquetVersionMinio string

const (
	// ParquetVersionMinioParquet10 1.0
	ParquetVersionMinioParquet10 ParquetVersionMinio = "PARQUET_1_0"
	// ParquetVersionMinioParquet24 2.4
	ParquetVersionMinioParquet24 ParquetVersionMinio = "PARQUET_2_4"
	// ParquetVersionMinioParquet26 2.6
	ParquetVersionMinioParquet26 ParquetVersionMinio = "PARQUET_2_6"
)

func (e ParquetVersionMinio) ToPointer() *ParquetVersionMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ParquetVersionMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6":
			return true
		}
	}
	return false
}

// DataPageVersionMinio - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionMinio string

const (
	// DataPageVersionMinioDataPageV1 V1
	DataPageVersionMinioDataPageV1 DataPageVersionMinio = "DATA_PAGE_V1"
	// DataPageVersionMinioDataPageV2 V2
	DataPageVersionMinioDataPageV2 DataPageVersionMinio = "DATA_PAGE_V2"
)

func (e DataPageVersionMinio) ToPointer() *DataPageVersionMinio {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataPageVersionMinio) IsExact() bool {
	if e != nil {
		switch *e {
		case "DATA_PAGE_V1", "DATA_PAGE_V2":
			return true
		}
	}
	return false
}

type KeyValueMetadatumMinio struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumMinio) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumMinio) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumMinio) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumMinio) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputMinio struct {
	// Unique ID for this output
	ID   string    `json:"id"`
	Type TypeMinio `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// MinIO service url (e.g. http://minioHost:9000)
	Endpoint string `json:"endpoint"`
	// Name of the destination MinIO bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodMinio `default:"auto" json:"awsAuthenticationMethod"`
	// Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// Region where the MinIO service/cluster is located
	Region *string `json:"region,omitempty"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.
	DestPath *string `json:"destPath,omitempty"`
	// Signature version to use for signing MinIO requests
	SignatureVersion *SignatureVersionMinio `default:"v4" json:"signatureVersion"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLMinio `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass *StorageClassMinio `json:"storageClass,omitempty"`
	// Server-side encryption for uploaded objects
	ServerSideEncryption *ServerSideEncryptionMinio `json:"serverSideEncryption,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatMinio `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorMinio `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionMinio `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	Description            *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionMinio `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelMinio `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionMinio `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionMinio `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumMinio `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputMinio) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMinio) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "endpoint", "bucket"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputMinio) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputMinio) GetType() TypeMinio {
	if o == nil {
		return TypeMinio("")
	}
	return o.Type
}

func (o *OutputMinio) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputMinio) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputMinio) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputMinio) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputMinio) GetEndpoint() string {
	if o == nil {
		return ""
	}
	return o.Endpoint
}

func (o *OutputMinio) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputMinio) GetAwsAuthenticationMethod() *AuthenticationMethodMinio {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputMinio) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputMinio) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputMinio) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputMinio) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputMinio) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputMinio) GetSignatureVersion() *SignatureVersionMinio {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputMinio) GetObjectACL() *ObjectACLMinio {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputMinio) GetStorageClass() *StorageClassMinio {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputMinio) GetServerSideEncryption() *ServerSideEncryptionMinio {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputMinio) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputMinio) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputMinio) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputMinio) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputMinio) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputMinio) GetFormat() *DataFormatMinio {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputMinio) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputMinio) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputMinio) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputMinio) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputMinio) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputMinio) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputMinio) GetOnBackpressure() *BackpressureBehaviorMinio {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputMinio) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputMinio) GetOnDiskFullBackpressure() *DiskSpaceProtectionMinio {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputMinio) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputMinio) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputMinio) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputMinio) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputMinio) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputMinio) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputMinio) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputMinio) GetCompress() *CompressionMinio {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputMinio) GetCompressionLevel() *CompressionLevelMinio {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputMinio) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputMinio) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputMinio) GetParquetVersion() *ParquetVersionMinio {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputMinio) GetParquetDataPageVersion() *DataPageVersionMinio {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputMinio) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputMinio) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputMinio) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputMinio) GetKeyValueMetadata() []KeyValueMetadatumMinio {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputMinio) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputMinio) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputMinio) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputMinio) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputMinio) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputMinio) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputMinio) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeCloudwatch string

const (
	TypeCloudwatchCloudwatch TypeCloudwatch = "cloudwatch"
)

func (e TypeCloudwatch) ToPointer() *TypeCloudwatch {
	return &e
}
func (e *TypeCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cloudwatch":
		*e = TypeCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCloudwatch: %v", v)
	}
}

// AuthenticationMethodCloudwatch - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodCloudwatch string

const (
	// AuthenticationMethodCloudwatchAuto Auto
	AuthenticationMethodCloudwatchAuto AuthenticationMethodCloudwatch = "auto"
	// AuthenticationMethodCloudwatchManual Manual
	AuthenticationMethodCloudwatchManual AuthenticationMethodCloudwatch = "manual"
	// AuthenticationMethodCloudwatchSecret Secret Key pair
	AuthenticationMethodCloudwatchSecret AuthenticationMethodCloudwatch = "secret"
)

func (e AuthenticationMethodCloudwatch) ToPointer() *AuthenticationMethodCloudwatch {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodCloudwatch) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// BackpressureBehaviorCloudwatch - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCloudwatch string

const (
	// BackpressureBehaviorCloudwatchBlock Block
	BackpressureBehaviorCloudwatchBlock BackpressureBehaviorCloudwatch = "block"
	// BackpressureBehaviorCloudwatchDrop Drop
	BackpressureBehaviorCloudwatchDrop BackpressureBehaviorCloudwatch = "drop"
	// BackpressureBehaviorCloudwatchQueue Persistent Queue
	BackpressureBehaviorCloudwatchQueue BackpressureBehaviorCloudwatch = "queue"
)

func (e BackpressureBehaviorCloudwatch) ToPointer() *BackpressureBehaviorCloudwatch {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorCloudwatch) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeCloudwatch - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeCloudwatch string

const (
	// ModeCloudwatchError Error
	ModeCloudwatchError ModeCloudwatch = "error"
	// ModeCloudwatchAlways Backpressure
	ModeCloudwatchAlways ModeCloudwatch = "always"
	// ModeCloudwatchBackpressure Always On
	ModeCloudwatchBackpressure ModeCloudwatch = "backpressure"
)

func (e ModeCloudwatch) ToPointer() *ModeCloudwatch {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeCloudwatch) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionCloudwatch - Codec to use to compress the persisted data
type CompressionCloudwatch string

const (
	// CompressionCloudwatchNone None
	CompressionCloudwatchNone CompressionCloudwatch = "none"
	// CompressionCloudwatchGzip Gzip
	CompressionCloudwatchGzip CompressionCloudwatch = "gzip"
)

func (e CompressionCloudwatch) ToPointer() *CompressionCloudwatch {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionCloudwatch) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorCloudwatch - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCloudwatch string

const (
	// QueueFullBehaviorCloudwatchBlock Block
	QueueFullBehaviorCloudwatchBlock QueueFullBehaviorCloudwatch = "block"
	// QueueFullBehaviorCloudwatchDrop Drop new data
	QueueFullBehaviorCloudwatchDrop QueueFullBehaviorCloudwatch = "drop"
)

func (e QueueFullBehaviorCloudwatch) ToPointer() *QueueFullBehaviorCloudwatch {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorCloudwatch) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsCloudwatch struct {
}

func (p PqControlsCloudwatch) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsCloudwatch) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputCloudwatch struct {
	// Unique ID for this output
	ID   string         `json:"id"`
	Type TypeCloudwatch `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// CloudWatch log group to associate events with
	LogGroupName string `json:"logGroupName"`
	// Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId
	LogStreamName string `json:"logStreamName"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodCloudwatch `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                         `json:"awsSecretKey,omitempty"`
	// Region where the CloudWatchLogs is located
	Region string `json:"region"`
	// CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access CloudWatchLogs
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of queued batches before blocking
	MaxQueueSize *float64 `default:"5" json:"maxQueueSize"`
	// Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size
	MaxRecordSizeKB *float64 `default:"1024" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCloudwatch `default:"block" json:"onBackpressure"`
	Description    *string                         `json:"description,omitempty"`
	AwsAPIKey      *string                         `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeCloudwatch `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionCloudwatch `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorCloudwatch `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsCloudwatch        `json:"pqControls,omitempty"`
}

func (o OutputCloudwatch) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCloudwatch) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "logGroupName", "logStreamName", "region"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCloudwatch) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputCloudwatch) GetType() TypeCloudwatch {
	if o == nil {
		return TypeCloudwatch("")
	}
	return o.Type
}

func (o *OutputCloudwatch) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCloudwatch) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCloudwatch) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCloudwatch) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCloudwatch) GetLogGroupName() string {
	if o == nil {
		return ""
	}
	return o.LogGroupName
}

func (o *OutputCloudwatch) GetLogStreamName() string {
	if o == nil {
		return ""
	}
	return o.LogStreamName
}

func (o *OutputCloudwatch) GetAwsAuthenticationMethod() *AuthenticationMethodCloudwatch {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputCloudwatch) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputCloudwatch) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputCloudwatch) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputCloudwatch) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputCloudwatch) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCloudwatch) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputCloudwatch) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputCloudwatch) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputCloudwatch) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputCloudwatch) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputCloudwatch) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputCloudwatch) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCloudwatch) GetOnBackpressure() *BackpressureBehaviorCloudwatch {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCloudwatch) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCloudwatch) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputCloudwatch) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputCloudwatch) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputCloudwatch) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputCloudwatch) GetPqMode() *ModeCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCloudwatch) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputCloudwatch) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputCloudwatch) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCloudwatch) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCloudwatch) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCloudwatch) GetPqCompress() *CompressionCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCloudwatch) GetPqOnBackpressure() *QueueFullBehaviorCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCloudwatch) GetPqControls() *PqControlsCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeInfluxdb string

const (
	TypeInfluxdbInfluxdb TypeInfluxdb = "influxdb"
)

func (e TypeInfluxdb) ToPointer() *TypeInfluxdb {
	return &e
}
func (e *TypeInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "influxdb":
		*e = TypeInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeInfluxdb: %v", v)
	}
}

// TimestampPrecision - Sets the precision for the supplied Unix time values. Defaults to milliseconds.
type TimestampPrecision string

const (
	// TimestampPrecisionNs Nanoseconds
	TimestampPrecisionNs TimestampPrecision = "ns"
	// TimestampPrecisionU Microseconds
	TimestampPrecisionU TimestampPrecision = "u"
	// TimestampPrecisionMs Milliseconds
	TimestampPrecisionMs TimestampPrecision = "ms"
	// TimestampPrecisionS Seconds
	TimestampPrecisionS TimestampPrecision = "s"
	// TimestampPrecisionM Minutes
	TimestampPrecisionM TimestampPrecision = "m"
	// TimestampPrecisionH Hours
	TimestampPrecisionH TimestampPrecision = "h"
)

func (e TimestampPrecision) ToPointer() *TimestampPrecision {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *TimestampPrecision) IsExact() bool {
	if e != nil {
		switch *e {
		case "ns", "u", "ms", "s", "m", "h":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderInfluxdb struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderInfluxdb) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderInfluxdb) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeInfluxdb - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeInfluxdb string

const (
	// FailedRequestLoggingModeInfluxdbPayload Payload
	FailedRequestLoggingModeInfluxdbPayload FailedRequestLoggingModeInfluxdb = "payload"
	// FailedRequestLoggingModeInfluxdbPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeInfluxdbPayloadAndHeaders FailedRequestLoggingModeInfluxdb = "payloadAndHeaders"
	// FailedRequestLoggingModeInfluxdbNone None
	FailedRequestLoggingModeInfluxdbNone FailedRequestLoggingModeInfluxdb = "none"
)

func (e FailedRequestLoggingModeInfluxdb) ToPointer() *FailedRequestLoggingModeInfluxdb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeInfluxdb) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingInfluxdb struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingInfluxdb) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingInfluxdb) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingInfluxdb) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingInfluxdb) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsInfluxdb struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsInfluxdb) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsInfluxdb) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsInfluxdb) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsInfluxdb) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorInfluxdb - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorInfluxdb string

const (
	// BackpressureBehaviorInfluxdbBlock Block
	BackpressureBehaviorInfluxdbBlock BackpressureBehaviorInfluxdb = "block"
	// BackpressureBehaviorInfluxdbDrop Drop
	BackpressureBehaviorInfluxdbDrop BackpressureBehaviorInfluxdb = "drop"
	// BackpressureBehaviorInfluxdbQueue Persistent Queue
	BackpressureBehaviorInfluxdbQueue BackpressureBehaviorInfluxdb = "queue"
)

func (e BackpressureBehaviorInfluxdb) ToPointer() *BackpressureBehaviorInfluxdb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorInfluxdb) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// AuthenticationTypeInfluxdb - InfluxDB authentication type
type AuthenticationTypeInfluxdb string

const (
	AuthenticationTypeInfluxdbNone              AuthenticationTypeInfluxdb = "none"
	AuthenticationTypeInfluxdbBasic             AuthenticationTypeInfluxdb = "basic"
	AuthenticationTypeInfluxdbCredentialsSecret AuthenticationTypeInfluxdb = "credentialsSecret"
	AuthenticationTypeInfluxdbToken             AuthenticationTypeInfluxdb = "token"
	AuthenticationTypeInfluxdbTextSecret        AuthenticationTypeInfluxdb = "textSecret"
	AuthenticationTypeInfluxdbOauth             AuthenticationTypeInfluxdb = "oauth"
)

func (e AuthenticationTypeInfluxdb) ToPointer() *AuthenticationTypeInfluxdb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationTypeInfluxdb) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "basic", "credentialsSecret", "token", "textSecret", "oauth":
			return true
		}
	}
	return false
}

// ModeInfluxdb - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeInfluxdb string

const (
	// ModeInfluxdbError Error
	ModeInfluxdbError ModeInfluxdb = "error"
	// ModeInfluxdbAlways Backpressure
	ModeInfluxdbAlways ModeInfluxdb = "always"
	// ModeInfluxdbBackpressure Always On
	ModeInfluxdbBackpressure ModeInfluxdb = "backpressure"
)

func (e ModeInfluxdb) ToPointer() *ModeInfluxdb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeInfluxdb) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionInfluxdb - Codec to use to compress the persisted data
type CompressionInfluxdb string

const (
	// CompressionInfluxdbNone None
	CompressionInfluxdbNone CompressionInfluxdb = "none"
	// CompressionInfluxdbGzip Gzip
	CompressionInfluxdbGzip CompressionInfluxdb = "gzip"
)

func (e CompressionInfluxdb) ToPointer() *CompressionInfluxdb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionInfluxdb) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorInfluxdb - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorInfluxdb string

const (
	// QueueFullBehaviorInfluxdbBlock Block
	QueueFullBehaviorInfluxdbBlock QueueFullBehaviorInfluxdb = "block"
	// QueueFullBehaviorInfluxdbDrop Drop new data
	QueueFullBehaviorInfluxdbDrop QueueFullBehaviorInfluxdb = "drop"
)

func (e QueueFullBehaviorInfluxdb) ToPointer() *QueueFullBehaviorInfluxdb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorInfluxdb) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsInfluxdb struct {
}

func (p PqControlsInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OauthParamInfluxdb struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o OauthParamInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthParamInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthParamInfluxdb) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamInfluxdb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderInfluxdb struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o OauthHeaderInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthHeaderInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthHeaderInfluxdb) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderInfluxdb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputInfluxdb struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type TypeInfluxdb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL of an InfluxDB cluster to send events to, e.g., http://localhost:8086/write
	URL string `json:"url"`
	// The v2 API can be enabled with InfluxDB versions 1.8 and later.
	UseV2API *bool `default:"false" json:"useV2API"`
	// Sets the precision for the supplied Unix time values. Defaults to milliseconds.
	TimestampPrecision *TimestampPrecision `default:"ms" json:"timestampPrecision"`
	// Enabling this will pull the value field from the metric name. E,g, 'db.query.user' will use 'db.query' as the measurement and 'user' as the value field.
	DynamicValueFieldName *bool `default:"true" json:"dynamicValueFieldName"`
	// Name of the field in which to store the metric when sending to InfluxDB. If dynamic generation is enabled and fails, this will be used as a fallback.
	ValueFieldName *string `default:"value" json:"valueFieldName"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderInfluxdb `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeInfluxdb `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingInfluxdb `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsInfluxdb  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorInfluxdb `default:"block" json:"onBackpressure"`
	// InfluxDB authentication type
	AuthType    *AuthenticationTypeInfluxdb `default:"none" json:"authType"`
	Description *string                     `json:"description,omitempty"`
	// Database to write to.
	Database *string `json:"database,omitempty"`
	// Bucket to write to.
	Bucket *string `json:"bucket,omitempty"`
	// Organization ID for this bucket.
	Org *string `json:"org,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeInfluxdb `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionInfluxdb `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorInfluxdb `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsInfluxdb        `json:"pqControls,omitempty"`
	Username         *string                    `json:"username,omitempty"`
	Password         *string                    `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamInfluxdb `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderInfluxdb `json:"oauthHeaders,omitempty"`
}

func (o OutputInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "url"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputInfluxdb) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputInfluxdb) GetType() TypeInfluxdb {
	if o == nil {
		return TypeInfluxdb("")
	}
	return o.Type
}

func (o *OutputInfluxdb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputInfluxdb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputInfluxdb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputInfluxdb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputInfluxdb) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputInfluxdb) GetUseV2API() *bool {
	if o == nil {
		return nil
	}
	return o.UseV2API
}

func (o *OutputInfluxdb) GetTimestampPrecision() *TimestampPrecision {
	if o == nil {
		return nil
	}
	return o.TimestampPrecision
}

func (o *OutputInfluxdb) GetDynamicValueFieldName() *bool {
	if o == nil {
		return nil
	}
	return o.DynamicValueFieldName
}

func (o *OutputInfluxdb) GetValueFieldName() *string {
	if o == nil {
		return nil
	}
	return o.ValueFieldName
}

func (o *OutputInfluxdb) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputInfluxdb) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputInfluxdb) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputInfluxdb) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputInfluxdb) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputInfluxdb) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputInfluxdb) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputInfluxdb) GetExtraHTTPHeaders() []ExtraHTTPHeaderInfluxdb {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputInfluxdb) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputInfluxdb) GetFailedRequestLoggingMode() *FailedRequestLoggingModeInfluxdb {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputInfluxdb) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputInfluxdb) GetResponseRetrySettings() []ResponseRetrySettingInfluxdb {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputInfluxdb) GetTimeoutRetrySettings() *TimeoutRetrySettingsInfluxdb {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputInfluxdb) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputInfluxdb) GetOnBackpressure() *BackpressureBehaviorInfluxdb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputInfluxdb) GetAuthType() *AuthenticationTypeInfluxdb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputInfluxdb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputInfluxdb) GetDatabase() *string {
	if o == nil {
		return nil
	}
	return o.Database
}

func (o *OutputInfluxdb) GetBucket() *string {
	if o == nil {
		return nil
	}
	return o.Bucket
}

func (o *OutputInfluxdb) GetOrg() *string {
	if o == nil {
		return nil
	}
	return o.Org
}

func (o *OutputInfluxdb) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputInfluxdb) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputInfluxdb) GetPqMode() *ModeInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputInfluxdb) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputInfluxdb) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputInfluxdb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputInfluxdb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputInfluxdb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputInfluxdb) GetPqCompress() *CompressionInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputInfluxdb) GetPqOnBackpressure() *QueueFullBehaviorInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputInfluxdb) GetPqControls() *PqControlsInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputInfluxdb) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputInfluxdb) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputInfluxdb) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputInfluxdb) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputInfluxdb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputInfluxdb) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputInfluxdb) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputInfluxdb) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputInfluxdb) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputInfluxdb) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputInfluxdb) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputInfluxdb) GetOauthParams() []OauthParamInfluxdb {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputInfluxdb) GetOauthHeaders() []OauthHeaderInfluxdb {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

type TypeNewrelicEvents string

const (
	TypeNewrelicEventsNewrelicEvents TypeNewrelicEvents = "newrelic_events"
)

func (e TypeNewrelicEvents) ToPointer() *TypeNewrelicEvents {
	return &e
}
func (e *TypeNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "newrelic_events":
		*e = TypeNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeNewrelicEvents: %v", v)
	}
}

// RegionNewrelicEvents - Which New Relic region endpoint to use.
type RegionNewrelicEvents string

const (
	// RegionNewrelicEventsUs US
	RegionNewrelicEventsUs RegionNewrelicEvents = "US"
	// RegionNewrelicEventsEu Europe
	RegionNewrelicEventsEu RegionNewrelicEvents = "EU"
	// RegionNewrelicEventsCustom Custom
	RegionNewrelicEventsCustom RegionNewrelicEvents = "Custom"
)

func (e RegionNewrelicEvents) ToPointer() *RegionNewrelicEvents {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *RegionNewrelicEvents) IsExact() bool {
	if e != nil {
		switch *e {
		case "US", "EU", "Custom":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderNewrelicEvents struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderNewrelicEvents) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderNewrelicEvents) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeNewrelicEvents - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeNewrelicEvents string

const (
	// FailedRequestLoggingModeNewrelicEventsPayload Payload
	FailedRequestLoggingModeNewrelicEventsPayload FailedRequestLoggingModeNewrelicEvents = "payload"
	// FailedRequestLoggingModeNewrelicEventsPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeNewrelicEventsPayloadAndHeaders FailedRequestLoggingModeNewrelicEvents = "payloadAndHeaders"
	// FailedRequestLoggingModeNewrelicEventsNone None
	FailedRequestLoggingModeNewrelicEventsNone FailedRequestLoggingModeNewrelicEvents = "none"
)

func (e FailedRequestLoggingModeNewrelicEvents) ToPointer() *FailedRequestLoggingModeNewrelicEvents {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeNewrelicEvents) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingNewrelicEvents struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingNewrelicEvents) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingNewrelicEvents) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingNewrelicEvents) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingNewrelicEvents) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsNewrelicEvents struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsNewrelicEvents) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsNewrelicEvents) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsNewrelicEvents) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsNewrelicEvents) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorNewrelicEvents - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorNewrelicEvents string

const (
	// BackpressureBehaviorNewrelicEventsBlock Block
	BackpressureBehaviorNewrelicEventsBlock BackpressureBehaviorNewrelicEvents = "block"
	// BackpressureBehaviorNewrelicEventsDrop Drop
	BackpressureBehaviorNewrelicEventsDrop BackpressureBehaviorNewrelicEvents = "drop"
	// BackpressureBehaviorNewrelicEventsQueue Persistent Queue
	BackpressureBehaviorNewrelicEventsQueue BackpressureBehaviorNewrelicEvents = "queue"
)

func (e BackpressureBehaviorNewrelicEvents) ToPointer() *BackpressureBehaviorNewrelicEvents {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorNewrelicEvents) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// AuthenticationMethodNewrelicEvents - Enter API key directly, or select a stored secret
type AuthenticationMethodNewrelicEvents string

const (
	AuthenticationMethodNewrelicEventsManual AuthenticationMethodNewrelicEvents = "manual"
	AuthenticationMethodNewrelicEventsSecret AuthenticationMethodNewrelicEvents = "secret"
)

func (e AuthenticationMethodNewrelicEvents) ToPointer() *AuthenticationMethodNewrelicEvents {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodNewrelicEvents) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

// ModeNewrelicEvents - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeNewrelicEvents string

const (
	// ModeNewrelicEventsError Error
	ModeNewrelicEventsError ModeNewrelicEvents = "error"
	// ModeNewrelicEventsAlways Backpressure
	ModeNewrelicEventsAlways ModeNewrelicEvents = "always"
	// ModeNewrelicEventsBackpressure Always On
	ModeNewrelicEventsBackpressure ModeNewrelicEvents = "backpressure"
)

func (e ModeNewrelicEvents) ToPointer() *ModeNewrelicEvents {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeNewrelicEvents) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionNewrelicEvents - Codec to use to compress the persisted data
type CompressionNewrelicEvents string

const (
	// CompressionNewrelicEventsNone None
	CompressionNewrelicEventsNone CompressionNewrelicEvents = "none"
	// CompressionNewrelicEventsGzip Gzip
	CompressionNewrelicEventsGzip CompressionNewrelicEvents = "gzip"
)

func (e CompressionNewrelicEvents) ToPointer() *CompressionNewrelicEvents {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionNewrelicEvents) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorNewrelicEvents - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorNewrelicEvents string

const (
	// QueueFullBehaviorNewrelicEventsBlock Block
	QueueFullBehaviorNewrelicEventsBlock QueueFullBehaviorNewrelicEvents = "block"
	// QueueFullBehaviorNewrelicEventsDrop Drop new data
	QueueFullBehaviorNewrelicEventsDrop QueueFullBehaviorNewrelicEvents = "drop"
)

func (e QueueFullBehaviorNewrelicEvents) ToPointer() *QueueFullBehaviorNewrelicEvents {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorNewrelicEvents) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsNewrelicEvents struct {
}

func (p PqControlsNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputNewrelicEvents struct {
	// Unique ID for this output
	ID   string             `json:"id"`
	Type TypeNewrelicEvents `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Which New Relic region endpoint to use.
	Region *RegionNewrelicEvents `default:"US" json:"region"`
	// New Relic account ID
	AccountID string `json:"accountId"`
	// Default eventType to use when not present in an event. For more information, see [here](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/data-requirements-limits-custom-event-data/#reserved-words).
	EventType string `json:"eventType"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderNewrelicEvents `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeNewrelicEvents `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingNewrelicEvents `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsNewrelicEvents  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorNewrelicEvents `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType    *AuthenticationMethodNewrelicEvents `default:"manual" json:"authType"`
	Description *string                             `json:"description,omitempty"`
	CustomURL   *string                             `json:"customUrl,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeNewrelicEvents `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionNewrelicEvents `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorNewrelicEvents `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsNewrelicEvents        `json:"pqControls,omitempty"`
	// New Relic API key. Can be overridden using __newRelic_apiKey field.
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "accountId", "eventType"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputNewrelicEvents) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputNewrelicEvents) GetType() TypeNewrelicEvents {
	if o == nil {
		return TypeNewrelicEvents("")
	}
	return o.Type
}

func (o *OutputNewrelicEvents) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputNewrelicEvents) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputNewrelicEvents) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputNewrelicEvents) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputNewrelicEvents) GetRegion() *RegionNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputNewrelicEvents) GetAccountID() string {
	if o == nil {
		return ""
	}
	return o.AccountID
}

func (o *OutputNewrelicEvents) GetEventType() string {
	if o == nil {
		return ""
	}
	return o.EventType
}

func (o *OutputNewrelicEvents) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputNewrelicEvents) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputNewrelicEvents) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputNewrelicEvents) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputNewrelicEvents) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputNewrelicEvents) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputNewrelicEvents) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputNewrelicEvents) GetExtraHTTPHeaders() []ExtraHTTPHeaderNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputNewrelicEvents) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputNewrelicEvents) GetFailedRequestLoggingMode() *FailedRequestLoggingModeNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputNewrelicEvents) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputNewrelicEvents) GetResponseRetrySettings() []ResponseRetrySettingNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputNewrelicEvents) GetTimeoutRetrySettings() *TimeoutRetrySettingsNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputNewrelicEvents) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputNewrelicEvents) GetOnBackpressure() *BackpressureBehaviorNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputNewrelicEvents) GetAuthType() *AuthenticationMethodNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputNewrelicEvents) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputNewrelicEvents) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputNewrelicEvents) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputNewrelicEvents) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputNewrelicEvents) GetPqMode() *ModeNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputNewrelicEvents) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputNewrelicEvents) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputNewrelicEvents) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputNewrelicEvents) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputNewrelicEvents) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputNewrelicEvents) GetPqCompress() *CompressionNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputNewrelicEvents) GetPqOnBackpressure() *QueueFullBehaviorNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputNewrelicEvents) GetPqControls() *PqControlsNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputNewrelicEvents) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputNewrelicEvents) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type TypeNewrelic string

const (
	TypeNewrelicNewrelic TypeNewrelic = "newrelic"
)

func (e TypeNewrelic) ToPointer() *TypeNewrelic {
	return &e
}
func (e *TypeNewrelic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "newrelic":
		*e = TypeNewrelic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeNewrelic: %v", v)
	}
}

// RegionNewrelic - Which New Relic region endpoint to use.
type RegionNewrelic string

const (
	// RegionNewrelicUs US
	RegionNewrelicUs RegionNewrelic = "US"
	// RegionNewrelicEu Europe
	RegionNewrelicEu RegionNewrelic = "EU"
	// RegionNewrelicCustom Custom
	RegionNewrelicCustom RegionNewrelic = "Custom"
)

func (e RegionNewrelic) ToPointer() *RegionNewrelic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *RegionNewrelic) IsExact() bool {
	if e != nil {
		switch *e {
		case "US", "EU", "Custom":
			return true
		}
	}
	return false
}

type FieldName string

const (
	FieldNameService   FieldName = "service"
	FieldNameHostname  FieldName = "hostname"
	FieldNameTimestamp FieldName = "timestamp"
	FieldNameAuditID   FieldName = "auditId"
)

func (e FieldName) ToPointer() *FieldName {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FieldName) IsExact() bool {
	if e != nil {
		switch *e {
		case "service", "hostname", "timestamp", "auditId":
			return true
		}
	}
	return false
}

type MetadatumNewrelic struct {
	Name FieldName `json:"name"`
	// JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)
	Value string `json:"value"`
}

func (m MetadatumNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(m, "", false)
}

func (m *MetadatumNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &m, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (m *MetadatumNewrelic) GetName() FieldName {
	if m == nil {
		return FieldName("")
	}
	return m.Name
}

func (m *MetadatumNewrelic) GetValue() string {
	if m == nil {
		return ""
	}
	return m.Value
}

type ExtraHTTPHeaderNewrelic struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderNewrelic) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderNewrelic) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeNewrelic - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeNewrelic string

const (
	// FailedRequestLoggingModeNewrelicPayload Payload
	FailedRequestLoggingModeNewrelicPayload FailedRequestLoggingModeNewrelic = "payload"
	// FailedRequestLoggingModeNewrelicPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeNewrelicPayloadAndHeaders FailedRequestLoggingModeNewrelic = "payloadAndHeaders"
	// FailedRequestLoggingModeNewrelicNone None
	FailedRequestLoggingModeNewrelicNone FailedRequestLoggingModeNewrelic = "none"
)

func (e FailedRequestLoggingModeNewrelic) ToPointer() *FailedRequestLoggingModeNewrelic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeNewrelic) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingNewrelic struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingNewrelic) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingNewrelic) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingNewrelic) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingNewrelic) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsNewrelic struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsNewrelic) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsNewrelic) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsNewrelic) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsNewrelic) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorNewrelic - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorNewrelic string

const (
	// BackpressureBehaviorNewrelicBlock Block
	BackpressureBehaviorNewrelicBlock BackpressureBehaviorNewrelic = "block"
	// BackpressureBehaviorNewrelicDrop Drop
	BackpressureBehaviorNewrelicDrop BackpressureBehaviorNewrelic = "drop"
	// BackpressureBehaviorNewrelicQueue Persistent Queue
	BackpressureBehaviorNewrelicQueue BackpressureBehaviorNewrelic = "queue"
)

func (e BackpressureBehaviorNewrelic) ToPointer() *BackpressureBehaviorNewrelic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorNewrelic) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// AuthenticationMethodNewrelic - Enter API key directly, or select a stored secret
type AuthenticationMethodNewrelic string

const (
	AuthenticationMethodNewrelicManual AuthenticationMethodNewrelic = "manual"
	AuthenticationMethodNewrelicSecret AuthenticationMethodNewrelic = "secret"
)

func (e AuthenticationMethodNewrelic) ToPointer() *AuthenticationMethodNewrelic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodNewrelic) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

// ModeNewrelic - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeNewrelic string

const (
	// ModeNewrelicError Error
	ModeNewrelicError ModeNewrelic = "error"
	// ModeNewrelicAlways Backpressure
	ModeNewrelicAlways ModeNewrelic = "always"
	// ModeNewrelicBackpressure Always On
	ModeNewrelicBackpressure ModeNewrelic = "backpressure"
)

func (e ModeNewrelic) ToPointer() *ModeNewrelic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeNewrelic) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionNewrelic - Codec to use to compress the persisted data
type CompressionNewrelic string

const (
	// CompressionNewrelicNone None
	CompressionNewrelicNone CompressionNewrelic = "none"
	// CompressionNewrelicGzip Gzip
	CompressionNewrelicGzip CompressionNewrelic = "gzip"
)

func (e CompressionNewrelic) ToPointer() *CompressionNewrelic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionNewrelic) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorNewrelic - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorNewrelic string

const (
	// QueueFullBehaviorNewrelicBlock Block
	QueueFullBehaviorNewrelicBlock QueueFullBehaviorNewrelic = "block"
	// QueueFullBehaviorNewrelicDrop Drop new data
	QueueFullBehaviorNewrelicDrop QueueFullBehaviorNewrelic = "drop"
)

func (e QueueFullBehaviorNewrelic) ToPointer() *QueueFullBehaviorNewrelic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorNewrelic) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsNewrelic struct {
}

func (p PqControlsNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputNewrelic struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type TypeNewrelic `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Which New Relic region endpoint to use.
	Region *RegionNewrelic `default:"US" json:"region"`
	// Name of the logtype to send with events, e.g.: observability, access_log. The event's 'sourcetype' field (if set) will override this value.
	LogType *string `default:"" json:"logType"`
	// Name of field to send as log message value. If not present, event will be serialized and sent as JSON.
	MessageField *string `default:"" json:"messageField"`
	// Fields to add to events from this input
	Metadata []MetadatumNewrelic `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderNewrelic `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeNewrelic `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingNewrelic `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsNewrelic  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorNewrelic `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType *AuthenticationMethodNewrelic `default:"manual" json:"authType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	CustomURL          *string  `json:"customUrl,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeNewrelic `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionNewrelic `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorNewrelic `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsNewrelic        `json:"pqControls,omitempty"`
	// New Relic API key. Can be overridden using __newRelic_apiKey field.
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputNewrelic) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputNewrelic) GetType() TypeNewrelic {
	if o == nil {
		return TypeNewrelic("")
	}
	return o.Type
}

func (o *OutputNewrelic) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputNewrelic) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputNewrelic) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputNewrelic) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputNewrelic) GetRegion() *RegionNewrelic {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputNewrelic) GetLogType() *string {
	if o == nil {
		return nil
	}
	return o.LogType
}

func (o *OutputNewrelic) GetMessageField() *string {
	if o == nil {
		return nil
	}
	return o.MessageField
}

func (o *OutputNewrelic) GetMetadata() []MetadatumNewrelic {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputNewrelic) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputNewrelic) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputNewrelic) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputNewrelic) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputNewrelic) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputNewrelic) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputNewrelic) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputNewrelic) GetExtraHTTPHeaders() []ExtraHTTPHeaderNewrelic {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputNewrelic) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputNewrelic) GetFailedRequestLoggingMode() *FailedRequestLoggingModeNewrelic {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputNewrelic) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputNewrelic) GetResponseRetrySettings() []ResponseRetrySettingNewrelic {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputNewrelic) GetTimeoutRetrySettings() *TimeoutRetrySettingsNewrelic {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputNewrelic) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputNewrelic) GetOnBackpressure() *BackpressureBehaviorNewrelic {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputNewrelic) GetAuthType() *AuthenticationMethodNewrelic {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputNewrelic) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputNewrelic) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputNewrelic) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputNewrelic) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputNewrelic) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputNewrelic) GetPqMode() *ModeNewrelic {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputNewrelic) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputNewrelic) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputNewrelic) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputNewrelic) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputNewrelic) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputNewrelic) GetPqCompress() *CompressionNewrelic {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputNewrelic) GetPqOnBackpressure() *QueueFullBehaviorNewrelic {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputNewrelic) GetPqControls() *PqControlsNewrelic {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputNewrelic) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputNewrelic) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type TypeElasticCloud string

const (
	TypeElasticCloudElasticCloud TypeElasticCloud = "elastic_cloud"
)

func (e TypeElasticCloud) ToPointer() *TypeElasticCloud {
	return &e
}
func (e *TypeElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "elastic_cloud":
		*e = TypeElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeElasticCloud: %v", v)
	}
}

type ExtraHTTPHeaderElasticCloud struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderElasticCloud) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderElasticCloud) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeElasticCloud - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeElasticCloud string

const (
	// FailedRequestLoggingModeElasticCloudPayload Payload
	FailedRequestLoggingModeElasticCloudPayload FailedRequestLoggingModeElasticCloud = "payload"
	// FailedRequestLoggingModeElasticCloudPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeElasticCloudPayloadAndHeaders FailedRequestLoggingModeElasticCloud = "payloadAndHeaders"
	// FailedRequestLoggingModeElasticCloudNone None
	FailedRequestLoggingModeElasticCloudNone FailedRequestLoggingModeElasticCloud = "none"
)

func (e FailedRequestLoggingModeElasticCloud) ToPointer() *FailedRequestLoggingModeElasticCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeElasticCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ExtraParamElasticCloud struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (e ExtraParamElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraParamElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraParamElasticCloud) GetName() string {
	if e == nil {
		return ""
	}
	return e.Name
}

func (e *ExtraParamElasticCloud) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// AuthenticationMethodElasticCloud - Enter credentials directly, or select a stored secret
type AuthenticationMethodElasticCloud string

const (
	AuthenticationMethodElasticCloudManual       AuthenticationMethodElasticCloud = "manual"
	AuthenticationMethodElasticCloudSecret       AuthenticationMethodElasticCloud = "secret"
	AuthenticationMethodElasticCloudManualAPIKey AuthenticationMethodElasticCloud = "manualAPIKey"
	AuthenticationMethodElasticCloudTextSecret   AuthenticationMethodElasticCloud = "textSecret"
)

func (e AuthenticationMethodElasticCloud) ToPointer() *AuthenticationMethodElasticCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodElasticCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret", "manualAPIKey", "textSecret":
			return true
		}
	}
	return false
}

type AuthElasticCloud struct {
	Disabled *bool   `default:"false" json:"disabled"`
	Username *string `json:"username,omitempty"`
	Password *string `json:"password,omitempty"`
	// Enter credentials directly, or select a stored secret
	AuthType *AuthenticationMethodElasticCloud `default:"manual" json:"authType"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Enter API key directly
	ManualAPIKey *string `json:"manualAPIKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (a AuthElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (a *AuthElasticCloud) GetDisabled() *bool {
	if a == nil {
		return nil
	}
	return a.Disabled
}

func (a *AuthElasticCloud) GetUsername() *string {
	if a == nil {
		return nil
	}
	return a.Username
}

func (a *AuthElasticCloud) GetPassword() *string {
	if a == nil {
		return nil
	}
	return a.Password
}

func (a *AuthElasticCloud) GetAuthType() *AuthenticationMethodElasticCloud {
	if a == nil {
		return nil
	}
	return a.AuthType
}

func (a *AuthElasticCloud) GetCredentialsSecret() *string {
	if a == nil {
		return nil
	}
	return a.CredentialsSecret
}

func (a *AuthElasticCloud) GetManualAPIKey() *string {
	if a == nil {
		return nil
	}
	return a.ManualAPIKey
}

func (a *AuthElasticCloud) GetTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.TextSecret
}

type ResponseRetrySettingElasticCloud struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingElasticCloud) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingElasticCloud) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingElasticCloud) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingElasticCloud) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsElasticCloud struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsElasticCloud) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsElasticCloud) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsElasticCloud) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsElasticCloud) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorElasticCloud - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorElasticCloud string

const (
	// BackpressureBehaviorElasticCloudBlock Block
	BackpressureBehaviorElasticCloudBlock BackpressureBehaviorElasticCloud = "block"
	// BackpressureBehaviorElasticCloudDrop Drop
	BackpressureBehaviorElasticCloudDrop BackpressureBehaviorElasticCloud = "drop"
	// BackpressureBehaviorElasticCloudQueue Persistent Queue
	BackpressureBehaviorElasticCloudQueue BackpressureBehaviorElasticCloud = "queue"
)

func (e BackpressureBehaviorElasticCloud) ToPointer() *BackpressureBehaviorElasticCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorElasticCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeElasticCloud - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeElasticCloud string

const (
	// ModeElasticCloudError Error
	ModeElasticCloudError ModeElasticCloud = "error"
	// ModeElasticCloudAlways Backpressure
	ModeElasticCloudAlways ModeElasticCloud = "always"
	// ModeElasticCloudBackpressure Always On
	ModeElasticCloudBackpressure ModeElasticCloud = "backpressure"
)

func (e ModeElasticCloud) ToPointer() *ModeElasticCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeElasticCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionElasticCloud - Codec to use to compress the persisted data
type CompressionElasticCloud string

const (
	// CompressionElasticCloudNone None
	CompressionElasticCloudNone CompressionElasticCloud = "none"
	// CompressionElasticCloudGzip Gzip
	CompressionElasticCloudGzip CompressionElasticCloud = "gzip"
)

func (e CompressionElasticCloud) ToPointer() *CompressionElasticCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionElasticCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorElasticCloud - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorElasticCloud string

const (
	// QueueFullBehaviorElasticCloudBlock Block
	QueueFullBehaviorElasticCloudBlock QueueFullBehaviorElasticCloud = "block"
	// QueueFullBehaviorElasticCloudDrop Drop new data
	QueueFullBehaviorElasticCloudDrop QueueFullBehaviorElasticCloud = "drop"
)

func (e QueueFullBehaviorElasticCloud) ToPointer() *QueueFullBehaviorElasticCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorElasticCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsElasticCloud struct {
}

func (p PqControlsElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputElasticCloud struct {
	// Unique ID for this output
	ID   string           `json:"id"`
	Type TypeElasticCloud `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter Cloud ID of the Elastic Cloud environment to send events to
	URL string `json:"url"`
	// Data stream or index to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.
	Index string `json:"index"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderElasticCloud `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeElasticCloud `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Extra parameters to use in HTTP requests
	ExtraParams []ExtraParamElasticCloud `json:"extraParams,omitempty"`
	Auth        *AuthElasticCloud        `json:"auth,omitempty"`
	// Optional Elastic Cloud Destination pipeline
	ElasticPipeline *string `json:"elasticPipeline,omitempty"`
	// Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)
	IncludeDocID *bool `default:"true" json:"includeDocId"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingElasticCloud `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsElasticCloud  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorElasticCloud `default:"block" json:"onBackpressure"`
	Description    *string                           `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeElasticCloud `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionElasticCloud `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorElasticCloud `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsElasticCloud        `json:"pqControls,omitempty"`
}

func (o OutputElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "url", "index"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputElasticCloud) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputElasticCloud) GetType() TypeElasticCloud {
	if o == nil {
		return TypeElasticCloud("")
	}
	return o.Type
}

func (o *OutputElasticCloud) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputElasticCloud) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputElasticCloud) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputElasticCloud) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputElasticCloud) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputElasticCloud) GetIndex() string {
	if o == nil {
		return ""
	}
	return o.Index
}

func (o *OutputElasticCloud) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputElasticCloud) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputElasticCloud) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputElasticCloud) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputElasticCloud) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputElasticCloud) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputElasticCloud) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputElasticCloud) GetExtraHTTPHeaders() []ExtraHTTPHeaderElasticCloud {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputElasticCloud) GetFailedRequestLoggingMode() *FailedRequestLoggingModeElasticCloud {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputElasticCloud) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputElasticCloud) GetExtraParams() []ExtraParamElasticCloud {
	if o == nil {
		return nil
	}
	return o.ExtraParams
}

func (o *OutputElasticCloud) GetAuth() *AuthElasticCloud {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputElasticCloud) GetElasticPipeline() *string {
	if o == nil {
		return nil
	}
	return o.ElasticPipeline
}

func (o *OutputElasticCloud) GetIncludeDocID() *bool {
	if o == nil {
		return nil
	}
	return o.IncludeDocID
}

func (o *OutputElasticCloud) GetResponseRetrySettings() []ResponseRetrySettingElasticCloud {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputElasticCloud) GetTimeoutRetrySettings() *TimeoutRetrySettingsElasticCloud {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputElasticCloud) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputElasticCloud) GetOnBackpressure() *BackpressureBehaviorElasticCloud {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputElasticCloud) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputElasticCloud) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputElasticCloud) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputElasticCloud) GetPqMode() *ModeElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputElasticCloud) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputElasticCloud) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputElasticCloud) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputElasticCloud) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputElasticCloud) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputElasticCloud) GetPqCompress() *CompressionElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputElasticCloud) GetPqOnBackpressure() *QueueFullBehaviorElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputElasticCloud) GetPqControls() *PqControlsElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeElastic string

const (
	CreateOutputTypeElasticElastic CreateOutputTypeElastic = "elastic"
)

func (e CreateOutputTypeElastic) ToPointer() *CreateOutputTypeElastic {
	return &e
}
func (e *CreateOutputTypeElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "elastic":
		*e = CreateOutputTypeElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeElastic: %v", v)
	}
}

type CreateOutputExtraHTTPHeaderElastic struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (c CreateOutputExtraHTTPHeaderElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputExtraHTTPHeaderElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputExtraHTTPHeaderElastic) GetName() *string {
	if c == nil {
		return nil
	}
	return c.Name
}

func (c *CreateOutputExtraHTTPHeaderElastic) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

// FailedRequestLoggingModeElastic - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeElastic string

const (
	// FailedRequestLoggingModeElasticPayload Payload
	FailedRequestLoggingModeElasticPayload FailedRequestLoggingModeElastic = "payload"
	// FailedRequestLoggingModeElasticPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeElasticPayloadAndHeaders FailedRequestLoggingModeElastic = "payloadAndHeaders"
	// FailedRequestLoggingModeElasticNone None
	FailedRequestLoggingModeElasticNone FailedRequestLoggingModeElastic = "none"
)

func (e FailedRequestLoggingModeElastic) ToPointer() *FailedRequestLoggingModeElastic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeElastic) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingElastic struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingElastic) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingElastic) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingElastic) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingElastic) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsElastic struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsElastic) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsElastic) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsElastic) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsElastic) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type ExtraParamElastic struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (e ExtraParamElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraParamElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraParamElastic) GetName() string {
	if e == nil {
		return ""
	}
	return e.Name
}

func (e *ExtraParamElastic) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// AuthAuthenticationMethodElastic - Enter credentials directly, or select a stored secret
type AuthAuthenticationMethodElastic string

const (
	AuthAuthenticationMethodElasticManual       AuthAuthenticationMethodElastic = "manual"
	AuthAuthenticationMethodElasticSecret       AuthAuthenticationMethodElastic = "secret"
	AuthAuthenticationMethodElasticManualAPIKey AuthAuthenticationMethodElastic = "manualAPIKey"
	AuthAuthenticationMethodElasticTextSecret   AuthAuthenticationMethodElastic = "textSecret"
)

func (e AuthAuthenticationMethodElastic) ToPointer() *AuthAuthenticationMethodElastic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthAuthenticationMethodElastic) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret", "manualAPIKey", "textSecret":
			return true
		}
	}
	return false
}

type AuthElastic struct {
	Disabled *bool   `default:"true" json:"disabled"`
	Username *string `json:"username,omitempty"`
	Password *string `json:"password,omitempty"`
	// Enter credentials directly, or select a stored secret
	AuthType *AuthAuthenticationMethodElastic `default:"manual" json:"authType"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Enter API key directly
	ManualAPIKey *string `json:"manualAPIKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (a AuthElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (a *AuthElastic) GetDisabled() *bool {
	if a == nil {
		return nil
	}
	return a.Disabled
}

func (a *AuthElastic) GetUsername() *string {
	if a == nil {
		return nil
	}
	return a.Username
}

func (a *AuthElastic) GetPassword() *string {
	if a == nil {
		return nil
	}
	return a.Password
}

func (a *AuthElastic) GetAuthType() *AuthAuthenticationMethodElastic {
	if a == nil {
		return nil
	}
	return a.AuthType
}

func (a *AuthElastic) GetCredentialsSecret() *string {
	if a == nil {
		return nil
	}
	return a.CredentialsSecret
}

func (a *AuthElastic) GetManualAPIKey() *string {
	if a == nil {
		return nil
	}
	return a.ManualAPIKey
}

func (a *AuthElastic) GetTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.TextSecret
}

// ElasticVersion - Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.
type ElasticVersion string

const (
	// ElasticVersionAuto Auto
	ElasticVersionAuto ElasticVersion = "auto"
	// ElasticVersionSix 6.x
	ElasticVersionSix ElasticVersion = "6"
	// ElasticVersionSeven 7.x
	ElasticVersionSeven ElasticVersion = "7"
)

func (e ElasticVersion) ToPointer() *ElasticVersion {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ElasticVersion) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "6", "7":
			return true
		}
	}
	return false
}

// WriteAction - Action to use when writing events. Must be set to `Create` when writing to a data stream.
type WriteAction string

const (
	// WriteActionIndex Index
	WriteActionIndex WriteAction = "index"
	// WriteActionCreate Create
	WriteActionCreate WriteAction = "create"
)

func (e WriteAction) ToPointer() *WriteAction {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *WriteAction) IsExact() bool {
	if e != nil {
		switch *e {
		case "index", "create":
			return true
		}
	}
	return false
}

// BackpressureBehaviorElastic - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorElastic string

const (
	// BackpressureBehaviorElasticBlock Block
	BackpressureBehaviorElasticBlock BackpressureBehaviorElastic = "block"
	// BackpressureBehaviorElasticDrop Drop
	BackpressureBehaviorElasticDrop BackpressureBehaviorElastic = "drop"
	// BackpressureBehaviorElasticQueue Persistent Queue
	BackpressureBehaviorElasticQueue BackpressureBehaviorElastic = "queue"
)

func (e BackpressureBehaviorElastic) ToPointer() *BackpressureBehaviorElastic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorElastic) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type URLElastic struct {
	// The URL to an Elastic node to send events to. Example: http://elastic:9200/_bulk
	URL string `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, []string{"url"}); err != nil {
		return err
	}
	return nil
}

func (u *URLElastic) GetURL() string {
	if u == nil {
		return ""
	}
	return u.URL
}

func (u *URLElastic) GetWeight() *float64 {
	if u == nil {
		return nil
	}
	return u.Weight
}

// CreateOutputModeElastic - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeElastic string

const (
	// CreateOutputModeElasticError Error
	CreateOutputModeElasticError CreateOutputModeElastic = "error"
	// CreateOutputModeElasticAlways Backpressure
	CreateOutputModeElasticAlways CreateOutputModeElastic = "always"
	// CreateOutputModeElasticBackpressure Always On
	CreateOutputModeElasticBackpressure CreateOutputModeElastic = "backpressure"
)

func (e CreateOutputModeElastic) ToPointer() *CreateOutputModeElastic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeElastic) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionElastic - Codec to use to compress the persisted data
type PqCompressCompressionElastic string

const (
	// PqCompressCompressionElasticNone None
	PqCompressCompressionElasticNone PqCompressCompressionElastic = "none"
	// PqCompressCompressionElasticGzip Gzip
	PqCompressCompressionElasticGzip PqCompressCompressionElastic = "gzip"
)

func (e PqCompressCompressionElastic) ToPointer() *PqCompressCompressionElastic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionElastic) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorElastic - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorElastic string

const (
	// QueueFullBehaviorElasticBlock Block
	QueueFullBehaviorElasticBlock QueueFullBehaviorElastic = "block"
	// QueueFullBehaviorElasticDrop Drop new data
	QueueFullBehaviorElasticDrop QueueFullBehaviorElastic = "drop"
)

func (e QueueFullBehaviorElastic) ToPointer() *QueueFullBehaviorElastic {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorElastic) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsElastic struct {
}

func (c CreateOutputPqControlsElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputElastic struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type CreateOutputTypeElastic `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// Index or data stream to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.
	Index string `json:"index"`
	// Document type to use for events. Can be overwritten by an event's __type field.
	DocType *string `json:"docType,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []CreateOutputExtraHTTPHeaderElastic `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeElastic `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingElastic `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsElastic  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool               `default:"true" json:"responseHonorRetryAfterHeader"`
	ExtraParams                   []ExtraParamElastic `json:"extraParams,omitempty"`
	Auth                          *AuthElastic        `json:"auth,omitempty"`
	// Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.
	ElasticVersion *ElasticVersion `default:"auto" json:"elasticVersion"`
	// Optional Elasticsearch destination pipeline
	ElasticPipeline *string `json:"elasticPipeline,omitempty"`
	// Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)
	IncludeDocID *bool `default:"false" json:"includeDocId"`
	// Action to use when writing events. Must be set to `Create` when writing to a data stream.
	WriteAction *WriteAction `default:"create" json:"writeAction"`
	// Retry failed events when a bulk request to Elastic is successful, but the response body returns an error for one or more events in the batch
	RetryPartialErrors *bool `default:"false" json:"retryPartialErrors"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorElastic `default:"block" json:"onBackpressure"`
	Description    *string                      `json:"description,omitempty"`
	// The Cloud ID or URL to an Elastic cluster to send events to. Example: http://elastic:9200/_bulk
	URL *string `json:"url,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool        `default:"false" json:"excludeSelf"`
	Urls        []URLElastic `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeElastic `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionElastic `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorElastic      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsElastic `json:"pqControls,omitempty"`
}

func (o OutputElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "index"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputElastic) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputElastic) GetType() CreateOutputTypeElastic {
	if o == nil {
		return CreateOutputTypeElastic("")
	}
	return o.Type
}

func (o *OutputElastic) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputElastic) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputElastic) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputElastic) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputElastic) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputElastic) GetIndex() string {
	if o == nil {
		return ""
	}
	return o.Index
}

func (o *OutputElastic) GetDocType() *string {
	if o == nil {
		return nil
	}
	return o.DocType
}

func (o *OutputElastic) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputElastic) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputElastic) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputElastic) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputElastic) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputElastic) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputElastic) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputElastic) GetExtraHTTPHeaders() []CreateOutputExtraHTTPHeaderElastic {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputElastic) GetFailedRequestLoggingMode() *FailedRequestLoggingModeElastic {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputElastic) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputElastic) GetResponseRetrySettings() []ResponseRetrySettingElastic {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputElastic) GetTimeoutRetrySettings() *TimeoutRetrySettingsElastic {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputElastic) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputElastic) GetExtraParams() []ExtraParamElastic {
	if o == nil {
		return nil
	}
	return o.ExtraParams
}

func (o *OutputElastic) GetAuth() *AuthElastic {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputElastic) GetElasticVersion() *ElasticVersion {
	if o == nil {
		return nil
	}
	return o.ElasticVersion
}

func (o *OutputElastic) GetElasticPipeline() *string {
	if o == nil {
		return nil
	}
	return o.ElasticPipeline
}

func (o *OutputElastic) GetIncludeDocID() *bool {
	if o == nil {
		return nil
	}
	return o.IncludeDocID
}

func (o *OutputElastic) GetWriteAction() *WriteAction {
	if o == nil {
		return nil
	}
	return o.WriteAction
}

func (o *OutputElastic) GetRetryPartialErrors() *bool {
	if o == nil {
		return nil
	}
	return o.RetryPartialErrors
}

func (o *OutputElastic) GetOnBackpressure() *BackpressureBehaviorElastic {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputElastic) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputElastic) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputElastic) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputElastic) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputElastic) GetUrls() []URLElastic {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputElastic) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputElastic) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputElastic) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputElastic) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputElastic) GetPqMode() *CreateOutputModeElastic {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputElastic) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputElastic) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputElastic) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputElastic) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputElastic) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputElastic) GetPqCompress() *PqCompressCompressionElastic {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputElastic) GetPqOnBackpressure() *QueueFullBehaviorElastic {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputElastic) GetPqControls() *CreateOutputPqControlsElastic {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeMsk string

const (
	CreateOutputTypeMskMsk CreateOutputTypeMsk = "msk"
)

func (e CreateOutputTypeMsk) ToPointer() *CreateOutputTypeMsk {
	return &e
}
func (e *CreateOutputTypeMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "msk":
		*e = CreateOutputTypeMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeMsk: %v", v)
	}
}

// AcknowledgmentsMsk - Control the number of required acknowledgments.
type AcknowledgmentsMsk int64

const (
	// AcknowledgmentsMskOne Leader
	AcknowledgmentsMskOne AcknowledgmentsMsk = 1
	// AcknowledgmentsMskZero None
	AcknowledgmentsMskZero AcknowledgmentsMsk = 0
	// AcknowledgmentsMskMinus1 All
	AcknowledgmentsMskMinus1 AcknowledgmentsMsk = -1
)

func (e AcknowledgmentsMsk) ToPointer() *AcknowledgmentsMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AcknowledgmentsMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case 1, 0, -1:
			return true
		}
	}
	return false
}

// RecordDataFormatMsk - Format to use to serialize events before writing to Kafka.
type RecordDataFormatMsk string

const (
	// RecordDataFormatMskJSON JSON
	RecordDataFormatMskJSON RecordDataFormatMsk = "json"
	// RecordDataFormatMskRaw Field _raw
	RecordDataFormatMskRaw RecordDataFormatMsk = "raw"
	// RecordDataFormatMskProtobuf Protobuf
	RecordDataFormatMskProtobuf RecordDataFormatMsk = "protobuf"
)

func (e RecordDataFormatMsk) ToPointer() *RecordDataFormatMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *RecordDataFormatMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "protobuf":
			return true
		}
	}
	return false
}

// CreateOutputCompressionMsk - Codec to use to compress the data before sending to Kafka
type CreateOutputCompressionMsk string

const (
	// CreateOutputCompressionMskNone None
	CreateOutputCompressionMskNone CreateOutputCompressionMsk = "none"
	// CreateOutputCompressionMskGzip Gzip
	CreateOutputCompressionMskGzip CreateOutputCompressionMsk = "gzip"
	// CreateOutputCompressionMskSnappy Snappy
	CreateOutputCompressionMskSnappy CreateOutputCompressionMsk = "snappy"
	// CreateOutputCompressionMskLz4 LZ4
	CreateOutputCompressionMskLz4 CreateOutputCompressionMsk = "lz4"
	// CreateOutputCompressionMskZstd ZSTD
	CreateOutputCompressionMskZstd CreateOutputCompressionMsk = "zstd"
)

func (e CreateOutputCompressionMsk) ToPointer() *CreateOutputCompressionMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputCompressionMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip", "snappy", "lz4", "zstd":
			return true
		}
	}
	return false
}

// CreateOutputAuthMsk - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type CreateOutputAuthMsk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (c CreateOutputAuthMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputAuthMsk) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputAuthMsk) GetCredentialsSecret() *string {
	if c == nil {
		return nil
	}
	return c.CredentialsSecret
}

type CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk string

const (
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv1  CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv11 CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv12 CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv13 CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk) ToPointer() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk string

const (
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv1  CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv11 CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv12 CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv13 CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk) ToPointer() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                              `json:"passphrase,omitempty"`
	MinVersion *CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk `json:"maxVersion,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetRejectUnauthorized() *bool {
	if c == nil {
		return nil
	}
	return c.RejectUnauthorized
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetServername() *string {
	if c == nil {
		return nil
	}
	return c.Servername
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCertificateName() *string {
	if c == nil {
		return nil
	}
	return c.CertificateName
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCaPath() *string {
	if c == nil {
		return nil
	}
	return c.CaPath
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetPrivKeyPath() *string {
	if c == nil {
		return nil
	}
	return c.PrivKeyPath
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCertPath() *string {
	if c == nil {
		return nil
	}
	return c.CertPath
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetPassphrase() *string {
	if c == nil {
		return nil
	}
	return c.Passphrase
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetMinVersion() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionMsk {
	if c == nil {
		return nil
	}
	return c.MinVersion
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetMaxVersion() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionMsk {
	if c == nil {
		return nil
	}
	return c.MaxVersion
}

type CreateOutputKafkaSchemaRegistryAuthenticationMsk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *CreateOutputAuthMsk                                     `json:"auth,omitempty"`
	TLS  *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryAuthenticationMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetSchemaRegistryURL() *string {
	if c == nil {
		return nil
	}
	return c.SchemaRegistryURL
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetConnectionTimeout() *float64 {
	if c == nil {
		return nil
	}
	return c.ConnectionTimeout
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetRequestTimeout() *float64 {
	if c == nil {
		return nil
	}
	return c.RequestTimeout
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetMaxRetries() *float64 {
	if c == nil {
		return nil
	}
	return c.MaxRetries
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetAuth() *CreateOutputAuthMsk {
	if c == nil {
		return nil
	}
	return c.Auth
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetTLS() *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideMsk {
	if c == nil {
		return nil
	}
	return c.TLS
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetDefaultKeySchemaID() *float64 {
	if c == nil {
		return nil
	}
	return c.DefaultKeySchemaID
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationMsk) GetDefaultValueSchemaID() *float64 {
	if c == nil {
		return nil
	}
	return c.DefaultValueSchemaID
}

// CreateOutputAuthenticationMethodMsk - AWS authentication method. Choose Auto to use IAM roles.
type CreateOutputAuthenticationMethodMsk string

const (
	// CreateOutputAuthenticationMethodMskAuto Auto
	CreateOutputAuthenticationMethodMskAuto CreateOutputAuthenticationMethodMsk = "auto"
	// CreateOutputAuthenticationMethodMskManual Manual
	CreateOutputAuthenticationMethodMskManual CreateOutputAuthenticationMethodMsk = "manual"
	// CreateOutputAuthenticationMethodMskSecret Secret Key pair
	CreateOutputAuthenticationMethodMskSecret CreateOutputAuthenticationMethodMsk = "secret"
)

func (e CreateOutputAuthenticationMethodMsk) ToPointer() *CreateOutputAuthenticationMethodMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationMethodMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// CreateOutputSignatureVersionMsk - Signature version to use for signing MSK cluster requests
type CreateOutputSignatureVersionMsk string

const (
	CreateOutputSignatureVersionMskV2 CreateOutputSignatureVersionMsk = "v2"
	CreateOutputSignatureVersionMskV4 CreateOutputSignatureVersionMsk = "v4"
)

func (e CreateOutputSignatureVersionMsk) ToPointer() *CreateOutputSignatureVersionMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputSignatureVersionMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

type CreateOutputMinimumTLSVersionMsk string

const (
	CreateOutputMinimumTLSVersionMskTlSv1  CreateOutputMinimumTLSVersionMsk = "TLSv1"
	CreateOutputMinimumTLSVersionMskTlSv11 CreateOutputMinimumTLSVersionMsk = "TLSv1.1"
	CreateOutputMinimumTLSVersionMskTlSv12 CreateOutputMinimumTLSVersionMsk = "TLSv1.2"
	CreateOutputMinimumTLSVersionMskTlSv13 CreateOutputMinimumTLSVersionMsk = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionMsk) ToPointer() *CreateOutputMinimumTLSVersionMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMinimumTLSVersionMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputMaximumTLSVersionMsk string

const (
	CreateOutputMaximumTLSVersionMskTlSv1  CreateOutputMaximumTLSVersionMsk = "TLSv1"
	CreateOutputMaximumTLSVersionMskTlSv11 CreateOutputMaximumTLSVersionMsk = "TLSv1.1"
	CreateOutputMaximumTLSVersionMskTlSv12 CreateOutputMaximumTLSVersionMsk = "TLSv1.2"
	CreateOutputMaximumTLSVersionMskTlSv13 CreateOutputMaximumTLSVersionMsk = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionMsk) ToPointer() *CreateOutputMaximumTLSVersionMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMaximumTLSVersionMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputTLSSettingsClientSideMsk struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                           `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionMsk `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionMsk `json:"maxVersion,omitempty"`
}

func (c CreateOutputTLSSettingsClientSideMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputTLSSettingsClientSideMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputTLSSettingsClientSideMsk) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputTLSSettingsClientSideMsk) GetRejectUnauthorized() *bool {
	if c == nil {
		return nil
	}
	return c.RejectUnauthorized
}

func (c *CreateOutputTLSSettingsClientSideMsk) GetServername() *string {
	if c == nil {
		return nil
	}
	return c.Servername
}

func (c *CreateOutputTLSSettingsClientSideMsk) GetCertificateName() *string {
	if c == nil {
		return nil
	}
	return c.CertificateName
}

func (c *CreateOutputTLSSettingsClientSideMsk) GetCaPath() *string {
	if c == nil {
		return nil
	}
	return c.CaPath
}

func (c *CreateOutputTLSSettingsClientSideMsk) GetPrivKeyPath() *string {
	if c == nil {
		return nil
	}
	return c.PrivKeyPath
}

func (c *CreateOutputTLSSettingsClientSideMsk) GetCertPath() *string {
	if c == nil {
		return nil
	}
	return c.CertPath
}

func (c *CreateOutputTLSSettingsClientSideMsk) GetPassphrase() *string {
	if c == nil {
		return nil
	}
	return c.Passphrase
}

func (c *CreateOutputTLSSettingsClientSideMsk) GetMinVersion() *CreateOutputMinimumTLSVersionMsk {
	if c == nil {
		return nil
	}
	return c.MinVersion
}

func (c *CreateOutputTLSSettingsClientSideMsk) GetMaxVersion() *CreateOutputMaximumTLSVersionMsk {
	if c == nil {
		return nil
	}
	return c.MaxVersion
}

// BackpressureBehaviorMsk - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorMsk string

const (
	// BackpressureBehaviorMskBlock Block
	BackpressureBehaviorMskBlock BackpressureBehaviorMsk = "block"
	// BackpressureBehaviorMskDrop Drop
	BackpressureBehaviorMskDrop BackpressureBehaviorMsk = "drop"
	// BackpressureBehaviorMskQueue Persistent Queue
	BackpressureBehaviorMskQueue BackpressureBehaviorMsk = "queue"
)

func (e BackpressureBehaviorMsk) ToPointer() *BackpressureBehaviorMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// CreateOutputModeMsk - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeMsk string

const (
	// CreateOutputModeMskError Error
	CreateOutputModeMskError CreateOutputModeMsk = "error"
	// CreateOutputModeMskAlways Backpressure
	CreateOutputModeMskAlways CreateOutputModeMsk = "always"
	// CreateOutputModeMskBackpressure Always On
	CreateOutputModeMskBackpressure CreateOutputModeMsk = "backpressure"
)

func (e CreateOutputModeMsk) ToPointer() *CreateOutputModeMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionMsk - Codec to use to compress the persisted data
type PqCompressCompressionMsk string

const (
	// PqCompressCompressionMskNone None
	PqCompressCompressionMskNone PqCompressCompressionMsk = "none"
	// PqCompressCompressionMskGzip Gzip
	PqCompressCompressionMskGzip PqCompressCompressionMsk = "gzip"
)

func (e PqCompressCompressionMsk) ToPointer() *PqCompressCompressionMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorMsk - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorMsk string

const (
	// QueueFullBehaviorMskBlock Block
	QueueFullBehaviorMskBlock QueueFullBehaviorMsk = "block"
	// QueueFullBehaviorMskDrop Drop new data
	QueueFullBehaviorMskDrop QueueFullBehaviorMsk = "drop"
)

func (e QueueFullBehaviorMsk) ToPointer() *QueueFullBehaviorMsk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorMsk) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsMsk struct {
}

func (c CreateOutputPqControlsMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputMsk struct {
	// Unique ID for this output
	ID   string              `json:"id"`
	Type CreateOutputTypeMsk `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.
	Brokers []string `json:"brokers"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsMsk `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatMsk `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *CreateOutputCompressionMsk `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                          `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *CreateOutputKafkaSchemaRegistryAuthenticationMsk `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *CreateOutputAuthenticationMethodMsk `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                              `json:"awsSecretKey,omitempty"`
	// Region where the MSK cluster is located
	Region string `json:"region"`
	// MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing MSK cluster requests
	SignatureVersion *CreateOutputSignatureVersionMsk `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access MSK
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64                              `default:"3600" json:"durationSeconds"`
	TLS             *CreateOutputTLSSettingsClientSideMsk `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorMsk `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// Select the type of object you want the Protobuf definitions to use for event encoding
	ProtobufEncodingID *string `json:"protobufEncodingId,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeMsk `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionMsk `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorMsk      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsMsk `json:"pqControls,omitempty"`
}

func (o OutputMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "brokers", "topic", "region"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputMsk) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputMsk) GetType() CreateOutputTypeMsk {
	if o == nil {
		return CreateOutputTypeMsk("")
	}
	return o.Type
}

func (o *OutputMsk) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputMsk) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputMsk) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputMsk) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputMsk) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputMsk) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputMsk) GetAck() *AcknowledgmentsMsk {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputMsk) GetFormat() *RecordDataFormatMsk {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputMsk) GetCompression() *CreateOutputCompressionMsk {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputMsk) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputMsk) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputMsk) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputMsk) GetKafkaSchemaRegistry() *CreateOutputKafkaSchemaRegistryAuthenticationMsk {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputMsk) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputMsk) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputMsk) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputMsk) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputMsk) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputMsk) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputMsk) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputMsk) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputMsk) GetAwsAuthenticationMethod() *CreateOutputAuthenticationMethodMsk {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputMsk) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputMsk) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputMsk) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputMsk) GetSignatureVersion() *CreateOutputSignatureVersionMsk {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputMsk) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputMsk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputMsk) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputMsk) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputMsk) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputMsk) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputMsk) GetTLS() *CreateOutputTLSSettingsClientSideMsk {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputMsk) GetOnBackpressure() *BackpressureBehaviorMsk {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputMsk) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputMsk) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputMsk) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputMsk) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputMsk) GetProtobufEncodingID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufEncodingID
}

func (o *OutputMsk) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputMsk) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputMsk) GetPqMode() *CreateOutputModeMsk {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputMsk) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputMsk) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputMsk) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputMsk) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputMsk) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputMsk) GetPqCompress() *PqCompressCompressionMsk {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputMsk) GetPqOnBackpressure() *QueueFullBehaviorMsk {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputMsk) GetPqControls() *CreateOutputPqControlsMsk {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeConfluentCloud string

const (
	CreateOutputTypeConfluentCloudConfluentCloud CreateOutputTypeConfluentCloud = "confluent_cloud"
)

func (e CreateOutputTypeConfluentCloud) ToPointer() *CreateOutputTypeConfluentCloud {
	return &e
}
func (e *CreateOutputTypeConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "confluent_cloud":
		*e = CreateOutputTypeConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeConfluentCloud: %v", v)
	}
}

type CreateOutputMinimumTLSVersionConfluentCloud string

const (
	CreateOutputMinimumTLSVersionConfluentCloudTlSv1  CreateOutputMinimumTLSVersionConfluentCloud = "TLSv1"
	CreateOutputMinimumTLSVersionConfluentCloudTlSv11 CreateOutputMinimumTLSVersionConfluentCloud = "TLSv1.1"
	CreateOutputMinimumTLSVersionConfluentCloudTlSv12 CreateOutputMinimumTLSVersionConfluentCloud = "TLSv1.2"
	CreateOutputMinimumTLSVersionConfluentCloudTlSv13 CreateOutputMinimumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionConfluentCloud) ToPointer() *CreateOutputMinimumTLSVersionConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMinimumTLSVersionConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputMaximumTLSVersionConfluentCloud string

const (
	CreateOutputMaximumTLSVersionConfluentCloudTlSv1  CreateOutputMaximumTLSVersionConfluentCloud = "TLSv1"
	CreateOutputMaximumTLSVersionConfluentCloudTlSv11 CreateOutputMaximumTLSVersionConfluentCloud = "TLSv1.1"
	CreateOutputMaximumTLSVersionConfluentCloudTlSv12 CreateOutputMaximumTLSVersionConfluentCloud = "TLSv1.2"
	CreateOutputMaximumTLSVersionConfluentCloudTlSv13 CreateOutputMaximumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionConfluentCloud) ToPointer() *CreateOutputMaximumTLSVersionConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMaximumTLSVersionConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputTLSSettingsClientSideConfluentCloud struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                      `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionConfluentCloud `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionConfluentCloud `json:"maxVersion,omitempty"`
}

func (c CreateOutputTLSSettingsClientSideConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) GetRejectUnauthorized() *bool {
	if c == nil {
		return nil
	}
	return c.RejectUnauthorized
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) GetServername() *string {
	if c == nil {
		return nil
	}
	return c.Servername
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) GetCertificateName() *string {
	if c == nil {
		return nil
	}
	return c.CertificateName
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) GetCaPath() *string {
	if c == nil {
		return nil
	}
	return c.CaPath
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) GetPrivKeyPath() *string {
	if c == nil {
		return nil
	}
	return c.PrivKeyPath
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) GetCertPath() *string {
	if c == nil {
		return nil
	}
	return c.CertPath
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) GetPassphrase() *string {
	if c == nil {
		return nil
	}
	return c.Passphrase
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) GetMinVersion() *CreateOutputMinimumTLSVersionConfluentCloud {
	if c == nil {
		return nil
	}
	return c.MinVersion
}

func (c *CreateOutputTLSSettingsClientSideConfluentCloud) GetMaxVersion() *CreateOutputMaximumTLSVersionConfluentCloud {
	if c == nil {
		return nil
	}
	return c.MaxVersion
}

// AcknowledgmentsConfluentCloud - Control the number of required acknowledgments.
type AcknowledgmentsConfluentCloud int64

const (
	// AcknowledgmentsConfluentCloudOne Leader
	AcknowledgmentsConfluentCloudOne AcknowledgmentsConfluentCloud = 1
	// AcknowledgmentsConfluentCloudZero None
	AcknowledgmentsConfluentCloudZero AcknowledgmentsConfluentCloud = 0
	// AcknowledgmentsConfluentCloudMinus1 All
	AcknowledgmentsConfluentCloudMinus1 AcknowledgmentsConfluentCloud = -1
)

func (e AcknowledgmentsConfluentCloud) ToPointer() *AcknowledgmentsConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AcknowledgmentsConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case 1, 0, -1:
			return true
		}
	}
	return false
}

// RecordDataFormatConfluentCloud - Format to use to serialize events before writing to Kafka.
type RecordDataFormatConfluentCloud string

const (
	// RecordDataFormatConfluentCloudJSON JSON
	RecordDataFormatConfluentCloudJSON RecordDataFormatConfluentCloud = "json"
	// RecordDataFormatConfluentCloudRaw Field _raw
	RecordDataFormatConfluentCloudRaw RecordDataFormatConfluentCloud = "raw"
	// RecordDataFormatConfluentCloudProtobuf Protobuf
	RecordDataFormatConfluentCloudProtobuf RecordDataFormatConfluentCloud = "protobuf"
)

func (e RecordDataFormatConfluentCloud) ToPointer() *RecordDataFormatConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *RecordDataFormatConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "protobuf":
			return true
		}
	}
	return false
}

// CreateOutputCompressionConfluentCloud - Codec to use to compress the data before sending to Kafka
type CreateOutputCompressionConfluentCloud string

const (
	// CreateOutputCompressionConfluentCloudNone None
	CreateOutputCompressionConfluentCloudNone CreateOutputCompressionConfluentCloud = "none"
	// CreateOutputCompressionConfluentCloudGzip Gzip
	CreateOutputCompressionConfluentCloudGzip CreateOutputCompressionConfluentCloud = "gzip"
	// CreateOutputCompressionConfluentCloudSnappy Snappy
	CreateOutputCompressionConfluentCloudSnappy CreateOutputCompressionConfluentCloud = "snappy"
	// CreateOutputCompressionConfluentCloudLz4 LZ4
	CreateOutputCompressionConfluentCloudLz4 CreateOutputCompressionConfluentCloud = "lz4"
	// CreateOutputCompressionConfluentCloudZstd ZSTD
	CreateOutputCompressionConfluentCloudZstd CreateOutputCompressionConfluentCloud = "zstd"
)

func (e CreateOutputCompressionConfluentCloud) ToPointer() *CreateOutputCompressionConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputCompressionConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip", "snappy", "lz4", "zstd":
			return true
		}
	}
	return false
}

// CreateOutputAuthConfluentCloud - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type CreateOutputAuthConfluentCloud struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (c CreateOutputAuthConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputAuthConfluentCloud) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputAuthConfluentCloud) GetCredentialsSecret() *string {
	if c == nil {
		return nil
	}
	return c.CredentialsSecret
}

type CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud string

const (
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv1  CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv11 CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv12 CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv13 CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud) ToPointer() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud string

const (
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv1  CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv11 CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv12 CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv13 CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud) ToPointer() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                                         `json:"passphrase,omitempty"`
	MinVersion *CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud `json:"maxVersion,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetRejectUnauthorized() *bool {
	if c == nil {
		return nil
	}
	return c.RejectUnauthorized
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetServername() *string {
	if c == nil {
		return nil
	}
	return c.Servername
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCertificateName() *string {
	if c == nil {
		return nil
	}
	return c.CertificateName
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCaPath() *string {
	if c == nil {
		return nil
	}
	return c.CaPath
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetPrivKeyPath() *string {
	if c == nil {
		return nil
	}
	return c.PrivKeyPath
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCertPath() *string {
	if c == nil {
		return nil
	}
	return c.CertPath
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetPassphrase() *string {
	if c == nil {
		return nil
	}
	return c.Passphrase
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetMinVersion() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud {
	if c == nil {
		return nil
	}
	return c.MinVersion
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetMaxVersion() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud {
	if c == nil {
		return nil
	}
	return c.MaxVersion
}

type CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud struct {
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *CreateOutputAuthConfluentCloud                                     `json:"auth,omitempty"`
	TLS  *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetSchemaRegistryURL() *string {
	if c == nil {
		return nil
	}
	return c.SchemaRegistryURL
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetConnectionTimeout() *float64 {
	if c == nil {
		return nil
	}
	return c.ConnectionTimeout
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetRequestTimeout() *float64 {
	if c == nil {
		return nil
	}
	return c.RequestTimeout
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetMaxRetries() *float64 {
	if c == nil {
		return nil
	}
	return c.MaxRetries
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetAuth() *CreateOutputAuthConfluentCloud {
	if c == nil {
		return nil
	}
	return c.Auth
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetTLS() *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud {
	if c == nil {
		return nil
	}
	return c.TLS
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDefaultKeySchemaID() *float64 {
	if c == nil {
		return nil
	}
	return c.DefaultKeySchemaID
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDefaultValueSchemaID() *float64 {
	if c == nil {
		return nil
	}
	return c.DefaultValueSchemaID
}

// CreateOutputAuthenticationMethodConfluentCloud - Enter credentials directly, or select a stored secret
type CreateOutputAuthenticationMethodConfluentCloud string

const (
	CreateOutputAuthenticationMethodConfluentCloudManual CreateOutputAuthenticationMethodConfluentCloud = "manual"
	CreateOutputAuthenticationMethodConfluentCloudSecret CreateOutputAuthenticationMethodConfluentCloud = "secret"
)

func (e CreateOutputAuthenticationMethodConfluentCloud) ToPointer() *CreateOutputAuthenticationMethodConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationMethodConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

type CreateOutputSASLMechanismConfluentCloud string

const (
	// CreateOutputSASLMechanismConfluentCloudPlain PLAIN
	CreateOutputSASLMechanismConfluentCloudPlain CreateOutputSASLMechanismConfluentCloud = "plain"
	// CreateOutputSASLMechanismConfluentCloudScramSha256 SCRAM-SHA-256
	CreateOutputSASLMechanismConfluentCloudScramSha256 CreateOutputSASLMechanismConfluentCloud = "scram-sha-256"
	// CreateOutputSASLMechanismConfluentCloudScramSha512 SCRAM-SHA-512
	CreateOutputSASLMechanismConfluentCloudScramSha512 CreateOutputSASLMechanismConfluentCloud = "scram-sha-512"
	// CreateOutputSASLMechanismConfluentCloudKerberos GSSAPI/Kerberos
	CreateOutputSASLMechanismConfluentCloudKerberos CreateOutputSASLMechanismConfluentCloud = "kerberos"
)

func (e CreateOutputSASLMechanismConfluentCloud) ToPointer() *CreateOutputSASLMechanismConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputSASLMechanismConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "plain", "scram-sha-256", "scram-sha-512", "kerberos":
			return true
		}
	}
	return false
}

type CreateOutputOauthParamConfluentCloud struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (c CreateOutputOauthParamConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputOauthParamConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputOauthParamConfluentCloud) GetName() string {
	if c == nil {
		return ""
	}
	return c.Name
}

func (c *CreateOutputOauthParamConfluentCloud) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

type CreateOutputSaslExtensionConfluentCloud struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (c CreateOutputSaslExtensionConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputSaslExtensionConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputSaslExtensionConfluentCloud) GetName() string {
	if c == nil {
		return ""
	}
	return c.Name
}

func (c *CreateOutputSaslExtensionConfluentCloud) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

// CreateOutputAuthenticationConfluentCloud - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type CreateOutputAuthenticationConfluentCloud struct {
	Disabled *bool   `default:"true" json:"disabled"`
	Username *string `json:"username,omitempty"`
	Password *string `json:"password,omitempty"`
	// Enter credentials directly, or select a stored secret
	AuthType *CreateOutputAuthenticationMethodConfluentCloud `default:"manual" json:"authType"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string                                  `json:"credentialsSecret,omitempty"`
	Mechanism         *CreateOutputSASLMechanismConfluentCloud `default:"plain" json:"mechanism"`
	// Location of keytab file for authentication principal
	KeytabLocation *string `json:"keytabLocation,omitempty"`
	// Authentication principal, such as `kafka_user@example.com`
	Principal *string `json:"principal,omitempty"`
	// Kerberos service class for Kafka brokers, such as `kafka`
	BrokerServiceClass *string `json:"brokerServiceClass,omitempty"`
	// Enable OAuth authentication
	OauthEnabled *bool `default:"false" json:"oauthEnabled"`
	// URL of the token endpoint to use for OAuth authentication
	TokenURL *string `json:"tokenUrl,omitempty"`
	// Client ID to use for OAuth authentication
	ClientID        *string `json:"clientId,omitempty"`
	OauthSecretType *string `default:"secret" json:"oauthSecretType"`
	// Select or create a stored text secret
	ClientTextSecret *string `json:"clientTextSecret,omitempty"`
	// Additional fields to send to the token endpoint, such as scope or audience
	OauthParams []CreateOutputOauthParamConfluentCloud `json:"oauthParams,omitempty"`
	// Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId
	SaslExtensions []CreateOutputSaslExtensionConfluentCloud `json:"saslExtensions,omitempty"`
}

func (c CreateOutputAuthenticationConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthenticationConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputAuthenticationConfluentCloud) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputAuthenticationConfluentCloud) GetUsername() *string {
	if c == nil {
		return nil
	}
	return c.Username
}

func (c *CreateOutputAuthenticationConfluentCloud) GetPassword() *string {
	if c == nil {
		return nil
	}
	return c.Password
}

func (c *CreateOutputAuthenticationConfluentCloud) GetAuthType() *CreateOutputAuthenticationMethodConfluentCloud {
	if c == nil {
		return nil
	}
	return c.AuthType
}

func (c *CreateOutputAuthenticationConfluentCloud) GetCredentialsSecret() *string {
	if c == nil {
		return nil
	}
	return c.CredentialsSecret
}

func (c *CreateOutputAuthenticationConfluentCloud) GetMechanism() *CreateOutputSASLMechanismConfluentCloud {
	if c == nil {
		return nil
	}
	return c.Mechanism
}

func (c *CreateOutputAuthenticationConfluentCloud) GetKeytabLocation() *string {
	if c == nil {
		return nil
	}
	return c.KeytabLocation
}

func (c *CreateOutputAuthenticationConfluentCloud) GetPrincipal() *string {
	if c == nil {
		return nil
	}
	return c.Principal
}

func (c *CreateOutputAuthenticationConfluentCloud) GetBrokerServiceClass() *string {
	if c == nil {
		return nil
	}
	return c.BrokerServiceClass
}

func (c *CreateOutputAuthenticationConfluentCloud) GetOauthEnabled() *bool {
	if c == nil {
		return nil
	}
	return c.OauthEnabled
}

func (c *CreateOutputAuthenticationConfluentCloud) GetTokenURL() *string {
	if c == nil {
		return nil
	}
	return c.TokenURL
}

func (c *CreateOutputAuthenticationConfluentCloud) GetClientID() *string {
	if c == nil {
		return nil
	}
	return c.ClientID
}

func (c *CreateOutputAuthenticationConfluentCloud) GetOauthSecretType() *string {
	if c == nil {
		return nil
	}
	return c.OauthSecretType
}

func (c *CreateOutputAuthenticationConfluentCloud) GetClientTextSecret() *string {
	if c == nil {
		return nil
	}
	return c.ClientTextSecret
}

func (c *CreateOutputAuthenticationConfluentCloud) GetOauthParams() []CreateOutputOauthParamConfluentCloud {
	if c == nil {
		return nil
	}
	return c.OauthParams
}

func (c *CreateOutputAuthenticationConfluentCloud) GetSaslExtensions() []CreateOutputSaslExtensionConfluentCloud {
	if c == nil {
		return nil
	}
	return c.SaslExtensions
}

// BackpressureBehaviorConfluentCloud - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorConfluentCloud string

const (
	// BackpressureBehaviorConfluentCloudBlock Block
	BackpressureBehaviorConfluentCloudBlock BackpressureBehaviorConfluentCloud = "block"
	// BackpressureBehaviorConfluentCloudDrop Drop
	BackpressureBehaviorConfluentCloudDrop BackpressureBehaviorConfluentCloud = "drop"
	// BackpressureBehaviorConfluentCloudQueue Persistent Queue
	BackpressureBehaviorConfluentCloudQueue BackpressureBehaviorConfluentCloud = "queue"
)

func (e BackpressureBehaviorConfluentCloud) ToPointer() *BackpressureBehaviorConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// CreateOutputModeConfluentCloud - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeConfluentCloud string

const (
	// CreateOutputModeConfluentCloudError Error
	CreateOutputModeConfluentCloudError CreateOutputModeConfluentCloud = "error"
	// CreateOutputModeConfluentCloudAlways Backpressure
	CreateOutputModeConfluentCloudAlways CreateOutputModeConfluentCloud = "always"
	// CreateOutputModeConfluentCloudBackpressure Always On
	CreateOutputModeConfluentCloudBackpressure CreateOutputModeConfluentCloud = "backpressure"
)

func (e CreateOutputModeConfluentCloud) ToPointer() *CreateOutputModeConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionConfluentCloud - Codec to use to compress the persisted data
type PqCompressCompressionConfluentCloud string

const (
	// PqCompressCompressionConfluentCloudNone None
	PqCompressCompressionConfluentCloudNone PqCompressCompressionConfluentCloud = "none"
	// PqCompressCompressionConfluentCloudGzip Gzip
	PqCompressCompressionConfluentCloudGzip PqCompressCompressionConfluentCloud = "gzip"
)

func (e PqCompressCompressionConfluentCloud) ToPointer() *PqCompressCompressionConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorConfluentCloud - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorConfluentCloud string

const (
	// QueueFullBehaviorConfluentCloudBlock Block
	QueueFullBehaviorConfluentCloudBlock QueueFullBehaviorConfluentCloud = "block"
	// QueueFullBehaviorConfluentCloudDrop Drop new data
	QueueFullBehaviorConfluentCloudDrop QueueFullBehaviorConfluentCloud = "drop"
)

func (e QueueFullBehaviorConfluentCloud) ToPointer() *QueueFullBehaviorConfluentCloud {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorConfluentCloud) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsConfluentCloud struct {
}

func (c CreateOutputPqControlsConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputConfluentCloud struct {
	// Unique ID for this output
	ID   string                         `json:"id"`
	Type CreateOutputTypeConfluentCloud `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092.
	Brokers []string                                         `json:"brokers"`
	TLS     *CreateOutputTLSSettingsClientSideConfluentCloud `json:"tls,omitempty"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsConfluentCloud `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatConfluentCloud `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *CreateOutputCompressionConfluentCloud `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                                     `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *CreateOutputAuthenticationConfluentCloud `json:"sasl,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorConfluentCloud `default:"block" json:"onBackpressure"`
	Description    *string                             `json:"description,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// Select the type of object you want the Protobuf definitions to use for event encoding
	ProtobufEncodingID *string `json:"protobufEncodingId,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeConfluentCloud `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionConfluentCloud `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorConfluentCloud      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsConfluentCloud `json:"pqControls,omitempty"`
}

func (o OutputConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "brokers", "topic"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputConfluentCloud) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputConfluentCloud) GetType() CreateOutputTypeConfluentCloud {
	if o == nil {
		return CreateOutputTypeConfluentCloud("")
	}
	return o.Type
}

func (o *OutputConfluentCloud) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputConfluentCloud) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputConfluentCloud) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputConfluentCloud) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputConfluentCloud) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputConfluentCloud) GetTLS() *CreateOutputTLSSettingsClientSideConfluentCloud {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputConfluentCloud) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputConfluentCloud) GetAck() *AcknowledgmentsConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputConfluentCloud) GetFormat() *RecordDataFormatConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputConfluentCloud) GetCompression() *CreateOutputCompressionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputConfluentCloud) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputConfluentCloud) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputConfluentCloud) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputConfluentCloud) GetKafkaSchemaRegistry() *CreateOutputKafkaSchemaRegistryAuthenticationConfluentCloud {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputConfluentCloud) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputConfluentCloud) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputConfluentCloud) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputConfluentCloud) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputConfluentCloud) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputConfluentCloud) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputConfluentCloud) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputConfluentCloud) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputConfluentCloud) GetSasl() *CreateOutputAuthenticationConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputConfluentCloud) GetOnBackpressure() *BackpressureBehaviorConfluentCloud {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputConfluentCloud) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputConfluentCloud) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputConfluentCloud) GetProtobufEncodingID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufEncodingID
}

func (o *OutputConfluentCloud) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputConfluentCloud) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputConfluentCloud) GetPqMode() *CreateOutputModeConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputConfluentCloud) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputConfluentCloud) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputConfluentCloud) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputConfluentCloud) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputConfluentCloud) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputConfluentCloud) GetPqCompress() *PqCompressCompressionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputConfluentCloud) GetPqOnBackpressure() *QueueFullBehaviorConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputConfluentCloud) GetPqControls() *CreateOutputPqControlsConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeKafka string

const (
	CreateOutputTypeKafkaKafka CreateOutputTypeKafka = "kafka"
)

func (e CreateOutputTypeKafka) ToPointer() *CreateOutputTypeKafka {
	return &e
}
func (e *CreateOutputTypeKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "kafka":
		*e = CreateOutputTypeKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeKafka: %v", v)
	}
}

// AcknowledgmentsKafka - Control the number of required acknowledgments.
type AcknowledgmentsKafka int64

const (
	// AcknowledgmentsKafkaOne Leader
	AcknowledgmentsKafkaOne AcknowledgmentsKafka = 1
	// AcknowledgmentsKafkaZero None
	AcknowledgmentsKafkaZero AcknowledgmentsKafka = 0
	// AcknowledgmentsKafkaMinus1 All
	AcknowledgmentsKafkaMinus1 AcknowledgmentsKafka = -1
)

func (e AcknowledgmentsKafka) ToPointer() *AcknowledgmentsKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AcknowledgmentsKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case 1, 0, -1:
			return true
		}
	}
	return false
}

// RecordDataFormatKafka - Format to use to serialize events before writing to Kafka.
type RecordDataFormatKafka string

const (
	// RecordDataFormatKafkaJSON JSON
	RecordDataFormatKafkaJSON RecordDataFormatKafka = "json"
	// RecordDataFormatKafkaRaw Field _raw
	RecordDataFormatKafkaRaw RecordDataFormatKafka = "raw"
	// RecordDataFormatKafkaProtobuf Protobuf
	RecordDataFormatKafkaProtobuf RecordDataFormatKafka = "protobuf"
)

func (e RecordDataFormatKafka) ToPointer() *RecordDataFormatKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *RecordDataFormatKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "protobuf":
			return true
		}
	}
	return false
}

// CreateOutputCompressionKafka - Codec to use to compress the data before sending to Kafka
type CreateOutputCompressionKafka string

const (
	// CreateOutputCompressionKafkaNone None
	CreateOutputCompressionKafkaNone CreateOutputCompressionKafka = "none"
	// CreateOutputCompressionKafkaGzip Gzip
	CreateOutputCompressionKafkaGzip CreateOutputCompressionKafka = "gzip"
	// CreateOutputCompressionKafkaSnappy Snappy
	CreateOutputCompressionKafkaSnappy CreateOutputCompressionKafka = "snappy"
	// CreateOutputCompressionKafkaLz4 LZ4
	CreateOutputCompressionKafkaLz4 CreateOutputCompressionKafka = "lz4"
	// CreateOutputCompressionKafkaZstd ZSTD
	CreateOutputCompressionKafkaZstd CreateOutputCompressionKafka = "zstd"
)

func (e CreateOutputCompressionKafka) ToPointer() *CreateOutputCompressionKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputCompressionKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip", "snappy", "lz4", "zstd":
			return true
		}
	}
	return false
}

// CreateOutputAuthKafka - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type CreateOutputAuthKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (c CreateOutputAuthKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputAuthKafka) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputAuthKafka) GetCredentialsSecret() *string {
	if c == nil {
		return nil
	}
	return c.CredentialsSecret
}

type CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka string

const (
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv1  CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv11 CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv12 CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv13 CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka) ToPointer() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka string

const (
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv1  CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv11 CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.1"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv12 CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.2"
	CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv13 CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.3"
)

func (e CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka) ToPointer() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                                `json:"passphrase,omitempty"`
	MinVersion *CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka `json:"maxVersion,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetRejectUnauthorized() *bool {
	if c == nil {
		return nil
	}
	return c.RejectUnauthorized
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetServername() *string {
	if c == nil {
		return nil
	}
	return c.Servername
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCertificateName() *string {
	if c == nil {
		return nil
	}
	return c.CertificateName
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCaPath() *string {
	if c == nil {
		return nil
	}
	return c.CaPath
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetPrivKeyPath() *string {
	if c == nil {
		return nil
	}
	return c.PrivKeyPath
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCertPath() *string {
	if c == nil {
		return nil
	}
	return c.CertPath
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetPassphrase() *string {
	if c == nil {
		return nil
	}
	return c.Passphrase
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetMinVersion() *CreateOutputKafkaSchemaRegistryMinimumTLSVersionKafka {
	if c == nil {
		return nil
	}
	return c.MinVersion
}

func (c *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetMaxVersion() *CreateOutputKafkaSchemaRegistryMaximumTLSVersionKafka {
	if c == nil {
		return nil
	}
	return c.MaxVersion
}

type CreateOutputKafkaSchemaRegistryAuthenticationKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *CreateOutputAuthKafka                                     `json:"auth,omitempty"`
	TLS  *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (c CreateOutputKafkaSchemaRegistryAuthenticationKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetSchemaRegistryURL() *string {
	if c == nil {
		return nil
	}
	return c.SchemaRegistryURL
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetConnectionTimeout() *float64 {
	if c == nil {
		return nil
	}
	return c.ConnectionTimeout
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetRequestTimeout() *float64 {
	if c == nil {
		return nil
	}
	return c.RequestTimeout
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetMaxRetries() *float64 {
	if c == nil {
		return nil
	}
	return c.MaxRetries
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetAuth() *CreateOutputAuthKafka {
	if c == nil {
		return nil
	}
	return c.Auth
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetTLS() *CreateOutputKafkaSchemaRegistryTLSSettingsClientSideKafka {
	if c == nil {
		return nil
	}
	return c.TLS
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetDefaultKeySchemaID() *float64 {
	if c == nil {
		return nil
	}
	return c.DefaultKeySchemaID
}

func (c *CreateOutputKafkaSchemaRegistryAuthenticationKafka) GetDefaultValueSchemaID() *float64 {
	if c == nil {
		return nil
	}
	return c.DefaultValueSchemaID
}

// CreateOutputAuthenticationMethodKafka - Enter credentials directly, or select a stored secret
type CreateOutputAuthenticationMethodKafka string

const (
	CreateOutputAuthenticationMethodKafkaManual CreateOutputAuthenticationMethodKafka = "manual"
	CreateOutputAuthenticationMethodKafkaSecret CreateOutputAuthenticationMethodKafka = "secret"
)

func (e CreateOutputAuthenticationMethodKafka) ToPointer() *CreateOutputAuthenticationMethodKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationMethodKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

type CreateOutputSASLMechanismKafka string

const (
	// CreateOutputSASLMechanismKafkaPlain PLAIN
	CreateOutputSASLMechanismKafkaPlain CreateOutputSASLMechanismKafka = "plain"
	// CreateOutputSASLMechanismKafkaScramSha256 SCRAM-SHA-256
	CreateOutputSASLMechanismKafkaScramSha256 CreateOutputSASLMechanismKafka = "scram-sha-256"
	// CreateOutputSASLMechanismKafkaScramSha512 SCRAM-SHA-512
	CreateOutputSASLMechanismKafkaScramSha512 CreateOutputSASLMechanismKafka = "scram-sha-512"
	// CreateOutputSASLMechanismKafkaKerberos GSSAPI/Kerberos
	CreateOutputSASLMechanismKafkaKerberos CreateOutputSASLMechanismKafka = "kerberos"
)

func (e CreateOutputSASLMechanismKafka) ToPointer() *CreateOutputSASLMechanismKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputSASLMechanismKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "plain", "scram-sha-256", "scram-sha-512", "kerberos":
			return true
		}
	}
	return false
}

type CreateOutputOauthParamKafka struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (c CreateOutputOauthParamKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputOauthParamKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputOauthParamKafka) GetName() string {
	if c == nil {
		return ""
	}
	return c.Name
}

func (c *CreateOutputOauthParamKafka) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

type CreateOutputSaslExtensionKafka struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (c CreateOutputSaslExtensionKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputSaslExtensionKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputSaslExtensionKafka) GetName() string {
	if c == nil {
		return ""
	}
	return c.Name
}

func (c *CreateOutputSaslExtensionKafka) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

// CreateOutputAuthenticationKafka - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type CreateOutputAuthenticationKafka struct {
	Disabled *bool   `default:"true" json:"disabled"`
	Username *string `json:"username,omitempty"`
	Password *string `json:"password,omitempty"`
	// Enter credentials directly, or select a stored secret
	AuthType *CreateOutputAuthenticationMethodKafka `default:"manual" json:"authType"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string                         `json:"credentialsSecret,omitempty"`
	Mechanism         *CreateOutputSASLMechanismKafka `default:"plain" json:"mechanism"`
	// Location of keytab file for authentication principal
	KeytabLocation *string `json:"keytabLocation,omitempty"`
	// Authentication principal, such as `kafka_user@example.com`
	Principal *string `json:"principal,omitempty"`
	// Kerberos service class for Kafka brokers, such as `kafka`
	BrokerServiceClass *string `json:"brokerServiceClass,omitempty"`
	// Enable OAuth authentication
	OauthEnabled *bool `default:"false" json:"oauthEnabled"`
	// URL of the token endpoint to use for OAuth authentication
	TokenURL *string `json:"tokenUrl,omitempty"`
	// Client ID to use for OAuth authentication
	ClientID        *string `json:"clientId,omitempty"`
	OauthSecretType *string `default:"secret" json:"oauthSecretType"`
	// Select or create a stored text secret
	ClientTextSecret *string `json:"clientTextSecret,omitempty"`
	// Additional fields to send to the token endpoint, such as scope or audience
	OauthParams []CreateOutputOauthParamKafka `json:"oauthParams,omitempty"`
	// Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId
	SaslExtensions []CreateOutputSaslExtensionKafka `json:"saslExtensions,omitempty"`
}

func (c CreateOutputAuthenticationKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputAuthenticationKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputAuthenticationKafka) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputAuthenticationKafka) GetUsername() *string {
	if c == nil {
		return nil
	}
	return c.Username
}

func (c *CreateOutputAuthenticationKafka) GetPassword() *string {
	if c == nil {
		return nil
	}
	return c.Password
}

func (c *CreateOutputAuthenticationKafka) GetAuthType() *CreateOutputAuthenticationMethodKafka {
	if c == nil {
		return nil
	}
	return c.AuthType
}

func (c *CreateOutputAuthenticationKafka) GetCredentialsSecret() *string {
	if c == nil {
		return nil
	}
	return c.CredentialsSecret
}

func (c *CreateOutputAuthenticationKafka) GetMechanism() *CreateOutputSASLMechanismKafka {
	if c == nil {
		return nil
	}
	return c.Mechanism
}

func (c *CreateOutputAuthenticationKafka) GetKeytabLocation() *string {
	if c == nil {
		return nil
	}
	return c.KeytabLocation
}

func (c *CreateOutputAuthenticationKafka) GetPrincipal() *string {
	if c == nil {
		return nil
	}
	return c.Principal
}

func (c *CreateOutputAuthenticationKafka) GetBrokerServiceClass() *string {
	if c == nil {
		return nil
	}
	return c.BrokerServiceClass
}

func (c *CreateOutputAuthenticationKafka) GetOauthEnabled() *bool {
	if c == nil {
		return nil
	}
	return c.OauthEnabled
}

func (c *CreateOutputAuthenticationKafka) GetTokenURL() *string {
	if c == nil {
		return nil
	}
	return c.TokenURL
}

func (c *CreateOutputAuthenticationKafka) GetClientID() *string {
	if c == nil {
		return nil
	}
	return c.ClientID
}

func (c *CreateOutputAuthenticationKafka) GetOauthSecretType() *string {
	if c == nil {
		return nil
	}
	return c.OauthSecretType
}

func (c *CreateOutputAuthenticationKafka) GetClientTextSecret() *string {
	if c == nil {
		return nil
	}
	return c.ClientTextSecret
}

func (c *CreateOutputAuthenticationKafka) GetOauthParams() []CreateOutputOauthParamKafka {
	if c == nil {
		return nil
	}
	return c.OauthParams
}

func (c *CreateOutputAuthenticationKafka) GetSaslExtensions() []CreateOutputSaslExtensionKafka {
	if c == nil {
		return nil
	}
	return c.SaslExtensions
}

type CreateOutputMinimumTLSVersionKafka string

const (
	CreateOutputMinimumTLSVersionKafkaTlSv1  CreateOutputMinimumTLSVersionKafka = "TLSv1"
	CreateOutputMinimumTLSVersionKafkaTlSv11 CreateOutputMinimumTLSVersionKafka = "TLSv1.1"
	CreateOutputMinimumTLSVersionKafkaTlSv12 CreateOutputMinimumTLSVersionKafka = "TLSv1.2"
	CreateOutputMinimumTLSVersionKafkaTlSv13 CreateOutputMinimumTLSVersionKafka = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionKafka) ToPointer() *CreateOutputMinimumTLSVersionKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMinimumTLSVersionKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputMaximumTLSVersionKafka string

const (
	CreateOutputMaximumTLSVersionKafkaTlSv1  CreateOutputMaximumTLSVersionKafka = "TLSv1"
	CreateOutputMaximumTLSVersionKafkaTlSv11 CreateOutputMaximumTLSVersionKafka = "TLSv1.1"
	CreateOutputMaximumTLSVersionKafkaTlSv12 CreateOutputMaximumTLSVersionKafka = "TLSv1.2"
	CreateOutputMaximumTLSVersionKafkaTlSv13 CreateOutputMaximumTLSVersionKafka = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionKafka) ToPointer() *CreateOutputMaximumTLSVersionKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMaximumTLSVersionKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputTLSSettingsClientSideKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                             `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionKafka `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionKafka `json:"maxVersion,omitempty"`
}

func (c CreateOutputTLSSettingsClientSideKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputTLSSettingsClientSideKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputTLSSettingsClientSideKafka) GetDisabled() *bool {
	if c == nil {
		return nil
	}
	return c.Disabled
}

func (c *CreateOutputTLSSettingsClientSideKafka) GetRejectUnauthorized() *bool {
	if c == nil {
		return nil
	}
	return c.RejectUnauthorized
}

func (c *CreateOutputTLSSettingsClientSideKafka) GetServername() *string {
	if c == nil {
		return nil
	}
	return c.Servername
}

func (c *CreateOutputTLSSettingsClientSideKafka) GetCertificateName() *string {
	if c == nil {
		return nil
	}
	return c.CertificateName
}

func (c *CreateOutputTLSSettingsClientSideKafka) GetCaPath() *string {
	if c == nil {
		return nil
	}
	return c.CaPath
}

func (c *CreateOutputTLSSettingsClientSideKafka) GetPrivKeyPath() *string {
	if c == nil {
		return nil
	}
	return c.PrivKeyPath
}

func (c *CreateOutputTLSSettingsClientSideKafka) GetCertPath() *string {
	if c == nil {
		return nil
	}
	return c.CertPath
}

func (c *CreateOutputTLSSettingsClientSideKafka) GetPassphrase() *string {
	if c == nil {
		return nil
	}
	return c.Passphrase
}

func (c *CreateOutputTLSSettingsClientSideKafka) GetMinVersion() *CreateOutputMinimumTLSVersionKafka {
	if c == nil {
		return nil
	}
	return c.MinVersion
}

func (c *CreateOutputTLSSettingsClientSideKafka) GetMaxVersion() *CreateOutputMaximumTLSVersionKafka {
	if c == nil {
		return nil
	}
	return c.MaxVersion
}

// BackpressureBehaviorKafka - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorKafka string

const (
	// BackpressureBehaviorKafkaBlock Block
	BackpressureBehaviorKafkaBlock BackpressureBehaviorKafka = "block"
	// BackpressureBehaviorKafkaDrop Drop
	BackpressureBehaviorKafkaDrop BackpressureBehaviorKafka = "drop"
	// BackpressureBehaviorKafkaQueue Persistent Queue
	BackpressureBehaviorKafkaQueue BackpressureBehaviorKafka = "queue"
)

func (e BackpressureBehaviorKafka) ToPointer() *BackpressureBehaviorKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// CreateOutputModeKafka - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeKafka string

const (
	// CreateOutputModeKafkaError Error
	CreateOutputModeKafkaError CreateOutputModeKafka = "error"
	// CreateOutputModeKafkaAlways Backpressure
	CreateOutputModeKafkaAlways CreateOutputModeKafka = "always"
	// CreateOutputModeKafkaBackpressure Always On
	CreateOutputModeKafkaBackpressure CreateOutputModeKafka = "backpressure"
)

func (e CreateOutputModeKafka) ToPointer() *CreateOutputModeKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionKafka - Codec to use to compress the persisted data
type PqCompressCompressionKafka string

const (
	// PqCompressCompressionKafkaNone None
	PqCompressCompressionKafkaNone PqCompressCompressionKafka = "none"
	// PqCompressCompressionKafkaGzip Gzip
	PqCompressCompressionKafkaGzip PqCompressCompressionKafka = "gzip"
)

func (e PqCompressCompressionKafka) ToPointer() *PqCompressCompressionKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorKafka - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorKafka string

const (
	// QueueFullBehaviorKafkaBlock Block
	QueueFullBehaviorKafkaBlock QueueFullBehaviorKafka = "block"
	// QueueFullBehaviorKafkaDrop Drop new data
	QueueFullBehaviorKafkaDrop QueueFullBehaviorKafka = "drop"
)

func (e QueueFullBehaviorKafka) ToPointer() *QueueFullBehaviorKafka {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorKafka) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsKafka struct {
}

func (c CreateOutputPqControlsKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputKafka struct {
	// Unique ID for this output
	ID   string                `json:"id"`
	Type CreateOutputTypeKafka `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.
	Brokers []string `json:"brokers"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsKafka `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatKafka `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *CreateOutputCompressionKafka `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                            `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *CreateOutputKafkaSchemaRegistryAuthenticationKafka `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *CreateOutputAuthenticationKafka        `json:"sasl,omitempty"`
	TLS  *CreateOutputTLSSettingsClientSideKafka `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorKafka `default:"block" json:"onBackpressure"`
	Description    *string                    `json:"description,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// Select the type of object you want the Protobuf definitions to use for event encoding
	ProtobufEncodingID *string `json:"protobufEncodingId,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeKafka `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionKafka `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorKafka      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsKafka `json:"pqControls,omitempty"`
}

func (o OutputKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "brokers", "topic"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafka) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputKafka) GetType() CreateOutputTypeKafka {
	if o == nil {
		return CreateOutputTypeKafka("")
	}
	return o.Type
}

func (o *OutputKafka) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputKafka) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputKafka) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputKafka) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputKafka) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputKafka) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputKafka) GetAck() *AcknowledgmentsKafka {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputKafka) GetFormat() *RecordDataFormatKafka {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputKafka) GetCompression() *CreateOutputCompressionKafka {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputKafka) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputKafka) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputKafka) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputKafka) GetKafkaSchemaRegistry() *CreateOutputKafkaSchemaRegistryAuthenticationKafka {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputKafka) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputKafka) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputKafka) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputKafka) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputKafka) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputKafka) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputKafka) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputKafka) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputKafka) GetSasl() *CreateOutputAuthenticationKafka {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputKafka) GetTLS() *CreateOutputTLSSettingsClientSideKafka {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputKafka) GetOnBackpressure() *BackpressureBehaviorKafka {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputKafka) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputKafka) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputKafka) GetProtobufEncodingID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufEncodingID
}

func (o *OutputKafka) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputKafka) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputKafka) GetPqMode() *CreateOutputModeKafka {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputKafka) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputKafka) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputKafka) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputKafka) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputKafka) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputKafka) GetPqCompress() *PqCompressCompressionKafka {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputKafka) GetPqOnBackpressure() *QueueFullBehaviorKafka {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputKafka) GetPqControls() *CreateOutputPqControlsKafka {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeExabeam string

const (
	TypeExabeamExabeam TypeExabeam = "exabeam"
)

func (e TypeExabeam) ToPointer() *TypeExabeam {
	return &e
}
func (e *TypeExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "exabeam":
		*e = TypeExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeExabeam: %v", v)
	}
}

// SignatureVersionExabeam - Signature version to use for signing Google Cloud Storage requests
type SignatureVersionExabeam string

const (
	SignatureVersionExabeamV2 SignatureVersionExabeam = "v2"
	SignatureVersionExabeamV4 SignatureVersionExabeam = "v4"
)

func (e SignatureVersionExabeam) ToPointer() *SignatureVersionExabeam {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SignatureVersionExabeam) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

// ObjectACLExabeam - Object ACL to assign to uploaded objects
type ObjectACLExabeam string

const (
	// ObjectACLExabeamPrivate private
	ObjectACLExabeamPrivate ObjectACLExabeam = "private"
	// ObjectACLExabeamBucketOwnerRead bucket-owner-read
	ObjectACLExabeamBucketOwnerRead ObjectACLExabeam = "bucket-owner-read"
	// ObjectACLExabeamBucketOwnerFullControl bucket-owner-full-control
	ObjectACLExabeamBucketOwnerFullControl ObjectACLExabeam = "bucket-owner-full-control"
	// ObjectACLExabeamProjectPrivate project-private
	ObjectACLExabeamProjectPrivate ObjectACLExabeam = "project-private"
	// ObjectACLExabeamAuthenticatedRead authenticated-read
	ObjectACLExabeamAuthenticatedRead ObjectACLExabeam = "authenticated-read"
	// ObjectACLExabeamPublicRead public-read
	ObjectACLExabeamPublicRead ObjectACLExabeam = "public-read"
)

func (e ObjectACLExabeam) ToPointer() *ObjectACLExabeam {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ObjectACLExabeam) IsExact() bool {
	if e != nil {
		switch *e {
		case "private", "bucket-owner-read", "bucket-owner-full-control", "project-private", "authenticated-read", "public-read":
			return true
		}
	}
	return false
}

// StorageClassExabeam - Storage class to select for uploaded objects
type StorageClassExabeam string

const (
	// StorageClassExabeamStandard Standard Storage
	StorageClassExabeamStandard StorageClassExabeam = "STANDARD"
	// StorageClassExabeamNearline Nearline Storage
	StorageClassExabeamNearline StorageClassExabeam = "NEARLINE"
	// StorageClassExabeamColdline Coldline Storage
	StorageClassExabeamColdline StorageClassExabeam = "COLDLINE"
	// StorageClassExabeamArchive Archive Storage
	StorageClassExabeamArchive StorageClassExabeam = "ARCHIVE"
)

func (e StorageClassExabeam) ToPointer() *StorageClassExabeam {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *StorageClassExabeam) IsExact() bool {
	if e != nil {
		switch *e {
		case "STANDARD", "NEARLINE", "COLDLINE", "ARCHIVE":
			return true
		}
	}
	return false
}

// BackpressureBehaviorExabeam - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorExabeam string

const (
	// BackpressureBehaviorExabeamBlock Block
	BackpressureBehaviorExabeamBlock BackpressureBehaviorExabeam = "block"
	// BackpressureBehaviorExabeamDrop Drop
	BackpressureBehaviorExabeamDrop BackpressureBehaviorExabeam = "drop"
)

func (e BackpressureBehaviorExabeam) ToPointer() *BackpressureBehaviorExabeam {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorExabeam) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionExabeam - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionExabeam string

const (
	// DiskSpaceProtectionExabeamBlock Block
	DiskSpaceProtectionExabeamBlock DiskSpaceProtectionExabeam = "block"
	// DiskSpaceProtectionExabeamDrop Drop
	DiskSpaceProtectionExabeamDrop DiskSpaceProtectionExabeam = "drop"
)

func (e DiskSpaceProtectionExabeam) ToPointer() *DiskSpaceProtectionExabeam {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionExabeam) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type OutputExabeam struct {
	// Unique ID for this output
	ID   string      `json:"id"`
	Type TypeExabeam `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// Region where the bucket is located
	Region string `json:"region"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Google Cloud Storage service endpoint
	Endpoint *string `default:"https://storage.googleapis.com" json:"endpoint"`
	// Signature version to use for signing Google Cloud Storage requests
	SignatureVersion *SignatureVersionExabeam `default:"v4" json:"signatureVersion"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLExabeam `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass *StorageClassExabeam `json:"storageClass,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorExabeam `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionExabeam `default:"block" json:"onDiskFullBackpressure"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"10" json:"maxFileSizeMB"`
	// Enter an encoded string containing Exabeam configurations
	EncodedConfiguration *string `json:"encodedConfiguration,omitempty"`
	// ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888
	//
	CollectorInstanceID string `json:"collectorInstanceId"`
	// Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants.
	SiteName *string `json:"siteName,omitempty"`
	// Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name.
	SiteID         *string `json:"siteId,omitempty"`
	TimezoneOffset *string `json:"timezoneOffset,omitempty"`
	// HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	Description  *string `json:"description,omitempty"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputExabeam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputExabeam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "bucket", "region", "collectorInstanceId"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputExabeam) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputExabeam) GetType() TypeExabeam {
	if o == nil {
		return TypeExabeam("")
	}
	return o.Type
}

func (o *OutputExabeam) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputExabeam) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputExabeam) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputExabeam) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputExabeam) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputExabeam) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputExabeam) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputExabeam) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputExabeam) GetSignatureVersion() *SignatureVersionExabeam {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputExabeam) GetObjectACL() *ObjectACLExabeam {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputExabeam) GetStorageClass() *StorageClassExabeam {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputExabeam) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputExabeam) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputExabeam) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputExabeam) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputExabeam) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputExabeam) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputExabeam) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputExabeam) GetOnBackpressure() *BackpressureBehaviorExabeam {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputExabeam) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputExabeam) GetOnDiskFullBackpressure() *DiskSpaceProtectionExabeam {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputExabeam) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputExabeam) GetEncodedConfiguration() *string {
	if o == nil {
		return nil
	}
	return o.EncodedConfiguration
}

func (o *OutputExabeam) GetCollectorInstanceID() string {
	if o == nil {
		return ""
	}
	return o.CollectorInstanceID
}

func (o *OutputExabeam) GetSiteName() *string {
	if o == nil {
		return nil
	}
	return o.SiteName
}

func (o *OutputExabeam) GetSiteID() *string {
	if o == nil {
		return nil
	}
	return o.SiteID
}

func (o *OutputExabeam) GetTimezoneOffset() *string {
	if o == nil {
		return nil
	}
	return o.TimezoneOffset
}

func (o *OutputExabeam) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputExabeam) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputExabeam) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputExabeam) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputExabeam) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputExabeam) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputExabeam) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type CreateOutputTypeGooglePubsub string

const (
	CreateOutputTypeGooglePubsubGooglePubsub CreateOutputTypeGooglePubsub = "google_pubsub"
)

func (e CreateOutputTypeGooglePubsub) ToPointer() *CreateOutputTypeGooglePubsub {
	return &e
}
func (e *CreateOutputTypeGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_pubsub":
		*e = CreateOutputTypeGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeGooglePubsub: %v", v)
	}
}

// CreateOutputGoogleAuthenticationMethodGooglePubsub - Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
type CreateOutputGoogleAuthenticationMethodGooglePubsub string

const (
	// CreateOutputGoogleAuthenticationMethodGooglePubsubAuto Auto
	CreateOutputGoogleAuthenticationMethodGooglePubsubAuto CreateOutputGoogleAuthenticationMethodGooglePubsub = "auto"
	// CreateOutputGoogleAuthenticationMethodGooglePubsubManual Manual
	CreateOutputGoogleAuthenticationMethodGooglePubsubManual CreateOutputGoogleAuthenticationMethodGooglePubsub = "manual"
	// CreateOutputGoogleAuthenticationMethodGooglePubsubSecret Secret
	CreateOutputGoogleAuthenticationMethodGooglePubsubSecret CreateOutputGoogleAuthenticationMethodGooglePubsub = "secret"
)

func (e CreateOutputGoogleAuthenticationMethodGooglePubsub) ToPointer() *CreateOutputGoogleAuthenticationMethodGooglePubsub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputGoogleAuthenticationMethodGooglePubsub) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// BackpressureBehaviorGooglePubsub - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGooglePubsub string

const (
	// BackpressureBehaviorGooglePubsubBlock Block
	BackpressureBehaviorGooglePubsubBlock BackpressureBehaviorGooglePubsub = "block"
	// BackpressureBehaviorGooglePubsubDrop Drop
	BackpressureBehaviorGooglePubsubDrop BackpressureBehaviorGooglePubsub = "drop"
	// BackpressureBehaviorGooglePubsubQueue Persistent Queue
	BackpressureBehaviorGooglePubsubQueue BackpressureBehaviorGooglePubsub = "queue"
)

func (e BackpressureBehaviorGooglePubsub) ToPointer() *BackpressureBehaviorGooglePubsub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorGooglePubsub) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// CreateOutputModeGooglePubsub - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeGooglePubsub string

const (
	// CreateOutputModeGooglePubsubError Error
	CreateOutputModeGooglePubsubError CreateOutputModeGooglePubsub = "error"
	// CreateOutputModeGooglePubsubAlways Backpressure
	CreateOutputModeGooglePubsubAlways CreateOutputModeGooglePubsub = "always"
	// CreateOutputModeGooglePubsubBackpressure Always On
	CreateOutputModeGooglePubsubBackpressure CreateOutputModeGooglePubsub = "backpressure"
)

func (e CreateOutputModeGooglePubsub) ToPointer() *CreateOutputModeGooglePubsub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeGooglePubsub) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionGooglePubsub - Codec to use to compress the persisted data
type PqCompressCompressionGooglePubsub string

const (
	// PqCompressCompressionGooglePubsubNone None
	PqCompressCompressionGooglePubsubNone PqCompressCompressionGooglePubsub = "none"
	// PqCompressCompressionGooglePubsubGzip Gzip
	PqCompressCompressionGooglePubsubGzip PqCompressCompressionGooglePubsub = "gzip"
)

func (e PqCompressCompressionGooglePubsub) ToPointer() *PqCompressCompressionGooglePubsub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionGooglePubsub) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorGooglePubsub - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGooglePubsub string

const (
	// QueueFullBehaviorGooglePubsubBlock Block
	QueueFullBehaviorGooglePubsubBlock QueueFullBehaviorGooglePubsub = "block"
	// QueueFullBehaviorGooglePubsubDrop Drop new data
	QueueFullBehaviorGooglePubsubDrop QueueFullBehaviorGooglePubsub = "drop"
)

func (e QueueFullBehaviorGooglePubsub) ToPointer() *QueueFullBehaviorGooglePubsub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorGooglePubsub) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsGooglePubsub struct {
}

func (c CreateOutputPqControlsGooglePubsub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsGooglePubsub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGooglePubsub struct {
	// Unique ID for this output
	ID   string                       `json:"id"`
	Type CreateOutputTypeGooglePubsub `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// ID of the topic to send events to.
	TopicName string `json:"topicName"`
	// If enabled, create topic if it does not exist.
	CreateTopic *bool `default:"false" json:"createTopic"`
	// If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled.
	OrderedDelivery *bool `default:"false" json:"orderedDelivery"`
	// Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.
	Region *string `json:"region,omitempty"`
	// Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
	GoogleAuthMethod *CreateOutputGoogleAuthenticationMethodGooglePubsub `default:"manual" json:"googleAuthMethod"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	Secret *string `json:"secret,omitempty"`
	// The maximum number of items the Google API should batch before it sends them to the topic.
	BatchSize *float64 `default:"1000" json:"batchSize"`
	// The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached).
	BatchTimeout *float64 `default:"100" json:"batchTimeout"`
	// Maximum number of queued batches before blocking.
	MaxQueueSize *float64 `default:"100" json:"maxQueueSize"`
	// Maximum size (KB) of batches to send.
	MaxRecordSizeKB *float64 `default:"256" json:"maxRecordSizeKB"`
	// Maximum time to wait before sending a batch (when batch size limit is not reached)
	FlushPeriod *float64 `default:"1" json:"flushPeriod"`
	// The maximum number of in-progress API requests before backpressure is applied.
	MaxInProgress *float64 `default:"10" json:"maxInProgress"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGooglePubsub `default:"block" json:"onBackpressure"`
	Description    *string                           `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeGooglePubsub `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionGooglePubsub `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGooglePubsub      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsGooglePubsub `json:"pqControls,omitempty"`
}

func (o OutputGooglePubsub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGooglePubsub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "topicName"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGooglePubsub) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGooglePubsub) GetType() CreateOutputTypeGooglePubsub {
	if o == nil {
		return CreateOutputTypeGooglePubsub("")
	}
	return o.Type
}

func (o *OutputGooglePubsub) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGooglePubsub) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGooglePubsub) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGooglePubsub) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGooglePubsub) GetTopicName() string {
	if o == nil {
		return ""
	}
	return o.TopicName
}

func (o *OutputGooglePubsub) GetCreateTopic() *bool {
	if o == nil {
		return nil
	}
	return o.CreateTopic
}

func (o *OutputGooglePubsub) GetOrderedDelivery() *bool {
	if o == nil {
		return nil
	}
	return o.OrderedDelivery
}

func (o *OutputGooglePubsub) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputGooglePubsub) GetGoogleAuthMethod() *CreateOutputGoogleAuthenticationMethodGooglePubsub {
	if o == nil {
		return nil
	}
	return o.GoogleAuthMethod
}

func (o *OutputGooglePubsub) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGooglePubsub) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputGooglePubsub) GetBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.BatchSize
}

func (o *OutputGooglePubsub) GetBatchTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.BatchTimeout
}

func (o *OutputGooglePubsub) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputGooglePubsub) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputGooglePubsub) GetFlushPeriod() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriod
}

func (o *OutputGooglePubsub) GetMaxInProgress() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxInProgress
}

func (o *OutputGooglePubsub) GetOnBackpressure() *BackpressureBehaviorGooglePubsub {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGooglePubsub) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGooglePubsub) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGooglePubsub) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGooglePubsub) GetPqMode() *CreateOutputModeGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGooglePubsub) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGooglePubsub) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGooglePubsub) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGooglePubsub) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGooglePubsub) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGooglePubsub) GetPqCompress() *PqCompressCompressionGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGooglePubsub) GetPqOnBackpressure() *QueueFullBehaviorGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGooglePubsub) GetPqControls() *CreateOutputPqControlsGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeGoogleCloudLogging string

const (
	TypeGoogleCloudLoggingGoogleCloudLogging TypeGoogleCloudLogging = "google_cloud_logging"
)

func (e TypeGoogleCloudLogging) ToPointer() *TypeGoogleCloudLogging {
	return &e
}
func (e *TypeGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_cloud_logging":
		*e = TypeGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleCloudLogging: %v", v)
	}
}

type LogLocationType string

const (
	// LogLocationTypeProject Project
	LogLocationTypeProject LogLocationType = "project"
	// LogLocationTypeOrganization Organization
	LogLocationTypeOrganization LogLocationType = "organization"
	// LogLocationTypeBillingAccount Billing Account
	LogLocationTypeBillingAccount LogLocationType = "billingAccount"
	// LogLocationTypeFolder Folder
	LogLocationTypeFolder LogLocationType = "folder"
)

func (e LogLocationType) ToPointer() *LogLocationType {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *LogLocationType) IsExact() bool {
	if e != nil {
		switch *e {
		case "project", "organization", "billingAccount", "folder":
			return true
		}
	}
	return false
}

// PayloadFormat - Format to use when sending payload. Defaults to Text.
type PayloadFormat string

const (
	// PayloadFormatText Text
	PayloadFormatText PayloadFormat = "text"
	// PayloadFormatJSON JSON
	PayloadFormatJSON PayloadFormat = "json"
)

func (e PayloadFormat) ToPointer() *PayloadFormat {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PayloadFormat) IsExact() bool {
	if e != nil {
		switch *e {
		case "text", "json":
			return true
		}
	}
	return false
}

type LogLabel struct {
	// Label name
	Label string `json:"label"`
	// JavaScript expression to compute the label's value.
	ValueExpression string `json:"valueExpression"`
}

func (l LogLabel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(l, "", false)
}

func (l *LogLabel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &l, "", false, []string{"label", "valueExpression"}); err != nil {
		return err
	}
	return nil
}

func (l *LogLabel) GetLabel() string {
	if l == nil {
		return ""
	}
	return l.Label
}

func (l *LogLabel) GetValueExpression() string {
	if l == nil {
		return ""
	}
	return l.ValueExpression
}

type ResourceTypeLabel struct {
	// Label name
	Label string `json:"label"`
	// JavaScript expression to compute the label's value.
	ValueExpression string `json:"valueExpression"`
}

func (r ResourceTypeLabel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResourceTypeLabel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"label", "valueExpression"}); err != nil {
		return err
	}
	return nil
}

func (r *ResourceTypeLabel) GetLabel() string {
	if r == nil {
		return ""
	}
	return r.Label
}

func (r *ResourceTypeLabel) GetValueExpression() string {
	if r == nil {
		return ""
	}
	return r.ValueExpression
}

// GoogleAuthenticationMethodGoogleCloudLogging - Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
type GoogleAuthenticationMethodGoogleCloudLogging string

const (
	// GoogleAuthenticationMethodGoogleCloudLoggingAuto Auto
	GoogleAuthenticationMethodGoogleCloudLoggingAuto GoogleAuthenticationMethodGoogleCloudLogging = "auto"
	// GoogleAuthenticationMethodGoogleCloudLoggingManual Manual
	GoogleAuthenticationMethodGoogleCloudLoggingManual GoogleAuthenticationMethodGoogleCloudLogging = "manual"
	// GoogleAuthenticationMethodGoogleCloudLoggingSecret Secret
	GoogleAuthenticationMethodGoogleCloudLoggingSecret GoogleAuthenticationMethodGoogleCloudLogging = "secret"
)

func (e GoogleAuthenticationMethodGoogleCloudLogging) ToPointer() *GoogleAuthenticationMethodGoogleCloudLogging {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *GoogleAuthenticationMethodGoogleCloudLogging) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// BackpressureBehaviorGoogleCloudLogging - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGoogleCloudLogging string

const (
	// BackpressureBehaviorGoogleCloudLoggingBlock Block
	BackpressureBehaviorGoogleCloudLoggingBlock BackpressureBehaviorGoogleCloudLogging = "block"
	// BackpressureBehaviorGoogleCloudLoggingDrop Drop
	BackpressureBehaviorGoogleCloudLoggingDrop BackpressureBehaviorGoogleCloudLogging = "drop"
	// BackpressureBehaviorGoogleCloudLoggingQueue Persistent Queue
	BackpressureBehaviorGoogleCloudLoggingQueue BackpressureBehaviorGoogleCloudLogging = "queue"
)

func (e BackpressureBehaviorGoogleCloudLogging) ToPointer() *BackpressureBehaviorGoogleCloudLogging {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorGoogleCloudLogging) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeGoogleCloudLogging - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeGoogleCloudLogging string

const (
	// ModeGoogleCloudLoggingError Error
	ModeGoogleCloudLoggingError ModeGoogleCloudLogging = "error"
	// ModeGoogleCloudLoggingAlways Backpressure
	ModeGoogleCloudLoggingAlways ModeGoogleCloudLogging = "always"
	// ModeGoogleCloudLoggingBackpressure Always On
	ModeGoogleCloudLoggingBackpressure ModeGoogleCloudLogging = "backpressure"
)

func (e ModeGoogleCloudLogging) ToPointer() *ModeGoogleCloudLogging {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeGoogleCloudLogging) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionGoogleCloudLogging - Codec to use to compress the persisted data
type CompressionGoogleCloudLogging string

const (
	// CompressionGoogleCloudLoggingNone None
	CompressionGoogleCloudLoggingNone CompressionGoogleCloudLogging = "none"
	// CompressionGoogleCloudLoggingGzip Gzip
	CompressionGoogleCloudLoggingGzip CompressionGoogleCloudLogging = "gzip"
)

func (e CompressionGoogleCloudLogging) ToPointer() *CompressionGoogleCloudLogging {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionGoogleCloudLogging) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorGoogleCloudLogging - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGoogleCloudLogging string

const (
	// QueueFullBehaviorGoogleCloudLoggingBlock Block
	QueueFullBehaviorGoogleCloudLoggingBlock QueueFullBehaviorGoogleCloudLogging = "block"
	// QueueFullBehaviorGoogleCloudLoggingDrop Drop new data
	QueueFullBehaviorGoogleCloudLoggingDrop QueueFullBehaviorGoogleCloudLogging = "drop"
)

func (e QueueFullBehaviorGoogleCloudLogging) ToPointer() *QueueFullBehaviorGoogleCloudLogging {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorGoogleCloudLogging) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsGoogleCloudLogging struct {
}

func (p PqControlsGoogleCloudLogging) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGoogleCloudLogging struct {
	// Unique ID for this output
	ID   string                 `json:"id"`
	Type TypeGoogleCloudLogging `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags      []string        `json:"streamtags,omitempty"`
	LogLocationType LogLocationType `json:"logLocationType"`
	// JavaScript expression to compute the value of the log name. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.
	LogNameExpression string `json:"logNameExpression"`
	SanitizeLogNames  *bool  `default:"false" json:"sanitizeLogNames"`
	// Format to use when sending payload. Defaults to Text.
	PayloadFormat *PayloadFormat `default:"text" json:"payloadFormat"`
	// Labels to apply to the log entry
	LogLabels []LogLabel `json:"logLabels,omitempty"`
	// JavaScript expression to compute the value of the managed resource type field. Must evaluate to one of the valid values [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types). Defaults to "global".
	ResourceTypeExpression *string `json:"resourceTypeExpression,omitempty"`
	// Labels to apply to the managed resource. These must correspond to the valid labels for the specified resource type (see [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types)). Otherwise, they will be dropped by Google Cloud Logging.
	ResourceTypeLabels []ResourceTypeLabel `json:"resourceTypeLabels,omitempty"`
	// JavaScript expression to compute the value of the severity field. Must evaluate to one of the severity values supported by Google Cloud Logging [here](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) (case insensitive). Defaults to "DEFAULT".
	SeverityExpression *string `json:"severityExpression,omitempty"`
	// JavaScript expression to compute the value of the insert ID field.
	InsertIDExpression *string `json:"insertIdExpression,omitempty"`
	// Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
	GoogleAuthMethod *GoogleAuthenticationMethodGoogleCloudLogging `default:"manual" json:"googleAuthMethod"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	Secret *string `json:"secret,omitempty"`
	// Maximum size, in KB, of the request body.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Max number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Maximum number of ongoing requests before blocking.
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it.
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum number of requests to limit to per second.
	ThrottleRateReqPerSec *int64 `json:"throttleRateReqPerSec,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request method as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestMethodExpression *string `json:"requestMethodExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request URL as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestURLExpression *string `json:"requestUrlExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestSizeExpression *string `json:"requestSizeExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request method as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	StatusExpression *string `json:"statusExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP response size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ResponseSizeExpression *string `json:"responseSizeExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request user agent as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	UserAgentExpression *string `json:"userAgentExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request remote IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RemoteIPExpression *string `json:"remoteIpExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request server IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ServerIPExpression *string `json:"serverIpExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request referer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RefererExpression *string `json:"refererExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request latency, formatted as <seconds>.<nanoseconds>s (for example, 1.23s). See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	LatencyExpression *string `json:"latencyExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache lookup as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheLookupExpression *string `json:"cacheLookupExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache hit as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheHitExpression *string `json:"cacheHitExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache validated with origin server as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheValidatedExpression *string `json:"cacheValidatedExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache fill bytes as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheFillBytesExpression *string `json:"cacheFillBytesExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request protocol as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ProtocolExpression *string `json:"protocolExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation ID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	IDExpression *string `json:"idExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation producer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	ProducerExpression *string `json:"producerExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation first flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	FirstExpression *string `json:"firstExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation last flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	LastExpression *string `json:"lastExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location file as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	FileExpression *string `json:"fileExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location line as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	LineExpression *string `json:"lineExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location function as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	FunctionExpression *string `json:"functionExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split UID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	UIDExpression *string `json:"uidExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split index as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	IndexExpression *string `json:"indexExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split total splits as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	TotalSplitsExpression *string `json:"totalSplitsExpression,omitempty"`
	// A JavaScript expression that evaluates to the REST resource name of the trace being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	TraceExpression *string `json:"traceExpression,omitempty"`
	// A JavaScript expression that evaluates to the ID of the cloud trace span associated with the current operation in which the log is being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	SpanIDExpression *string `json:"spanIdExpression,omitempty"`
	// A JavaScript expression that evaluates to the the sampling decision of the span associated with the log entry. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	TraceSampledExpression *string `json:"traceSampledExpression,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGoogleCloudLogging `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// JavaScript expression to compute the value of the folder ID with which log entries should be associated. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.
	LogLocationExpression string `json:"logLocationExpression"`
	// JavaScript expression to compute the value of the payload. Must evaluate to a JavaScript object value. If an invalid value is encountered it will result in the default value instead. Defaults to the entire event.
	PayloadExpression *string `json:"payloadExpression,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeGoogleCloudLogging `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionGoogleCloudLogging `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGoogleCloudLogging `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsGoogleCloudLogging        `json:"pqControls,omitempty"`
}

func (o OutputGoogleCloudLogging) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "logLocationType", "logNameExpression", "logLocationExpression"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleCloudLogging) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGoogleCloudLogging) GetType() TypeGoogleCloudLogging {
	if o == nil {
		return TypeGoogleCloudLogging("")
	}
	return o.Type
}

func (o *OutputGoogleCloudLogging) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleCloudLogging) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleCloudLogging) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleCloudLogging) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleCloudLogging) GetLogLocationType() LogLocationType {
	if o == nil {
		return LogLocationType("")
	}
	return o.LogLocationType
}

func (o *OutputGoogleCloudLogging) GetLogNameExpression() string {
	if o == nil {
		return ""
	}
	return o.LogNameExpression
}

func (o *OutputGoogleCloudLogging) GetSanitizeLogNames() *bool {
	if o == nil {
		return nil
	}
	return o.SanitizeLogNames
}

func (o *OutputGoogleCloudLogging) GetPayloadFormat() *PayloadFormat {
	if o == nil {
		return nil
	}
	return o.PayloadFormat
}

func (o *OutputGoogleCloudLogging) GetLogLabels() []LogLabel {
	if o == nil {
		return nil
	}
	return o.LogLabels
}

func (o *OutputGoogleCloudLogging) GetResourceTypeExpression() *string {
	if o == nil {
		return nil
	}
	return o.ResourceTypeExpression
}

func (o *OutputGoogleCloudLogging) GetResourceTypeLabels() []ResourceTypeLabel {
	if o == nil {
		return nil
	}
	return o.ResourceTypeLabels
}

func (o *OutputGoogleCloudLogging) GetSeverityExpression() *string {
	if o == nil {
		return nil
	}
	return o.SeverityExpression
}

func (o *OutputGoogleCloudLogging) GetInsertIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.InsertIDExpression
}

func (o *OutputGoogleCloudLogging) GetGoogleAuthMethod() *GoogleAuthenticationMethodGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.GoogleAuthMethod
}

func (o *OutputGoogleCloudLogging) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGoogleCloudLogging) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputGoogleCloudLogging) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGoogleCloudLogging) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGoogleCloudLogging) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGoogleCloudLogging) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGoogleCloudLogging) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputGoogleCloudLogging) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGoogleCloudLogging) GetThrottleRateReqPerSec() *int64 {
	if o == nil {
		return nil
	}
	return o.ThrottleRateReqPerSec
}

func (o *OutputGoogleCloudLogging) GetRequestMethodExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestMethodExpression
}

func (o *OutputGoogleCloudLogging) GetRequestURLExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestURLExpression
}

func (o *OutputGoogleCloudLogging) GetRequestSizeExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestSizeExpression
}

func (o *OutputGoogleCloudLogging) GetStatusExpression() *string {
	if o == nil {
		return nil
	}
	return o.StatusExpression
}

func (o *OutputGoogleCloudLogging) GetResponseSizeExpression() *string {
	if o == nil {
		return nil
	}
	return o.ResponseSizeExpression
}

func (o *OutputGoogleCloudLogging) GetUserAgentExpression() *string {
	if o == nil {
		return nil
	}
	return o.UserAgentExpression
}

func (o *OutputGoogleCloudLogging) GetRemoteIPExpression() *string {
	if o == nil {
		return nil
	}
	return o.RemoteIPExpression
}

func (o *OutputGoogleCloudLogging) GetServerIPExpression() *string {
	if o == nil {
		return nil
	}
	return o.ServerIPExpression
}

func (o *OutputGoogleCloudLogging) GetRefererExpression() *string {
	if o == nil {
		return nil
	}
	return o.RefererExpression
}

func (o *OutputGoogleCloudLogging) GetLatencyExpression() *string {
	if o == nil {
		return nil
	}
	return o.LatencyExpression
}

func (o *OutputGoogleCloudLogging) GetCacheLookupExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheLookupExpression
}

func (o *OutputGoogleCloudLogging) GetCacheHitExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheHitExpression
}

func (o *OutputGoogleCloudLogging) GetCacheValidatedExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheValidatedExpression
}

func (o *OutputGoogleCloudLogging) GetCacheFillBytesExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheFillBytesExpression
}

func (o *OutputGoogleCloudLogging) GetProtocolExpression() *string {
	if o == nil {
		return nil
	}
	return o.ProtocolExpression
}

func (o *OutputGoogleCloudLogging) GetIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.IDExpression
}

func (o *OutputGoogleCloudLogging) GetProducerExpression() *string {
	if o == nil {
		return nil
	}
	return o.ProducerExpression
}

func (o *OutputGoogleCloudLogging) GetFirstExpression() *string {
	if o == nil {
		return nil
	}
	return o.FirstExpression
}

func (o *OutputGoogleCloudLogging) GetLastExpression() *string {
	if o == nil {
		return nil
	}
	return o.LastExpression
}

func (o *OutputGoogleCloudLogging) GetFileExpression() *string {
	if o == nil {
		return nil
	}
	return o.FileExpression
}

func (o *OutputGoogleCloudLogging) GetLineExpression() *string {
	if o == nil {
		return nil
	}
	return o.LineExpression
}

func (o *OutputGoogleCloudLogging) GetFunctionExpression() *string {
	if o == nil {
		return nil
	}
	return o.FunctionExpression
}

func (o *OutputGoogleCloudLogging) GetUIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.UIDExpression
}

func (o *OutputGoogleCloudLogging) GetIndexExpression() *string {
	if o == nil {
		return nil
	}
	return o.IndexExpression
}

func (o *OutputGoogleCloudLogging) GetTotalSplitsExpression() *string {
	if o == nil {
		return nil
	}
	return o.TotalSplitsExpression
}

func (o *OutputGoogleCloudLogging) GetTraceExpression() *string {
	if o == nil {
		return nil
	}
	return o.TraceExpression
}

func (o *OutputGoogleCloudLogging) GetSpanIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.SpanIDExpression
}

func (o *OutputGoogleCloudLogging) GetTraceSampledExpression() *string {
	if o == nil {
		return nil
	}
	return o.TraceSampledExpression
}

func (o *OutputGoogleCloudLogging) GetOnBackpressure() *BackpressureBehaviorGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleCloudLogging) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputGoogleCloudLogging) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleCloudLogging) GetLogLocationExpression() string {
	if o == nil {
		return ""
	}
	return o.LogLocationExpression
}

func (o *OutputGoogleCloudLogging) GetPayloadExpression() *string {
	if o == nil {
		return nil
	}
	return o.PayloadExpression
}

func (o *OutputGoogleCloudLogging) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGoogleCloudLogging) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGoogleCloudLogging) GetPqMode() *ModeGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGoogleCloudLogging) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGoogleCloudLogging) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGoogleCloudLogging) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGoogleCloudLogging) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGoogleCloudLogging) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGoogleCloudLogging) GetPqCompress() *CompressionGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGoogleCloudLogging) GetPqOnBackpressure() *QueueFullBehaviorGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGoogleCloudLogging) GetPqControls() *PqControlsGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeGoogleCloudStorage string

const (
	TypeGoogleCloudStorageGoogleCloudStorage TypeGoogleCloudStorage = "google_cloud_storage"
)

func (e TypeGoogleCloudStorage) ToPointer() *TypeGoogleCloudStorage {
	return &e
}
func (e *TypeGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_cloud_storage":
		*e = TypeGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleCloudStorage: %v", v)
	}
}

// SignatureVersionGoogleCloudStorage - Signature version to use for signing Google Cloud Storage requests
type SignatureVersionGoogleCloudStorage string

const (
	SignatureVersionGoogleCloudStorageV2 SignatureVersionGoogleCloudStorage = "v2"
	SignatureVersionGoogleCloudStorageV4 SignatureVersionGoogleCloudStorage = "v4"
)

func (e SignatureVersionGoogleCloudStorage) ToPointer() *SignatureVersionGoogleCloudStorage {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SignatureVersionGoogleCloudStorage) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

type AuthenticationMethodGoogleCloudStorage string

const (
	// AuthenticationMethodGoogleCloudStorageAuto auto
	AuthenticationMethodGoogleCloudStorageAuto AuthenticationMethodGoogleCloudStorage = "auto"
	// AuthenticationMethodGoogleCloudStorageManual manual
	AuthenticationMethodGoogleCloudStorageManual AuthenticationMethodGoogleCloudStorage = "manual"
	// AuthenticationMethodGoogleCloudStorageSecret Secret Key pair
	AuthenticationMethodGoogleCloudStorageSecret AuthenticationMethodGoogleCloudStorage = "secret"
)

func (e AuthenticationMethodGoogleCloudStorage) ToPointer() *AuthenticationMethodGoogleCloudStorage {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodGoogleCloudStorage) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// ObjectACLGoogleCloudStorage - Object ACL to assign to uploaded objects
type ObjectACLGoogleCloudStorage string

const (
	// ObjectACLGoogleCloudStoragePrivate private
	ObjectACLGoogleCloudStoragePrivate ObjectACLGoogleCloudStorage = "private"
	// ObjectACLGoogleCloudStorageBucketOwnerRead bucket-owner-read
	ObjectACLGoogleCloudStorageBucketOwnerRead ObjectACLGoogleCloudStorage = "bucket-owner-read"
	// ObjectACLGoogleCloudStorageBucketOwnerFullControl bucket-owner-full-control
	ObjectACLGoogleCloudStorageBucketOwnerFullControl ObjectACLGoogleCloudStorage = "bucket-owner-full-control"
	// ObjectACLGoogleCloudStorageProjectPrivate project-private
	ObjectACLGoogleCloudStorageProjectPrivate ObjectACLGoogleCloudStorage = "project-private"
	// ObjectACLGoogleCloudStorageAuthenticatedRead authenticated-read
	ObjectACLGoogleCloudStorageAuthenticatedRead ObjectACLGoogleCloudStorage = "authenticated-read"
	// ObjectACLGoogleCloudStoragePublicRead public-read
	ObjectACLGoogleCloudStoragePublicRead ObjectACLGoogleCloudStorage = "public-read"
)

func (e ObjectACLGoogleCloudStorage) ToPointer() *ObjectACLGoogleCloudStorage {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ObjectACLGoogleCloudStorage) IsExact() bool {
	if e != nil {
		switch *e {
		case "private", "bucket-owner-read", "bucket-owner-full-control", "project-private", "authenticated-read", "public-read":
			return true
		}
	}
	return false
}

// StorageClassGoogleCloudStorage - Storage class to select for uploaded objects
type StorageClassGoogleCloudStorage string

const (
	// StorageClassGoogleCloudStorageStandard Standard Storage
	StorageClassGoogleCloudStorageStandard StorageClassGoogleCloudStorage = "STANDARD"
	// StorageClassGoogleCloudStorageNearline Nearline Storage
	StorageClassGoogleCloudStorageNearline StorageClassGoogleCloudStorage = "NEARLINE"
	// StorageClassGoogleCloudStorageColdline Coldline Storage
	StorageClassGoogleCloudStorageColdline StorageClassGoogleCloudStorage = "COLDLINE"
	// StorageClassGoogleCloudStorageArchive Archive Storage
	StorageClassGoogleCloudStorageArchive StorageClassGoogleCloudStorage = "ARCHIVE"
)

func (e StorageClassGoogleCloudStorage) ToPointer() *StorageClassGoogleCloudStorage {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *StorageClassGoogleCloudStorage) IsExact() bool {
	if e != nil {
		switch *e {
		case "STANDARD", "NEARLINE", "COLDLINE", "ARCHIVE":
			return true
		}
	}
	return false
}

// DataFormatGoogleCloudStorage - Format of the output data
type DataFormatGoogleCloudStorage string

const (
	// DataFormatGoogleCloudStorageJSON JSON
	DataFormatGoogleCloudStorageJSON DataFormatGoogleCloudStorage = "json"
	// DataFormatGoogleCloudStorageRaw Raw
	DataFormatGoogleCloudStorageRaw DataFormatGoogleCloudStorage = "raw"
	// DataFormatGoogleCloudStorageParquet Parquet
	DataFormatGoogleCloudStorageParquet DataFormatGoogleCloudStorage = "parquet"
)

func (e DataFormatGoogleCloudStorage) ToPointer() *DataFormatGoogleCloudStorage {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataFormatGoogleCloudStorage) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "parquet":
			return true
		}
	}
	return false
}

// BackpressureBehaviorGoogleCloudStorage - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGoogleCloudStorage string

const (
	// BackpressureBehaviorGoogleCloudStorageBlock Block
	BackpressureBehaviorGoogleCloudStorageBlock BackpressureBehaviorGoogleCloudStorage = "block"
	// BackpressureBehaviorGoogleCloudStorageDrop Drop
	BackpressureBehaviorGoogleCloudStorageDrop BackpressureBehaviorGoogleCloudStorage = "drop"
)

func (e BackpressureBehaviorGoogleCloudStorage) ToPointer() *BackpressureBehaviorGoogleCloudStorage {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorGoogleCloudStorage) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionGoogleCloudStorage - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionGoogleCloudStorage string

const (
	// DiskSpaceProtectionGoogleCloudStorageBlock Block
	DiskSpaceProtectionGoogleCloudStorageBlock DiskSpaceProtectionGoogleCloudStorage = "block"
	// DiskSpaceProtectionGoogleCloudStorageDrop Drop
	DiskSpaceProtectionGoogleCloudStorageDrop DiskSpaceProtectionGoogleCloudStorage = "drop"
)

func (e DiskSpaceProtectionGoogleCloudStorage) ToPointer() *DiskSpaceProtectionGoogleCloudStorage {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionGoogleCloudStorage) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// CompressionGoogleCloudStorage - Data compression format to apply to HTTP content before it is delivered
type CompressionGoogleCloudStorage string

const (
	CompressionGoogleCloudStorageNone CompressionGoogleCloudStorage = "none"
	CompressionGoogleCloudStorageGzip CompressionGoogleCloudStorage = "gzip"
)

func (e CompressionGoogleCloudStorage) ToPointer() *CompressionGoogleCloudStorage {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionGoogleCloudStorage) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// CompressionLevelGoogleCloudStorage - Compression level to apply before moving files to final destination
type CompressionLevelGoogleCloudStorage string

const (
	// CompressionLevelGoogleCloudStorageBestSpeed Best Speed
	CompressionLevelGoogleCloudStorageBestSpeed CompressionLevelGoogleCloudStorage = "best_speed"
	// CompressionLevelGoogleCloudStorageNormal Normal
	CompressionLevelGoogleCloudStorageNormal CompressionLevelGoogleCloudStorage = "normal"
	// CompressionLevelGoogleCloudStorageBestCompression Best Compression
	CompressionLevelGoogleCloudStorageBestCompression CompressionLevelGoogleCloudStorage = "best_compression"
)

func (e CompressionLevelGoogleCloudStorage) ToPointer() *CompressionLevelGoogleCloudStorage {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionLevelGoogleCloudStorage) IsExact() bool {
	if e != nil {
		switch *e {
		case "best_speed", "normal", "best_compression":
			return true
		}
	}
	return false
}

// ParquetVersionGoogleCloudStorage - Determines which data types are supported and how they are represented
type ParquetVersionGoogleCloudStorage string

const (
	// ParquetVersionGoogleCloudStorageParquet10 1.0
	ParquetVersionGoogleCloudStorageParquet10 ParquetVersionGoogleCloudStorage = "PARQUET_1_0"
	// ParquetVersionGoogleCloudStorageParquet24 2.4
	ParquetVersionGoogleCloudStorageParquet24 ParquetVersionGoogleCloudStorage = "PARQUET_2_4"
	// ParquetVersionGoogleCloudStorageParquet26 2.6
	ParquetVersionGoogleCloudStorageParquet26 ParquetVersionGoogleCloudStorage = "PARQUET_2_6"
)

func (e ParquetVersionGoogleCloudStorage) ToPointer() *ParquetVersionGoogleCloudStorage {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ParquetVersionGoogleCloudStorage) IsExact() bool {
	if e != nil {
		switch *e {
		case "PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6":
			return true
		}
	}
	return false
}

// DataPageVersionGoogleCloudStorage - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionGoogleCloudStorage string

const (
	// DataPageVersionGoogleCloudStorageDataPageV1 V1
	DataPageVersionGoogleCloudStorageDataPageV1 DataPageVersionGoogleCloudStorage = "DATA_PAGE_V1"
	// DataPageVersionGoogleCloudStorageDataPageV2 V2
	DataPageVersionGoogleCloudStorageDataPageV2 DataPageVersionGoogleCloudStorage = "DATA_PAGE_V2"
)

func (e DataPageVersionGoogleCloudStorage) ToPointer() *DataPageVersionGoogleCloudStorage {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataPageVersionGoogleCloudStorage) IsExact() bool {
	if e != nil {
		switch *e {
		case "DATA_PAGE_V1", "DATA_PAGE_V2":
			return true
		}
	}
	return false
}

type KeyValueMetadatumGoogleCloudStorage struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumGoogleCloudStorage) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumGoogleCloudStorage) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumGoogleCloudStorage) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputGoogleCloudStorage struct {
	// Unique ID for this output
	ID   string                 `json:"id"`
	Type TypeGoogleCloudStorage `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// Region where the bucket is located
	Region string `json:"region"`
	// Google Cloud Storage service endpoint
	Endpoint *string `default:"https://storage.googleapis.com" json:"endpoint"`
	// Signature version to use for signing Google Cloud Storage requests
	SignatureVersion        *SignatureVersionGoogleCloudStorage     `default:"v4" json:"signatureVersion"`
	AwsAuthenticationMethod *AuthenticationMethodGoogleCloudStorage `default:"manual" json:"awsAuthenticationMethod"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`
	DestPath *string `default:"" json:"destPath"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLGoogleCloudStorage `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass *StorageClassGoogleCloudStorage `json:"storageClass,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatGoogleCloudStorage `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGoogleCloudStorage `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionGoogleCloudStorage `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool   `default:"false" json:"forceCloseOnShutdown"`
	Description          *string `json:"description,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionGoogleCloudStorage `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelGoogleCloudStorage `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionGoogleCloudStorage `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionGoogleCloudStorage `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumGoogleCloudStorage `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
	// HMAC access key. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// HMAC secret. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
}

func (o OutputGoogleCloudStorage) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "bucket", "region"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleCloudStorage) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGoogleCloudStorage) GetType() TypeGoogleCloudStorage {
	if o == nil {
		return TypeGoogleCloudStorage("")
	}
	return o.Type
}

func (o *OutputGoogleCloudStorage) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleCloudStorage) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleCloudStorage) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleCloudStorage) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleCloudStorage) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputGoogleCloudStorage) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputGoogleCloudStorage) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputGoogleCloudStorage) GetSignatureVersion() *SignatureVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputGoogleCloudStorage) GetAwsAuthenticationMethod() *AuthenticationMethodGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputGoogleCloudStorage) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputGoogleCloudStorage) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputGoogleCloudStorage) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputGoogleCloudStorage) GetObjectACL() *ObjectACLGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputGoogleCloudStorage) GetStorageClass() *StorageClassGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputGoogleCloudStorage) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputGoogleCloudStorage) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGoogleCloudStorage) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputGoogleCloudStorage) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputGoogleCloudStorage) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputGoogleCloudStorage) GetFormat() *DataFormatGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputGoogleCloudStorage) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputGoogleCloudStorage) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputGoogleCloudStorage) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputGoogleCloudStorage) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputGoogleCloudStorage) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputGoogleCloudStorage) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputGoogleCloudStorage) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputGoogleCloudStorage) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputGoogleCloudStorage) GetOnBackpressure() *BackpressureBehaviorGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleCloudStorage) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputGoogleCloudStorage) GetOnDiskFullBackpressure() *DiskSpaceProtectionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputGoogleCloudStorage) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputGoogleCloudStorage) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleCloudStorage) GetCompress() *CompressionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGoogleCloudStorage) GetCompressionLevel() *CompressionLevelGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputGoogleCloudStorage) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputGoogleCloudStorage) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputGoogleCloudStorage) GetParquetVersion() *ParquetVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputGoogleCloudStorage) GetParquetDataPageVersion() *DataPageVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputGoogleCloudStorage) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputGoogleCloudStorage) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputGoogleCloudStorage) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputGoogleCloudStorage) GetKeyValueMetadata() []KeyValueMetadatumGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputGoogleCloudStorage) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputGoogleCloudStorage) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputGoogleCloudStorage) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputGoogleCloudStorage) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputGoogleCloudStorage) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputGoogleCloudStorage) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputGoogleCloudStorage) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputGoogleCloudStorage) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputGoogleCloudStorage) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputGoogleCloudStorage) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

type TypeGoogleChronicle string

const (
	TypeGoogleChronicleGoogleChronicle TypeGoogleChronicle = "google_chronicle"
)

func (e TypeGoogleChronicle) ToPointer() *TypeGoogleChronicle {
	return &e
}
func (e *TypeGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_chronicle":
		*e = TypeGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleChronicle: %v", v)
	}
}

type CreateOutputAPIVersion string

const (
	// CreateOutputAPIVersionV1 V1
	CreateOutputAPIVersionV1 CreateOutputAPIVersion = "v1"
	// CreateOutputAPIVersionV2 V2
	CreateOutputAPIVersionV2 CreateOutputAPIVersion = "v2"
)

func (e CreateOutputAPIVersion) ToPointer() *CreateOutputAPIVersion {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAPIVersion) IsExact() bool {
	if e != nil {
		switch *e {
		case "v1", "v2":
			return true
		}
	}
	return false
}

type AuthenticationMethodGoogleChronicle string

const (
	// AuthenticationMethodGoogleChronicleManual API key
	AuthenticationMethodGoogleChronicleManual AuthenticationMethodGoogleChronicle = "manual"
	// AuthenticationMethodGoogleChronicleSecret API key secret
	AuthenticationMethodGoogleChronicleSecret AuthenticationMethodGoogleChronicle = "secret"
	// AuthenticationMethodGoogleChronicleServiceAccount Service account credentials
	AuthenticationMethodGoogleChronicleServiceAccount AuthenticationMethodGoogleChronicle = "serviceAccount"
	// AuthenticationMethodGoogleChronicleServiceAccountSecret Service account credentials secret
	AuthenticationMethodGoogleChronicleServiceAccountSecret AuthenticationMethodGoogleChronicle = "serviceAccountSecret"
)

func (e AuthenticationMethodGoogleChronicle) ToPointer() *AuthenticationMethodGoogleChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodGoogleChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret", "serviceAccount", "serviceAccountSecret":
			return true
		}
	}
	return false
}

type ResponseRetrySettingGoogleChronicle struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingGoogleChronicle) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingGoogleChronicle) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingGoogleChronicle) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingGoogleChronicle) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsGoogleChronicle struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsGoogleChronicle) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsGoogleChronicle) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsGoogleChronicle) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsGoogleChronicle) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type SendEventsAs string

const (
	// SendEventsAsUnstructured Unstructured
	SendEventsAsUnstructured SendEventsAs = "unstructured"
	// SendEventsAsUdm UDM
	SendEventsAsUdm SendEventsAs = "udm"
)

func (e SendEventsAs) ToPointer() *SendEventsAs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SendEventsAs) IsExact() bool {
	if e != nil {
		switch *e {
		case "unstructured", "udm":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderGoogleChronicle struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderGoogleChronicle) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderGoogleChronicle) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeGoogleChronicle - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeGoogleChronicle string

const (
	// FailedRequestLoggingModeGoogleChroniclePayload Payload
	FailedRequestLoggingModeGoogleChroniclePayload FailedRequestLoggingModeGoogleChronicle = "payload"
	// FailedRequestLoggingModeGoogleChroniclePayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeGoogleChroniclePayloadAndHeaders FailedRequestLoggingModeGoogleChronicle = "payloadAndHeaders"
	// FailedRequestLoggingModeGoogleChronicleNone None
	FailedRequestLoggingModeGoogleChronicleNone FailedRequestLoggingModeGoogleChronicle = "none"
)

func (e FailedRequestLoggingModeGoogleChronicle) ToPointer() *FailedRequestLoggingModeGoogleChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeGoogleChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

// BackpressureBehaviorGoogleChronicle - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGoogleChronicle string

const (
	// BackpressureBehaviorGoogleChronicleBlock Block
	BackpressureBehaviorGoogleChronicleBlock BackpressureBehaviorGoogleChronicle = "block"
	// BackpressureBehaviorGoogleChronicleDrop Drop
	BackpressureBehaviorGoogleChronicleDrop BackpressureBehaviorGoogleChronicle = "drop"
	// BackpressureBehaviorGoogleChronicleQueue Persistent Queue
	BackpressureBehaviorGoogleChronicleQueue BackpressureBehaviorGoogleChronicle = "queue"
)

func (e BackpressureBehaviorGoogleChronicle) ToPointer() *BackpressureBehaviorGoogleChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorGoogleChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type ExtraLogType struct {
	LogType     string  `json:"logType"`
	Description *string `json:"description,omitempty"`
}

func (e ExtraLogType) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraLogType) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"logType"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraLogType) GetLogType() string {
	if e == nil {
		return ""
	}
	return e.LogType
}

func (e *ExtraLogType) GetDescription() *string {
	if e == nil {
		return nil
	}
	return e.Description
}

type CustomLabelGoogleChronicle struct {
	Key   string `json:"key"`
	Value string `json:"value"`
}

func (c CustomLabelGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CustomLabelGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"key", "value"}); err != nil {
		return err
	}
	return nil
}

func (c *CustomLabelGoogleChronicle) GetKey() string {
	if c == nil {
		return ""
	}
	return c.Key
}

func (c *CustomLabelGoogleChronicle) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

// UDMType - Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent.
type UDMType string

const (
	UDMTypeEntities UDMType = "entities"
	UDMTypeLogs     UDMType = "logs"
)

func (e UDMType) ToPointer() *UDMType {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *UDMType) IsExact() bool {
	if e != nil {
		switch *e {
		case "entities", "logs":
			return true
		}
	}
	return false
}

// ModeGoogleChronicle - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeGoogleChronicle string

const (
	// ModeGoogleChronicleError Error
	ModeGoogleChronicleError ModeGoogleChronicle = "error"
	// ModeGoogleChronicleAlways Backpressure
	ModeGoogleChronicleAlways ModeGoogleChronicle = "always"
	// ModeGoogleChronicleBackpressure Always On
	ModeGoogleChronicleBackpressure ModeGoogleChronicle = "backpressure"
)

func (e ModeGoogleChronicle) ToPointer() *ModeGoogleChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeGoogleChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionGoogleChronicle - Codec to use to compress the persisted data
type CompressionGoogleChronicle string

const (
	// CompressionGoogleChronicleNone None
	CompressionGoogleChronicleNone CompressionGoogleChronicle = "none"
	// CompressionGoogleChronicleGzip Gzip
	CompressionGoogleChronicleGzip CompressionGoogleChronicle = "gzip"
)

func (e CompressionGoogleChronicle) ToPointer() *CompressionGoogleChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionGoogleChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorGoogleChronicle - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGoogleChronicle string

const (
	// QueueFullBehaviorGoogleChronicleBlock Block
	QueueFullBehaviorGoogleChronicleBlock QueueFullBehaviorGoogleChronicle = "block"
	// QueueFullBehaviorGoogleChronicleDrop Drop new data
	QueueFullBehaviorGoogleChronicleDrop QueueFullBehaviorGoogleChronicle = "drop"
)

func (e QueueFullBehaviorGoogleChronicle) ToPointer() *QueueFullBehaviorGoogleChronicle {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorGoogleChronicle) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsGoogleChronicle struct {
}

func (p PqControlsGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGoogleChronicle struct {
	// Unique ID for this output
	ID   string              `json:"id"`
	Type TypeGoogleChronicle `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags           []string                             `json:"streamtags,omitempty"`
	APIVersion           *CreateOutputAPIVersion              `default:"v1" json:"apiVersion"`
	AuthenticationMethod *AuthenticationMethodGoogleChronicle `default:"serviceAccount" json:"authenticationMethod"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingGoogleChronicle `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsGoogleChronicle  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool         `default:"false" json:"responseHonorRetryAfterHeader"`
	LogFormatType                 *SendEventsAs `default:"unstructured" json:"logFormatType"`
	// Regional endpoint to send events to
	Region *string `json:"region,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"90" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderGoogleChronicle `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeGoogleChronicle `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGoogleChronicle `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Custom log types. If the value "Custom" is selected in the setting "Default log type" above, the first custom log type in this table will be automatically selected as default log type.
	ExtraLogTypes []ExtraLogType `json:"extraLogTypes,omitempty"`
	// Default log type value to send to SecOps. Can be overwritten by event field __logType.
	LogType *string `json:"logType,omitempty"`
	// Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.
	LogTextField *string `json:"logTextField,omitempty"`
	// A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication.
	CustomerID *string `json:"customerId,omitempty"`
	// User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.
	Namespace *string `json:"namespace,omitempty"`
	// Custom labels to be added to every batch
	CustomLabels []CustomLabelGoogleChronicle `json:"customLabels,omitempty"`
	// Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent.
	UdmType *UDMType `default:"logs" json:"udmType"`
	// Organization's API key in Google SecOps
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	APIKeySecret *string `json:"apiKeySecret,omitempty"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	ServiceAccountCredentialsSecret *string `json:"serviceAccountCredentialsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeGoogleChronicle `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionGoogleChronicle `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGoogleChronicle `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsGoogleChronicle        `json:"pqControls,omitempty"`
}

func (o OutputGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleChronicle) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGoogleChronicle) GetType() TypeGoogleChronicle {
	if o == nil {
		return TypeGoogleChronicle("")
	}
	return o.Type
}

func (o *OutputGoogleChronicle) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleChronicle) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleChronicle) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleChronicle) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleChronicle) GetAPIVersion() *CreateOutputAPIVersion {
	if o == nil {
		return nil
	}
	return o.APIVersion
}

func (o *OutputGoogleChronicle) GetAuthenticationMethod() *AuthenticationMethodGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.AuthenticationMethod
}

func (o *OutputGoogleChronicle) GetResponseRetrySettings() []ResponseRetrySettingGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputGoogleChronicle) GetTimeoutRetrySettings() *TimeoutRetrySettingsGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputGoogleChronicle) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputGoogleChronicle) GetLogFormatType() *SendEventsAs {
	if o == nil {
		return nil
	}
	return o.LogFormatType
}

func (o *OutputGoogleChronicle) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputGoogleChronicle) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGoogleChronicle) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGoogleChronicle) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGoogleChronicle) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGoogleChronicle) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGoogleChronicle) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGoogleChronicle) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGoogleChronicle) GetExtraHTTPHeaders() []ExtraHTTPHeaderGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputGoogleChronicle) GetFailedRequestLoggingMode() *FailedRequestLoggingModeGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputGoogleChronicle) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputGoogleChronicle) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputGoogleChronicle) GetOnBackpressure() *BackpressureBehaviorGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleChronicle) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputGoogleChronicle) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleChronicle) GetExtraLogTypes() []ExtraLogType {
	if o == nil {
		return nil
	}
	return o.ExtraLogTypes
}

func (o *OutputGoogleChronicle) GetLogType() *string {
	if o == nil {
		return nil
	}
	return o.LogType
}

func (o *OutputGoogleChronicle) GetLogTextField() *string {
	if o == nil {
		return nil
	}
	return o.LogTextField
}

func (o *OutputGoogleChronicle) GetCustomerID() *string {
	if o == nil {
		return nil
	}
	return o.CustomerID
}

func (o *OutputGoogleChronicle) GetNamespace() *string {
	if o == nil {
		return nil
	}
	return o.Namespace
}

func (o *OutputGoogleChronicle) GetCustomLabels() []CustomLabelGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.CustomLabels
}

func (o *OutputGoogleChronicle) GetUdmType() *UDMType {
	if o == nil {
		return nil
	}
	return o.UdmType
}

func (o *OutputGoogleChronicle) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputGoogleChronicle) GetAPIKeySecret() *string {
	if o == nil {
		return nil
	}
	return o.APIKeySecret
}

func (o *OutputGoogleChronicle) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGoogleChronicle) GetServiceAccountCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentialsSecret
}

func (o *OutputGoogleChronicle) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGoogleChronicle) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGoogleChronicle) GetPqMode() *ModeGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGoogleChronicle) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGoogleChronicle) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGoogleChronicle) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGoogleChronicle) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGoogleChronicle) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGoogleChronicle) GetPqCompress() *CompressionGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGoogleChronicle) GetPqOnBackpressure() *QueueFullBehaviorGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGoogleChronicle) GetPqControls() *PqControlsGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeAzureEventhub string

const (
	TypeAzureEventhubAzureEventhub TypeAzureEventhub = "azure_eventhub"
)

func (e TypeAzureEventhub) ToPointer() *TypeAzureEventhub {
	return &e
}
func (e *TypeAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_eventhub":
		*e = TypeAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureEventhub: %v", v)
	}
}

// AcknowledgmentsAzureEventhub - Control the number of required acknowledgments
type AcknowledgmentsAzureEventhub int64

const (
	// AcknowledgmentsAzureEventhubOne Leader
	AcknowledgmentsAzureEventhubOne AcknowledgmentsAzureEventhub = 1
	// AcknowledgmentsAzureEventhubZero None
	AcknowledgmentsAzureEventhubZero AcknowledgmentsAzureEventhub = 0
	// AcknowledgmentsAzureEventhubMinus1 All
	AcknowledgmentsAzureEventhubMinus1 AcknowledgmentsAzureEventhub = -1
)

func (e AcknowledgmentsAzureEventhub) ToPointer() *AcknowledgmentsAzureEventhub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AcknowledgmentsAzureEventhub) IsExact() bool {
	if e != nil {
		switch *e {
		case 1, 0, -1:
			return true
		}
	}
	return false
}

// RecordDataFormatAzureEventhub - Format to use to serialize events before writing to the Event Hubs Kafka brokers
type RecordDataFormatAzureEventhub string

const (
	// RecordDataFormatAzureEventhubJSON JSON
	RecordDataFormatAzureEventhubJSON RecordDataFormatAzureEventhub = "json"
	// RecordDataFormatAzureEventhubRaw Field _raw
	RecordDataFormatAzureEventhubRaw RecordDataFormatAzureEventhub = "raw"
)

func (e RecordDataFormatAzureEventhub) ToPointer() *RecordDataFormatAzureEventhub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *RecordDataFormatAzureEventhub) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw":
			return true
		}
	}
	return false
}

// AuthTypeAuthenticationMethodAzureEventhub - Enter password directly, or select a stored secret
type AuthTypeAuthenticationMethodAzureEventhub string

const (
	AuthTypeAuthenticationMethodAzureEventhubManual AuthTypeAuthenticationMethodAzureEventhub = "manual"
	AuthTypeAuthenticationMethodAzureEventhubSecret AuthTypeAuthenticationMethodAzureEventhub = "secret"
)

func (e AuthTypeAuthenticationMethodAzureEventhub) ToPointer() *AuthTypeAuthenticationMethodAzureEventhub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthTypeAuthenticationMethodAzureEventhub) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

type SASLMechanismAzureEventhub string

const (
	// SASLMechanismAzureEventhubPlain PLAIN
	SASLMechanismAzureEventhubPlain SASLMechanismAzureEventhub = "plain"
	// SASLMechanismAzureEventhubOauthbearer OAUTHBEARER
	SASLMechanismAzureEventhubOauthbearer SASLMechanismAzureEventhub = "oauthbearer"
)

func (e SASLMechanismAzureEventhub) ToPointer() *SASLMechanismAzureEventhub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SASLMechanismAzureEventhub) IsExact() bool {
	if e != nil {
		switch *e {
		case "plain", "oauthbearer":
			return true
		}
	}
	return false
}

type ClientSecretAuthTypeAuthenticationMethodAzureEventhub string

const (
	ClientSecretAuthTypeAuthenticationMethodAzureEventhubManual      ClientSecretAuthTypeAuthenticationMethodAzureEventhub = "manual"
	ClientSecretAuthTypeAuthenticationMethodAzureEventhubSecret      ClientSecretAuthTypeAuthenticationMethodAzureEventhub = "secret"
	ClientSecretAuthTypeAuthenticationMethodAzureEventhubCertificate ClientSecretAuthTypeAuthenticationMethodAzureEventhub = "certificate"
)

func (e ClientSecretAuthTypeAuthenticationMethodAzureEventhub) ToPointer() *ClientSecretAuthTypeAuthenticationMethodAzureEventhub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ClientSecretAuthTypeAuthenticationMethodAzureEventhub) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret", "certificate":
			return true
		}
	}
	return false
}

// MicrosoftEntraIDAuthenticationEndpointAzureEventhub - Endpoint used to acquire authentication tokens from Azure
type MicrosoftEntraIDAuthenticationEndpointAzureEventhub string

const (
	MicrosoftEntraIDAuthenticationEndpointAzureEventhubHTTPSLoginMicrosoftonlineCom       MicrosoftEntraIDAuthenticationEndpointAzureEventhub = "https://login.microsoftonline.com"
	MicrosoftEntraIDAuthenticationEndpointAzureEventhubHTTPSLoginMicrosoftonlineUs        MicrosoftEntraIDAuthenticationEndpointAzureEventhub = "https://login.microsoftonline.us"
	MicrosoftEntraIDAuthenticationEndpointAzureEventhubHTTPSLoginPartnerMicrosoftonlineCn MicrosoftEntraIDAuthenticationEndpointAzureEventhub = "https://login.partner.microsoftonline.cn"
)

func (e MicrosoftEntraIDAuthenticationEndpointAzureEventhub) ToPointer() *MicrosoftEntraIDAuthenticationEndpointAzureEventhub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MicrosoftEntraIDAuthenticationEndpointAzureEventhub) IsExact() bool {
	if e != nil {
		switch *e {
		case "https://login.microsoftonline.com", "https://login.microsoftonline.us", "https://login.partner.microsoftonline.cn":
			return true
		}
	}
	return false
}

// AuthenticationAzureEventhub - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type AuthenticationAzureEventhub struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Enter password directly, or select a stored secret
	AuthType *AuthTypeAuthenticationMethodAzureEventhub `default:"manual" json:"authType"`
	// Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace
	Password *string `json:"password,omitempty"`
	// Select or create a stored text secret
	TextSecret *string                     `json:"textSecret,omitempty"`
	Mechanism  *SASLMechanismAzureEventhub `default:"plain" json:"mechanism"`
	// The username for authentication. For Event Hubs, this should always be $ConnectionString.
	Username             *string                                                `default:"$ConnectionString" json:"username"`
	ClientSecretAuthType *ClientSecretAuthTypeAuthenticationMethodAzureEventhub `default:"manual" json:"clientSecretAuthType"`
	// client_secret to pass in the OAuth request parameter
	ClientSecret *string `json:"clientSecret,omitempty"`
	// Select or create a stored text secret
	ClientTextSecret *string `json:"clientTextSecret,omitempty"`
	// Select or create a stored certificate
	CertificateName *string `json:"certificateName,omitempty"`
	CertPath        *string `json:"certPath,omitempty"`
	PrivKeyPath     *string `json:"privKeyPath,omitempty"`
	Passphrase      *string `json:"passphrase,omitempty"`
	// Endpoint used to acquire authentication tokens from Azure
	OauthEndpoint *MicrosoftEntraIDAuthenticationEndpointAzureEventhub `default:"https://login.microsoftonline.com" json:"oauthEndpoint"`
	// client_id to pass in the OAuth request parameter
	ClientID *string `json:"clientId,omitempty"`
	// Directory ID (tenant identifier) in Azure Active Directory
	TenantID *string `json:"tenantId,omitempty"`
	// Scope to pass in the OAuth request parameter
	Scope *string `json:"scope,omitempty"`
}

func (a AuthenticationAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthenticationAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (a *AuthenticationAzureEventhub) GetDisabled() *bool {
	if a == nil {
		return nil
	}
	return a.Disabled
}

func (a *AuthenticationAzureEventhub) GetAuthType() *AuthTypeAuthenticationMethodAzureEventhub {
	if a == nil {
		return nil
	}
	return a.AuthType
}

func (a *AuthenticationAzureEventhub) GetPassword() *string {
	if a == nil {
		return nil
	}
	return a.Password
}

func (a *AuthenticationAzureEventhub) GetTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.TextSecret
}

func (a *AuthenticationAzureEventhub) GetMechanism() *SASLMechanismAzureEventhub {
	if a == nil {
		return nil
	}
	return a.Mechanism
}

func (a *AuthenticationAzureEventhub) GetUsername() *string {
	if a == nil {
		return nil
	}
	return a.Username
}

func (a *AuthenticationAzureEventhub) GetClientSecretAuthType() *ClientSecretAuthTypeAuthenticationMethodAzureEventhub {
	if a == nil {
		return nil
	}
	return a.ClientSecretAuthType
}

func (a *AuthenticationAzureEventhub) GetClientSecret() *string {
	if a == nil {
		return nil
	}
	return a.ClientSecret
}

func (a *AuthenticationAzureEventhub) GetClientTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.ClientTextSecret
}

func (a *AuthenticationAzureEventhub) GetCertificateName() *string {
	if a == nil {
		return nil
	}
	return a.CertificateName
}

func (a *AuthenticationAzureEventhub) GetCertPath() *string {
	if a == nil {
		return nil
	}
	return a.CertPath
}

func (a *AuthenticationAzureEventhub) GetPrivKeyPath() *string {
	if a == nil {
		return nil
	}
	return a.PrivKeyPath
}

func (a *AuthenticationAzureEventhub) GetPassphrase() *string {
	if a == nil {
		return nil
	}
	return a.Passphrase
}

func (a *AuthenticationAzureEventhub) GetOauthEndpoint() *MicrosoftEntraIDAuthenticationEndpointAzureEventhub {
	if a == nil {
		return nil
	}
	return a.OauthEndpoint
}

func (a *AuthenticationAzureEventhub) GetClientID() *string {
	if a == nil {
		return nil
	}
	return a.ClientID
}

func (a *AuthenticationAzureEventhub) GetTenantID() *string {
	if a == nil {
		return nil
	}
	return a.TenantID
}

func (a *AuthenticationAzureEventhub) GetScope() *string {
	if a == nil {
		return nil
	}
	return a.Scope
}

type TLSSettingsClientSideAzureEventhub struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
}

func (t TLSSettingsClientSideAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideAzureEventhub) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideAzureEventhub) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

// BackpressureBehaviorAzureEventhub - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureEventhub string

const (
	// BackpressureBehaviorAzureEventhubBlock Block
	BackpressureBehaviorAzureEventhubBlock BackpressureBehaviorAzureEventhub = "block"
	// BackpressureBehaviorAzureEventhubDrop Drop
	BackpressureBehaviorAzureEventhubDrop BackpressureBehaviorAzureEventhub = "drop"
	// BackpressureBehaviorAzureEventhubQueue Persistent Queue
	BackpressureBehaviorAzureEventhubQueue BackpressureBehaviorAzureEventhub = "queue"
)

func (e BackpressureBehaviorAzureEventhub) ToPointer() *BackpressureBehaviorAzureEventhub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorAzureEventhub) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeAzureEventhub - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureEventhub string

const (
	// ModeAzureEventhubError Error
	ModeAzureEventhubError ModeAzureEventhub = "error"
	// ModeAzureEventhubAlways Backpressure
	ModeAzureEventhubAlways ModeAzureEventhub = "always"
	// ModeAzureEventhubBackpressure Always On
	ModeAzureEventhubBackpressure ModeAzureEventhub = "backpressure"
)

func (e ModeAzureEventhub) ToPointer() *ModeAzureEventhub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeAzureEventhub) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionAzureEventhub - Codec to use to compress the persisted data
type CompressionAzureEventhub string

const (
	// CompressionAzureEventhubNone None
	CompressionAzureEventhubNone CompressionAzureEventhub = "none"
	// CompressionAzureEventhubGzip Gzip
	CompressionAzureEventhubGzip CompressionAzureEventhub = "gzip"
)

func (e CompressionAzureEventhub) ToPointer() *CompressionAzureEventhub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionAzureEventhub) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorAzureEventhub - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureEventhub string

const (
	// QueueFullBehaviorAzureEventhubBlock Block
	QueueFullBehaviorAzureEventhubBlock QueueFullBehaviorAzureEventhub = "block"
	// QueueFullBehaviorAzureEventhubDrop Drop new data
	QueueFullBehaviorAzureEventhubDrop QueueFullBehaviorAzureEventhub = "drop"
)

func (e QueueFullBehaviorAzureEventhub) ToPointer() *QueueFullBehaviorAzureEventhub {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorAzureEventhub) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsAzureEventhub struct {
}

func (p PqControlsAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputAzureEventhub struct {
	// Unique ID for this output
	ID   string            `json:"id"`
	Type TypeAzureEventhub `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// List of Event Hubs Kafka brokers to connect to, eg. yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.
	Brokers []string `json:"brokers"`
	// The name of the Event Hub (Kafka Topic) to publish events. Can be overwritten using field __topicOut.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments
	Ack *AcknowledgmentsAzureEventhub `default:"1" json:"ack"`
	// Format to use to serialize events before writing to the Event Hubs Kafka brokers
	Format *RecordDataFormatAzureEventhub `default:"json" json:"format"`
	// Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// Maximum number of events in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *AuthenticationAzureEventhub        `json:"sasl,omitempty"`
	TLS  *TLSSettingsClientSideAzureEventhub `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureEventhub `default:"block" json:"onBackpressure"`
	Description    *string                            `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeAzureEventhub `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionAzureEventhub `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorAzureEventhub `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsAzureEventhub        `json:"pqControls,omitempty"`
}

func (o OutputAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "brokers", "topic"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureEventhub) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputAzureEventhub) GetType() TypeAzureEventhub {
	if o == nil {
		return TypeAzureEventhub("")
	}
	return o.Type
}

func (o *OutputAzureEventhub) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureEventhub) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureEventhub) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureEventhub) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureEventhub) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputAzureEventhub) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputAzureEventhub) GetAck() *AcknowledgmentsAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputAzureEventhub) GetFormat() *RecordDataFormatAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureEventhub) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputAzureEventhub) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputAzureEventhub) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureEventhub) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputAzureEventhub) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputAzureEventhub) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputAzureEventhub) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputAzureEventhub) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputAzureEventhub) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputAzureEventhub) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputAzureEventhub) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputAzureEventhub) GetSasl() *AuthenticationAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputAzureEventhub) GetTLS() *TLSSettingsClientSideAzureEventhub {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputAzureEventhub) GetOnBackpressure() *BackpressureBehaviorAzureEventhub {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureEventhub) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureEventhub) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputAzureEventhub) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputAzureEventhub) GetPqMode() *ModeAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureEventhub) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputAzureEventhub) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputAzureEventhub) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureEventhub) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureEventhub) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureEventhub) GetPqCompress() *CompressionAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureEventhub) GetPqOnBackpressure() *QueueFullBehaviorAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureEventhub) GetPqControls() *PqControlsAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeHoneycomb string

const (
	TypeHoneycombHoneycomb TypeHoneycomb = "honeycomb"
)

func (e TypeHoneycomb) ToPointer() *TypeHoneycomb {
	return &e
}
func (e *TypeHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "honeycomb":
		*e = TypeHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeHoneycomb: %v", v)
	}
}

type ExtraHTTPHeaderHoneycomb struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderHoneycomb) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderHoneycomb) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeHoneycomb - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeHoneycomb string

const (
	// FailedRequestLoggingModeHoneycombPayload Payload
	FailedRequestLoggingModeHoneycombPayload FailedRequestLoggingModeHoneycomb = "payload"
	// FailedRequestLoggingModeHoneycombPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeHoneycombPayloadAndHeaders FailedRequestLoggingModeHoneycomb = "payloadAndHeaders"
	// FailedRequestLoggingModeHoneycombNone None
	FailedRequestLoggingModeHoneycombNone FailedRequestLoggingModeHoneycomb = "none"
)

func (e FailedRequestLoggingModeHoneycomb) ToPointer() *FailedRequestLoggingModeHoneycomb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeHoneycomb) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingHoneycomb struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingHoneycomb) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingHoneycomb) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingHoneycomb) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingHoneycomb) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsHoneycomb struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsHoneycomb) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsHoneycomb) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsHoneycomb) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsHoneycomb) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorHoneycomb - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorHoneycomb string

const (
	// BackpressureBehaviorHoneycombBlock Block
	BackpressureBehaviorHoneycombBlock BackpressureBehaviorHoneycomb = "block"
	// BackpressureBehaviorHoneycombDrop Drop
	BackpressureBehaviorHoneycombDrop BackpressureBehaviorHoneycomb = "drop"
	// BackpressureBehaviorHoneycombQueue Persistent Queue
	BackpressureBehaviorHoneycombQueue BackpressureBehaviorHoneycomb = "queue"
)

func (e BackpressureBehaviorHoneycomb) ToPointer() *BackpressureBehaviorHoneycomb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorHoneycomb) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// AuthenticationMethodHoneycomb - Enter API key directly, or select a stored secret
type AuthenticationMethodHoneycomb string

const (
	AuthenticationMethodHoneycombManual AuthenticationMethodHoneycomb = "manual"
	AuthenticationMethodHoneycombSecret AuthenticationMethodHoneycomb = "secret"
)

func (e AuthenticationMethodHoneycomb) ToPointer() *AuthenticationMethodHoneycomb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodHoneycomb) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

// ModeHoneycomb - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeHoneycomb string

const (
	// ModeHoneycombError Error
	ModeHoneycombError ModeHoneycomb = "error"
	// ModeHoneycombAlways Backpressure
	ModeHoneycombAlways ModeHoneycomb = "always"
	// ModeHoneycombBackpressure Always On
	ModeHoneycombBackpressure ModeHoneycomb = "backpressure"
)

func (e ModeHoneycomb) ToPointer() *ModeHoneycomb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeHoneycomb) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionHoneycomb - Codec to use to compress the persisted data
type CompressionHoneycomb string

const (
	// CompressionHoneycombNone None
	CompressionHoneycombNone CompressionHoneycomb = "none"
	// CompressionHoneycombGzip Gzip
	CompressionHoneycombGzip CompressionHoneycomb = "gzip"
)

func (e CompressionHoneycomb) ToPointer() *CompressionHoneycomb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionHoneycomb) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorHoneycomb - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorHoneycomb string

const (
	// QueueFullBehaviorHoneycombBlock Block
	QueueFullBehaviorHoneycombBlock QueueFullBehaviorHoneycomb = "block"
	// QueueFullBehaviorHoneycombDrop Drop new data
	QueueFullBehaviorHoneycombDrop QueueFullBehaviorHoneycomb = "drop"
)

func (e QueueFullBehaviorHoneycomb) ToPointer() *QueueFullBehaviorHoneycomb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorHoneycomb) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsHoneycomb struct {
}

func (p PqControlsHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputHoneycomb struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeHoneycomb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the dataset to send events to – e.g., observability
	Dataset string `json:"dataset"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderHoneycomb `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeHoneycomb `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingHoneycomb `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsHoneycomb  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorHoneycomb `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType    *AuthenticationMethodHoneycomb `default:"manual" json:"authType"`
	Description *string                        `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeHoneycomb `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionHoneycomb `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorHoneycomb `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsHoneycomb        `json:"pqControls,omitempty"`
	// Team API key where the dataset belongs
	Team *string `json:"team,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "dataset"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputHoneycomb) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputHoneycomb) GetType() TypeHoneycomb {
	if o == nil {
		return TypeHoneycomb("")
	}
	return o.Type
}

func (o *OutputHoneycomb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputHoneycomb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputHoneycomb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputHoneycomb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputHoneycomb) GetDataset() string {
	if o == nil {
		return ""
	}
	return o.Dataset
}

func (o *OutputHoneycomb) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputHoneycomb) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputHoneycomb) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputHoneycomb) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputHoneycomb) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputHoneycomb) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputHoneycomb) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputHoneycomb) GetExtraHTTPHeaders() []ExtraHTTPHeaderHoneycomb {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputHoneycomb) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputHoneycomb) GetFailedRequestLoggingMode() *FailedRequestLoggingModeHoneycomb {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputHoneycomb) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputHoneycomb) GetResponseRetrySettings() []ResponseRetrySettingHoneycomb {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputHoneycomb) GetTimeoutRetrySettings() *TimeoutRetrySettingsHoneycomb {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputHoneycomb) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputHoneycomb) GetOnBackpressure() *BackpressureBehaviorHoneycomb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputHoneycomb) GetAuthType() *AuthenticationMethodHoneycomb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputHoneycomb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputHoneycomb) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputHoneycomb) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputHoneycomb) GetPqMode() *ModeHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputHoneycomb) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputHoneycomb) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputHoneycomb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputHoneycomb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputHoneycomb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputHoneycomb) GetPqCompress() *CompressionHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputHoneycomb) GetPqOnBackpressure() *QueueFullBehaviorHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputHoneycomb) GetPqControls() *PqControlsHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputHoneycomb) GetTeam() *string {
	if o == nil {
		return nil
	}
	return o.Team
}

func (o *OutputHoneycomb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type CreateOutputTypeKinesis string

const (
	CreateOutputTypeKinesisKinesis CreateOutputTypeKinesis = "kinesis"
)

func (e CreateOutputTypeKinesis) ToPointer() *CreateOutputTypeKinesis {
	return &e
}
func (e *CreateOutputTypeKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "kinesis":
		*e = CreateOutputTypeKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeKinesis: %v", v)
	}
}

// CreateOutputAuthenticationMethodKinesis - AWS authentication method. Choose Auto to use IAM roles.
type CreateOutputAuthenticationMethodKinesis string

const (
	// CreateOutputAuthenticationMethodKinesisAuto Auto
	CreateOutputAuthenticationMethodKinesisAuto CreateOutputAuthenticationMethodKinesis = "auto"
	// CreateOutputAuthenticationMethodKinesisManual Manual
	CreateOutputAuthenticationMethodKinesisManual CreateOutputAuthenticationMethodKinesis = "manual"
	// CreateOutputAuthenticationMethodKinesisSecret Secret Key pair
	CreateOutputAuthenticationMethodKinesisSecret CreateOutputAuthenticationMethodKinesis = "secret"
)

func (e CreateOutputAuthenticationMethodKinesis) ToPointer() *CreateOutputAuthenticationMethodKinesis {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationMethodKinesis) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// CreateOutputSignatureVersionKinesis - Signature version to use for signing Kinesis stream requests
type CreateOutputSignatureVersionKinesis string

const (
	CreateOutputSignatureVersionKinesisV2 CreateOutputSignatureVersionKinesis = "v2"
	CreateOutputSignatureVersionKinesisV4 CreateOutputSignatureVersionKinesis = "v4"
)

func (e CreateOutputSignatureVersionKinesis) ToPointer() *CreateOutputSignatureVersionKinesis {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputSignatureVersionKinesis) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

// CreateOutputCompressionKinesis - Compression type to use for records
type CreateOutputCompressionKinesis string

const (
	// CreateOutputCompressionKinesisNone None
	CreateOutputCompressionKinesisNone CreateOutputCompressionKinesis = "none"
	// CreateOutputCompressionKinesisGzip Gzip
	CreateOutputCompressionKinesisGzip CreateOutputCompressionKinesis = "gzip"
)

func (e CreateOutputCompressionKinesis) ToPointer() *CreateOutputCompressionKinesis {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputCompressionKinesis) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// BackpressureBehaviorKinesis - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorKinesis string

const (
	// BackpressureBehaviorKinesisBlock Block
	BackpressureBehaviorKinesisBlock BackpressureBehaviorKinesis = "block"
	// BackpressureBehaviorKinesisDrop Drop
	BackpressureBehaviorKinesisDrop BackpressureBehaviorKinesis = "drop"
	// BackpressureBehaviorKinesisQueue Persistent Queue
	BackpressureBehaviorKinesisQueue BackpressureBehaviorKinesis = "queue"
)

func (e BackpressureBehaviorKinesis) ToPointer() *BackpressureBehaviorKinesis {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorKinesis) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// CreateOutputModeKinesis - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeKinesis string

const (
	// CreateOutputModeKinesisError Error
	CreateOutputModeKinesisError CreateOutputModeKinesis = "error"
	// CreateOutputModeKinesisAlways Backpressure
	CreateOutputModeKinesisAlways CreateOutputModeKinesis = "always"
	// CreateOutputModeKinesisBackpressure Always On
	CreateOutputModeKinesisBackpressure CreateOutputModeKinesis = "backpressure"
)

func (e CreateOutputModeKinesis) ToPointer() *CreateOutputModeKinesis {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeKinesis) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionKinesis - Codec to use to compress the persisted data
type PqCompressCompressionKinesis string

const (
	// PqCompressCompressionKinesisNone None
	PqCompressCompressionKinesisNone PqCompressCompressionKinesis = "none"
	// PqCompressCompressionKinesisGzip Gzip
	PqCompressCompressionKinesisGzip PqCompressCompressionKinesis = "gzip"
)

func (e PqCompressCompressionKinesis) ToPointer() *PqCompressCompressionKinesis {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionKinesis) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorKinesis - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorKinesis string

const (
	// QueueFullBehaviorKinesisBlock Block
	QueueFullBehaviorKinesisBlock QueueFullBehaviorKinesis = "block"
	// QueueFullBehaviorKinesisDrop Drop new data
	QueueFullBehaviorKinesisDrop QueueFullBehaviorKinesis = "drop"
)

func (e QueueFullBehaviorKinesis) ToPointer() *QueueFullBehaviorKinesis {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorKinesis) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsKinesis struct {
}

func (c CreateOutputPqControlsKinesis) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsKinesis) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputKinesis struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type CreateOutputTypeKinesis `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Kinesis stream name to send events to.
	StreamName string `json:"streamName"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *CreateOutputAuthenticationMethodKinesis `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                                  `json:"awsSecretKey,omitempty"`
	// Region where the Kinesis stream is located
	Region string `json:"region"`
	// Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing Kinesis stream requests
	SignatureVersion *CreateOutputSignatureVersionKinesis `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access Kinesis stream
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of ongoing put requests before blocking.
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size
	MaxRecordSizeKB *float64 `default:"1024" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Compression type to use for records
	Compression *CreateOutputCompressionKinesis `default:"gzip" json:"compression"`
	// Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details.
	UseListShards *bool `default:"false" json:"useListShards"`
	// Batch events into a single record as NDJSON
	AsNdjson *bool `default:"true" json:"asNdjson"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorKinesis `default:"block" json:"onBackpressure"`
	Description    *string                      `json:"description,omitempty"`
	AwsAPIKey      *string                      `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Maximum number of records to send in a single request
	MaxEventsPerFlush *float64 `default:"500" json:"maxEventsPerFlush"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeKinesis `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionKinesis `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorKinesis      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsKinesis `json:"pqControls,omitempty"`
}

func (o OutputKinesis) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKinesis) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "streamName", "region"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputKinesis) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputKinesis) GetType() CreateOutputTypeKinesis {
	if o == nil {
		return CreateOutputTypeKinesis("")
	}
	return o.Type
}

func (o *OutputKinesis) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputKinesis) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputKinesis) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputKinesis) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputKinesis) GetStreamName() string {
	if o == nil {
		return ""
	}
	return o.StreamName
}

func (o *OutputKinesis) GetAwsAuthenticationMethod() *CreateOutputAuthenticationMethodKinesis {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputKinesis) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputKinesis) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputKinesis) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputKinesis) GetSignatureVersion() *CreateOutputSignatureVersionKinesis {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputKinesis) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputKinesis) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputKinesis) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputKinesis) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputKinesis) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputKinesis) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputKinesis) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputKinesis) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputKinesis) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputKinesis) GetCompression() *CreateOutputCompressionKinesis {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputKinesis) GetUseListShards() *bool {
	if o == nil {
		return nil
	}
	return o.UseListShards
}

func (o *OutputKinesis) GetAsNdjson() *bool {
	if o == nil {
		return nil
	}
	return o.AsNdjson
}

func (o *OutputKinesis) GetOnBackpressure() *BackpressureBehaviorKinesis {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputKinesis) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputKinesis) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputKinesis) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputKinesis) GetMaxEventsPerFlush() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxEventsPerFlush
}

func (o *OutputKinesis) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputKinesis) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputKinesis) GetPqMode() *CreateOutputModeKinesis {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputKinesis) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputKinesis) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputKinesis) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputKinesis) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputKinesis) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputKinesis) GetPqCompress() *PqCompressCompressionKinesis {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputKinesis) GetPqOnBackpressure() *QueueFullBehaviorKinesis {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputKinesis) GetPqControls() *CreateOutputPqControlsKinesis {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeAzureLogs string

const (
	TypeAzureLogsAzureLogs TypeAzureLogs = "azure_logs"
)

func (e TypeAzureLogs) ToPointer() *TypeAzureLogs {
	return &e
}
func (e *TypeAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_logs":
		*e = TypeAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureLogs: %v", v)
	}
}

type ExtraHTTPHeaderAzureLogs struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderAzureLogs) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderAzureLogs) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeAzureLogs - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeAzureLogs string

const (
	// FailedRequestLoggingModeAzureLogsPayload Payload
	FailedRequestLoggingModeAzureLogsPayload FailedRequestLoggingModeAzureLogs = "payload"
	// FailedRequestLoggingModeAzureLogsPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeAzureLogsPayloadAndHeaders FailedRequestLoggingModeAzureLogs = "payloadAndHeaders"
	// FailedRequestLoggingModeAzureLogsNone None
	FailedRequestLoggingModeAzureLogsNone FailedRequestLoggingModeAzureLogs = "none"
)

func (e FailedRequestLoggingModeAzureLogs) ToPointer() *FailedRequestLoggingModeAzureLogs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeAzureLogs) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingAzureLogs struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingAzureLogs) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingAzureLogs) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingAzureLogs) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingAzureLogs) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsAzureLogs struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsAzureLogs) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsAzureLogs) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsAzureLogs) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsAzureLogs) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorAzureLogs - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureLogs string

const (
	// BackpressureBehaviorAzureLogsBlock Block
	BackpressureBehaviorAzureLogsBlock BackpressureBehaviorAzureLogs = "block"
	// BackpressureBehaviorAzureLogsDrop Drop
	BackpressureBehaviorAzureLogsDrop BackpressureBehaviorAzureLogs = "drop"
	// BackpressureBehaviorAzureLogsQueue Persistent Queue
	BackpressureBehaviorAzureLogsQueue BackpressureBehaviorAzureLogs = "queue"
)

func (e BackpressureBehaviorAzureLogs) ToPointer() *BackpressureBehaviorAzureLogs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorAzureLogs) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// AuthenticationMethodAzureLogs - Enter workspace ID and workspace key directly, or select a stored secret
type AuthenticationMethodAzureLogs string

const (
	AuthenticationMethodAzureLogsManual AuthenticationMethodAzureLogs = "manual"
	AuthenticationMethodAzureLogsSecret AuthenticationMethodAzureLogs = "secret"
)

func (e AuthenticationMethodAzureLogs) ToPointer() *AuthenticationMethodAzureLogs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodAzureLogs) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

// ModeAzureLogs - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureLogs string

const (
	// ModeAzureLogsError Error
	ModeAzureLogsError ModeAzureLogs = "error"
	// ModeAzureLogsAlways Backpressure
	ModeAzureLogsAlways ModeAzureLogs = "always"
	// ModeAzureLogsBackpressure Always On
	ModeAzureLogsBackpressure ModeAzureLogs = "backpressure"
)

func (e ModeAzureLogs) ToPointer() *ModeAzureLogs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeAzureLogs) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionAzureLogs - Codec to use to compress the persisted data
type CompressionAzureLogs string

const (
	// CompressionAzureLogsNone None
	CompressionAzureLogsNone CompressionAzureLogs = "none"
	// CompressionAzureLogsGzip Gzip
	CompressionAzureLogsGzip CompressionAzureLogs = "gzip"
)

func (e CompressionAzureLogs) ToPointer() *CompressionAzureLogs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionAzureLogs) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorAzureLogs - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureLogs string

const (
	// QueueFullBehaviorAzureLogsBlock Block
	QueueFullBehaviorAzureLogsBlock QueueFullBehaviorAzureLogs = "block"
	// QueueFullBehaviorAzureLogsDrop Drop new data
	QueueFullBehaviorAzureLogsDrop QueueFullBehaviorAzureLogs = "drop"
)

func (e QueueFullBehaviorAzureLogs) ToPointer() *QueueFullBehaviorAzureLogs {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorAzureLogs) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsAzureLogs struct {
}

func (p PqControlsAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputAzureLogs struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeAzureLogs `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType.
	LogType *string `default:"Cribl" json:"logType"`
	// Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.
	ResourceID *string `json:"resourceId,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	Compress         *bool    `json:"compress,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderAzureLogs `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeAzureLogs `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// The DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>.
	APIURL *string `default:".ods.opinsights.azure.com" json:"apiUrl"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingAzureLogs `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsAzureLogs  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureLogs `default:"block" json:"onBackpressure"`
	// Enter workspace ID and workspace key directly, or select a stored secret
	AuthType    *AuthenticationMethodAzureLogs `default:"manual" json:"authType"`
	Description *string                        `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeAzureLogs `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionAzureLogs `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorAzureLogs `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsAzureLogs        `json:"pqControls,omitempty"`
	// Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.
	WorkspaceID *string `json:"workspaceId,omitempty"`
	// Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.
	WorkspaceKey *string `json:"workspaceKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	KeypairSecret *string `json:"keypairSecret,omitempty"`
}

func (o OutputAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureLogs) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputAzureLogs) GetType() TypeAzureLogs {
	if o == nil {
		return TypeAzureLogs("")
	}
	return o.Type
}

func (o *OutputAzureLogs) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureLogs) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureLogs) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureLogs) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureLogs) GetLogType() *string {
	if o == nil {
		return nil
	}
	return o.LogType
}

func (o *OutputAzureLogs) GetResourceID() *string {
	if o == nil {
		return nil
	}
	return o.ResourceID
}

func (o *OutputAzureLogs) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputAzureLogs) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputAzureLogs) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputAzureLogs) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureLogs) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputAzureLogs) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputAzureLogs) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureLogs) GetExtraHTTPHeaders() []ExtraHTTPHeaderAzureLogs {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputAzureLogs) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputAzureLogs) GetFailedRequestLoggingMode() *FailedRequestLoggingModeAzureLogs {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputAzureLogs) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputAzureLogs) GetAPIURL() *string {
	if o == nil {
		return nil
	}
	return o.APIURL
}

func (o *OutputAzureLogs) GetResponseRetrySettings() []ResponseRetrySettingAzureLogs {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputAzureLogs) GetTimeoutRetrySettings() *TimeoutRetrySettingsAzureLogs {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputAzureLogs) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputAzureLogs) GetOnBackpressure() *BackpressureBehaviorAzureLogs {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureLogs) GetAuthType() *AuthenticationMethodAzureLogs {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputAzureLogs) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureLogs) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputAzureLogs) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputAzureLogs) GetPqMode() *ModeAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureLogs) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputAzureLogs) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputAzureLogs) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureLogs) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureLogs) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureLogs) GetPqCompress() *CompressionAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureLogs) GetPqOnBackpressure() *QueueFullBehaviorAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureLogs) GetPqControls() *PqControlsAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputAzureLogs) GetWorkspaceID() *string {
	if o == nil {
		return nil
	}
	return o.WorkspaceID
}

func (o *OutputAzureLogs) GetWorkspaceKey() *string {
	if o == nil {
		return nil
	}
	return o.WorkspaceKey
}

func (o *OutputAzureLogs) GetKeypairSecret() *string {
	if o == nil {
		return nil
	}
	return o.KeypairSecret
}

type TypeAzureDataExplorer string

const (
	TypeAzureDataExplorerAzureDataExplorer TypeAzureDataExplorer = "azure_data_explorer"
)

func (e TypeAzureDataExplorer) ToPointer() *TypeAzureDataExplorer {
	return &e
}
func (e *TypeAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_data_explorer":
		*e = TypeAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureDataExplorer: %v", v)
	}
}

type IngestionMode string

const (
	// IngestionModeBatching Batching
	IngestionModeBatching IngestionMode = "batching"
	// IngestionModeStreaming Streaming
	IngestionModeStreaming IngestionMode = "streaming"
)

func (e IngestionMode) ToPointer() *IngestionMode {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *IngestionMode) IsExact() bool {
	if e != nil {
		switch *e {
		case "batching", "streaming":
			return true
		}
	}
	return false
}

// MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer - Endpoint used to acquire authentication tokens from Azure
type MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer string

const (
	MicrosoftEntraIDAuthenticationEndpointAzureDataExplorerHTTPSLoginMicrosoftonlineCom       MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer = "https://login.microsoftonline.com"
	MicrosoftEntraIDAuthenticationEndpointAzureDataExplorerHTTPSLoginMicrosoftonlineUs        MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer = "https://login.microsoftonline.us"
	MicrosoftEntraIDAuthenticationEndpointAzureDataExplorerHTTPSLoginPartnerMicrosoftonlineCn MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer = "https://login.partner.microsoftonline.cn"
)

func (e MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer) ToPointer() *MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer) IsExact() bool {
	if e != nil {
		switch *e {
		case "https://login.microsoftonline.com", "https://login.microsoftonline.us", "https://login.partner.microsoftonline.cn":
			return true
		}
	}
	return false
}

// OauthTypeAuthenticationMethod - The type of OAuth 2.0 client credentials grant flow to use
type OauthTypeAuthenticationMethod string

const (
	// OauthTypeAuthenticationMethodClientSecret Client secret
	OauthTypeAuthenticationMethodClientSecret OauthTypeAuthenticationMethod = "clientSecret"
	// OauthTypeAuthenticationMethodClientTextSecret Client secret (text secret)
	OauthTypeAuthenticationMethodClientTextSecret OauthTypeAuthenticationMethod = "clientTextSecret"
	// OauthTypeAuthenticationMethodCertificate Certificate
	OauthTypeAuthenticationMethodCertificate OauthTypeAuthenticationMethod = "certificate"
)

func (e OauthTypeAuthenticationMethod) ToPointer() *OauthTypeAuthenticationMethod {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *OauthTypeAuthenticationMethod) IsExact() bool {
	if e != nil {
		switch *e {
		case "clientSecret", "clientTextSecret", "certificate":
			return true
		}
	}
	return false
}

type CertificateAzureDataExplorer struct {
	// The certificate you registered as credentials for your app in the Azure portal
	CertificateName *string `json:"certificateName,omitempty"`
}

func (c CertificateAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CertificateAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CertificateAzureDataExplorer) GetCertificateName() *string {
	if c == nil {
		return nil
	}
	return c.CertificateName
}

// DataFormatAzureDataExplorer - Format of the output data
type DataFormatAzureDataExplorer string

const (
	// DataFormatAzureDataExplorerJSON JSON
	DataFormatAzureDataExplorerJSON DataFormatAzureDataExplorer = "json"
	// DataFormatAzureDataExplorerRaw Raw
	DataFormatAzureDataExplorerRaw DataFormatAzureDataExplorer = "raw"
	// DataFormatAzureDataExplorerParquet Parquet
	DataFormatAzureDataExplorerParquet DataFormatAzureDataExplorer = "parquet"
)

func (e DataFormatAzureDataExplorer) ToPointer() *DataFormatAzureDataExplorer {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataFormatAzureDataExplorer) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "parquet":
			return true
		}
	}
	return false
}

// CompressCompressionAzureDataExplorer - Data compression format to apply to HTTP content before it is delivered
type CompressCompressionAzureDataExplorer string

const (
	CompressCompressionAzureDataExplorerNone CompressCompressionAzureDataExplorer = "none"
	CompressCompressionAzureDataExplorerGzip CompressCompressionAzureDataExplorer = "gzip"
)

func (e CompressCompressionAzureDataExplorer) ToPointer() *CompressCompressionAzureDataExplorer {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressCompressionAzureDataExplorer) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// CompressionLevelAzureDataExplorer - Compression level to apply before moving files to final destination
type CompressionLevelAzureDataExplorer string

const (
	// CompressionLevelAzureDataExplorerBestSpeed Best Speed
	CompressionLevelAzureDataExplorerBestSpeed CompressionLevelAzureDataExplorer = "best_speed"
	// CompressionLevelAzureDataExplorerNormal Normal
	CompressionLevelAzureDataExplorerNormal CompressionLevelAzureDataExplorer = "normal"
	// CompressionLevelAzureDataExplorerBestCompression Best Compression
	CompressionLevelAzureDataExplorerBestCompression CompressionLevelAzureDataExplorer = "best_compression"
)

func (e CompressionLevelAzureDataExplorer) ToPointer() *CompressionLevelAzureDataExplorer {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionLevelAzureDataExplorer) IsExact() bool {
	if e != nil {
		switch *e {
		case "best_speed", "normal", "best_compression":
			return true
		}
	}
	return false
}

// ParquetVersionAzureDataExplorer - Determines which data types are supported and how they are represented
type ParquetVersionAzureDataExplorer string

const (
	// ParquetVersionAzureDataExplorerParquet10 1.0
	ParquetVersionAzureDataExplorerParquet10 ParquetVersionAzureDataExplorer = "PARQUET_1_0"
	// ParquetVersionAzureDataExplorerParquet24 2.4
	ParquetVersionAzureDataExplorerParquet24 ParquetVersionAzureDataExplorer = "PARQUET_2_4"
	// ParquetVersionAzureDataExplorerParquet26 2.6
	ParquetVersionAzureDataExplorerParquet26 ParquetVersionAzureDataExplorer = "PARQUET_2_6"
)

func (e ParquetVersionAzureDataExplorer) ToPointer() *ParquetVersionAzureDataExplorer {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ParquetVersionAzureDataExplorer) IsExact() bool {
	if e != nil {
		switch *e {
		case "PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6":
			return true
		}
	}
	return false
}

// DataPageVersionAzureDataExplorer - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionAzureDataExplorer string

const (
	// DataPageVersionAzureDataExplorerDataPageV1 V1
	DataPageVersionAzureDataExplorerDataPageV1 DataPageVersionAzureDataExplorer = "DATA_PAGE_V1"
	// DataPageVersionAzureDataExplorerDataPageV2 V2
	DataPageVersionAzureDataExplorerDataPageV2 DataPageVersionAzureDataExplorer = "DATA_PAGE_V2"
)

func (e DataPageVersionAzureDataExplorer) ToPointer() *DataPageVersionAzureDataExplorer {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataPageVersionAzureDataExplorer) IsExact() bool {
	if e != nil {
		switch *e {
		case "DATA_PAGE_V1", "DATA_PAGE_V2":
			return true
		}
	}
	return false
}

type KeyValueMetadatumAzureDataExplorer struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumAzureDataExplorer) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumAzureDataExplorer) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

// BackpressureBehaviorAzureDataExplorer - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureDataExplorer string

const (
	// BackpressureBehaviorAzureDataExplorerBlock Block
	BackpressureBehaviorAzureDataExplorerBlock BackpressureBehaviorAzureDataExplorer = "block"
	// BackpressureBehaviorAzureDataExplorerDrop Drop
	BackpressureBehaviorAzureDataExplorerDrop BackpressureBehaviorAzureDataExplorer = "drop"
	// BackpressureBehaviorAzureDataExplorerQueue Persistent Queue
	BackpressureBehaviorAzureDataExplorerQueue BackpressureBehaviorAzureDataExplorer = "queue"
)

func (e BackpressureBehaviorAzureDataExplorer) ToPointer() *BackpressureBehaviorAzureDataExplorer {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorAzureDataExplorer) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionAzureDataExplorer - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionAzureDataExplorer string

const (
	// DiskSpaceProtectionAzureDataExplorerBlock Block
	DiskSpaceProtectionAzureDataExplorerBlock DiskSpaceProtectionAzureDataExplorer = "block"
	// DiskSpaceProtectionAzureDataExplorerDrop Drop
	DiskSpaceProtectionAzureDataExplorerDrop DiskSpaceProtectionAzureDataExplorer = "drop"
)

func (e DiskSpaceProtectionAzureDataExplorer) ToPointer() *DiskSpaceProtectionAzureDataExplorer {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionAzureDataExplorer) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PrefixOptional string

const (
	// PrefixOptionalDropBy drop-by
	PrefixOptionalDropBy PrefixOptional = "dropBy"
	// PrefixOptionalIngestBy ingest-by
	PrefixOptionalIngestBy PrefixOptional = "ingestBy"
)

func (e PrefixOptional) ToPointer() *PrefixOptional {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PrefixOptional) IsExact() bool {
	if e != nil {
		switch *e {
		case "dropBy", "ingestBy":
			return true
		}
	}
	return false
}

type ExtentTag struct {
	Prefix *PrefixOptional `json:"prefix,omitempty"`
	Value  string          `json:"value"`
}

func (e ExtentTag) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtentTag) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtentTag) GetPrefix() *PrefixOptional {
	if e == nil {
		return nil
	}
	return e.Prefix
}

func (e *ExtentTag) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

type IngestIfNotExist struct {
	Value string `json:"value"`
}

func (i IngestIfNotExist) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(i, "", false)
}

func (i *IngestIfNotExist) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &i, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (i *IngestIfNotExist) GetValue() string {
	if i == nil {
		return ""
	}
	return i.Value
}

// ReportLevel - Level of ingestion status reporting. Defaults to FailuresOnly.
type ReportLevel string

const (
	// ReportLevelFailuresOnly FailuresOnly
	ReportLevelFailuresOnly ReportLevel = "failuresOnly"
	// ReportLevelDoNotReport DoNotReport
	ReportLevelDoNotReport ReportLevel = "doNotReport"
	// ReportLevelFailuresAndSuccesses FailuresAndSuccesses
	ReportLevelFailuresAndSuccesses ReportLevel = "failuresAndSuccesses"
)

func (e ReportLevel) ToPointer() *ReportLevel {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ReportLevel) IsExact() bool {
	if e != nil {
		switch *e {
		case "failuresOnly", "doNotReport", "failuresAndSuccesses":
			return true
		}
	}
	return false
}

// ReportMethod - Target of the ingestion status reporting. Defaults to Queue.
type ReportMethod string

const (
	// ReportMethodQueue Queue
	ReportMethodQueue ReportMethod = "queue"
	// ReportMethodTable Table
	ReportMethodTable ReportMethod = "table"
	// ReportMethodQueueAndTable QueueAndTable
	ReportMethodQueueAndTable ReportMethod = "queueAndTable"
)

func (e ReportMethod) ToPointer() *ReportMethod {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ReportMethod) IsExact() bool {
	if e != nil {
		switch *e {
		case "queue", "table", "queueAndTable":
			return true
		}
	}
	return false
}

type AdditionalProperty struct {
	Key   string `json:"key"`
	Value string `json:"value"`
}

func (a AdditionalProperty) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AdditionalProperty) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, []string{"key", "value"}); err != nil {
		return err
	}
	return nil
}

func (a *AdditionalProperty) GetKey() string {
	if a == nil {
		return ""
	}
	return a.Key
}

func (a *AdditionalProperty) GetValue() string {
	if a == nil {
		return ""
	}
	return a.Value
}

type ResponseRetrySettingAzureDataExplorer struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingAzureDataExplorer) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingAzureDataExplorer) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingAzureDataExplorer) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingAzureDataExplorer) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsAzureDataExplorer struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsAzureDataExplorer) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsAzureDataExplorer) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsAzureDataExplorer) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsAzureDataExplorer) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// ModeAzureDataExplorer - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureDataExplorer string

const (
	// ModeAzureDataExplorerError Error
	ModeAzureDataExplorerError ModeAzureDataExplorer = "error"
	// ModeAzureDataExplorerAlways Backpressure
	ModeAzureDataExplorerAlways ModeAzureDataExplorer = "always"
	// ModeAzureDataExplorerBackpressure Always On
	ModeAzureDataExplorerBackpressure ModeAzureDataExplorer = "backpressure"
)

func (e ModeAzureDataExplorer) ToPointer() *ModeAzureDataExplorer {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeAzureDataExplorer) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionAzureDataExplorer - Codec to use to compress the persisted data
type PqCompressCompressionAzureDataExplorer string

const (
	// PqCompressCompressionAzureDataExplorerNone None
	PqCompressCompressionAzureDataExplorerNone PqCompressCompressionAzureDataExplorer = "none"
	// PqCompressCompressionAzureDataExplorerGzip Gzip
	PqCompressCompressionAzureDataExplorerGzip PqCompressCompressionAzureDataExplorer = "gzip"
)

func (e PqCompressCompressionAzureDataExplorer) ToPointer() *PqCompressCompressionAzureDataExplorer {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionAzureDataExplorer) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorAzureDataExplorer - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureDataExplorer string

const (
	// QueueFullBehaviorAzureDataExplorerBlock Block
	QueueFullBehaviorAzureDataExplorerBlock QueueFullBehaviorAzureDataExplorer = "block"
	// QueueFullBehaviorAzureDataExplorerDrop Drop new data
	QueueFullBehaviorAzureDataExplorerDrop QueueFullBehaviorAzureDataExplorer = "drop"
)

func (e QueueFullBehaviorAzureDataExplorer) ToPointer() *QueueFullBehaviorAzureDataExplorer {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorAzureDataExplorer) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsAzureDataExplorer struct {
}

func (p PqControlsAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputAzureDataExplorer struct {
	// Unique ID for this output
	ID   string                `json:"id"`
	Type TypeAzureDataExplorer `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.
	ClusterURL string `json:"clusterUrl"`
	// Name of the database containing the table where data will be ingested
	Database string `json:"database"`
	// Name of the table to ingest data into
	Table string `json:"table"`
	// When saving or starting the Destination, validate the database name and credentials; also validate table name, except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.
	ValidateDatabaseSettings *bool          `default:"true" json:"validateDatabaseSettings"`
	IngestMode               *IngestionMode `default:"batching" json:"ingestMode"`
	// Endpoint used to acquire authentication tokens from Azure
	OauthEndpoint *MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer `default:"https://login.microsoftonline.com" json:"oauthEndpoint"`
	// Directory ID (tenant identifier) in Azure Active Directory
	TenantID string `json:"tenantId"`
	// client_id to pass in the OAuth request parameter
	ClientID string `json:"clientId"`
	// Scope to pass in the OAuth request parameter
	Scope string `json:"scope"`
	// The type of OAuth 2.0 client credentials grant flow to use
	OauthType   *OauthTypeAuthenticationMethod `default:"clientSecret" json:"oauthType"`
	Description *string                        `json:"description,omitempty"`
	// The client secret that you generated for your app in the Azure portal
	ClientSecret *string `json:"clientSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret  *string                       `json:"textSecret,omitempty"`
	Certificate *CertificateAzureDataExplorer `json:"certificate,omitempty"`
	// Format of the output data
	Format *DataFormatAzureDataExplorer `default:"json" json:"format"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressCompressionAzureDataExplorer `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelAzureDataExplorer `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionAzureDataExplorer `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionAzureDataExplorer `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumAzureDataExplorer `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
	// Send a JSON mapping object instead of specifying an existing named data mapping
	IsMappingObj *bool `default:"false" json:"isMappingObj"`
	// Enter a JSON object that defines your desired data mapping
	MappingObj *string `json:"mappingObj,omitempty"`
	// Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.
	MappingRef *string `json:"mappingRef,omitempty"`
	// The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.
	IngestURL *string `json:"ingestUrl,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureDataExplorer `default:"block" json:"onBackpressure"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// Maximum number of parts to upload in parallel per file
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionAzureDataExplorer `default:"block" json:"onDiskFullBackpressure"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Bypass the data management service's aggregation mechanism
	FlushImmediately *bool `default:"false" json:"flushImmediately"`
	// Prevent blob deletion after ingestion is complete
	RetainBlobOnSuccess *bool `default:"false" json:"retainBlobOnSuccess"`
	// Strings or tags associated with the extent (ingested data shard)
	ExtentTags []ExtentTag `json:"extentTags,omitempty"`
	// Prevents duplicate ingestion by verifying whether an extent with the specified ingest-by tag already exists
	IngestIfNotExists []IngestIfNotExist `json:"ingestIfNotExists,omitempty"`
	// Level of ingestion status reporting. Defaults to FailuresOnly.
	ReportLevel *ReportLevel `default:"failuresOnly" json:"reportLevel"`
	// Target of the ingestion status reporting. Defaults to Queue.
	ReportMethod *ReportMethod `default:"queue" json:"reportMethod"`
	// Optionally, enter additional configuration properties to send to the ingestion service
	AdditionalProperties []AdditionalProperty `json:"additionalProperties,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingAzureDataExplorer `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsAzureDataExplorer  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeAzureDataExplorer `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionAzureDataExplorer `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorAzureDataExplorer `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsAzureDataExplorer        `json:"pqControls,omitempty"`
}

func (o OutputAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "clusterUrl", "database", "table", "tenantId", "clientId", "scope"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureDataExplorer) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputAzureDataExplorer) GetType() TypeAzureDataExplorer {
	if o == nil {
		return TypeAzureDataExplorer("")
	}
	return o.Type
}

func (o *OutputAzureDataExplorer) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureDataExplorer) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureDataExplorer) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureDataExplorer) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureDataExplorer) GetClusterURL() string {
	if o == nil {
		return ""
	}
	return o.ClusterURL
}

func (o *OutputAzureDataExplorer) GetDatabase() string {
	if o == nil {
		return ""
	}
	return o.Database
}

func (o *OutputAzureDataExplorer) GetTable() string {
	if o == nil {
		return ""
	}
	return o.Table
}

func (o *OutputAzureDataExplorer) GetValidateDatabaseSettings() *bool {
	if o == nil {
		return nil
	}
	return o.ValidateDatabaseSettings
}

func (o *OutputAzureDataExplorer) GetIngestMode() *IngestionMode {
	if o == nil {
		return nil
	}
	return o.IngestMode
}

func (o *OutputAzureDataExplorer) GetOauthEndpoint() *MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.OauthEndpoint
}

func (o *OutputAzureDataExplorer) GetTenantID() string {
	if o == nil {
		return ""
	}
	return o.TenantID
}

func (o *OutputAzureDataExplorer) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputAzureDataExplorer) GetScope() string {
	if o == nil {
		return ""
	}
	return o.Scope
}

func (o *OutputAzureDataExplorer) GetOauthType() *OauthTypeAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.OauthType
}

func (o *OutputAzureDataExplorer) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureDataExplorer) GetClientSecret() *string {
	if o == nil {
		return nil
	}
	return o.ClientSecret
}

func (o *OutputAzureDataExplorer) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputAzureDataExplorer) GetCertificate() *CertificateAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Certificate
}

func (o *OutputAzureDataExplorer) GetFormat() *DataFormatAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureDataExplorer) GetCompress() *CompressCompressionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureDataExplorer) GetCompressionLevel() *CompressionLevelAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputAzureDataExplorer) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputAzureDataExplorer) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputAzureDataExplorer) GetParquetVersion() *ParquetVersionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputAzureDataExplorer) GetParquetDataPageVersion() *DataPageVersionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputAzureDataExplorer) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputAzureDataExplorer) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputAzureDataExplorer) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputAzureDataExplorer) GetKeyValueMetadata() []KeyValueMetadatumAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputAzureDataExplorer) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputAzureDataExplorer) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputAzureDataExplorer) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputAzureDataExplorer) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputAzureDataExplorer) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputAzureDataExplorer) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputAzureDataExplorer) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputAzureDataExplorer) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputAzureDataExplorer) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputAzureDataExplorer) GetIsMappingObj() *bool {
	if o == nil {
		return nil
	}
	return o.IsMappingObj
}

func (o *OutputAzureDataExplorer) GetMappingObj() *string {
	if o == nil {
		return nil
	}
	return o.MappingObj
}

func (o *OutputAzureDataExplorer) GetMappingRef() *string {
	if o == nil {
		return nil
	}
	return o.MappingRef
}

func (o *OutputAzureDataExplorer) GetIngestURL() *string {
	if o == nil {
		return nil
	}
	return o.IngestURL
}

func (o *OutputAzureDataExplorer) GetOnBackpressure() *BackpressureBehaviorAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureDataExplorer) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputAzureDataExplorer) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputAzureDataExplorer) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputAzureDataExplorer) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputAzureDataExplorer) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputAzureDataExplorer) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputAzureDataExplorer) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputAzureDataExplorer) GetOnDiskFullBackpressure() *DiskSpaceProtectionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputAzureDataExplorer) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputAzureDataExplorer) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputAzureDataExplorer) GetFlushImmediately() *bool {
	if o == nil {
		return nil
	}
	return o.FlushImmediately
}

func (o *OutputAzureDataExplorer) GetRetainBlobOnSuccess() *bool {
	if o == nil {
		return nil
	}
	return o.RetainBlobOnSuccess
}

func (o *OutputAzureDataExplorer) GetExtentTags() []ExtentTag {
	if o == nil {
		return nil
	}
	return o.ExtentTags
}

func (o *OutputAzureDataExplorer) GetIngestIfNotExists() []IngestIfNotExist {
	if o == nil {
		return nil
	}
	return o.IngestIfNotExists
}

func (o *OutputAzureDataExplorer) GetReportLevel() *ReportLevel {
	if o == nil {
		return nil
	}
	return o.ReportLevel
}

func (o *OutputAzureDataExplorer) GetReportMethod() *ReportMethod {
	if o == nil {
		return nil
	}
	return o.ReportMethod
}

func (o *OutputAzureDataExplorer) GetAdditionalProperties() []AdditionalProperty {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

func (o *OutputAzureDataExplorer) GetResponseRetrySettings() []ResponseRetrySettingAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputAzureDataExplorer) GetTimeoutRetrySettings() *TimeoutRetrySettingsAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputAzureDataExplorer) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputAzureDataExplorer) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputAzureDataExplorer) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputAzureDataExplorer) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputAzureDataExplorer) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureDataExplorer) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputAzureDataExplorer) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputAzureDataExplorer) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputAzureDataExplorer) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputAzureDataExplorer) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputAzureDataExplorer) GetPqMode() *ModeAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureDataExplorer) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputAzureDataExplorer) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputAzureDataExplorer) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureDataExplorer) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureDataExplorer) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureDataExplorer) GetPqCompress() *PqCompressCompressionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureDataExplorer) GetPqOnBackpressure() *QueueFullBehaviorAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureDataExplorer) GetPqControls() *PqControlsAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeAzureBlob string

const (
	CreateOutputTypeAzureBlobAzureBlob CreateOutputTypeAzureBlob = "azure_blob"
)

func (e CreateOutputTypeAzureBlob) ToPointer() *CreateOutputTypeAzureBlob {
	return &e
}
func (e *CreateOutputTypeAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_blob":
		*e = CreateOutputTypeAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeAzureBlob: %v", v)
	}
}

// DataFormatAzureBlob - Format of the output data
type DataFormatAzureBlob string

const (
	// DataFormatAzureBlobJSON JSON
	DataFormatAzureBlobJSON DataFormatAzureBlob = "json"
	// DataFormatAzureBlobRaw Raw
	DataFormatAzureBlobRaw DataFormatAzureBlob = "raw"
	// DataFormatAzureBlobParquet Parquet
	DataFormatAzureBlobParquet DataFormatAzureBlob = "parquet"
)

func (e DataFormatAzureBlob) ToPointer() *DataFormatAzureBlob {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataFormatAzureBlob) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "parquet":
			return true
		}
	}
	return false
}

// BackpressureBehaviorAzureBlob - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureBlob string

const (
	// BackpressureBehaviorAzureBlobBlock Block
	BackpressureBehaviorAzureBlobBlock BackpressureBehaviorAzureBlob = "block"
	// BackpressureBehaviorAzureBlobDrop Drop
	BackpressureBehaviorAzureBlobDrop BackpressureBehaviorAzureBlob = "drop"
)

func (e BackpressureBehaviorAzureBlob) ToPointer() *BackpressureBehaviorAzureBlob {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorAzureBlob) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionAzureBlob - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionAzureBlob string

const (
	// DiskSpaceProtectionAzureBlobBlock Block
	DiskSpaceProtectionAzureBlobBlock DiskSpaceProtectionAzureBlob = "block"
	// DiskSpaceProtectionAzureBlobDrop Drop
	DiskSpaceProtectionAzureBlobDrop DiskSpaceProtectionAzureBlob = "drop"
)

func (e DiskSpaceProtectionAzureBlob) ToPointer() *DiskSpaceProtectionAzureBlob {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionAzureBlob) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputAuthenticationMethodAzureBlob string

const (
	CreateOutputAuthenticationMethodAzureBlobManual       CreateOutputAuthenticationMethodAzureBlob = "manual"
	CreateOutputAuthenticationMethodAzureBlobSecret       CreateOutputAuthenticationMethodAzureBlob = "secret"
	CreateOutputAuthenticationMethodAzureBlobClientSecret CreateOutputAuthenticationMethodAzureBlob = "clientSecret"
	CreateOutputAuthenticationMethodAzureBlobClientCert   CreateOutputAuthenticationMethodAzureBlob = "clientCert"
)

func (e CreateOutputAuthenticationMethodAzureBlob) ToPointer() *CreateOutputAuthenticationMethodAzureBlob {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationMethodAzureBlob) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret", "clientSecret", "clientCert":
			return true
		}
	}
	return false
}

type BlobAccessTier string

const (
	// BlobAccessTierInferred Default account access tier
	BlobAccessTierInferred BlobAccessTier = "Inferred"
	// BlobAccessTierHot Hot tier
	BlobAccessTierHot BlobAccessTier = "Hot"
	// BlobAccessTierCool Cool tier
	BlobAccessTierCool BlobAccessTier = "Cool"
	// BlobAccessTierCold Cold tier
	BlobAccessTierCold BlobAccessTier = "Cold"
	// BlobAccessTierArchive Archive tier
	BlobAccessTierArchive BlobAccessTier = "Archive"
)

func (e BlobAccessTier) ToPointer() *BlobAccessTier {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BlobAccessTier) IsExact() bool {
	if e != nil {
		switch *e {
		case "Inferred", "Hot", "Cool", "Cold", "Archive":
			return true
		}
	}
	return false
}

// CreateOutputCompressionAzureBlob - Data compression format to apply to HTTP content before it is delivered
type CreateOutputCompressionAzureBlob string

const (
	CreateOutputCompressionAzureBlobNone CreateOutputCompressionAzureBlob = "none"
	CreateOutputCompressionAzureBlobGzip CreateOutputCompressionAzureBlob = "gzip"
)

func (e CreateOutputCompressionAzureBlob) ToPointer() *CreateOutputCompressionAzureBlob {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputCompressionAzureBlob) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// CompressionLevelAzureBlob - Compression level to apply before moving files to final destination
type CompressionLevelAzureBlob string

const (
	// CompressionLevelAzureBlobBestSpeed Best Speed
	CompressionLevelAzureBlobBestSpeed CompressionLevelAzureBlob = "best_speed"
	// CompressionLevelAzureBlobNormal Normal
	CompressionLevelAzureBlobNormal CompressionLevelAzureBlob = "normal"
	// CompressionLevelAzureBlobBestCompression Best Compression
	CompressionLevelAzureBlobBestCompression CompressionLevelAzureBlob = "best_compression"
)

func (e CompressionLevelAzureBlob) ToPointer() *CompressionLevelAzureBlob {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionLevelAzureBlob) IsExact() bool {
	if e != nil {
		switch *e {
		case "best_speed", "normal", "best_compression":
			return true
		}
	}
	return false
}

// ParquetVersionAzureBlob - Determines which data types are supported and how they are represented
type ParquetVersionAzureBlob string

const (
	// ParquetVersionAzureBlobParquet10 1.0
	ParquetVersionAzureBlobParquet10 ParquetVersionAzureBlob = "PARQUET_1_0"
	// ParquetVersionAzureBlobParquet24 2.4
	ParquetVersionAzureBlobParquet24 ParquetVersionAzureBlob = "PARQUET_2_4"
	// ParquetVersionAzureBlobParquet26 2.6
	ParquetVersionAzureBlobParquet26 ParquetVersionAzureBlob = "PARQUET_2_6"
)

func (e ParquetVersionAzureBlob) ToPointer() *ParquetVersionAzureBlob {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ParquetVersionAzureBlob) IsExact() bool {
	if e != nil {
		switch *e {
		case "PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6":
			return true
		}
	}
	return false
}

// DataPageVersionAzureBlob - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionAzureBlob string

const (
	// DataPageVersionAzureBlobDataPageV1 V1
	DataPageVersionAzureBlobDataPageV1 DataPageVersionAzureBlob = "DATA_PAGE_V1"
	// DataPageVersionAzureBlobDataPageV2 V2
	DataPageVersionAzureBlobDataPageV2 DataPageVersionAzureBlob = "DATA_PAGE_V2"
)

func (e DataPageVersionAzureBlob) ToPointer() *DataPageVersionAzureBlob {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataPageVersionAzureBlob) IsExact() bool {
	if e != nil {
		switch *e {
		case "DATA_PAGE_V1", "DATA_PAGE_V2":
			return true
		}
	}
	return false
}

type KeyValueMetadatumAzureBlob struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumAzureBlob) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumAzureBlob) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumAzureBlob) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumAzureBlob) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type CreateOutputCertificateAzureBlob struct {
	// The certificate you registered as credentials for your app in the Azure portal
	CertificateName string `json:"certificateName"`
}

func (c CreateOutputCertificateAzureBlob) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputCertificateAzureBlob) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"certificateName"}); err != nil {
		return err
	}
	return nil
}

func (c *CreateOutputCertificateAzureBlob) GetCertificateName() string {
	if c == nil {
		return ""
	}
	return c.CertificateName
}

type OutputAzureBlob struct {
	// Unique ID for this output
	ID   string                    `json:"id"`
	Type CreateOutputTypeAzureBlob `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env["CRIBL_WORKER_ID"]}`.
	ContainerName string `json:"containerName"`
	// Create the configured container in Azure Blob Storage if it does not already exist
	CreateContainer *bool `default:"false" json:"createContainer"`
	// Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`.
	DestPath *string `json:"destPath,omitempty"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Maximum number of parts to upload in parallel per file
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatAzureBlob `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureBlob `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionAzureBlob `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool                                      `default:"false" json:"forceCloseOnShutdown"`
	AuthType             *CreateOutputAuthenticationMethodAzureBlob `default:"manual" json:"authType"`
	StorageClass         *BlobAccessTier                            `default:"Inferred" json:"storageClass"`
	Description          *string                                    `json:"description,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CreateOutputCompressionAzureBlob `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelAzureBlob `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionAzureBlob `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionAzureBlob `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumAzureBlob `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
	// Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.
	ConnectionString *string `json:"connectionString,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The name of your Azure storage account
	StorageAccountName *string `json:"storageAccountName,omitempty"`
	// The service principal's tenant ID
	TenantID *string `json:"tenantId,omitempty"`
	// The service principal's client ID
	ClientID *string `json:"clientId,omitempty"`
	// The Azure cloud to use. Defaults to Azure Public Cloud.
	AzureCloud *string `json:"azureCloud,omitempty"`
	// Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.
	EndpointSuffix *string `json:"endpointSuffix,omitempty"`
	// Select or create a stored text secret
	ClientTextSecret *string                           `json:"clientTextSecret,omitempty"`
	Certificate      *CreateOutputCertificateAzureBlob `json:"certificate,omitempty"`
}

func (o OutputAzureBlob) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureBlob) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "containerName"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureBlob) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputAzureBlob) GetType() CreateOutputTypeAzureBlob {
	if o == nil {
		return CreateOutputTypeAzureBlob("")
	}
	return o.Type
}

func (o *OutputAzureBlob) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureBlob) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureBlob) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureBlob) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureBlob) GetContainerName() string {
	if o == nil {
		return ""
	}
	return o.ContainerName
}

func (o *OutputAzureBlob) GetCreateContainer() *bool {
	if o == nil {
		return nil
	}
	return o.CreateContainer
}

func (o *OutputAzureBlob) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputAzureBlob) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputAzureBlob) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputAzureBlob) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputAzureBlob) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputAzureBlob) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputAzureBlob) GetFormat() *DataFormatAzureBlob {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureBlob) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputAzureBlob) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputAzureBlob) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputAzureBlob) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputAzureBlob) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputAzureBlob) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputAzureBlob) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputAzureBlob) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputAzureBlob) GetOnBackpressure() *BackpressureBehaviorAzureBlob {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureBlob) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputAzureBlob) GetOnDiskFullBackpressure() *DiskSpaceProtectionAzureBlob {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputAzureBlob) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputAzureBlob) GetAuthType() *CreateOutputAuthenticationMethodAzureBlob {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputAzureBlob) GetStorageClass() *BlobAccessTier {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputAzureBlob) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureBlob) GetCompress() *CreateOutputCompressionAzureBlob {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureBlob) GetCompressionLevel() *CompressionLevelAzureBlob {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputAzureBlob) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputAzureBlob) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputAzureBlob) GetParquetVersion() *ParquetVersionAzureBlob {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputAzureBlob) GetParquetDataPageVersion() *DataPageVersionAzureBlob {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputAzureBlob) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputAzureBlob) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputAzureBlob) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputAzureBlob) GetKeyValueMetadata() []KeyValueMetadatumAzureBlob {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputAzureBlob) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputAzureBlob) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputAzureBlob) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputAzureBlob) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputAzureBlob) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputAzureBlob) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputAzureBlob) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputAzureBlob) GetConnectionString() *string {
	if o == nil {
		return nil
	}
	return o.ConnectionString
}

func (o *OutputAzureBlob) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputAzureBlob) GetStorageAccountName() *string {
	if o == nil {
		return nil
	}
	return o.StorageAccountName
}

func (o *OutputAzureBlob) GetTenantID() *string {
	if o == nil {
		return nil
	}
	return o.TenantID
}

func (o *OutputAzureBlob) GetClientID() *string {
	if o == nil {
		return nil
	}
	return o.ClientID
}

func (o *OutputAzureBlob) GetAzureCloud() *string {
	if o == nil {
		return nil
	}
	return o.AzureCloud
}

func (o *OutputAzureBlob) GetEndpointSuffix() *string {
	if o == nil {
		return nil
	}
	return o.EndpointSuffix
}

func (o *OutputAzureBlob) GetClientTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.ClientTextSecret
}

func (o *OutputAzureBlob) GetCertificate() *CreateOutputCertificateAzureBlob {
	if o == nil {
		return nil
	}
	return o.Certificate
}

type CreateOutputTypeS3 string

const (
	CreateOutputTypeS3S3 CreateOutputTypeS3 = "s3"
)

func (e CreateOutputTypeS3) ToPointer() *CreateOutputTypeS3 {
	return &e
}
func (e *CreateOutputTypeS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "s3":
		*e = CreateOutputTypeS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeS3: %v", v)
	}
}

// CreateOutputAuthenticationMethodS3 - AWS authentication method. Choose Auto to use IAM roles.
type CreateOutputAuthenticationMethodS3 string

const (
	// CreateOutputAuthenticationMethodS3Auto Auto
	CreateOutputAuthenticationMethodS3Auto CreateOutputAuthenticationMethodS3 = "auto"
	// CreateOutputAuthenticationMethodS3Manual Manual
	CreateOutputAuthenticationMethodS3Manual CreateOutputAuthenticationMethodS3 = "manual"
	// CreateOutputAuthenticationMethodS3Secret Secret Key pair
	CreateOutputAuthenticationMethodS3Secret CreateOutputAuthenticationMethodS3 = "secret"
)

func (e CreateOutputAuthenticationMethodS3) ToPointer() *CreateOutputAuthenticationMethodS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationMethodS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "auto", "manual", "secret":
			return true
		}
	}
	return false
}

// CreateOutputSignatureVersionS3 - Signature version to use for signing S3 requests
type CreateOutputSignatureVersionS3 string

const (
	CreateOutputSignatureVersionS3V2 CreateOutputSignatureVersionS3 = "v2"
	CreateOutputSignatureVersionS3V4 CreateOutputSignatureVersionS3 = "v4"
)

func (e CreateOutputSignatureVersionS3) ToPointer() *CreateOutputSignatureVersionS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputSignatureVersionS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "v2", "v4":
			return true
		}
	}
	return false
}

// ObjectACLS3 - Object ACL to assign to uploaded objects
type ObjectACLS3 string

const (
	// ObjectACLS3Private Private
	ObjectACLS3Private ObjectACLS3 = "private"
	// ObjectACLS3PublicRead Public Read Only
	ObjectACLS3PublicRead ObjectACLS3 = "public-read"
	// ObjectACLS3PublicReadWrite Public Read/Write
	ObjectACLS3PublicReadWrite ObjectACLS3 = "public-read-write"
	// ObjectACLS3AuthenticatedRead Authenticated Read Only
	ObjectACLS3AuthenticatedRead ObjectACLS3 = "authenticated-read"
	// ObjectACLS3AwsExecRead AWS EC2 AMI Read Only
	ObjectACLS3AwsExecRead ObjectACLS3 = "aws-exec-read"
	// ObjectACLS3BucketOwnerRead Bucket Owner Read Only
	ObjectACLS3BucketOwnerRead ObjectACLS3 = "bucket-owner-read"
	// ObjectACLS3BucketOwnerFullControl Bucket Owner Full Control
	ObjectACLS3BucketOwnerFullControl ObjectACLS3 = "bucket-owner-full-control"
)

func (e ObjectACLS3) ToPointer() *ObjectACLS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ObjectACLS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "private", "public-read", "public-read-write", "authenticated-read", "aws-exec-read", "bucket-owner-read", "bucket-owner-full-control":
			return true
		}
	}
	return false
}

// StorageClassS3 - Storage class to select for uploaded objects
type StorageClassS3 string

const (
	// StorageClassS3Standard Standard
	StorageClassS3Standard StorageClassS3 = "STANDARD"
	// StorageClassS3ReducedRedundancy Reduced Redundancy Storage
	StorageClassS3ReducedRedundancy StorageClassS3 = "REDUCED_REDUNDANCY"
	// StorageClassS3StandardIa Standard, Infrequent Access
	StorageClassS3StandardIa StorageClassS3 = "STANDARD_IA"
	// StorageClassS3OnezoneIa One Zone, Infrequent Access
	StorageClassS3OnezoneIa StorageClassS3 = "ONEZONE_IA"
	// StorageClassS3IntelligentTiering Intelligent Tiering
	StorageClassS3IntelligentTiering StorageClassS3 = "INTELLIGENT_TIERING"
	// StorageClassS3Glacier Glacier Flexible Retrieval
	StorageClassS3Glacier StorageClassS3 = "GLACIER"
	// StorageClassS3GlacierIr Glacier Instant Retrieval
	StorageClassS3GlacierIr StorageClassS3 = "GLACIER_IR"
	// StorageClassS3DeepArchive Glacier Deep Archive
	StorageClassS3DeepArchive StorageClassS3 = "DEEP_ARCHIVE"
)

func (e StorageClassS3) ToPointer() *StorageClassS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *StorageClassS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "STANDARD", "REDUCED_REDUNDANCY", "STANDARD_IA", "ONEZONE_IA", "INTELLIGENT_TIERING", "GLACIER", "GLACIER_IR", "DEEP_ARCHIVE":
			return true
		}
	}
	return false
}

type ServerSideEncryptionForUploadedObjectsS3 string

const (
	// ServerSideEncryptionForUploadedObjectsS3Aes256 Amazon S3 Managed Key
	ServerSideEncryptionForUploadedObjectsS3Aes256 ServerSideEncryptionForUploadedObjectsS3 = "AES256"
	// ServerSideEncryptionForUploadedObjectsS3AwsKms AWS KMS Managed Key
	ServerSideEncryptionForUploadedObjectsS3AwsKms ServerSideEncryptionForUploadedObjectsS3 = "aws:kms"
)

func (e ServerSideEncryptionForUploadedObjectsS3) ToPointer() *ServerSideEncryptionForUploadedObjectsS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ServerSideEncryptionForUploadedObjectsS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "AES256", "aws:kms":
			return true
		}
	}
	return false
}

// DataFormatS3 - Format of the output data
type DataFormatS3 string

const (
	// DataFormatS3JSON JSON
	DataFormatS3JSON DataFormatS3 = "json"
	// DataFormatS3Raw Raw
	DataFormatS3Raw DataFormatS3 = "raw"
	// DataFormatS3Parquet Parquet
	DataFormatS3Parquet DataFormatS3 = "parquet"
)

func (e DataFormatS3) ToPointer() *DataFormatS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataFormatS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "parquet":
			return true
		}
	}
	return false
}

// BackpressureBehaviorS3 - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorS3 string

const (
	// BackpressureBehaviorS3Block Block
	BackpressureBehaviorS3Block BackpressureBehaviorS3 = "block"
	// BackpressureBehaviorS3Drop Drop
	BackpressureBehaviorS3Drop BackpressureBehaviorS3 = "drop"
)

func (e BackpressureBehaviorS3) ToPointer() *BackpressureBehaviorS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionS3 - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionS3 string

const (
	// DiskSpaceProtectionS3Block Block
	DiskSpaceProtectionS3Block DiskSpaceProtectionS3 = "block"
	// DiskSpaceProtectionS3Drop Drop
	DiskSpaceProtectionS3Drop DiskSpaceProtectionS3 = "drop"
)

func (e DiskSpaceProtectionS3) ToPointer() *DiskSpaceProtectionS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// CreateOutputCompressionS3 - Data compression format to apply to HTTP content before it is delivered
type CreateOutputCompressionS3 string

const (
	CreateOutputCompressionS3None CreateOutputCompressionS3 = "none"
	CreateOutputCompressionS3Gzip CreateOutputCompressionS3 = "gzip"
)

func (e CreateOutputCompressionS3) ToPointer() *CreateOutputCompressionS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputCompressionS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// CompressionLevelS3 - Compression level to apply before moving files to final destination
type CompressionLevelS3 string

const (
	// CompressionLevelS3BestSpeed Best Speed
	CompressionLevelS3BestSpeed CompressionLevelS3 = "best_speed"
	// CompressionLevelS3Normal Normal
	CompressionLevelS3Normal CompressionLevelS3 = "normal"
	// CompressionLevelS3BestCompression Best Compression
	CompressionLevelS3BestCompression CompressionLevelS3 = "best_compression"
)

func (e CompressionLevelS3) ToPointer() *CompressionLevelS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionLevelS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "best_speed", "normal", "best_compression":
			return true
		}
	}
	return false
}

// ParquetVersionS3 - Determines which data types are supported and how they are represented
type ParquetVersionS3 string

const (
	// ParquetVersionS3Parquet10 1.0
	ParquetVersionS3Parquet10 ParquetVersionS3 = "PARQUET_1_0"
	// ParquetVersionS3Parquet24 2.4
	ParquetVersionS3Parquet24 ParquetVersionS3 = "PARQUET_2_4"
	// ParquetVersionS3Parquet26 2.6
	ParquetVersionS3Parquet26 ParquetVersionS3 = "PARQUET_2_6"
)

func (e ParquetVersionS3) ToPointer() *ParquetVersionS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ParquetVersionS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6":
			return true
		}
	}
	return false
}

// DataPageVersionS3 - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionS3 string

const (
	// DataPageVersionS3DataPageV1 V1
	DataPageVersionS3DataPageV1 DataPageVersionS3 = "DATA_PAGE_V1"
	// DataPageVersionS3DataPageV2 V2
	DataPageVersionS3DataPageV2 DataPageVersionS3 = "DATA_PAGE_V2"
)

func (e DataPageVersionS3) ToPointer() *DataPageVersionS3 {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataPageVersionS3) IsExact() bool {
	if e != nil {
		switch *e {
		case "DATA_PAGE_V1", "DATA_PAGE_V2":
			return true
		}
	}
	return false
}

type KeyValueMetadatumS3 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumS3) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumS3) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputS3 struct {
	// Unique ID for this output
	ID   string             `json:"id"`
	Type CreateOutputTypeS3 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// Region where the S3 bucket is located
	Region *string `json:"region,omitempty"`
	// Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *CreateOutputAuthenticationMethodS3 `default:"auto" json:"awsAuthenticationMethod"`
	// S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing S3 requests
	SignatureVersion *CreateOutputSignatureVersionS3 `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`
	DestPath *string `default:"" json:"destPath"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLS3 `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass         *StorageClassS3                           `json:"storageClass,omitempty"`
	ServerSideEncryption *ServerSideEncryptionForUploadedObjectsS3 `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatS3 `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorS3 `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionS3 `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	Description                   *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CreateOutputCompressionS3 `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelS3 `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionS3 `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionS3 `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumS3 `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "bucket"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputS3) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputS3) GetType() CreateOutputTypeS3 {
	if o == nil {
		return CreateOutputTypeS3("")
	}
	return o.Type
}

func (o *OutputS3) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputS3) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputS3) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputS3) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputS3) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputS3) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputS3) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputS3) GetAwsAuthenticationMethod() *CreateOutputAuthenticationMethodS3 {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputS3) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputS3) GetSignatureVersion() *CreateOutputSignatureVersionS3 {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputS3) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputS3) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputS3) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputS3) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputS3) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputS3) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputS3) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputS3) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputS3) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputS3) GetObjectACL() *ObjectACLS3 {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputS3) GetStorageClass() *StorageClassS3 {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputS3) GetServerSideEncryption() *ServerSideEncryptionForUploadedObjectsS3 {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputS3) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputS3) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputS3) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputS3) GetFormat() *DataFormatS3 {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputS3) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputS3) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputS3) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputS3) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputS3) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputS3) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputS3) GetOnBackpressure() *BackpressureBehaviorS3 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputS3) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputS3) GetOnDiskFullBackpressure() *DiskSpaceProtectionS3 {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputS3) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputS3) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputS3) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputS3) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputS3) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputS3) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputS3) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputS3) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputS3) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputS3) GetCompress() *CreateOutputCompressionS3 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputS3) GetCompressionLevel() *CompressionLevelS3 {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputS3) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputS3) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputS3) GetParquetVersion() *ParquetVersionS3 {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputS3) GetParquetDataPageVersion() *DataPageVersionS3 {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputS3) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputS3) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputS3) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputS3) GetKeyValueMetadata() []KeyValueMetadatumS3 {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputS3) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputS3) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputS3) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputS3) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputS3) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputS3) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputS3) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeFilesystem string

const (
	TypeFilesystemFilesystem TypeFilesystem = "filesystem"
)

func (e TypeFilesystem) ToPointer() *TypeFilesystem {
	return &e
}
func (e *TypeFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "filesystem":
		*e = TypeFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeFilesystem: %v", v)
	}
}

// DataFormatFilesystem - Format of the output data
type DataFormatFilesystem string

const (
	// DataFormatFilesystemJSON JSON
	DataFormatFilesystemJSON DataFormatFilesystem = "json"
	// DataFormatFilesystemRaw Raw
	DataFormatFilesystemRaw DataFormatFilesystem = "raw"
	// DataFormatFilesystemParquet Parquet
	DataFormatFilesystemParquet DataFormatFilesystem = "parquet"
)

func (e DataFormatFilesystem) ToPointer() *DataFormatFilesystem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataFormatFilesystem) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "raw", "parquet":
			return true
		}
	}
	return false
}

// BackpressureBehaviorFilesystem - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorFilesystem string

const (
	// BackpressureBehaviorFilesystemBlock Block
	BackpressureBehaviorFilesystemBlock BackpressureBehaviorFilesystem = "block"
	// BackpressureBehaviorFilesystemDrop Drop
	BackpressureBehaviorFilesystemDrop BackpressureBehaviorFilesystem = "drop"
)

func (e BackpressureBehaviorFilesystem) ToPointer() *BackpressureBehaviorFilesystem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorFilesystem) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// DiskSpaceProtectionFilesystem - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionFilesystem string

const (
	// DiskSpaceProtectionFilesystemBlock Block
	DiskSpaceProtectionFilesystemBlock DiskSpaceProtectionFilesystem = "block"
	// DiskSpaceProtectionFilesystemDrop Drop
	DiskSpaceProtectionFilesystemDrop DiskSpaceProtectionFilesystem = "drop"
)

func (e DiskSpaceProtectionFilesystem) ToPointer() *DiskSpaceProtectionFilesystem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DiskSpaceProtectionFilesystem) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

// CompressionFilesystem - Data compression format to apply to HTTP content before it is delivered
type CompressionFilesystem string

const (
	CompressionFilesystemNone CompressionFilesystem = "none"
	CompressionFilesystemGzip CompressionFilesystem = "gzip"
)

func (e CompressionFilesystem) ToPointer() *CompressionFilesystem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionFilesystem) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// CompressionLevelFilesystem - Compression level to apply before moving files to final destination
type CompressionLevelFilesystem string

const (
	// CompressionLevelFilesystemBestSpeed Best Speed
	CompressionLevelFilesystemBestSpeed CompressionLevelFilesystem = "best_speed"
	// CompressionLevelFilesystemNormal Normal
	CompressionLevelFilesystemNormal CompressionLevelFilesystem = "normal"
	// CompressionLevelFilesystemBestCompression Best Compression
	CompressionLevelFilesystemBestCompression CompressionLevelFilesystem = "best_compression"
)

func (e CompressionLevelFilesystem) ToPointer() *CompressionLevelFilesystem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionLevelFilesystem) IsExact() bool {
	if e != nil {
		switch *e {
		case "best_speed", "normal", "best_compression":
			return true
		}
	}
	return false
}

// ParquetVersionFilesystem - Determines which data types are supported and how they are represented
type ParquetVersionFilesystem string

const (
	// ParquetVersionFilesystemParquet10 1.0
	ParquetVersionFilesystemParquet10 ParquetVersionFilesystem = "PARQUET_1_0"
	// ParquetVersionFilesystemParquet24 2.4
	ParquetVersionFilesystemParquet24 ParquetVersionFilesystem = "PARQUET_2_4"
	// ParquetVersionFilesystemParquet26 2.6
	ParquetVersionFilesystemParquet26 ParquetVersionFilesystem = "PARQUET_2_6"
)

func (e ParquetVersionFilesystem) ToPointer() *ParquetVersionFilesystem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ParquetVersionFilesystem) IsExact() bool {
	if e != nil {
		switch *e {
		case "PARQUET_1_0", "PARQUET_2_4", "PARQUET_2_6":
			return true
		}
	}
	return false
}

// DataPageVersionFilesystem - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionFilesystem string

const (
	// DataPageVersionFilesystemDataPageV1 V1
	DataPageVersionFilesystemDataPageV1 DataPageVersionFilesystem = "DATA_PAGE_V1"
	// DataPageVersionFilesystemDataPageV2 V2
	DataPageVersionFilesystemDataPageV2 DataPageVersionFilesystem = "DATA_PAGE_V2"
)

func (e DataPageVersionFilesystem) ToPointer() *DataPageVersionFilesystem {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *DataPageVersionFilesystem) IsExact() bool {
	if e != nil {
		switch *e {
		case "DATA_PAGE_V1", "DATA_PAGE_V2":
			return true
		}
	}
	return false
}

type KeyValueMetadatumFilesystem struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumFilesystem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumFilesystem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumFilesystem) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumFilesystem) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputFilesystem struct {
	// Unique ID for this output
	ID   string         `json:"id"`
	Type TypeFilesystem `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Final destination for the output files
	DestPath string `json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `json:"stagePath,omitempty"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatFilesystem `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorFilesystem `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionFilesystem `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool   `default:"false" json:"forceCloseOnShutdown"`
	Description          *string `json:"description,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionFilesystem `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelFilesystem `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionFilesystem `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionFilesystem `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumFilesystem `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputFilesystem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputFilesystem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "destPath"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputFilesystem) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputFilesystem) GetType() TypeFilesystem {
	if o == nil {
		return TypeFilesystem("")
	}
	return o.Type
}

func (o *OutputFilesystem) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputFilesystem) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputFilesystem) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputFilesystem) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputFilesystem) GetDestPath() string {
	if o == nil {
		return ""
	}
	return o.DestPath
}

func (o *OutputFilesystem) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputFilesystem) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputFilesystem) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputFilesystem) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputFilesystem) GetFormat() *DataFormatFilesystem {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputFilesystem) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputFilesystem) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputFilesystem) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputFilesystem) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputFilesystem) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputFilesystem) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputFilesystem) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputFilesystem) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputFilesystem) GetOnBackpressure() *BackpressureBehaviorFilesystem {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputFilesystem) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputFilesystem) GetOnDiskFullBackpressure() *DiskSpaceProtectionFilesystem {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputFilesystem) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputFilesystem) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputFilesystem) GetCompress() *CompressionFilesystem {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputFilesystem) GetCompressionLevel() *CompressionLevelFilesystem {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputFilesystem) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputFilesystem) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputFilesystem) GetParquetVersion() *ParquetVersionFilesystem {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputFilesystem) GetParquetDataPageVersion() *DataPageVersionFilesystem {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputFilesystem) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputFilesystem) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputFilesystem) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputFilesystem) GetKeyValueMetadata() []KeyValueMetadatumFilesystem {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputFilesystem) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputFilesystem) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputFilesystem) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputFilesystem) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputFilesystem) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputFilesystem) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputFilesystem) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type TypeSignalfx string

const (
	TypeSignalfxSignalfx TypeSignalfx = "signalfx"
)

func (e TypeSignalfx) ToPointer() *TypeSignalfx {
	return &e
}
func (e *TypeSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "signalfx":
		*e = TypeSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSignalfx: %v", v)
	}
}

// AuthenticationMethodSignalfx - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSignalfx string

const (
	AuthenticationMethodSignalfxManual AuthenticationMethodSignalfx = "manual"
	AuthenticationMethodSignalfxSecret AuthenticationMethodSignalfx = "secret"
)

func (e AuthenticationMethodSignalfx) ToPointer() *AuthenticationMethodSignalfx {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodSignalfx) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderSignalfx struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderSignalfx) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderSignalfx) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeSignalfx - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSignalfx string

const (
	// FailedRequestLoggingModeSignalfxPayload Payload
	FailedRequestLoggingModeSignalfxPayload FailedRequestLoggingModeSignalfx = "payload"
	// FailedRequestLoggingModeSignalfxPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeSignalfxPayloadAndHeaders FailedRequestLoggingModeSignalfx = "payloadAndHeaders"
	// FailedRequestLoggingModeSignalfxNone None
	FailedRequestLoggingModeSignalfxNone FailedRequestLoggingModeSignalfx = "none"
)

func (e FailedRequestLoggingModeSignalfx) ToPointer() *FailedRequestLoggingModeSignalfx {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeSignalfx) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingSignalfx struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingSignalfx) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingSignalfx) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingSignalfx) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingSignalfx) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsSignalfx struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsSignalfx) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsSignalfx) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsSignalfx) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsSignalfx) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorSignalfx - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSignalfx string

const (
	// BackpressureBehaviorSignalfxBlock Block
	BackpressureBehaviorSignalfxBlock BackpressureBehaviorSignalfx = "block"
	// BackpressureBehaviorSignalfxDrop Drop
	BackpressureBehaviorSignalfxDrop BackpressureBehaviorSignalfx = "drop"
	// BackpressureBehaviorSignalfxQueue Persistent Queue
	BackpressureBehaviorSignalfxQueue BackpressureBehaviorSignalfx = "queue"
)

func (e BackpressureBehaviorSignalfx) ToPointer() *BackpressureBehaviorSignalfx {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorSignalfx) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeSignalfx - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSignalfx string

const (
	// ModeSignalfxError Error
	ModeSignalfxError ModeSignalfx = "error"
	// ModeSignalfxAlways Backpressure
	ModeSignalfxAlways ModeSignalfx = "always"
	// ModeSignalfxBackpressure Always On
	ModeSignalfxBackpressure ModeSignalfx = "backpressure"
)

func (e ModeSignalfx) ToPointer() *ModeSignalfx {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeSignalfx) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionSignalfx - Codec to use to compress the persisted data
type CompressionSignalfx string

const (
	// CompressionSignalfxNone None
	CompressionSignalfxNone CompressionSignalfx = "none"
	// CompressionSignalfxGzip Gzip
	CompressionSignalfxGzip CompressionSignalfx = "gzip"
)

func (e CompressionSignalfx) ToPointer() *CompressionSignalfx {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionSignalfx) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorSignalfx - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSignalfx string

const (
	// QueueFullBehaviorSignalfxBlock Block
	QueueFullBehaviorSignalfxBlock QueueFullBehaviorSignalfx = "block"
	// QueueFullBehaviorSignalfxDrop Drop new data
	QueueFullBehaviorSignalfxDrop QueueFullBehaviorSignalfx = "drop"
)

func (e QueueFullBehaviorSignalfx) ToPointer() *QueueFullBehaviorSignalfx {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorSignalfx) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsSignalfx struct {
}

func (p PqControlsSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSignalfx struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type TypeSignalfx `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodSignalfx `default:"manual" json:"authType"`
	// SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions).
	Realm *string `default:"us0" json:"realm"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderSignalfx `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSignalfx `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSignalfx `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSignalfx  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSignalfx `default:"block" json:"onBackpressure"`
	Description    *string                       `json:"description,omitempty"`
	// SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSignalfx `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSignalfx `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSignalfx `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsSignalfx        `json:"pqControls,omitempty"`
}

func (o OutputSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSignalfx) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSignalfx) GetType() TypeSignalfx {
	if o == nil {
		return TypeSignalfx("")
	}
	return o.Type
}

func (o *OutputSignalfx) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSignalfx) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSignalfx) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSignalfx) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSignalfx) GetAuthType() *AuthenticationMethodSignalfx {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSignalfx) GetRealm() *string {
	if o == nil {
		return nil
	}
	return o.Realm
}

func (o *OutputSignalfx) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSignalfx) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSignalfx) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSignalfx) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSignalfx) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSignalfx) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSignalfx) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSignalfx) GetExtraHTTPHeaders() []ExtraHTTPHeaderSignalfx {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSignalfx) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSignalfx) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSignalfx {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSignalfx) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSignalfx) GetResponseRetrySettings() []ResponseRetrySettingSignalfx {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSignalfx) GetTimeoutRetrySettings() *TimeoutRetrySettingsSignalfx {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSignalfx) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSignalfx) GetOnBackpressure() *BackpressureBehaviorSignalfx {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSignalfx) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSignalfx) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputSignalfx) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSignalfx) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSignalfx) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSignalfx) GetPqMode() *ModeSignalfx {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSignalfx) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSignalfx) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSignalfx) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSignalfx) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSignalfx) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSignalfx) GetPqCompress() *CompressionSignalfx {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSignalfx) GetPqOnBackpressure() *QueueFullBehaviorSignalfx {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSignalfx) GetPqControls() *PqControlsSignalfx {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeWavefront string

const (
	TypeWavefrontWavefront TypeWavefront = "wavefront"
)

func (e TypeWavefront) ToPointer() *TypeWavefront {
	return &e
}
func (e *TypeWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "wavefront":
		*e = TypeWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeWavefront: %v", v)
	}
}

// AuthenticationMethodWavefront - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodWavefront string

const (
	AuthenticationMethodWavefrontManual AuthenticationMethodWavefront = "manual"
	AuthenticationMethodWavefrontSecret AuthenticationMethodWavefront = "secret"
)

func (e AuthenticationMethodWavefront) ToPointer() *AuthenticationMethodWavefront {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodWavefront) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderWavefront struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderWavefront) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderWavefront) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeWavefront - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeWavefront string

const (
	// FailedRequestLoggingModeWavefrontPayload Payload
	FailedRequestLoggingModeWavefrontPayload FailedRequestLoggingModeWavefront = "payload"
	// FailedRequestLoggingModeWavefrontPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeWavefrontPayloadAndHeaders FailedRequestLoggingModeWavefront = "payloadAndHeaders"
	// FailedRequestLoggingModeWavefrontNone None
	FailedRequestLoggingModeWavefrontNone FailedRequestLoggingModeWavefront = "none"
)

func (e FailedRequestLoggingModeWavefront) ToPointer() *FailedRequestLoggingModeWavefront {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeWavefront) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingWavefront struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingWavefront) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingWavefront) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingWavefront) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingWavefront) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsWavefront struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsWavefront) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsWavefront) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsWavefront) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsWavefront) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorWavefront - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorWavefront string

const (
	// BackpressureBehaviorWavefrontBlock Block
	BackpressureBehaviorWavefrontBlock BackpressureBehaviorWavefront = "block"
	// BackpressureBehaviorWavefrontDrop Drop
	BackpressureBehaviorWavefrontDrop BackpressureBehaviorWavefront = "drop"
	// BackpressureBehaviorWavefrontQueue Persistent Queue
	BackpressureBehaviorWavefrontQueue BackpressureBehaviorWavefront = "queue"
)

func (e BackpressureBehaviorWavefront) ToPointer() *BackpressureBehaviorWavefront {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorWavefront) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeWavefront - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeWavefront string

const (
	// ModeWavefrontError Error
	ModeWavefrontError ModeWavefront = "error"
	// ModeWavefrontAlways Backpressure
	ModeWavefrontAlways ModeWavefront = "always"
	// ModeWavefrontBackpressure Always On
	ModeWavefrontBackpressure ModeWavefront = "backpressure"
)

func (e ModeWavefront) ToPointer() *ModeWavefront {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeWavefront) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionWavefront - Codec to use to compress the persisted data
type CompressionWavefront string

const (
	// CompressionWavefrontNone None
	CompressionWavefrontNone CompressionWavefront = "none"
	// CompressionWavefrontGzip Gzip
	CompressionWavefrontGzip CompressionWavefront = "gzip"
)

func (e CompressionWavefront) ToPointer() *CompressionWavefront {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionWavefront) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorWavefront - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorWavefront string

const (
	// QueueFullBehaviorWavefrontBlock Block
	QueueFullBehaviorWavefrontBlock QueueFullBehaviorWavefront = "block"
	// QueueFullBehaviorWavefrontDrop Drop new data
	QueueFullBehaviorWavefrontDrop QueueFullBehaviorWavefront = "drop"
)

func (e QueueFullBehaviorWavefront) ToPointer() *QueueFullBehaviorWavefront {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorWavefront) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsWavefront struct {
}

func (p PqControlsWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputWavefront struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeWavefront `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodWavefront `default:"manual" json:"authType"`
	// WaveFront domain name, e.g. "longboard"
	Domain *string `default:"longboard" json:"domain"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderWavefront `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeWavefront `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingWavefront `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsWavefront  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorWavefront `default:"block" json:"onBackpressure"`
	Description    *string                        `json:"description,omitempty"`
	// WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeWavefront `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionWavefront `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorWavefront `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsWavefront        `json:"pqControls,omitempty"`
}

func (o OutputWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputWavefront) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputWavefront) GetType() TypeWavefront {
	if o == nil {
		return TypeWavefront("")
	}
	return o.Type
}

func (o *OutputWavefront) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputWavefront) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputWavefront) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputWavefront) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputWavefront) GetAuthType() *AuthenticationMethodWavefront {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputWavefront) GetDomain() *string {
	if o == nil {
		return nil
	}
	return o.Domain
}

func (o *OutputWavefront) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputWavefront) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputWavefront) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputWavefront) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputWavefront) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputWavefront) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputWavefront) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputWavefront) GetExtraHTTPHeaders() []ExtraHTTPHeaderWavefront {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputWavefront) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputWavefront) GetFailedRequestLoggingMode() *FailedRequestLoggingModeWavefront {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputWavefront) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputWavefront) GetResponseRetrySettings() []ResponseRetrySettingWavefront {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputWavefront) GetTimeoutRetrySettings() *TimeoutRetrySettingsWavefront {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputWavefront) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputWavefront) GetOnBackpressure() *BackpressureBehaviorWavefront {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputWavefront) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputWavefront) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputWavefront) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputWavefront) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputWavefront) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputWavefront) GetPqMode() *ModeWavefront {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputWavefront) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputWavefront) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputWavefront) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputWavefront) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputWavefront) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputWavefront) GetPqCompress() *CompressionWavefront {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputWavefront) GetPqOnBackpressure() *QueueFullBehaviorWavefront {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputWavefront) GetPqControls() *PqControlsWavefront {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type CreateOutputTypeTcpjson string

const (
	CreateOutputTypeTcpjsonTcpjson CreateOutputTypeTcpjson = "tcpjson"
)

func (e CreateOutputTypeTcpjson) ToPointer() *CreateOutputTypeTcpjson {
	return &e
}
func (e *CreateOutputTypeTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "tcpjson":
		*e = CreateOutputTypeTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeTcpjson: %v", v)
	}
}

// CreateOutputCompressionTcpjson - Codec to use to compress the data before sending
type CreateOutputCompressionTcpjson string

const (
	// CreateOutputCompressionTcpjsonNone None
	CreateOutputCompressionTcpjsonNone CreateOutputCompressionTcpjson = "none"
	// CreateOutputCompressionTcpjsonGzip Gzip
	CreateOutputCompressionTcpjsonGzip CreateOutputCompressionTcpjson = "gzip"
)

func (e CreateOutputCompressionTcpjson) ToPointer() *CreateOutputCompressionTcpjson {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputCompressionTcpjson) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

type CreateOutputMinimumTLSVersionTcpjson string

const (
	CreateOutputMinimumTLSVersionTcpjsonTlSv1  CreateOutputMinimumTLSVersionTcpjson = "TLSv1"
	CreateOutputMinimumTLSVersionTcpjsonTlSv11 CreateOutputMinimumTLSVersionTcpjson = "TLSv1.1"
	CreateOutputMinimumTLSVersionTcpjsonTlSv12 CreateOutputMinimumTLSVersionTcpjson = "TLSv1.2"
	CreateOutputMinimumTLSVersionTcpjsonTlSv13 CreateOutputMinimumTLSVersionTcpjson = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionTcpjson) ToPointer() *CreateOutputMinimumTLSVersionTcpjson {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMinimumTLSVersionTcpjson) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputMaximumTLSVersionTcpjson string

const (
	CreateOutputMaximumTLSVersionTcpjsonTlSv1  CreateOutputMaximumTLSVersionTcpjson = "TLSv1"
	CreateOutputMaximumTLSVersionTcpjsonTlSv11 CreateOutputMaximumTLSVersionTcpjson = "TLSv1.1"
	CreateOutputMaximumTLSVersionTcpjsonTlSv12 CreateOutputMaximumTLSVersionTcpjson = "TLSv1.2"
	CreateOutputMaximumTLSVersionTcpjsonTlSv13 CreateOutputMaximumTLSVersionTcpjson = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionTcpjson) ToPointer() *CreateOutputMaximumTLSVersionTcpjson {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMaximumTLSVersionTcpjson) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideTcpjson struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                               `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionTcpjson `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionTcpjson `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideTcpjson) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideTcpjson) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideTcpjson) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideTcpjson) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideTcpjson) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideTcpjson) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideTcpjson) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideTcpjson) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideTcpjson) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideTcpjson) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideTcpjson) GetMinVersion() *CreateOutputMinimumTLSVersionTcpjson {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideTcpjson) GetMaxVersion() *CreateOutputMaximumTLSVersionTcpjson {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// BackpressureBehaviorTcpjson - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorTcpjson string

const (
	// BackpressureBehaviorTcpjsonBlock Block
	BackpressureBehaviorTcpjsonBlock BackpressureBehaviorTcpjson = "block"
	// BackpressureBehaviorTcpjsonDrop Drop
	BackpressureBehaviorTcpjsonDrop BackpressureBehaviorTcpjson = "drop"
	// BackpressureBehaviorTcpjsonQueue Persistent Queue
	BackpressureBehaviorTcpjsonQueue BackpressureBehaviorTcpjson = "queue"
)

func (e BackpressureBehaviorTcpjson) ToPointer() *BackpressureBehaviorTcpjson {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorTcpjson) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// CreateOutputAuthenticationMethodTcpjson - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type CreateOutputAuthenticationMethodTcpjson string

const (
	CreateOutputAuthenticationMethodTcpjsonManual CreateOutputAuthenticationMethodTcpjson = "manual"
	CreateOutputAuthenticationMethodTcpjsonSecret CreateOutputAuthenticationMethodTcpjson = "secret"
)

func (e CreateOutputAuthenticationMethodTcpjson) ToPointer() *CreateOutputAuthenticationMethodTcpjson {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationMethodTcpjson) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

// TLSTcpjson - Whether to inherit TLS configs from group setting or disable TLS
type TLSTcpjson string

const (
	TLSTcpjsonInherit TLSTcpjson = "inherit"
	TLSTcpjsonOff     TLSTcpjson = "off"
)

func (e TLSTcpjson) ToPointer() *TLSTcpjson {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *TLSTcpjson) IsExact() bool {
	if e != nil {
		switch *e {
		case "inherit", "off":
			return true
		}
	}
	return false
}

type HostTcpjson struct {
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port float64 `json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS
	TLS *TLSTcpjson `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostTcpjson) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostTcpjson) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host", "port"}); err != nil {
		return err
	}
	return nil
}

func (h *HostTcpjson) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostTcpjson) GetPort() float64 {
	if h == nil {
		return 0.0
	}
	return h.Port
}

func (h *HostTcpjson) GetTLS() *TLSTcpjson {
	if h == nil {
		return nil
	}
	return h.TLS
}

func (h *HostTcpjson) GetServername() *string {
	if h == nil {
		return nil
	}
	return h.Servername
}

func (h *HostTcpjson) GetWeight() *float64 {
	if h == nil {
		return nil
	}
	return h.Weight
}

// CreateOutputModeTcpjson - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeTcpjson string

const (
	// CreateOutputModeTcpjsonError Error
	CreateOutputModeTcpjsonError CreateOutputModeTcpjson = "error"
	// CreateOutputModeTcpjsonAlways Backpressure
	CreateOutputModeTcpjsonAlways CreateOutputModeTcpjson = "always"
	// CreateOutputModeTcpjsonBackpressure Always On
	CreateOutputModeTcpjsonBackpressure CreateOutputModeTcpjson = "backpressure"
)

func (e CreateOutputModeTcpjson) ToPointer() *CreateOutputModeTcpjson {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeTcpjson) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionTcpjson - Codec to use to compress the persisted data
type PqCompressCompressionTcpjson string

const (
	// PqCompressCompressionTcpjsonNone None
	PqCompressCompressionTcpjsonNone PqCompressCompressionTcpjson = "none"
	// PqCompressCompressionTcpjsonGzip Gzip
	PqCompressCompressionTcpjsonGzip PqCompressCompressionTcpjson = "gzip"
)

func (e PqCompressCompressionTcpjson) ToPointer() *PqCompressCompressionTcpjson {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionTcpjson) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorTcpjson - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorTcpjson string

const (
	// QueueFullBehaviorTcpjsonBlock Block
	QueueFullBehaviorTcpjsonBlock QueueFullBehaviorTcpjson = "block"
	// QueueFullBehaviorTcpjsonDrop Drop new data
	QueueFullBehaviorTcpjsonDrop QueueFullBehaviorTcpjson = "drop"
)

func (e QueueFullBehaviorTcpjson) ToPointer() *QueueFullBehaviorTcpjson {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorTcpjson) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsTcpjson struct {
}

func (c CreateOutputPqControlsTcpjson) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsTcpjson) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputTcpjson struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type CreateOutputTypeTcpjson `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Use load-balanced destinations
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// Codec to use to compress the data before sending
	Compression *CreateOutputCompressionTcpjson `default:"gzip" json:"compression"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string                       `default:"0" json:"throttleRatePerSec"`
	TLS                *TLSSettingsClientSideTcpjson `json:"tls,omitempty"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// The number of minutes before the internally generated authentication token expires, valid values between 1 and 60
	TokenTTLMinutes *float64 `default:"60" json:"tokenTTLMinutes"`
	// Upon connection, send a header-like record containing the auth token and other metadata.This record will not contain an actual event – only subsequent records will.
	SendHeader *bool `default:"true" json:"sendHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorTcpjson `default:"block" json:"onBackpressure"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType    *CreateOutputAuthenticationMethodTcpjson `default:"manual" json:"authType"`
	Description *string                                  `json:"description,omitempty"`
	// The hostname of the receiver
	Host *string `json:"host,omitempty"`
	// The port to connect to on the provided host
	Port *float64 `json:"port,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of hosts to load-balance data to
	Hosts []HostTcpjson `json:"hosts,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeTcpjson `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionTcpjson `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorTcpjson      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsTcpjson `json:"pqControls,omitempty"`
	// Optional authentication token to include as part of the connection header
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputTcpjson) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputTcpjson) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputTcpjson) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputTcpjson) GetType() CreateOutputTypeTcpjson {
	if o == nil {
		return CreateOutputTypeTcpjson("")
	}
	return o.Type
}

func (o *OutputTcpjson) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputTcpjson) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputTcpjson) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputTcpjson) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputTcpjson) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputTcpjson) GetCompression() *CreateOutputCompressionTcpjson {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputTcpjson) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputTcpjson) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputTcpjson) GetTLS() *TLSSettingsClientSideTcpjson {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputTcpjson) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputTcpjson) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputTcpjson) GetTokenTTLMinutes() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTTLMinutes
}

func (o *OutputTcpjson) GetSendHeader() *bool {
	if o == nil {
		return nil
	}
	return o.SendHeader
}

func (o *OutputTcpjson) GetOnBackpressure() *BackpressureBehaviorTcpjson {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputTcpjson) GetAuthType() *CreateOutputAuthenticationMethodTcpjson {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputTcpjson) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputTcpjson) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputTcpjson) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputTcpjson) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputTcpjson) GetHosts() []HostTcpjson {
	if o == nil {
		return nil
	}
	return o.Hosts
}

func (o *OutputTcpjson) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputTcpjson) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputTcpjson) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputTcpjson) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputTcpjson) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputTcpjson) GetPqMode() *CreateOutputModeTcpjson {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputTcpjson) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputTcpjson) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputTcpjson) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputTcpjson) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputTcpjson) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputTcpjson) GetPqCompress() *PqCompressCompressionTcpjson {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputTcpjson) GetPqOnBackpressure() *QueueFullBehaviorTcpjson {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputTcpjson) GetPqControls() *CreateOutputPqControlsTcpjson {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputTcpjson) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *OutputTcpjson) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type CreateOutputTypeSplunkHec string

const (
	CreateOutputTypeSplunkHecSplunkHec CreateOutputTypeSplunkHec = "splunk_hec"
)

func (e CreateOutputTypeSplunkHec) ToPointer() *CreateOutputTypeSplunkHec {
	return &e
}
func (e *CreateOutputTypeSplunkHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk_hec":
		*e = CreateOutputTypeSplunkHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeSplunkHec: %v", v)
	}
}

type CreateOutputMinimumTLSVersionSplunkHec string

const (
	CreateOutputMinimumTLSVersionSplunkHecTlSv1  CreateOutputMinimumTLSVersionSplunkHec = "TLSv1"
	CreateOutputMinimumTLSVersionSplunkHecTlSv11 CreateOutputMinimumTLSVersionSplunkHec = "TLSv1.1"
	CreateOutputMinimumTLSVersionSplunkHecTlSv12 CreateOutputMinimumTLSVersionSplunkHec = "TLSv1.2"
	CreateOutputMinimumTLSVersionSplunkHecTlSv13 CreateOutputMinimumTLSVersionSplunkHec = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionSplunkHec) ToPointer() *CreateOutputMinimumTLSVersionSplunkHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMinimumTLSVersionSplunkHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputMaximumTLSVersionSplunkHec string

const (
	CreateOutputMaximumTLSVersionSplunkHecTlSv1  CreateOutputMaximumTLSVersionSplunkHec = "TLSv1"
	CreateOutputMaximumTLSVersionSplunkHecTlSv11 CreateOutputMaximumTLSVersionSplunkHec = "TLSv1.1"
	CreateOutputMaximumTLSVersionSplunkHecTlSv12 CreateOutputMaximumTLSVersionSplunkHec = "TLSv1.2"
	CreateOutputMaximumTLSVersionSplunkHecTlSv13 CreateOutputMaximumTLSVersionSplunkHec = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionSplunkHec) ToPointer() *CreateOutputMaximumTLSVersionSplunkHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMaximumTLSVersionSplunkHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideSplunkHec struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                 `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionSplunkHec `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionSplunkHec `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideSplunkHec) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideSplunkHec) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideSplunkHec) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideSplunkHec) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideSplunkHec) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideSplunkHec) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideSplunkHec) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideSplunkHec) GetMinVersion() *CreateOutputMinimumTLSVersionSplunkHec {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideSplunkHec) GetMaxVersion() *CreateOutputMaximumTLSVersionSplunkHec {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

type ExtraHTTPHeaderSplunkHec struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderSplunkHec) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderSplunkHec) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeSplunkHec - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSplunkHec string

const (
	// FailedRequestLoggingModeSplunkHecPayload Payload
	FailedRequestLoggingModeSplunkHecPayload FailedRequestLoggingModeSplunkHec = "payload"
	// FailedRequestLoggingModeSplunkHecPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeSplunkHecPayloadAndHeaders FailedRequestLoggingModeSplunkHec = "payloadAndHeaders"
	// FailedRequestLoggingModeSplunkHecNone None
	FailedRequestLoggingModeSplunkHecNone FailedRequestLoggingModeSplunkHec = "none"
)

func (e FailedRequestLoggingModeSplunkHec) ToPointer() *FailedRequestLoggingModeSplunkHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeSplunkHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

// CreateOutputAuthenticationMethodSplunkHec - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type CreateOutputAuthenticationMethodSplunkHec string

const (
	CreateOutputAuthenticationMethodSplunkHecManual CreateOutputAuthenticationMethodSplunkHec = "manual"
	CreateOutputAuthenticationMethodSplunkHecSecret CreateOutputAuthenticationMethodSplunkHec = "secret"
)

func (e CreateOutputAuthenticationMethodSplunkHec) ToPointer() *CreateOutputAuthenticationMethodSplunkHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputAuthenticationMethodSplunkHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

type ResponseRetrySettingSplunkHec struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingSplunkHec) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingSplunkHec) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingSplunkHec) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingSplunkHec) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsSplunkHec struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsSplunkHec) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsSplunkHec) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsSplunkHec) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsSplunkHec) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorSplunkHec - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSplunkHec string

const (
	// BackpressureBehaviorSplunkHecBlock Block
	BackpressureBehaviorSplunkHecBlock BackpressureBehaviorSplunkHec = "block"
	// BackpressureBehaviorSplunkHecDrop Drop
	BackpressureBehaviorSplunkHecDrop BackpressureBehaviorSplunkHec = "drop"
	// BackpressureBehaviorSplunkHecQueue Persistent Queue
	BackpressureBehaviorSplunkHecQueue BackpressureBehaviorSplunkHec = "queue"
)

func (e BackpressureBehaviorSplunkHec) ToPointer() *BackpressureBehaviorSplunkHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorSplunkHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type URLSplunkHec struct {
	// URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event
	URL *string `default:"http://localhost:8088/services/collector/event" json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (u *URLSplunkHec) GetURL() *string {
	if u == nil {
		return nil
	}
	return u.URL
}

func (u *URLSplunkHec) GetWeight() *float64 {
	if u == nil {
		return nil
	}
	return u.Weight
}

// CreateOutputModeSplunkHec - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeSplunkHec string

const (
	// CreateOutputModeSplunkHecError Error
	CreateOutputModeSplunkHecError CreateOutputModeSplunkHec = "error"
	// CreateOutputModeSplunkHecAlways Backpressure
	CreateOutputModeSplunkHecAlways CreateOutputModeSplunkHec = "always"
	// CreateOutputModeSplunkHecBackpressure Always On
	CreateOutputModeSplunkHecBackpressure CreateOutputModeSplunkHec = "backpressure"
)

func (e CreateOutputModeSplunkHec) ToPointer() *CreateOutputModeSplunkHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeSplunkHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionSplunkHec - Codec to use to compress the persisted data
type PqCompressCompressionSplunkHec string

const (
	// PqCompressCompressionSplunkHecNone None
	PqCompressCompressionSplunkHecNone PqCompressCompressionSplunkHec = "none"
	// PqCompressCompressionSplunkHecGzip Gzip
	PqCompressCompressionSplunkHecGzip PqCompressCompressionSplunkHec = "gzip"
)

func (e PqCompressCompressionSplunkHec) ToPointer() *PqCompressCompressionSplunkHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionSplunkHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorSplunkHec - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSplunkHec string

const (
	// QueueFullBehaviorSplunkHecBlock Block
	QueueFullBehaviorSplunkHecBlock QueueFullBehaviorSplunkHec = "block"
	// QueueFullBehaviorSplunkHecDrop Drop new data
	QueueFullBehaviorSplunkHecDrop QueueFullBehaviorSplunkHec = "drop"
)

func (e QueueFullBehaviorSplunkHec) ToPointer() *QueueFullBehaviorSplunkHec {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorSplunkHec) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsSplunkHec struct {
}

func (c CreateOutputPqControlsSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSplunkHec struct {
	// Unique ID for this output
	ID   string                    `json:"id"`
	Type CreateOutputTypeSplunkHec `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// In the Splunk app, define which Splunk processing queue to send the events after HEC processing.
	NextQueue *string `default:"indexQueue" json:"nextQueue"`
	// In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set.
	TCPRouting *string                         `default:"nowhere" json:"tcpRouting"`
	TLS        *TLSSettingsClientSideSplunkHec `json:"tls,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderSplunkHec `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSplunkHec `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Output metrics in multiple-metric format, supported in Splunk 8.0 and above to allow multiple metrics in a single event.
	EnableMultiMetrics *bool `default:"false" json:"enableMultiMetrics"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *CreateOutputAuthenticationMethodSplunkHec `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSplunkHec `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSplunkHec  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSplunkHec `default:"block" json:"onBackpressure"`
	Description    *string                        `json:"description,omitempty"`
	// URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event
	URL *string `default:"http://localhost:8088/services/collector/event" json:"url"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool          `default:"false" json:"excludeSelf"`
	Urls        []URLSplunkHec `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Splunk HEC authentication token
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeSplunkHec `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionSplunkHec `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSplunkHec      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsSplunkHec `json:"pqControls,omitempty"`
}

func (o OutputSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSplunkHec) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSplunkHec) GetType() CreateOutputTypeSplunkHec {
	if o == nil {
		return CreateOutputTypeSplunkHec("")
	}
	return o.Type
}

func (o *OutputSplunkHec) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSplunkHec) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSplunkHec) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSplunkHec) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSplunkHec) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputSplunkHec) GetNextQueue() *string {
	if o == nil {
		return nil
	}
	return o.NextQueue
}

func (o *OutputSplunkHec) GetTCPRouting() *string {
	if o == nil {
		return nil
	}
	return o.TCPRouting
}

func (o *OutputSplunkHec) GetTLS() *TLSSettingsClientSideSplunkHec {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSplunkHec) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSplunkHec) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSplunkHec) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSplunkHec) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSplunkHec) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSplunkHec) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSplunkHec) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSplunkHec) GetExtraHTTPHeaders() []ExtraHTTPHeaderSplunkHec {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSplunkHec) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSplunkHec {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSplunkHec) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSplunkHec) GetEnableMultiMetrics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableMultiMetrics
}

func (o *OutputSplunkHec) GetAuthType() *CreateOutputAuthenticationMethodSplunkHec {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSplunkHec) GetResponseRetrySettings() []ResponseRetrySettingSplunkHec {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSplunkHec) GetTimeoutRetrySettings() *TimeoutRetrySettingsSplunkHec {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSplunkHec) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSplunkHec) GetOnBackpressure() *BackpressureBehaviorSplunkHec {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSplunkHec) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSplunkHec) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputSplunkHec) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSplunkHec) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputSplunkHec) GetUrls() []URLSplunkHec {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputSplunkHec) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSplunkHec) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputSplunkHec) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputSplunkHec) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSplunkHec) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSplunkHec) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSplunkHec) GetPqMode() *CreateOutputModeSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSplunkHec) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSplunkHec) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSplunkHec) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSplunkHec) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSplunkHec) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSplunkHec) GetPqCompress() *PqCompressCompressionSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSplunkHec) GetPqOnBackpressure() *QueueFullBehaviorSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSplunkHec) GetPqControls() *CreateOutputPqControlsSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeSplunkLb string

const (
	TypeSplunkLbSplunkLb TypeSplunkLb = "splunk_lb"
)

func (e TypeSplunkLb) ToPointer() *TypeSplunkLb {
	return &e
}
func (e *TypeSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk_lb":
		*e = TypeSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSplunkLb: %v", v)
	}
}

// NestedFieldSerializationSplunkLb - How to serialize nested fields into index-time fields
type NestedFieldSerializationSplunkLb string

const (
	// NestedFieldSerializationSplunkLbJSON JSON
	NestedFieldSerializationSplunkLbJSON NestedFieldSerializationSplunkLb = "json"
	// NestedFieldSerializationSplunkLbNone None
	NestedFieldSerializationSplunkLbNone NestedFieldSerializationSplunkLb = "none"
)

func (e NestedFieldSerializationSplunkLb) ToPointer() *NestedFieldSerializationSplunkLb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *NestedFieldSerializationSplunkLb) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "none":
			return true
		}
	}
	return false
}

type MinimumTLSVersionSplunkLb string

const (
	MinimumTLSVersionSplunkLbTlSv1  MinimumTLSVersionSplunkLb = "TLSv1"
	MinimumTLSVersionSplunkLbTlSv11 MinimumTLSVersionSplunkLb = "TLSv1.1"
	MinimumTLSVersionSplunkLbTlSv12 MinimumTLSVersionSplunkLb = "TLSv1.2"
	MinimumTLSVersionSplunkLbTlSv13 MinimumTLSVersionSplunkLb = "TLSv1.3"
)

func (e MinimumTLSVersionSplunkLb) ToPointer() *MinimumTLSVersionSplunkLb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MinimumTLSVersionSplunkLb) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type MaximumTLSVersionSplunkLb string

const (
	MaximumTLSVersionSplunkLbTlSv1  MaximumTLSVersionSplunkLb = "TLSv1"
	MaximumTLSVersionSplunkLbTlSv11 MaximumTLSVersionSplunkLb = "TLSv1.1"
	MaximumTLSVersionSplunkLbTlSv12 MaximumTLSVersionSplunkLb = "TLSv1.2"
	MaximumTLSVersionSplunkLbTlSv13 MaximumTLSVersionSplunkLb = "TLSv1.3"
)

func (e MaximumTLSVersionSplunkLb) ToPointer() *MaximumTLSVersionSplunkLb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MaximumTLSVersionSplunkLb) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideSplunkLb struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                    `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionSplunkLb `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionSplunkLb `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideSplunkLb) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideSplunkLb) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideSplunkLb) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideSplunkLb) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideSplunkLb) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideSplunkLb) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideSplunkLb) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideSplunkLb) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideSplunkLb) GetMinVersion() *MinimumTLSVersionSplunkLb {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideSplunkLb) GetMaxVersion() *MaximumTLSVersionSplunkLb {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// MaxS2SVersionSplunkLb - The highest S2S protocol version to advertise during handshake
type MaxS2SVersionSplunkLb string

const (
	MaxS2SVersionSplunkLbV3 MaxS2SVersionSplunkLb = "v3"
	MaxS2SVersionSplunkLbV4 MaxS2SVersionSplunkLb = "v4"
)

func (e MaxS2SVersionSplunkLb) ToPointer() *MaxS2SVersionSplunkLb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MaxS2SVersionSplunkLb) IsExact() bool {
	if e != nil {
		switch *e {
		case "v3", "v4":
			return true
		}
	}
	return false
}

// BackpressureBehaviorSplunkLb - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSplunkLb string

const (
	// BackpressureBehaviorSplunkLbBlock Block
	BackpressureBehaviorSplunkLbBlock BackpressureBehaviorSplunkLb = "block"
	// BackpressureBehaviorSplunkLbDrop Drop
	BackpressureBehaviorSplunkLbDrop BackpressureBehaviorSplunkLb = "drop"
	// BackpressureBehaviorSplunkLbQueue Persistent Queue
	BackpressureBehaviorSplunkLbQueue BackpressureBehaviorSplunkLb = "queue"
)

func (e BackpressureBehaviorSplunkLb) ToPointer() *BackpressureBehaviorSplunkLb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorSplunkLb) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// AuthenticationMethodSplunkLb - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSplunkLb string

const (
	AuthenticationMethodSplunkLbManual AuthenticationMethodSplunkLb = "manual"
	AuthenticationMethodSplunkLbSecret AuthenticationMethodSplunkLb = "secret"
)

func (e AuthenticationMethodSplunkLb) ToPointer() *AuthenticationMethodSplunkLb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodSplunkLb) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

// CompressCompressionSplunkLb - Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
type CompressCompressionSplunkLb string

const (
	// CompressCompressionSplunkLbDisabled Disabled
	CompressCompressionSplunkLbDisabled CompressCompressionSplunkLb = "disabled"
	// CompressCompressionSplunkLbAuto Automatic
	CompressCompressionSplunkLbAuto CompressCompressionSplunkLb = "auto"
	// CompressCompressionSplunkLbAlways Always
	CompressCompressionSplunkLbAlways CompressCompressionSplunkLb = "always"
)

func (e CompressCompressionSplunkLb) ToPointer() *CompressCompressionSplunkLb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressCompressionSplunkLb) IsExact() bool {
	if e != nil {
		switch *e {
		case "disabled", "auto", "always":
			return true
		}
	}
	return false
}

// IndexerDiscoveryConfigsAuthTokenAuthenticationMethod - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type IndexerDiscoveryConfigsAuthTokenAuthenticationMethod string

const (
	IndexerDiscoveryConfigsAuthTokenAuthenticationMethodManual IndexerDiscoveryConfigsAuthTokenAuthenticationMethod = "manual"
	IndexerDiscoveryConfigsAuthTokenAuthenticationMethodSecret IndexerDiscoveryConfigsAuthTokenAuthenticationMethod = "secret"
)

func (e IndexerDiscoveryConfigsAuthTokenAuthenticationMethod) ToPointer() *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

type IndexerDiscoveryConfigsAuthToken struct {
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod `default:"manual" json:"authType"`
	// Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (i IndexerDiscoveryConfigsAuthToken) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(i, "", false)
}

func (i *IndexerDiscoveryConfigsAuthToken) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &i, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (i *IndexerDiscoveryConfigsAuthToken) GetAuthType() *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod {
	if i == nil {
		return nil
	}
	return i.AuthType
}

func (i *IndexerDiscoveryConfigsAuthToken) GetAuthToken() *string {
	if i == nil {
		return nil
	}
	return i.AuthToken
}

func (i *IndexerDiscoveryConfigsAuthToken) GetTextSecret() *string {
	if i == nil {
		return nil
	}
	return i.TextSecret
}

// IndexerDiscoveryConfigsAuthenticationMethod - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type IndexerDiscoveryConfigsAuthenticationMethod string

const (
	IndexerDiscoveryConfigsAuthenticationMethodManual IndexerDiscoveryConfigsAuthenticationMethod = "manual"
	IndexerDiscoveryConfigsAuthenticationMethodSecret IndexerDiscoveryConfigsAuthenticationMethod = "secret"
)

func (e IndexerDiscoveryConfigsAuthenticationMethod) ToPointer() *IndexerDiscoveryConfigsAuthenticationMethod {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *IndexerDiscoveryConfigsAuthenticationMethod) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

// IndexerDiscoveryConfigs - List of configurations to set up indexer discovery in Splunk Indexer clustering environment.
type IndexerDiscoveryConfigs struct {
	// Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site.
	Site *string `default:"default" json:"site"`
	// Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089
	MasterURI string `json:"masterUri"`
	// Time interval, in seconds, between two consecutive indexer list fetches from cluster manager
	RefreshIntervalSec *float64 `default:"300" json:"refreshIntervalSec"`
	// During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates.
	RejectUnauthorized *bool `default:"false" json:"rejectUnauthorized"`
	// Tokens required to authenticate to cluster manager for indexer discovery
	AuthTokens []IndexerDiscoveryConfigsAuthToken `json:"authTokens,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *IndexerDiscoveryConfigsAuthenticationMethod `default:"manual" json:"authType"`
	// Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (i IndexerDiscoveryConfigs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(i, "", false)
}

func (i *IndexerDiscoveryConfigs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &i, "", false, []string{"masterUri"}); err != nil {
		return err
	}
	return nil
}

func (i *IndexerDiscoveryConfigs) GetSite() *string {
	if i == nil {
		return nil
	}
	return i.Site
}

func (i *IndexerDiscoveryConfigs) GetMasterURI() string {
	if i == nil {
		return ""
	}
	return i.MasterURI
}

func (i *IndexerDiscoveryConfigs) GetRefreshIntervalSec() *float64 {
	if i == nil {
		return nil
	}
	return i.RefreshIntervalSec
}

func (i *IndexerDiscoveryConfigs) GetRejectUnauthorized() *bool {
	if i == nil {
		return nil
	}
	return i.RejectUnauthorized
}

func (i *IndexerDiscoveryConfigs) GetAuthTokens() []IndexerDiscoveryConfigsAuthToken {
	if i == nil {
		return nil
	}
	return i.AuthTokens
}

func (i *IndexerDiscoveryConfigs) GetAuthType() *IndexerDiscoveryConfigsAuthenticationMethod {
	if i == nil {
		return nil
	}
	return i.AuthType
}

func (i *IndexerDiscoveryConfigs) GetAuthToken() *string {
	if i == nil {
		return nil
	}
	return i.AuthToken
}

func (i *IndexerDiscoveryConfigs) GetTextSecret() *string {
	if i == nil {
		return nil
	}
	return i.TextSecret
}

// TLSSplunkLb - Whether to inherit TLS configs from group setting or disable TLS
type TLSSplunkLb string

const (
	TLSSplunkLbInherit TLSSplunkLb = "inherit"
	TLSSplunkLbOff     TLSSplunkLb = "off"
)

func (e TLSSplunkLb) ToPointer() *TLSSplunkLb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *TLSSplunkLb) IsExact() bool {
	if e != nil {
		switch *e {
		case "inherit", "off":
			return true
		}
	}
	return false
}

type HostSplunkLb struct {
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port *float64 `default:"9997" json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS
	TLS *TLSSplunkLb `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host"}); err != nil {
		return err
	}
	return nil
}

func (h *HostSplunkLb) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostSplunkLb) GetPort() *float64 {
	if h == nil {
		return nil
	}
	return h.Port
}

func (h *HostSplunkLb) GetTLS() *TLSSplunkLb {
	if h == nil {
		return nil
	}
	return h.TLS
}

func (h *HostSplunkLb) GetServername() *string {
	if h == nil {
		return nil
	}
	return h.Servername
}

func (h *HostSplunkLb) GetWeight() *float64 {
	if h == nil {
		return nil
	}
	return h.Weight
}

// ModeSplunkLb - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSplunkLb string

const (
	// ModeSplunkLbError Error
	ModeSplunkLbError ModeSplunkLb = "error"
	// ModeSplunkLbAlways Backpressure
	ModeSplunkLbAlways ModeSplunkLb = "always"
	// ModeSplunkLbBackpressure Always On
	ModeSplunkLbBackpressure ModeSplunkLb = "backpressure"
)

func (e ModeSplunkLb) ToPointer() *ModeSplunkLb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeSplunkLb) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionSplunkLb - Codec to use to compress the persisted data
type PqCompressCompressionSplunkLb string

const (
	// PqCompressCompressionSplunkLbNone None
	PqCompressCompressionSplunkLbNone PqCompressCompressionSplunkLb = "none"
	// PqCompressCompressionSplunkLbGzip Gzip
	PqCompressCompressionSplunkLbGzip PqCompressCompressionSplunkLb = "gzip"
)

func (e PqCompressCompressionSplunkLb) ToPointer() *PqCompressCompressionSplunkLb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionSplunkLb) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorSplunkLb - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSplunkLb string

const (
	// QueueFullBehaviorSplunkLbBlock Block
	QueueFullBehaviorSplunkLbBlock QueueFullBehaviorSplunkLb = "block"
	// QueueFullBehaviorSplunkLbDrop Drop new data
	QueueFullBehaviorSplunkLbDrop QueueFullBehaviorSplunkLb = "drop"
)

func (e QueueFullBehaviorSplunkLb) ToPointer() *QueueFullBehaviorSplunkLb {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorSplunkLb) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsSplunkLb struct {
}

func (p PqControlsSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSplunkLb struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type TypeSplunkLb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// How to serialize nested fields into index-time fields
	NestedFields *NestedFieldSerializationSplunkLb `default:"none" json:"nestedFields"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64                       `default:"60000" json:"writeTimeout"`
	TLS          *TLSSettingsClientSideSplunkLb `json:"tls,omitempty"`
	// Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.
	EnableMultiMetrics *bool `default:"false" json:"enableMultiMetrics"`
	// Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.
	EnableACK *bool `default:"true" json:"enableACK"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// The highest S2S protocol version to advertise during handshake
	MaxS2Sversion *MaxS2SVersionSplunkLb `default:"v3" json:"maxS2Sversion"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSplunkLb `default:"block" json:"onBackpressure"`
	// Automatically discover indexers in indexer clustering environment.
	IndexerDiscovery *bool `default:"false" json:"indexerDiscovery"`
	// How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute.
	SenderUnhealthyTimeAllowance *float64 `default:"100" json:"senderUnhealthyTimeAllowance"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType    *AuthenticationMethodSplunkLb `default:"manual" json:"authType"`
	Description *string                       `json:"description,omitempty"`
	// Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.
	MaxFailedHealthChecks *float64 `default:"1" json:"maxFailedHealthChecks"`
	// Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
	Compress *CompressCompressionSplunkLb `default:"disabled" json:"compress"`
	// List of configurations to set up indexer discovery in Splunk Indexer clustering environment.
	IndexerDiscoveryConfigs *IndexerDiscoveryConfigs `json:"indexerDiscoveryConfigs,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of Splunk indexers to load-balance data to.
	Hosts []HostSplunkLb `json:"hosts"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSplunkLb `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionSplunkLb `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSplunkLb `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsSplunkLb        `json:"pqControls,omitempty"`
	// Shared secret token to use when establishing a connection to a Splunk indexer.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "hosts"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSplunkLb) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSplunkLb) GetType() TypeSplunkLb {
	if o == nil {
		return TypeSplunkLb("")
	}
	return o.Type
}

func (o *OutputSplunkLb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSplunkLb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSplunkLb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSplunkLb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSplunkLb) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSplunkLb) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputSplunkLb) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputSplunkLb) GetNestedFields() *NestedFieldSerializationSplunkLb {
	if o == nil {
		return nil
	}
	return o.NestedFields
}

func (o *OutputSplunkLb) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputSplunkLb) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputSplunkLb) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputSplunkLb) GetTLS() *TLSSettingsClientSideSplunkLb {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSplunkLb) GetEnableMultiMetrics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableMultiMetrics
}

func (o *OutputSplunkLb) GetEnableACK() *bool {
	if o == nil {
		return nil
	}
	return o.EnableACK
}

func (o *OutputSplunkLb) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputSplunkLb) GetMaxS2Sversion() *MaxS2SVersionSplunkLb {
	if o == nil {
		return nil
	}
	return o.MaxS2Sversion
}

func (o *OutputSplunkLb) GetOnBackpressure() *BackpressureBehaviorSplunkLb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSplunkLb) GetIndexerDiscovery() *bool {
	if o == nil {
		return nil
	}
	return o.IndexerDiscovery
}

func (o *OutputSplunkLb) GetSenderUnhealthyTimeAllowance() *float64 {
	if o == nil {
		return nil
	}
	return o.SenderUnhealthyTimeAllowance
}

func (o *OutputSplunkLb) GetAuthType() *AuthenticationMethodSplunkLb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSplunkLb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSplunkLb) GetMaxFailedHealthChecks() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFailedHealthChecks
}

func (o *OutputSplunkLb) GetCompress() *CompressCompressionSplunkLb {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSplunkLb) GetIndexerDiscoveryConfigs() *IndexerDiscoveryConfigs {
	if o == nil {
		return nil
	}
	return o.IndexerDiscoveryConfigs
}

func (o *OutputSplunkLb) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputSplunkLb) GetHosts() []HostSplunkLb {
	if o == nil {
		return []HostSplunkLb{}
	}
	return o.Hosts
}

func (o *OutputSplunkLb) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSplunkLb) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSplunkLb) GetPqMode() *ModeSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSplunkLb) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSplunkLb) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSplunkLb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSplunkLb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSplunkLb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSplunkLb) GetPqCompress() *PqCompressCompressionSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSplunkLb) GetPqOnBackpressure() *QueueFullBehaviorSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSplunkLb) GetPqControls() *PqControlsSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSplunkLb) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *OutputSplunkLb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type CreateOutputTypeSplunk string

const (
	CreateOutputTypeSplunkSplunk CreateOutputTypeSplunk = "splunk"
)

func (e CreateOutputTypeSplunk) ToPointer() *CreateOutputTypeSplunk {
	return &e
}
func (e *CreateOutputTypeSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk":
		*e = CreateOutputTypeSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeSplunk: %v", v)
	}
}

// NestedFieldSerializationSplunk - How to serialize nested fields into index-time fields
type NestedFieldSerializationSplunk string

const (
	// NestedFieldSerializationSplunkJSON JSON
	NestedFieldSerializationSplunkJSON NestedFieldSerializationSplunk = "json"
	// NestedFieldSerializationSplunkNone None
	NestedFieldSerializationSplunkNone NestedFieldSerializationSplunk = "none"
)

func (e NestedFieldSerializationSplunk) ToPointer() *NestedFieldSerializationSplunk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *NestedFieldSerializationSplunk) IsExact() bool {
	if e != nil {
		switch *e {
		case "json", "none":
			return true
		}
	}
	return false
}

type CreateOutputMinimumTLSVersionSplunk string

const (
	CreateOutputMinimumTLSVersionSplunkTlSv1  CreateOutputMinimumTLSVersionSplunk = "TLSv1"
	CreateOutputMinimumTLSVersionSplunkTlSv11 CreateOutputMinimumTLSVersionSplunk = "TLSv1.1"
	CreateOutputMinimumTLSVersionSplunkTlSv12 CreateOutputMinimumTLSVersionSplunk = "TLSv1.2"
	CreateOutputMinimumTLSVersionSplunkTlSv13 CreateOutputMinimumTLSVersionSplunk = "TLSv1.3"
)

func (e CreateOutputMinimumTLSVersionSplunk) ToPointer() *CreateOutputMinimumTLSVersionSplunk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMinimumTLSVersionSplunk) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type CreateOutputMaximumTLSVersionSplunk string

const (
	CreateOutputMaximumTLSVersionSplunkTlSv1  CreateOutputMaximumTLSVersionSplunk = "TLSv1"
	CreateOutputMaximumTLSVersionSplunkTlSv11 CreateOutputMaximumTLSVersionSplunk = "TLSv1.1"
	CreateOutputMaximumTLSVersionSplunkTlSv12 CreateOutputMaximumTLSVersionSplunk = "TLSv1.2"
	CreateOutputMaximumTLSVersionSplunkTlSv13 CreateOutputMaximumTLSVersionSplunk = "TLSv1.3"
)

func (e CreateOutputMaximumTLSVersionSplunk) ToPointer() *CreateOutputMaximumTLSVersionSplunk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMaximumTLSVersionSplunk) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideSplunk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                              `json:"passphrase,omitempty"`
	MinVersion *CreateOutputMinimumTLSVersionSplunk `json:"minVersion,omitempty"`
	MaxVersion *CreateOutputMaximumTLSVersionSplunk `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSplunk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSplunk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideSplunk) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideSplunk) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideSplunk) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideSplunk) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideSplunk) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideSplunk) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideSplunk) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideSplunk) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideSplunk) GetMinVersion() *CreateOutputMinimumTLSVersionSplunk {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideSplunk) GetMaxVersion() *CreateOutputMaximumTLSVersionSplunk {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// CreateOutputMaxS2SVersionSplunk - The highest S2S protocol version to advertise during handshake
type CreateOutputMaxS2SVersionSplunk string

const (
	CreateOutputMaxS2SVersionSplunkV3 CreateOutputMaxS2SVersionSplunk = "v3"
	CreateOutputMaxS2SVersionSplunkV4 CreateOutputMaxS2SVersionSplunk = "v4"
)

func (e CreateOutputMaxS2SVersionSplunk) ToPointer() *CreateOutputMaxS2SVersionSplunk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputMaxS2SVersionSplunk) IsExact() bool {
	if e != nil {
		switch *e {
		case "v3", "v4":
			return true
		}
	}
	return false
}

// BackpressureBehaviorSplunk - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSplunk string

const (
	// BackpressureBehaviorSplunkBlock Block
	BackpressureBehaviorSplunkBlock BackpressureBehaviorSplunk = "block"
	// BackpressureBehaviorSplunkDrop Drop
	BackpressureBehaviorSplunkDrop BackpressureBehaviorSplunk = "drop"
	// BackpressureBehaviorSplunkQueue Persistent Queue
	BackpressureBehaviorSplunkQueue BackpressureBehaviorSplunk = "queue"
)

func (e BackpressureBehaviorSplunk) ToPointer() *BackpressureBehaviorSplunk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorSplunk) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// AuthenticationMethodSplunk - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSplunk string

const (
	AuthenticationMethodSplunkManual AuthenticationMethodSplunk = "manual"
	AuthenticationMethodSplunkSecret AuthenticationMethodSplunk = "secret"
)

func (e AuthenticationMethodSplunk) ToPointer() *AuthenticationMethodSplunk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationMethodSplunk) IsExact() bool {
	if e != nil {
		switch *e {
		case "manual", "secret":
			return true
		}
	}
	return false
}

// CreateOutputCompressCompressionSplunk - Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
type CreateOutputCompressCompressionSplunk string

const (
	// CreateOutputCompressCompressionSplunkDisabled Disabled
	CreateOutputCompressCompressionSplunkDisabled CreateOutputCompressCompressionSplunk = "disabled"
	// CreateOutputCompressCompressionSplunkAuto Automatic
	CreateOutputCompressCompressionSplunkAuto CreateOutputCompressCompressionSplunk = "auto"
	// CreateOutputCompressCompressionSplunkAlways Always
	CreateOutputCompressCompressionSplunkAlways CreateOutputCompressCompressionSplunk = "always"
)

func (e CreateOutputCompressCompressionSplunk) ToPointer() *CreateOutputCompressCompressionSplunk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputCompressCompressionSplunk) IsExact() bool {
	if e != nil {
		switch *e {
		case "disabled", "auto", "always":
			return true
		}
	}
	return false
}

// CreateOutputModeSplunk - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type CreateOutputModeSplunk string

const (
	// CreateOutputModeSplunkError Error
	CreateOutputModeSplunkError CreateOutputModeSplunk = "error"
	// CreateOutputModeSplunkAlways Backpressure
	CreateOutputModeSplunkAlways CreateOutputModeSplunk = "always"
	// CreateOutputModeSplunkBackpressure Always On
	CreateOutputModeSplunkBackpressure CreateOutputModeSplunk = "backpressure"
)

func (e CreateOutputModeSplunk) ToPointer() *CreateOutputModeSplunk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CreateOutputModeSplunk) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// PqCompressCompressionSplunk - Codec to use to compress the persisted data
type PqCompressCompressionSplunk string

const (
	// PqCompressCompressionSplunkNone None
	PqCompressCompressionSplunkNone PqCompressCompressionSplunk = "none"
	// PqCompressCompressionSplunkGzip Gzip
	PqCompressCompressionSplunkGzip PqCompressCompressionSplunk = "gzip"
)

func (e PqCompressCompressionSplunk) ToPointer() *PqCompressCompressionSplunk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *PqCompressCompressionSplunk) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorSplunk - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSplunk string

const (
	// QueueFullBehaviorSplunkBlock Block
	QueueFullBehaviorSplunkBlock QueueFullBehaviorSplunk = "block"
	// QueueFullBehaviorSplunkDrop Drop new data
	QueueFullBehaviorSplunkDrop QueueFullBehaviorSplunk = "drop"
)

func (e QueueFullBehaviorSplunk) ToPointer() *QueueFullBehaviorSplunk {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorSplunk) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type CreateOutputPqControlsSplunk struct {
}

func (c CreateOutputPqControlsSplunk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CreateOutputPqControlsSplunk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSplunk struct {
	// Unique ID for this output
	ID   string                 `json:"id"`
	Type CreateOutputTypeSplunk `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port *float64 `default:"9997" json:"port"`
	// How to serialize nested fields into index-time fields
	NestedFields *NestedFieldSerializationSplunk `default:"none" json:"nestedFields"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64                     `default:"60000" json:"writeTimeout"`
	TLS          *TLSSettingsClientSideSplunk `json:"tls,omitempty"`
	// Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.
	EnableMultiMetrics *bool `default:"false" json:"enableMultiMetrics"`
	// Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.
	EnableACK *bool `default:"true" json:"enableACK"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// The highest S2S protocol version to advertise during handshake
	MaxS2Sversion *CreateOutputMaxS2SVersionSplunk `default:"v3" json:"maxS2Sversion"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSplunk `default:"block" json:"onBackpressure"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType    *AuthenticationMethodSplunk `default:"manual" json:"authType"`
	Description *string                     `json:"description,omitempty"`
	// Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.
	MaxFailedHealthChecks *float64 `default:"1" json:"maxFailedHealthChecks"`
	// Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
	Compress *CreateOutputCompressCompressionSplunk `default:"disabled" json:"compress"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *CreateOutputModeSplunk `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionSplunk `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSplunk      `default:"block" json:"pqOnBackpressure"`
	PqControls       *CreateOutputPqControlsSplunk `json:"pqControls,omitempty"`
	// Shared secret token to use when establishing a connection to a Splunk indexer.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (o OutputSplunk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSplunk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "host"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSplunk) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSplunk) GetType() CreateOutputTypeSplunk {
	if o == nil {
		return CreateOutputTypeSplunk("")
	}
	return o.Type
}

func (o *OutputSplunk) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSplunk) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSplunk) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSplunk) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSplunk) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputSplunk) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputSplunk) GetNestedFields() *NestedFieldSerializationSplunk {
	if o == nil {
		return nil
	}
	return o.NestedFields
}

func (o *OutputSplunk) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputSplunk) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputSplunk) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputSplunk) GetTLS() *TLSSettingsClientSideSplunk {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSplunk) GetEnableMultiMetrics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableMultiMetrics
}

func (o *OutputSplunk) GetEnableACK() *bool {
	if o == nil {
		return nil
	}
	return o.EnableACK
}

func (o *OutputSplunk) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputSplunk) GetMaxS2Sversion() *CreateOutputMaxS2SVersionSplunk {
	if o == nil {
		return nil
	}
	return o.MaxS2Sversion
}

func (o *OutputSplunk) GetOnBackpressure() *BackpressureBehaviorSplunk {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSplunk) GetAuthType() *AuthenticationMethodSplunk {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSplunk) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSplunk) GetMaxFailedHealthChecks() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFailedHealthChecks
}

func (o *OutputSplunk) GetCompress() *CreateOutputCompressCompressionSplunk {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSplunk) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSplunk) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSplunk) GetPqMode() *CreateOutputModeSplunk {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSplunk) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSplunk) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSplunk) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSplunk) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSplunk) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSplunk) GetPqCompress() *PqCompressCompressionSplunk {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSplunk) GetPqOnBackpressure() *QueueFullBehaviorSplunk {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSplunk) GetPqControls() *CreateOutputPqControlsSplunk {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSplunk) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *OutputSplunk) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

type CreateOutputTypeSyslog string

const (
	CreateOutputTypeSyslogSyslog CreateOutputTypeSyslog = "syslog"
)

func (e CreateOutputTypeSyslog) ToPointer() *CreateOutputTypeSyslog {
	return &e
}
func (e *CreateOutputTypeSyslog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "syslog":
		*e = CreateOutputTypeSyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CreateOutputTypeSyslog: %v", v)
	}
}

// ProtocolSyslog - The network protocol to use for sending out syslog messages
type ProtocolSyslog string

const (
	// ProtocolSyslogTCP TCP
	ProtocolSyslogTCP ProtocolSyslog = "tcp"
	// ProtocolSyslogUDP UDP
	ProtocolSyslogUDP ProtocolSyslog = "udp"
)

func (e ProtocolSyslog) ToPointer() *ProtocolSyslog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ProtocolSyslog) IsExact() bool {
	if e != nil {
		switch *e {
		case "tcp", "udp":
			return true
		}
	}
	return false
}

// Facility - Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.
type Facility int64

const (
	FacilityZero      Facility = 0
	FacilityOne       Facility = 1
	FacilityTwo       Facility = 2
	FacilityThree     Facility = 3
	FacilityFour      Facility = 4
	FacilityFive      Facility = 5
	FacilitySix       Facility = 6
	FacilitySeven     Facility = 7
	FacilityEight     Facility = 8
	FacilityNine      Facility = 9
	FacilityTen       Facility = 10
	FacilityEleven    Facility = 11
	FacilityTwelve    Facility = 12
	FacilityThirteen  Facility = 13
	FacilityFourteen  Facility = 14
	FacilityFifteen   Facility = 15
	FacilitySixteen   Facility = 16
	FacilitySeventeen Facility = 17
	FacilityEighteen  Facility = 18
	FacilityNineteen  Facility = 19
	FacilityTwenty    Facility = 20
	FacilityTwentyOne Facility = 21
)

func (e Facility) ToPointer() *Facility {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *Facility) IsExact() bool {
	if e != nil {
		switch *e {
		case 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21:
			return true
		}
	}
	return false
}

// SeveritySyslog - Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.
type SeveritySyslog int64

const (
	// SeveritySyslogZero emergency
	SeveritySyslogZero SeveritySyslog = 0
	// SeveritySyslogOne alert
	SeveritySyslogOne SeveritySyslog = 1
	// SeveritySyslogTwo critical
	SeveritySyslogTwo SeveritySyslog = 2
	// SeveritySyslogThree error
	SeveritySyslogThree SeveritySyslog = 3
	// SeveritySyslogFour warning
	SeveritySyslogFour SeveritySyslog = 4
	// SeveritySyslogFive notice
	SeveritySyslogFive SeveritySyslog = 5
	// SeveritySyslogSix info
	SeveritySyslogSix SeveritySyslog = 6
	// SeveritySyslogSeven debug
	SeveritySyslogSeven SeveritySyslog = 7
)

func (e SeveritySyslog) ToPointer() *SeveritySyslog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *SeveritySyslog) IsExact() bool {
	if e != nil {
		switch *e {
		case 0, 1, 2, 3, 4, 5, 6, 7:
			return true
		}
	}
	return false
}

// MessageFormatSyslog - The syslog message format depending on the receiver's support
type MessageFormatSyslog string

const (
	// MessageFormatSyslogRfc3164 RFC3164
	MessageFormatSyslogRfc3164 MessageFormatSyslog = "rfc3164"
	// MessageFormatSyslogRfc5424 RFC5424
	MessageFormatSyslogRfc5424 MessageFormatSyslog = "rfc5424"
)

func (e MessageFormatSyslog) ToPointer() *MessageFormatSyslog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MessageFormatSyslog) IsExact() bool {
	if e != nil {
		switch *e {
		case "rfc3164", "rfc5424":
			return true
		}
	}
	return false
}

// TimestampFormat - Timestamp format to use when serializing event's time field
type TimestampFormat string

const (
	// TimestampFormatSyslog Syslog
	TimestampFormatSyslog TimestampFormat = "syslog"
	// TimestampFormatIso8601 ISO8601
	TimestampFormatIso8601 TimestampFormat = "iso8601"
)

func (e TimestampFormat) ToPointer() *TimestampFormat {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *TimestampFormat) IsExact() bool {
	if e != nil {
		switch *e {
		case "syslog", "iso8601":
			return true
		}
	}
	return false
}

// TLSSyslog - Whether to inherit TLS configs from group setting or disable TLS
type TLSSyslog string

const (
	TLSSyslogInherit TLSSyslog = "inherit"
	TLSSyslogOff     TLSSyslog = "off"
)

func (e TLSSyslog) ToPointer() *TLSSyslog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *TLSSyslog) IsExact() bool {
	if e != nil {
		switch *e {
		case "inherit", "off":
			return true
		}
	}
	return false
}

type HostSyslog struct {
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port float64 `json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS
	TLS *TLSSyslog `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostSyslog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostSyslog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host", "port"}); err != nil {
		return err
	}
	return nil
}

func (h *HostSyslog) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostSyslog) GetPort() float64 {
	if h == nil {
		return 0.0
	}
	return h.Port
}

func (h *HostSyslog) GetTLS() *TLSSyslog {
	if h == nil {
		return nil
	}
	return h.TLS
}

func (h *HostSyslog) GetServername() *string {
	if h == nil {
		return nil
	}
	return h.Servername
}

func (h *HostSyslog) GetWeight() *float64 {
	if h == nil {
		return nil
	}
	return h.Weight
}

type MinimumTLSVersionSyslog string

const (
	MinimumTLSVersionSyslogTlSv1  MinimumTLSVersionSyslog = "TLSv1"
	MinimumTLSVersionSyslogTlSv11 MinimumTLSVersionSyslog = "TLSv1.1"
	MinimumTLSVersionSyslogTlSv12 MinimumTLSVersionSyslog = "TLSv1.2"
	MinimumTLSVersionSyslogTlSv13 MinimumTLSVersionSyslog = "TLSv1.3"
)

func (e MinimumTLSVersionSyslog) ToPointer() *MinimumTLSVersionSyslog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MinimumTLSVersionSyslog) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type MaximumTLSVersionSyslog string

const (
	MaximumTLSVersionSyslogTlSv1  MaximumTLSVersionSyslog = "TLSv1"
	MaximumTLSVersionSyslogTlSv11 MaximumTLSVersionSyslog = "TLSv1.1"
	MaximumTLSVersionSyslogTlSv12 MaximumTLSVersionSyslog = "TLSv1.2"
	MaximumTLSVersionSyslogTlSv13 MaximumTLSVersionSyslog = "TLSv1.3"
)

func (e MaximumTLSVersionSyslog) ToPointer() *MaximumTLSVersionSyslog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MaximumTLSVersionSyslog) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideSyslog struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                  `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionSyslog `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionSyslog `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSyslog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSyslog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideSyslog) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideSyslog) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideSyslog) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideSyslog) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideSyslog) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideSyslog) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideSyslog) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideSyslog) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideSyslog) GetMinVersion() *MinimumTLSVersionSyslog {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideSyslog) GetMaxVersion() *MaximumTLSVersionSyslog {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// BackpressureBehaviorSyslog - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSyslog string

const (
	// BackpressureBehaviorSyslogBlock Block
	BackpressureBehaviorSyslogBlock BackpressureBehaviorSyslog = "block"
	// BackpressureBehaviorSyslogDrop Drop
	BackpressureBehaviorSyslogDrop BackpressureBehaviorSyslog = "drop"
	// BackpressureBehaviorSyslogQueue Persistent Queue
	BackpressureBehaviorSyslogQueue BackpressureBehaviorSyslog = "queue"
)

func (e BackpressureBehaviorSyslog) ToPointer() *BackpressureBehaviorSyslog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorSyslog) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// ModeSyslog - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSyslog string

const (
	// ModeSyslogError Error
	ModeSyslogError ModeSyslog = "error"
	// ModeSyslogAlways Backpressure
	ModeSyslogAlways ModeSyslog = "always"
	// ModeSyslogBackpressure Always On
	ModeSyslogBackpressure ModeSyslog = "backpressure"
)

func (e ModeSyslog) ToPointer() *ModeSyslog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeSyslog) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionSyslog - Codec to use to compress the persisted data
type CompressionSyslog string

const (
	// CompressionSyslogNone None
	CompressionSyslogNone CompressionSyslog = "none"
	// CompressionSyslogGzip Gzip
	CompressionSyslogGzip CompressionSyslog = "gzip"
)

func (e CompressionSyslog) ToPointer() *CompressionSyslog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionSyslog) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorSyslog - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSyslog string

const (
	// QueueFullBehaviorSyslogBlock Block
	QueueFullBehaviorSyslogBlock QueueFullBehaviorSyslog = "block"
	// QueueFullBehaviorSyslogDrop Drop new data
	QueueFullBehaviorSyslogDrop QueueFullBehaviorSyslog = "drop"
)

func (e QueueFullBehaviorSyslog) ToPointer() *QueueFullBehaviorSyslog {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorSyslog) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsSyslog struct {
}

func (p PqControlsSyslog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSyslog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSyslog struct {
	// Unique ID for this output
	ID   string                 `json:"id"`
	Type CreateOutputTypeSyslog `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The network protocol to use for sending out syslog messages
	Protocol *ProtocolSyslog `default:"tcp" json:"protocol"`
	// Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.
	Facility *Facility `default:"1" json:"facility"`
	// Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.
	Severity *SeveritySyslog `default:"5" json:"severity"`
	// Default name for device or application that originated the message. Defaults to Cribl, but will be overwritten by value of __appname if set.
	AppName *string `default:"Cribl" json:"appName"`
	// The syslog message format depending on the receiver's support
	MessageFormat *MessageFormatSyslog `default:"rfc3164" json:"messageFormat"`
	// Timestamp format to use when serializing event's time field
	TimestampFormat *TimestampFormat `default:"syslog" json:"timestampFormat"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Prefix messages with the byte count of the message. If disabled, no prefix will be set, and the message will be appended with a \n.
	OctetCountFraming *bool `json:"octetCountFraming,omitempty"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool   `default:"false" json:"logFailedRequests"`
	Description       *string `json:"description,omitempty"`
	// For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs.  If this setting is disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// The hostname of the receiver
	Host *string `json:"host,omitempty"`
	// The port to connect to on the provided host
	Port *float64 `json:"port,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of hosts to load-balance data to
	Hosts []HostSyslog `json:"hosts,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64                     `default:"60000" json:"writeTimeout"`
	TLS          *TLSSettingsClientSideSyslog `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSyslog `default:"block" json:"onBackpressure"`
	// Maximum size of syslog messages. Make sure this value is less than or equal to the MTU to avoid UDP packet fragmentation.
	MaxRecordSize *float64 `default:"1500" json:"maxRecordSize"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every message sent will incur a DNS lookup.
	UDPDNSResolvePeriodSec *float64 `default:"0" json:"udpDnsResolvePeriodSec"`
	// Send Syslog traffic using the original event's Source IP and port. To enable this, you must install the external `udp-sender` helper binary at `/usr/bin/udp-sender` on all Worker Nodes and grant it the `CAP_NET_RAW` capability.
	EnableIPSpoofing *bool `default:"false" json:"enableIpSpoofing"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSyslog `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSyslog `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSyslog `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsSyslog        `json:"pqControls,omitempty"`
}

func (o OutputSyslog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSyslog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSyslog) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSyslog) GetType() CreateOutputTypeSyslog {
	if o == nil {
		return CreateOutputTypeSyslog("")
	}
	return o.Type
}

func (o *OutputSyslog) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSyslog) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSyslog) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSyslog) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSyslog) GetProtocol() *ProtocolSyslog {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputSyslog) GetFacility() *Facility {
	if o == nil {
		return nil
	}
	return o.Facility
}

func (o *OutputSyslog) GetSeverity() *SeveritySyslog {
	if o == nil {
		return nil
	}
	return o.Severity
}

func (o *OutputSyslog) GetAppName() *string {
	if o == nil {
		return nil
	}
	return o.AppName
}

func (o *OutputSyslog) GetMessageFormat() *MessageFormatSyslog {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputSyslog) GetTimestampFormat() *TimestampFormat {
	if o == nil {
		return nil
	}
	return o.TimestampFormat
}

func (o *OutputSyslog) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputSyslog) GetOctetCountFraming() *bool {
	if o == nil {
		return nil
	}
	return o.OctetCountFraming
}

func (o *OutputSyslog) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputSyslog) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSyslog) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputSyslog) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputSyslog) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputSyslog) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputSyslog) GetHosts() []HostSyslog {
	if o == nil {
		return nil
	}
	return o.Hosts
}

func (o *OutputSyslog) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSyslog) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputSyslog) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputSyslog) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputSyslog) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputSyslog) GetTLS() *TLSSettingsClientSideSyslog {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSyslog) GetOnBackpressure() *BackpressureBehaviorSyslog {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSyslog) GetMaxRecordSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSize
}

func (o *OutputSyslog) GetUDPDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.UDPDNSResolvePeriodSec
}

func (o *OutputSyslog) GetEnableIPSpoofing() *bool {
	if o == nil {
		return nil
	}
	return o.EnableIPSpoofing
}

func (o *OutputSyslog) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSyslog) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSyslog) GetPqMode() *ModeSyslog {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSyslog) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSyslog) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSyslog) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSyslog) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSyslog) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSyslog) GetPqCompress() *CompressionSyslog {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSyslog) GetPqOnBackpressure() *QueueFullBehaviorSyslog {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSyslog) GetPqControls() *PqControlsSyslog {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type TypeDevnull string

const (
	TypeDevnullDevnull TypeDevnull = "devnull"
)

func (e TypeDevnull) ToPointer() *TypeDevnull {
	return &e
}
func (e *TypeDevnull) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "devnull":
		*e = TypeDevnull(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDevnull: %v", v)
	}
}

type OutputDevnull struct {
	// Unique ID for this output
	ID   string      `json:"id"`
	Type TypeDevnull `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
}

func (o OutputDevnull) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDevnull) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDevnull) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDevnull) GetType() TypeDevnull {
	if o == nil {
		return TypeDevnull("")
	}
	return o.Type
}

func (o *OutputDevnull) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDevnull) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDevnull) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDevnull) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

type TypeSentinel string

const (
	TypeSentinelSentinel TypeSentinel = "sentinel"
)

func (e TypeSentinel) ToPointer() *TypeSentinel {
	return &e
}
func (e *TypeSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sentinel":
		*e = TypeSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSentinel: %v", v)
	}
}

type ExtraHTTPHeaderSentinel struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderSentinel) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderSentinel) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeSentinel - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSentinel string

const (
	// FailedRequestLoggingModeSentinelPayload Payload
	FailedRequestLoggingModeSentinelPayload FailedRequestLoggingModeSentinel = "payload"
	// FailedRequestLoggingModeSentinelPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeSentinelPayloadAndHeaders FailedRequestLoggingModeSentinel = "payloadAndHeaders"
	// FailedRequestLoggingModeSentinelNone None
	FailedRequestLoggingModeSentinelNone FailedRequestLoggingModeSentinel = "none"
)

func (e FailedRequestLoggingModeSentinel) ToPointer() *FailedRequestLoggingModeSentinel {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeSentinel) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingSentinel struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingSentinel) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingSentinel) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingSentinel) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingSentinel) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsSentinel struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsSentinel) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsSentinel) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsSentinel) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsSentinel) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorSentinel - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSentinel string

const (
	// BackpressureBehaviorSentinelBlock Block
	BackpressureBehaviorSentinelBlock BackpressureBehaviorSentinel = "block"
	// BackpressureBehaviorSentinelDrop Drop
	BackpressureBehaviorSentinelDrop BackpressureBehaviorSentinel = "drop"
	// BackpressureBehaviorSentinelQueue Persistent Queue
	BackpressureBehaviorSentinelQueue BackpressureBehaviorSentinel = "queue"
)

func (e BackpressureBehaviorSentinel) ToPointer() *BackpressureBehaviorSentinel {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorSentinel) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

type AuthType string

const (
	AuthTypeOauth AuthType = "oauth"
)

func (e AuthType) ToPointer() *AuthType {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthType) IsExact() bool {
	if e != nil {
		switch *e {
		case "oauth":
			return true
		}
	}
	return false
}

// EndpointConfiguration - Enter the data collection endpoint URL or the individual ID
type EndpointConfiguration string

const (
	// EndpointConfigurationURL URL
	EndpointConfigurationURL EndpointConfiguration = "url"
	// EndpointConfigurationID ID
	EndpointConfigurationID EndpointConfiguration = "ID"
)

func (e EndpointConfiguration) ToPointer() *EndpointConfiguration {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *EndpointConfiguration) IsExact() bool {
	if e != nil {
		switch *e {
		case "url", "ID":
			return true
		}
	}
	return false
}

type FormatSentinel string

const (
	FormatSentinelNdjson    FormatSentinel = "ndjson"
	FormatSentinelJSONArray FormatSentinel = "json_array"
	FormatSentinelCustom    FormatSentinel = "custom"
	FormatSentinelAdvanced  FormatSentinel = "advanced"
)

func (e FormatSentinel) ToPointer() *FormatSentinel {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FormatSentinel) IsExact() bool {
	if e != nil {
		switch *e {
		case "ndjson", "json_array", "custom", "advanced":
			return true
		}
	}
	return false
}

// ModeSentinel - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSentinel string

const (
	// ModeSentinelError Error
	ModeSentinelError ModeSentinel = "error"
	// ModeSentinelAlways Backpressure
	ModeSentinelAlways ModeSentinel = "always"
	// ModeSentinelBackpressure Always On
	ModeSentinelBackpressure ModeSentinel = "backpressure"
)

func (e ModeSentinel) ToPointer() *ModeSentinel {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeSentinel) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionSentinel - Codec to use to compress the persisted data
type CompressionSentinel string

const (
	// CompressionSentinelNone None
	CompressionSentinelNone CompressionSentinel = "none"
	// CompressionSentinelGzip Gzip
	CompressionSentinelGzip CompressionSentinel = "gzip"
)

func (e CompressionSentinel) ToPointer() *CompressionSentinel {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionSentinel) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorSentinel - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSentinel string

const (
	// QueueFullBehaviorSentinelBlock Block
	QueueFullBehaviorSentinelBlock QueueFullBehaviorSentinel = "block"
	// QueueFullBehaviorSentinelDrop Drop new data
	QueueFullBehaviorSentinelDrop QueueFullBehaviorSentinel = "drop"
)

func (e QueueFullBehaviorSentinel) ToPointer() *QueueFullBehaviorSentinel {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorSentinel) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsSentinel struct {
}

func (p PqControlsSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSentinel struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type TypeSentinel `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB)
	MaxPayloadSizeKB *float64 `default:"1000" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
	ExtraHTTPHeaders []ExtraHTTPHeaderSentinel `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSentinel `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSentinel `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSentinel  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSentinel `default:"block" json:"onBackpressure"`
	AuthType       *AuthType                     `json:"authType,omitempty"`
	// URL for OAuth
	LoginURL string `json:"loginUrl"`
	// Secret parameter value to pass in request body
	Secret string `json:"secret"`
	// JavaScript expression to compute the Client ID for the Azure application. Can be a constant.
	ClientID string `json:"client_id"`
	// Scope to pass in the OAuth request
	Scope *string `default:"https://monitor.azure.com/.default" json:"scope"`
	// Enter the data collection endpoint URL or the individual ID
	EndpointURLConfiguration *EndpointConfiguration `default:"url" json:"endpointURLConfiguration"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64        `json:"totalMemoryLimitKB,omitempty"`
	Description        *string         `json:"description,omitempty"`
	Format             *FormatSentinel `json:"format,omitempty"`
	// Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.
	CustomSourceExpression *string `default:"__httpOut" json:"customSourceExpression"`
	// Whether to drop events when the source expression evaluates to null
	CustomDropWhenNull *bool `default:"false" json:"customDropWhenNull"`
	// Delimiter string to insert between individual events. Defaults to newline character.
	CustomEventDelimiter *string `default:"\\n" json:"customEventDelimiter"`
	// Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.
	CustomContentType *string `default:"application/x-ndjson" json:"customContentType"`
	// Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.
	CustomPayloadExpression *string `default:"\\${events}" json:"customPayloadExpression"`
	// HTTP content-type header value
	AdvancedContentType *string `default:"application/json" json:"advancedContentType"`
	// Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatEventCode *string `json:"formatEventCode,omitempty"`
	// Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatPayloadCode *string `json:"formatPayloadCode,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSentinel `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSentinel `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSentinel `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsSentinel        `json:"pqControls,omitempty"`
	// URL to send events to. Can be overwritten by an event's __url field.
	URL *string `json:"url,omitempty"`
	// Immutable ID for the Data Collection Rule (DCR)
	DcrID *string `json:"dcrID,omitempty"`
	// Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`
	DceEndpoint *string `json:"dceEndpoint,omitempty"`
	// The name of the stream (Sentinel table) in which to store the events
	StreamName *string `json:"streamName,omitempty"`
}

func (o OutputSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "loginUrl", "secret", "client_id"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSentinel) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSentinel) GetType() TypeSentinel {
	if o == nil {
		return TypeSentinel("")
	}
	return o.Type
}

func (o *OutputSentinel) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSentinel) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSentinel) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSentinel) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSentinel) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputSentinel) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSentinel) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSentinel) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSentinel) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSentinel) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSentinel) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSentinel) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSentinel) GetExtraHTTPHeaders() []ExtraHTTPHeaderSentinel {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSentinel) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSentinel) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSentinel {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSentinel) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSentinel) GetResponseRetrySettings() []ResponseRetrySettingSentinel {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSentinel) GetTimeoutRetrySettings() *TimeoutRetrySettingsSentinel {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSentinel) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSentinel) GetOnBackpressure() *BackpressureBehaviorSentinel {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSentinel) GetAuthType() *AuthType {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSentinel) GetLoginURL() string {
	if o == nil {
		return ""
	}
	return o.LoginURL
}

func (o *OutputSentinel) GetSecret() string {
	if o == nil {
		return ""
	}
	return o.Secret
}

func (o *OutputSentinel) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputSentinel) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputSentinel) GetEndpointURLConfiguration() *EndpointConfiguration {
	if o == nil {
		return nil
	}
	return o.EndpointURLConfiguration
}

func (o *OutputSentinel) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputSentinel) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSentinel) GetFormat() *FormatSentinel {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputSentinel) GetCustomSourceExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomSourceExpression
}

func (o *OutputSentinel) GetCustomDropWhenNull() *bool {
	if o == nil {
		return nil
	}
	return o.CustomDropWhenNull
}

func (o *OutputSentinel) GetCustomEventDelimiter() *string {
	if o == nil {
		return nil
	}
	return o.CustomEventDelimiter
}

func (o *OutputSentinel) GetCustomContentType() *string {
	if o == nil {
		return nil
	}
	return o.CustomContentType
}

func (o *OutputSentinel) GetCustomPayloadExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomPayloadExpression
}

func (o *OutputSentinel) GetAdvancedContentType() *string {
	if o == nil {
		return nil
	}
	return o.AdvancedContentType
}

func (o *OutputSentinel) GetFormatEventCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatEventCode
}

func (o *OutputSentinel) GetFormatPayloadCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatPayloadCode
}

func (o *OutputSentinel) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSentinel) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSentinel) GetPqMode() *ModeSentinel {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSentinel) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSentinel) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSentinel) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSentinel) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSentinel) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSentinel) GetPqCompress() *CompressionSentinel {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSentinel) GetPqOnBackpressure() *QueueFullBehaviorSentinel {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSentinel) GetPqControls() *PqControlsSentinel {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSentinel) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputSentinel) GetDcrID() *string {
	if o == nil {
		return nil
	}
	return o.DcrID
}

func (o *OutputSentinel) GetDceEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.DceEndpoint
}

func (o *OutputSentinel) GetStreamName() *string {
	if o == nil {
		return nil
	}
	return o.StreamName
}

type TypeWebhook string

const (
	TypeWebhookWebhook TypeWebhook = "webhook"
)

func (e TypeWebhook) ToPointer() *TypeWebhook {
	return &e
}
func (e *TypeWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "webhook":
		*e = TypeWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeWebhook: %v", v)
	}
}

// MethodWebhook - The method to use when sending events
type MethodWebhook string

const (
	MethodWebhookPost  MethodWebhook = "POST"
	MethodWebhookPut   MethodWebhook = "PUT"
	MethodWebhookPatch MethodWebhook = "PATCH"
)

func (e MethodWebhook) ToPointer() *MethodWebhook {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MethodWebhook) IsExact() bool {
	if e != nil {
		switch *e {
		case "POST", "PUT", "PATCH":
			return true
		}
	}
	return false
}

// FormatWebhook - How to format events before sending out
type FormatWebhook string

const (
	// FormatWebhookNdjson NDJSON (Newline Delimited JSON)
	FormatWebhookNdjson FormatWebhook = "ndjson"
	// FormatWebhookJSONArray JSON Array
	FormatWebhookJSONArray FormatWebhook = "json_array"
	// FormatWebhookCustom Custom
	FormatWebhookCustom FormatWebhook = "custom"
	// FormatWebhookAdvanced Advanced
	FormatWebhookAdvanced FormatWebhook = "advanced"
)

func (e FormatWebhook) ToPointer() *FormatWebhook {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FormatWebhook) IsExact() bool {
	if e != nil {
		switch *e {
		case "ndjson", "json_array", "custom", "advanced":
			return true
		}
	}
	return false
}

type ExtraHTTPHeaderWebhook struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderWebhook) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderWebhook) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeWebhook - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeWebhook string

const (
	// FailedRequestLoggingModeWebhookPayload Payload
	FailedRequestLoggingModeWebhookPayload FailedRequestLoggingModeWebhook = "payload"
	// FailedRequestLoggingModeWebhookPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeWebhookPayloadAndHeaders FailedRequestLoggingModeWebhook = "payloadAndHeaders"
	// FailedRequestLoggingModeWebhookNone None
	FailedRequestLoggingModeWebhookNone FailedRequestLoggingModeWebhook = "none"
)

func (e FailedRequestLoggingModeWebhook) ToPointer() *FailedRequestLoggingModeWebhook {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *FailedRequestLoggingModeWebhook) IsExact() bool {
	if e != nil {
		switch *e {
		case "payload", "payloadAndHeaders", "none":
			return true
		}
	}
	return false
}

type ResponseRetrySettingWebhook struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingWebhook) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingWebhook) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingWebhook) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingWebhook) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsWebhook struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsWebhook) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsWebhook) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsWebhook) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsWebhook) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorWebhook - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorWebhook string

const (
	// BackpressureBehaviorWebhookBlock Block
	BackpressureBehaviorWebhookBlock BackpressureBehaviorWebhook = "block"
	// BackpressureBehaviorWebhookDrop Drop
	BackpressureBehaviorWebhookDrop BackpressureBehaviorWebhook = "drop"
	// BackpressureBehaviorWebhookQueue Persistent Queue
	BackpressureBehaviorWebhookQueue BackpressureBehaviorWebhook = "queue"
)

func (e BackpressureBehaviorWebhook) ToPointer() *BackpressureBehaviorWebhook {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *BackpressureBehaviorWebhook) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop", "queue":
			return true
		}
	}
	return false
}

// AuthenticationTypeWebhook - Authentication method to use for the HTTP request
type AuthenticationTypeWebhook string

const (
	// AuthenticationTypeWebhookNone None
	AuthenticationTypeWebhookNone AuthenticationTypeWebhook = "none"
	// AuthenticationTypeWebhookBasic Basic
	AuthenticationTypeWebhookBasic AuthenticationTypeWebhook = "basic"
	// AuthenticationTypeWebhookCredentialsSecret Basic (credentials secret)
	AuthenticationTypeWebhookCredentialsSecret AuthenticationTypeWebhook = "credentialsSecret"
	// AuthenticationTypeWebhookToken Token
	AuthenticationTypeWebhookToken AuthenticationTypeWebhook = "token"
	// AuthenticationTypeWebhookTextSecret Token (text secret)
	AuthenticationTypeWebhookTextSecret AuthenticationTypeWebhook = "textSecret"
	// AuthenticationTypeWebhookOauth OAuth
	AuthenticationTypeWebhookOauth AuthenticationTypeWebhook = "oauth"
)

func (e AuthenticationTypeWebhook) ToPointer() *AuthenticationTypeWebhook {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *AuthenticationTypeWebhook) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "basic", "credentialsSecret", "token", "textSecret", "oauth":
			return true
		}
	}
	return false
}

type MinimumTLSVersionWebhook string

const (
	MinimumTLSVersionWebhookTlSv1  MinimumTLSVersionWebhook = "TLSv1"
	MinimumTLSVersionWebhookTlSv11 MinimumTLSVersionWebhook = "TLSv1.1"
	MinimumTLSVersionWebhookTlSv12 MinimumTLSVersionWebhook = "TLSv1.2"
	MinimumTLSVersionWebhookTlSv13 MinimumTLSVersionWebhook = "TLSv1.3"
)

func (e MinimumTLSVersionWebhook) ToPointer() *MinimumTLSVersionWebhook {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MinimumTLSVersionWebhook) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type MaximumTLSVersionWebhook string

const (
	MaximumTLSVersionWebhookTlSv1  MaximumTLSVersionWebhook = "TLSv1"
	MaximumTLSVersionWebhookTlSv11 MaximumTLSVersionWebhook = "TLSv1.1"
	MaximumTLSVersionWebhookTlSv12 MaximumTLSVersionWebhook = "TLSv1.2"
	MaximumTLSVersionWebhookTlSv13 MaximumTLSVersionWebhook = "TLSv1.3"
)

func (e MaximumTLSVersionWebhook) ToPointer() *MaximumTLSVersionWebhook {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *MaximumTLSVersionWebhook) IsExact() bool {
	if e != nil {
		switch *e {
		case "TLSv1", "TLSv1.1", "TLSv1.2", "TLSv1.3":
			return true
		}
	}
	return false
}

type TLSSettingsClientSideWebhook struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                   `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionWebhook `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionWebhook `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideWebhook) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideWebhook) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideWebhook) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideWebhook) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideWebhook) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideWebhook) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideWebhook) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideWebhook) GetMinVersion() *MinimumTLSVersionWebhook {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideWebhook) GetMaxVersion() *MaximumTLSVersionWebhook {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// ModeWebhook - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeWebhook string

const (
	// ModeWebhookError Error
	ModeWebhookError ModeWebhook = "error"
	// ModeWebhookAlways Backpressure
	ModeWebhookAlways ModeWebhook = "always"
	// ModeWebhookBackpressure Always On
	ModeWebhookBackpressure ModeWebhook = "backpressure"
)

func (e ModeWebhook) ToPointer() *ModeWebhook {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *ModeWebhook) IsExact() bool {
	if e != nil {
		switch *e {
		case "error", "always", "backpressure":
			return true
		}
	}
	return false
}

// CompressionWebhook - Codec to use to compress the persisted data
type CompressionWebhook string

const (
	// CompressionWebhookNone None
	CompressionWebhookNone CompressionWebhook = "none"
	// CompressionWebhookGzip Gzip
	CompressionWebhookGzip CompressionWebhook = "gzip"
)

func (e CompressionWebhook) ToPointer() *CompressionWebhook {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *CompressionWebhook) IsExact() bool {
	if e != nil {
		switch *e {
		case "none", "gzip":
			return true
		}
	}
	return false
}

// QueueFullBehaviorWebhook - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorWebhook string

const (
	// QueueFullBehaviorWebhookBlock Block
	QueueFullBehaviorWebhookBlock QueueFullBehaviorWebhook = "block"
	// QueueFullBehaviorWebhookDrop Drop new data
	QueueFullBehaviorWebhookDrop QueueFullBehaviorWebhook = "drop"
)

func (e QueueFullBehaviorWebhook) ToPointer() *QueueFullBehaviorWebhook {
	return &e
}

// IsExact returns true if the value matches a known enum value, false otherwise.
func (e *QueueFullBehaviorWebhook) IsExact() bool {
	if e != nil {
		switch *e {
		case "block", "drop":
			return true
		}
	}
	return false
}

type PqControlsWebhook struct {
}

func (p PqControlsWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OauthParamWebhook struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o OauthParamWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthParamWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthParamWebhook) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamWebhook) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderWebhook struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o OauthHeaderWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthHeaderWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthHeaderWebhook) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderWebhook) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type URLWebhook struct {
	// URL of a webhook endpoint to send events to, such as http://localhost:10200
	URL string `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, []string{"url"}); err != nil {
		return err
	}
	return nil
}

func (u *URLWebhook) GetURL() string {
	if u == nil {
		return ""
	}
	return u.URL
}

func (u *URLWebhook) GetWeight() *float64 {
	if u == nil {
		return nil
	}
	return u.Weight
}

type OutputWebhook struct {
	// Unique ID for this output
	ID   string      `json:"id"`
	Type TypeWebhook `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The method to use when sending events
	Method *MethodWebhook `default:"POST" json:"method"`
	// How to format events before sending out
	Format *FormatWebhook `default:"ndjson" json:"format"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
	ExtraHTTPHeaders []ExtraHTTPHeaderWebhook `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeWebhook `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingWebhook `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsWebhook  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorWebhook `default:"block" json:"onBackpressure"`
	// Authentication method to use for the HTTP request
	AuthType *AuthenticationTypeWebhook    `default:"none" json:"authType"`
	TLS      *TLSSettingsClientSideWebhook `json:"tls,omitempty"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool   `default:"false" json:"loadBalanced"`
	Description  *string `json:"description,omitempty"`
	// Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.
	CustomSourceExpression *string `default:"__httpOut" json:"customSourceExpression"`
	// Whether to drop events when the source expression evaluates to null
	CustomDropWhenNull *bool `default:"false" json:"customDropWhenNull"`
	// Delimiter string to insert between individual events. Defaults to newline character.
	CustomEventDelimiter *string `default:"\\n" json:"customEventDelimiter"`
	// Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.
	CustomContentType *string `default:"application/x-ndjson" json:"customContentType"`
	// Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.
	CustomPayloadExpression *string `default:"\\${events}" json:"customPayloadExpression"`
	// HTTP content-type header value
	AdvancedContentType *string `default:"application/json" json:"advancedContentType"`
	// Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatEventCode *string `json:"formatEventCode,omitempty"`
	// Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatPayloadCode *string `json:"formatPayloadCode,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeWebhook `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionWebhook `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorWebhook `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsWebhook        `json:"pqControls,omitempty"`
	Username         *string                   `json:"username,omitempty"`
	Password         *string                   `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamWebhook `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderWebhook `json:"oauthHeaders,omitempty"`
	// URL of a webhook endpoint to send events to, such as http://localhost:10200
	URL *string `json:"url,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool        `default:"false" json:"excludeSelf"`
	Urls        []URLWebhook `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
}

func (o OutputWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputWebhook) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputWebhook) GetType() TypeWebhook {
	if o == nil {
		return TypeWebhook("")
	}
	return o.Type
}

func (o *OutputWebhook) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputWebhook) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputWebhook) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputWebhook) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputWebhook) GetMethod() *MethodWebhook {
	if o == nil {
		return nil
	}
	return o.Method
}

func (o *OutputWebhook) GetFormat() *FormatWebhook {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputWebhook) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputWebhook) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputWebhook) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputWebhook) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputWebhook) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputWebhook) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputWebhook) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputWebhook) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputWebhook) GetExtraHTTPHeaders() []ExtraHTTPHeaderWebhook {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputWebhook) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputWebhook) GetFailedRequestLoggingMode() *FailedRequestLoggingModeWebhook {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputWebhook) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputWebhook) GetResponseRetrySettings() []ResponseRetrySettingWebhook {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputWebhook) GetTimeoutRetrySettings() *TimeoutRetrySettingsWebhook {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputWebhook) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputWebhook) GetOnBackpressure() *BackpressureBehaviorWebhook {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputWebhook) GetAuthType() *AuthenticationTypeWebhook {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputWebhook) GetTLS() *TLSSettingsClientSideWebhook {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputWebhook) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputWebhook) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputWebhook) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputWebhook) GetCustomSourceExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomSourceExpression
}

func (o *OutputWebhook) GetCustomDropWhenNull() *bool {
	if o == nil {
		return nil
	}
	return o.CustomDropWhenNull
}

func (o *OutputWebhook) GetCustomEventDelimiter() *string {
	if o == nil {
		return nil
	}
	return o.CustomEventDelimiter
}

func (o *OutputWebhook) GetCustomContentType() *string {
	if o == nil {
		return nil
	}
	return o.CustomContentType
}

func (o *OutputWebhook) GetCustomPayloadExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomPayloadExpression
}

func (o *OutputWebhook) GetAdvancedContentType() *string {
	if o == nil {
		return nil
	}
	return o.AdvancedContentType
}

func (o *OutputWebhook) GetFormatEventCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatEventCode
}

func (o *OutputWebhook) GetFormatPayloadCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatPayloadCode
}

func (o *OutputWebhook) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputWebhook) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputWebhook) GetPqMode() *ModeWebhook {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputWebhook) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputWebhook) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputWebhook) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputWebhook) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputWebhook) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputWebhook) GetPqCompress() *CompressionWebhook {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputWebhook) GetPqOnBackpressure() *QueueFullBehaviorWebhook {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputWebhook) GetPqControls() *PqControlsWebhook {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputWebhook) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputWebhook) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputWebhook) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputWebhook) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputWebhook) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputWebhook) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputWebhook) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputWebhook) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputWebhook) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputWebhook) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputWebhook) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputWebhook) GetOauthParams() []OauthParamWebhook {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputWebhook) GetOauthHeaders() []OauthHeaderWebhook {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputWebhook) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputWebhook) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputWebhook) GetUrls() []URLWebhook {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputWebhook) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputWebhook) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

type TypeDefault string

const (
	TypeDefaultDefault TypeDefault = "default"
)

func (e TypeDefault) ToPointer() *TypeDefault {
	return &e
}
func (e *TypeDefault) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "default":
		*e = TypeDefault(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDefault: %v", v)
	}
}

type OutputDefault struct {
	// Unique ID for this output
	ID   string      `json:"id"`
	Type TypeDefault `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.
	DefaultID string `json:"defaultId"`
}

func (o OutputDefault) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDefault) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"id", "type", "defaultId"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDefault) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDefault) GetType() TypeDefault {
	if o == nil {
		return TypeDefault("")
	}
	return o.Type
}

func (o *OutputDefault) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDefault) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDefault) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDefault) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDefault) GetDefaultID() string {
	if o == nil {
		return ""
	}
	return o.DefaultID
}

type CreateOutputRequestType string

const (
	CreateOutputRequestTypeDefault                CreateOutputRequestType = "default"
	CreateOutputRequestTypeWebhook                CreateOutputRequestType = "webhook"
	CreateOutputRequestTypeSentinel               CreateOutputRequestType = "sentinel"
	CreateOutputRequestTypeDevnull                CreateOutputRequestType = "devnull"
	CreateOutputRequestTypeSyslog                 CreateOutputRequestType = "syslog"
	CreateOutputRequestTypeSplunk                 CreateOutputRequestType = "splunk"
	CreateOutputRequestTypeSplunkLb               CreateOutputRequestType = "splunk_lb"
	CreateOutputRequestTypeSplunkHec              CreateOutputRequestType = "splunk_hec"
	CreateOutputRequestTypeTcpjson                CreateOutputRequestType = "tcpjson"
	CreateOutputRequestTypeWavefront              CreateOutputRequestType = "wavefront"
	CreateOutputRequestTypeSignalfx               CreateOutputRequestType = "signalfx"
	CreateOutputRequestTypeFilesystem             CreateOutputRequestType = "filesystem"
	CreateOutputRequestTypeS3                     CreateOutputRequestType = "s3"
	CreateOutputRequestTypeAzureBlob              CreateOutputRequestType = "azure_blob"
	CreateOutputRequestTypeAzureDataExplorer      CreateOutputRequestType = "azure_data_explorer"
	CreateOutputRequestTypeAzureLogs              CreateOutputRequestType = "azure_logs"
	CreateOutputRequestTypeKinesis                CreateOutputRequestType = "kinesis"
	CreateOutputRequestTypeHoneycomb              CreateOutputRequestType = "honeycomb"
	CreateOutputRequestTypeAzureEventhub          CreateOutputRequestType = "azure_eventhub"
	CreateOutputRequestTypeGoogleChronicle        CreateOutputRequestType = "google_chronicle"
	CreateOutputRequestTypeGoogleCloudStorage     CreateOutputRequestType = "google_cloud_storage"
	CreateOutputRequestTypeGoogleCloudLogging     CreateOutputRequestType = "google_cloud_logging"
	CreateOutputRequestTypeGooglePubsub           CreateOutputRequestType = "google_pubsub"
	CreateOutputRequestTypeExabeam                CreateOutputRequestType = "exabeam"
	CreateOutputRequestTypeKafka                  CreateOutputRequestType = "kafka"
	CreateOutputRequestTypeConfluentCloud         CreateOutputRequestType = "confluent_cloud"
	CreateOutputRequestTypeMsk                    CreateOutputRequestType = "msk"
	CreateOutputRequestTypeElastic                CreateOutputRequestType = "elastic"
	CreateOutputRequestTypeElasticCloud           CreateOutputRequestType = "elastic_cloud"
	CreateOutputRequestTypeNewrelic               CreateOutputRequestType = "newrelic"
	CreateOutputRequestTypeNewrelicEvents         CreateOutputRequestType = "newrelic_events"
	CreateOutputRequestTypeInfluxdb               CreateOutputRequestType = "influxdb"
	CreateOutputRequestTypeCloudwatch             CreateOutputRequestType = "cloudwatch"
	CreateOutputRequestTypeMinio                  CreateOutputRequestType = "minio"
	CreateOutputRequestTypeStatsd                 CreateOutputRequestType = "statsd"
	CreateOutputRequestTypeStatsdExt              CreateOutputRequestType = "statsd_ext"
	CreateOutputRequestTypeGraphite               CreateOutputRequestType = "graphite"
	CreateOutputRequestTypeRouter                 CreateOutputRequestType = "router"
	CreateOutputRequestTypeSns                    CreateOutputRequestType = "sns"
	CreateOutputRequestTypeSqs                    CreateOutputRequestType = "sqs"
	CreateOutputRequestTypeSnmp                   CreateOutputRequestType = "snmp"
	CreateOutputRequestTypeSumoLogic              CreateOutputRequestType = "sumo_logic"
	CreateOutputRequestTypeDatadog                CreateOutputRequestType = "datadog"
	CreateOutputRequestTypeGrafanaCloud           CreateOutputRequestType = "grafana_cloud"
	CreateOutputRequestTypeLoki                   CreateOutputRequestType = "loki"
	CreateOutputRequestTypePrometheus             CreateOutputRequestType = "prometheus"
	CreateOutputRequestTypeRing                   CreateOutputRequestType = "ring"
	CreateOutputRequestTypeOpenTelemetry          CreateOutputRequestType = "open_telemetry"
	CreateOutputRequestTypeServiceNow             CreateOutputRequestType = "service_now"
	CreateOutputRequestTypeDataset                CreateOutputRequestType = "dataset"
	CreateOutputRequestTypeCriblTCP               CreateOutputRequestType = "cribl_tcp"
	CreateOutputRequestTypeCriblHTTP              CreateOutputRequestType = "cribl_http"
	CreateOutputRequestTypeCriblSearchEngine      CreateOutputRequestType = "cribl_search_engine"
	CreateOutputRequestTypeHumioHec               CreateOutputRequestType = "humio_hec"
	CreateOutputRequestTypeCrowdstrikeNextGenSiem CreateOutputRequestType = "crowdstrike_next_gen_siem"
	CreateOutputRequestTypeDlS3                   CreateOutputRequestType = "dl_s3"
	CreateOutputRequestTypeSecurityLake           CreateOutputRequestType = "security_lake"
	CreateOutputRequestTypeCriblLake              CreateOutputRequestType = "cribl_lake"
	CreateOutputRequestTypeDiskSpool              CreateOutputRequestType = "disk_spool"
	CreateOutputRequestTypeClickHouse             CreateOutputRequestType = "click_house"
	CreateOutputRequestTypeXsiam                  CreateOutputRequestType = "xsiam"
	CreateOutputRequestTypeNetflow                CreateOutputRequestType = "netflow"
	CreateOutputRequestTypeDynatraceHTTP          CreateOutputRequestType = "dynatrace_http"
	CreateOutputRequestTypeDynatraceOtlp          CreateOutputRequestType = "dynatrace_otlp"
	CreateOutputRequestTypeSentinelOneAiSiem      CreateOutputRequestType = "sentinel_one_ai_siem"
	CreateOutputRequestTypeChronicle              CreateOutputRequestType = "chronicle"
	CreateOutputRequestTypeDatabricks             CreateOutputRequestType = "databricks"
	CreateOutputRequestTypeMicrosoftFabric        CreateOutputRequestType = "microsoft_fabric"
	CreateOutputRequestTypeCloudflareR2           CreateOutputRequestType = "cloudflare_r2"
)

// CreateOutputRequest - Output object
type CreateOutputRequest struct {
	OutputDefault                *OutputDefault                `queryParam:"inline,name=RequestBody" union:"member"`
	OutputWebhook                *OutputWebhook                `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSentinel               *OutputSentinel               `queryParam:"inline,name=RequestBody" union:"member"`
	OutputDevnull                *OutputDevnull                `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSyslog                 *OutputSyslog                 `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSplunk                 *OutputSplunk                 `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSplunkLb               *OutputSplunkLb               `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSplunkHec              *OutputSplunkHec              `queryParam:"inline,name=RequestBody" union:"member"`
	OutputTcpjson                *OutputTcpjson                `queryParam:"inline,name=RequestBody" union:"member"`
	OutputWavefront              *OutputWavefront              `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSignalfx               *OutputSignalfx               `queryParam:"inline,name=RequestBody" union:"member"`
	OutputFilesystem             *OutputFilesystem             `queryParam:"inline,name=RequestBody" union:"member"`
	OutputS3                     *OutputS3                     `queryParam:"inline,name=RequestBody" union:"member"`
	OutputAzureBlob              *OutputAzureBlob              `queryParam:"inline,name=RequestBody" union:"member"`
	OutputAzureDataExplorer      *OutputAzureDataExplorer      `queryParam:"inline,name=RequestBody" union:"member"`
	OutputAzureLogs              *OutputAzureLogs              `queryParam:"inline,name=RequestBody" union:"member"`
	OutputKinesis                *OutputKinesis                `queryParam:"inline,name=RequestBody" union:"member"`
	OutputHoneycomb              *OutputHoneycomb              `queryParam:"inline,name=RequestBody" union:"member"`
	OutputAzureEventhub          *OutputAzureEventhub          `queryParam:"inline,name=RequestBody" union:"member"`
	OutputGoogleChronicle        *OutputGoogleChronicle        `queryParam:"inline,name=RequestBody" union:"member"`
	OutputGoogleCloudStorage     *OutputGoogleCloudStorage     `queryParam:"inline,name=RequestBody" union:"member"`
	OutputGoogleCloudLogging     *OutputGoogleCloudLogging     `queryParam:"inline,name=RequestBody" union:"member"`
	OutputGooglePubsub           *OutputGooglePubsub           `queryParam:"inline,name=RequestBody" union:"member"`
	OutputExabeam                *OutputExabeam                `queryParam:"inline,name=RequestBody" union:"member"`
	OutputKafka                  *OutputKafka                  `queryParam:"inline,name=RequestBody" union:"member"`
	OutputConfluentCloud         *OutputConfluentCloud         `queryParam:"inline,name=RequestBody" union:"member"`
	OutputMsk                    *OutputMsk                    `queryParam:"inline,name=RequestBody" union:"member"`
	OutputElastic                *OutputElastic                `queryParam:"inline,name=RequestBody" union:"member"`
	OutputElasticCloud           *OutputElasticCloud           `queryParam:"inline,name=RequestBody" union:"member"`
	OutputNewrelic               *OutputNewrelic               `queryParam:"inline,name=RequestBody" union:"member"`
	OutputNewrelicEvents         *OutputNewrelicEvents         `queryParam:"inline,name=RequestBody" union:"member"`
	OutputInfluxdb               *OutputInfluxdb               `queryParam:"inline,name=RequestBody" union:"member"`
	OutputCloudwatch             *OutputCloudwatch             `queryParam:"inline,name=RequestBody" union:"member"`
	OutputMinio                  *OutputMinio                  `queryParam:"inline,name=RequestBody" union:"member"`
	OutputStatsd                 *OutputStatsd                 `queryParam:"inline,name=RequestBody" union:"member"`
	OutputStatsdExt              *OutputStatsdExt              `queryParam:"inline,name=RequestBody" union:"member"`
	OutputGraphite               *OutputGraphite               `queryParam:"inline,name=RequestBody" union:"member"`
	OutputRouter                 *OutputRouter                 `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSns                    *OutputSns                    `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSqs                    *OutputSqs                    `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSnmp                   *OutputSnmp                   `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSumoLogic              *OutputSumoLogic              `queryParam:"inline,name=RequestBody" union:"member"`
	OutputDatadog                *OutputDatadog                `queryParam:"inline,name=RequestBody" union:"member"`
	OutputGrafanaCloud           *OutputGrafanaCloud           `queryParam:"inline,name=RequestBody" union:"member"`
	OutputLoki                   *OutputLoki                   `queryParam:"inline,name=RequestBody" union:"member"`
	OutputPrometheus             *OutputPrometheus             `queryParam:"inline,name=RequestBody" union:"member"`
	OutputRing                   *OutputRing                   `queryParam:"inline,name=RequestBody" union:"member"`
	OutputOpenTelemetry          *OutputOpenTelemetry          `queryParam:"inline,name=RequestBody" union:"member"`
	OutputServiceNow             *OutputServiceNow             `queryParam:"inline,name=RequestBody" union:"member"`
	OutputDataset                *OutputDataset                `queryParam:"inline,name=RequestBody" union:"member"`
	OutputCriblTCP               *OutputCriblTCP               `queryParam:"inline,name=RequestBody" union:"member"`
	OutputCriblHTTP              *OutputCriblHTTP              `queryParam:"inline,name=RequestBody" union:"member"`
	OutputCriblSearchEngine      *OutputCriblSearchEngine      `queryParam:"inline,name=RequestBody" union:"member"`
	OutputHumioHec               *OutputHumioHec               `queryParam:"inline,name=RequestBody" union:"member"`
	OutputCrowdstrikeNextGenSiem *OutputCrowdstrikeNextGenSiem `queryParam:"inline,name=RequestBody" union:"member"`
	OutputDlS3                   *OutputDlS3                   `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSecurityLake           *OutputSecurityLake           `queryParam:"inline,name=RequestBody" union:"member"`
	OutputCriblLake              *OutputCriblLake              `queryParam:"inline,name=RequestBody" union:"member"`
	OutputDiskSpool              *OutputDiskSpool              `queryParam:"inline,name=RequestBody" union:"member"`
	OutputClickHouse             *OutputClickHouse             `queryParam:"inline,name=RequestBody" union:"member"`
	OutputXsiam                  *OutputXsiam                  `queryParam:"inline,name=RequestBody" union:"member"`
	OutputNetflow                *OutputNetflow                `queryParam:"inline,name=RequestBody" union:"member"`
	OutputDynatraceHTTP          *OutputDynatraceHTTP          `queryParam:"inline,name=RequestBody" union:"member"`
	OutputDynatraceOtlp          *OutputDynatraceOtlp          `queryParam:"inline,name=RequestBody" union:"member"`
	OutputSentinelOneAiSiem      *OutputSentinelOneAiSiem      `queryParam:"inline,name=RequestBody" union:"member"`
	OutputChronicle              *OutputChronicle              `queryParam:"inline,name=RequestBody" union:"member"`
	OutputDatabricks             *OutputDatabricks             `queryParam:"inline,name=RequestBody" union:"member"`
	OutputMicrosoftFabric        *OutputMicrosoftFabric        `queryParam:"inline,name=RequestBody" union:"member"`
	OutputCloudflareR2           *OutputCloudflareR2           `queryParam:"inline,name=RequestBody" union:"member"`

	Type CreateOutputRequestType
}

func CreateCreateOutputRequestDefault(defaultT OutputDefault) CreateOutputRequest {
	typ := CreateOutputRequestTypeDefault

	typStr := TypeDefault(typ)
	defaultT.Type = typStr

	return CreateOutputRequest{
		OutputDefault: &defaultT,
		Type:          typ,
	}
}

func CreateCreateOutputRequestWebhook(webhook OutputWebhook) CreateOutputRequest {
	typ := CreateOutputRequestTypeWebhook

	typStr := TypeWebhook(typ)
	webhook.Type = typStr

	return CreateOutputRequest{
		OutputWebhook: &webhook,
		Type:          typ,
	}
}

func CreateCreateOutputRequestSentinel(sentinel OutputSentinel) CreateOutputRequest {
	typ := CreateOutputRequestTypeSentinel

	typStr := TypeSentinel(typ)
	sentinel.Type = typStr

	return CreateOutputRequest{
		OutputSentinel: &sentinel,
		Type:           typ,
	}
}

func CreateCreateOutputRequestDevnull(devnull OutputDevnull) CreateOutputRequest {
	typ := CreateOutputRequestTypeDevnull

	typStr := TypeDevnull(typ)
	devnull.Type = typStr

	return CreateOutputRequest{
		OutputDevnull: &devnull,
		Type:          typ,
	}
}

func CreateCreateOutputRequestSyslog(syslog OutputSyslog) CreateOutputRequest {
	typ := CreateOutputRequestTypeSyslog

	typStr := CreateOutputTypeSyslog(typ)
	syslog.Type = typStr

	return CreateOutputRequest{
		OutputSyslog: &syslog,
		Type:         typ,
	}
}

func CreateCreateOutputRequestSplunk(splunk OutputSplunk) CreateOutputRequest {
	typ := CreateOutputRequestTypeSplunk

	typStr := CreateOutputTypeSplunk(typ)
	splunk.Type = typStr

	return CreateOutputRequest{
		OutputSplunk: &splunk,
		Type:         typ,
	}
}

func CreateCreateOutputRequestSplunkLb(splunkLb OutputSplunkLb) CreateOutputRequest {
	typ := CreateOutputRequestTypeSplunkLb

	typStr := TypeSplunkLb(typ)
	splunkLb.Type = typStr

	return CreateOutputRequest{
		OutputSplunkLb: &splunkLb,
		Type:           typ,
	}
}

func CreateCreateOutputRequestSplunkHec(splunkHec OutputSplunkHec) CreateOutputRequest {
	typ := CreateOutputRequestTypeSplunkHec

	typStr := CreateOutputTypeSplunkHec(typ)
	splunkHec.Type = typStr

	return CreateOutputRequest{
		OutputSplunkHec: &splunkHec,
		Type:            typ,
	}
}

func CreateCreateOutputRequestTcpjson(tcpjson OutputTcpjson) CreateOutputRequest {
	typ := CreateOutputRequestTypeTcpjson

	typStr := CreateOutputTypeTcpjson(typ)
	tcpjson.Type = typStr

	return CreateOutputRequest{
		OutputTcpjson: &tcpjson,
		Type:          typ,
	}
}

func CreateCreateOutputRequestWavefront(wavefront OutputWavefront) CreateOutputRequest {
	typ := CreateOutputRequestTypeWavefront

	typStr := TypeWavefront(typ)
	wavefront.Type = typStr

	return CreateOutputRequest{
		OutputWavefront: &wavefront,
		Type:            typ,
	}
}

func CreateCreateOutputRequestSignalfx(signalfx OutputSignalfx) CreateOutputRequest {
	typ := CreateOutputRequestTypeSignalfx

	typStr := TypeSignalfx(typ)
	signalfx.Type = typStr

	return CreateOutputRequest{
		OutputSignalfx: &signalfx,
		Type:           typ,
	}
}

func CreateCreateOutputRequestFilesystem(filesystem OutputFilesystem) CreateOutputRequest {
	typ := CreateOutputRequestTypeFilesystem

	typStr := TypeFilesystem(typ)
	filesystem.Type = typStr

	return CreateOutputRequest{
		OutputFilesystem: &filesystem,
		Type:             typ,
	}
}

func CreateCreateOutputRequestS3(s3 OutputS3) CreateOutputRequest {
	typ := CreateOutputRequestTypeS3

	typStr := CreateOutputTypeS3(typ)
	s3.Type = typStr

	return CreateOutputRequest{
		OutputS3: &s3,
		Type:     typ,
	}
}

func CreateCreateOutputRequestAzureBlob(azureBlob OutputAzureBlob) CreateOutputRequest {
	typ := CreateOutputRequestTypeAzureBlob

	typStr := CreateOutputTypeAzureBlob(typ)
	azureBlob.Type = typStr

	return CreateOutputRequest{
		OutputAzureBlob: &azureBlob,
		Type:            typ,
	}
}

func CreateCreateOutputRequestAzureDataExplorer(azureDataExplorer OutputAzureDataExplorer) CreateOutputRequest {
	typ := CreateOutputRequestTypeAzureDataExplorer

	typStr := TypeAzureDataExplorer(typ)
	azureDataExplorer.Type = typStr

	return CreateOutputRequest{
		OutputAzureDataExplorer: &azureDataExplorer,
		Type:                    typ,
	}
}

func CreateCreateOutputRequestAzureLogs(azureLogs OutputAzureLogs) CreateOutputRequest {
	typ := CreateOutputRequestTypeAzureLogs

	typStr := TypeAzureLogs(typ)
	azureLogs.Type = typStr

	return CreateOutputRequest{
		OutputAzureLogs: &azureLogs,
		Type:            typ,
	}
}

func CreateCreateOutputRequestKinesis(kinesis OutputKinesis) CreateOutputRequest {
	typ := CreateOutputRequestTypeKinesis

	typStr := CreateOutputTypeKinesis(typ)
	kinesis.Type = typStr

	return CreateOutputRequest{
		OutputKinesis: &kinesis,
		Type:          typ,
	}
}

func CreateCreateOutputRequestHoneycomb(honeycomb OutputHoneycomb) CreateOutputRequest {
	typ := CreateOutputRequestTypeHoneycomb

	typStr := TypeHoneycomb(typ)
	honeycomb.Type = typStr

	return CreateOutputRequest{
		OutputHoneycomb: &honeycomb,
		Type:            typ,
	}
}

func CreateCreateOutputRequestAzureEventhub(azureEventhub OutputAzureEventhub) CreateOutputRequest {
	typ := CreateOutputRequestTypeAzureEventhub

	typStr := TypeAzureEventhub(typ)
	azureEventhub.Type = typStr

	return CreateOutputRequest{
		OutputAzureEventhub: &azureEventhub,
		Type:                typ,
	}
}

func CreateCreateOutputRequestGoogleChronicle(googleChronicle OutputGoogleChronicle) CreateOutputRequest {
	typ := CreateOutputRequestTypeGoogleChronicle

	typStr := TypeGoogleChronicle(typ)
	googleChronicle.Type = typStr

	return CreateOutputRequest{
		OutputGoogleChronicle: &googleChronicle,
		Type:                  typ,
	}
}

func CreateCreateOutputRequestGoogleCloudStorage(googleCloudStorage OutputGoogleCloudStorage) CreateOutputRequest {
	typ := CreateOutputRequestTypeGoogleCloudStorage

	typStr := TypeGoogleCloudStorage(typ)
	googleCloudStorage.Type = typStr

	return CreateOutputRequest{
		OutputGoogleCloudStorage: &googleCloudStorage,
		Type:                     typ,
	}
}

func CreateCreateOutputRequestGoogleCloudLogging(googleCloudLogging OutputGoogleCloudLogging) CreateOutputRequest {
	typ := CreateOutputRequestTypeGoogleCloudLogging

	typStr := TypeGoogleCloudLogging(typ)
	googleCloudLogging.Type = typStr

	return CreateOutputRequest{
		OutputGoogleCloudLogging: &googleCloudLogging,
		Type:                     typ,
	}
}

func CreateCreateOutputRequestGooglePubsub(googlePubsub OutputGooglePubsub) CreateOutputRequest {
	typ := CreateOutputRequestTypeGooglePubsub

	typStr := CreateOutputTypeGooglePubsub(typ)
	googlePubsub.Type = typStr

	return CreateOutputRequest{
		OutputGooglePubsub: &googlePubsub,
		Type:               typ,
	}
}

func CreateCreateOutputRequestExabeam(exabeam OutputExabeam) CreateOutputRequest {
	typ := CreateOutputRequestTypeExabeam

	typStr := TypeExabeam(typ)
	exabeam.Type = typStr

	return CreateOutputRequest{
		OutputExabeam: &exabeam,
		Type:          typ,
	}
}

func CreateCreateOutputRequestKafka(kafka OutputKafka) CreateOutputRequest {
	typ := CreateOutputRequestTypeKafka

	typStr := CreateOutputTypeKafka(typ)
	kafka.Type = typStr

	return CreateOutputRequest{
		OutputKafka: &kafka,
		Type:        typ,
	}
}

func CreateCreateOutputRequestConfluentCloud(confluentCloud OutputConfluentCloud) CreateOutputRequest {
	typ := CreateOutputRequestTypeConfluentCloud

	typStr := CreateOutputTypeConfluentCloud(typ)
	confluentCloud.Type = typStr

	return CreateOutputRequest{
		OutputConfluentCloud: &confluentCloud,
		Type:                 typ,
	}
}

func CreateCreateOutputRequestMsk(msk OutputMsk) CreateOutputRequest {
	typ := CreateOutputRequestTypeMsk

	typStr := CreateOutputTypeMsk(typ)
	msk.Type = typStr

	return CreateOutputRequest{
		OutputMsk: &msk,
		Type:      typ,
	}
}

func CreateCreateOutputRequestElastic(elastic OutputElastic) CreateOutputRequest {
	typ := CreateOutputRequestTypeElastic

	typStr := CreateOutputTypeElastic(typ)
	elastic.Type = typStr

	return CreateOutputRequest{
		OutputElastic: &elastic,
		Type:          typ,
	}
}

func CreateCreateOutputRequestElasticCloud(elasticCloud OutputElasticCloud) CreateOutputRequest {
	typ := CreateOutputRequestTypeElasticCloud

	typStr := TypeElasticCloud(typ)
	elasticCloud.Type = typStr

	return CreateOutputRequest{
		OutputElasticCloud: &elasticCloud,
		Type:               typ,
	}
}

func CreateCreateOutputRequestNewrelic(newrelic OutputNewrelic) CreateOutputRequest {
	typ := CreateOutputRequestTypeNewrelic

	typStr := TypeNewrelic(typ)
	newrelic.Type = typStr

	return CreateOutputRequest{
		OutputNewrelic: &newrelic,
		Type:           typ,
	}
}

func CreateCreateOutputRequestNewrelicEvents(newrelicEvents OutputNewrelicEvents) CreateOutputRequest {
	typ := CreateOutputRequestTypeNewrelicEvents

	typStr := TypeNewrelicEvents(typ)
	newrelicEvents.Type = typStr

	return CreateOutputRequest{
		OutputNewrelicEvents: &newrelicEvents,
		Type:                 typ,
	}
}

func CreateCreateOutputRequestInfluxdb(influxdb OutputInfluxdb) CreateOutputRequest {
	typ := CreateOutputRequestTypeInfluxdb

	typStr := TypeInfluxdb(typ)
	influxdb.Type = typStr

	return CreateOutputRequest{
		OutputInfluxdb: &influxdb,
		Type:           typ,
	}
}

func CreateCreateOutputRequestCloudwatch(cloudwatch OutputCloudwatch) CreateOutputRequest {
	typ := CreateOutputRequestTypeCloudwatch

	typStr := TypeCloudwatch(typ)
	cloudwatch.Type = typStr

	return CreateOutputRequest{
		OutputCloudwatch: &cloudwatch,
		Type:             typ,
	}
}

func CreateCreateOutputRequestMinio(minio OutputMinio) CreateOutputRequest {
	typ := CreateOutputRequestTypeMinio

	typStr := TypeMinio(typ)
	minio.Type = typStr

	return CreateOutputRequest{
		OutputMinio: &minio,
		Type:        typ,
	}
}

func CreateCreateOutputRequestStatsd(statsd OutputStatsd) CreateOutputRequest {
	typ := CreateOutputRequestTypeStatsd

	typStr := TypeStatsd(typ)
	statsd.Type = typStr

	return CreateOutputRequest{
		OutputStatsd: &statsd,
		Type:         typ,
	}
}

func CreateCreateOutputRequestStatsdExt(statsdExt OutputStatsdExt) CreateOutputRequest {
	typ := CreateOutputRequestTypeStatsdExt

	typStr := TypeStatsdExt(typ)
	statsdExt.Type = typStr

	return CreateOutputRequest{
		OutputStatsdExt: &statsdExt,
		Type:            typ,
	}
}

func CreateCreateOutputRequestGraphite(graphite OutputGraphite) CreateOutputRequest {
	typ := CreateOutputRequestTypeGraphite

	typStr := TypeGraphite(typ)
	graphite.Type = typStr

	return CreateOutputRequest{
		OutputGraphite: &graphite,
		Type:           typ,
	}
}

func CreateCreateOutputRequestRouter(router OutputRouter) CreateOutputRequest {
	typ := CreateOutputRequestTypeRouter

	typStr := TypeRouter(typ)
	router.Type = typStr

	return CreateOutputRequest{
		OutputRouter: &router,
		Type:         typ,
	}
}

func CreateCreateOutputRequestSns(sns OutputSns) CreateOutputRequest {
	typ := CreateOutputRequestTypeSns

	typStr := TypeSns(typ)
	sns.Type = typStr

	return CreateOutputRequest{
		OutputSns: &sns,
		Type:      typ,
	}
}

func CreateCreateOutputRequestSqs(sqs OutputSqs) CreateOutputRequest {
	typ := CreateOutputRequestTypeSqs

	typStr := CreateOutputTypeSqs(typ)
	sqs.Type = typStr

	return CreateOutputRequest{
		OutputSqs: &sqs,
		Type:      typ,
	}
}

func CreateCreateOutputRequestSnmp(snmp OutputSnmp) CreateOutputRequest {
	typ := CreateOutputRequestTypeSnmp

	typStr := CreateOutputTypeSnmp(typ)
	snmp.Type = typStr

	return CreateOutputRequest{
		OutputSnmp: &snmp,
		Type:       typ,
	}
}

func CreateCreateOutputRequestSumoLogic(sumoLogic OutputSumoLogic) CreateOutputRequest {
	typ := CreateOutputRequestTypeSumoLogic

	typStr := TypeSumoLogic(typ)
	sumoLogic.Type = typStr

	return CreateOutputRequest{
		OutputSumoLogic: &sumoLogic,
		Type:            typ,
	}
}

func CreateCreateOutputRequestDatadog(datadog OutputDatadog) CreateOutputRequest {
	typ := CreateOutputRequestTypeDatadog

	typStr := TypeDatadog(typ)
	datadog.Type = typStr

	return CreateOutputRequest{
		OutputDatadog: &datadog,
		Type:          typ,
	}
}

func CreateCreateOutputRequestGrafanaCloud(grafanaCloud OutputGrafanaCloud) CreateOutputRequest {
	typ := CreateOutputRequestTypeGrafanaCloud

	return CreateOutputRequest{
		OutputGrafanaCloud: &grafanaCloud,
		Type:               typ,
	}
}

func CreateCreateOutputRequestLoki(loki OutputLoki) CreateOutputRequest {
	typ := CreateOutputRequestTypeLoki

	typStr := CreateOutputTypeLoki(typ)
	loki.Type = typStr

	return CreateOutputRequest{
		OutputLoki: &loki,
		Type:       typ,
	}
}

func CreateCreateOutputRequestPrometheus(prometheus OutputPrometheus) CreateOutputRequest {
	typ := CreateOutputRequestTypePrometheus

	typStr := CreateOutputTypePrometheus(typ)
	prometheus.Type = typStr

	return CreateOutputRequest{
		OutputPrometheus: &prometheus,
		Type:             typ,
	}
}

func CreateCreateOutputRequestRing(ring OutputRing) CreateOutputRequest {
	typ := CreateOutputRequestTypeRing

	typStr := TypeRing(typ)
	ring.Type = typStr

	return CreateOutputRequest{
		OutputRing: &ring,
		Type:       typ,
	}
}

func CreateCreateOutputRequestOpenTelemetry(openTelemetry OutputOpenTelemetry) CreateOutputRequest {
	typ := CreateOutputRequestTypeOpenTelemetry

	typStr := CreateOutputTypeOpenTelemetry(typ)
	openTelemetry.Type = typStr

	return CreateOutputRequest{
		OutputOpenTelemetry: &openTelemetry,
		Type:                typ,
	}
}

func CreateCreateOutputRequestServiceNow(serviceNow OutputServiceNow) CreateOutputRequest {
	typ := CreateOutputRequestTypeServiceNow

	typStr := TypeServiceNow(typ)
	serviceNow.Type = typStr

	return CreateOutputRequest{
		OutputServiceNow: &serviceNow,
		Type:             typ,
	}
}

func CreateCreateOutputRequestDataset(dataset OutputDataset) CreateOutputRequest {
	typ := CreateOutputRequestTypeDataset

	typStr := TypeDataset(typ)
	dataset.Type = typStr

	return CreateOutputRequest{
		OutputDataset: &dataset,
		Type:          typ,
	}
}

func CreateCreateOutputRequestCriblTCP(criblTCP OutputCriblTCP) CreateOutputRequest {
	typ := CreateOutputRequestTypeCriblTCP

	typStr := CreateOutputTypeCriblTCP(typ)
	criblTCP.Type = typStr

	return CreateOutputRequest{
		OutputCriblTCP: &criblTCP,
		Type:           typ,
	}
}

func CreateCreateOutputRequestCriblHTTP(criblHTTP OutputCriblHTTP) CreateOutputRequest {
	typ := CreateOutputRequestTypeCriblHTTP

	typStr := CreateOutputTypeCriblHTTP(typ)
	criblHTTP.Type = typStr

	return CreateOutputRequest{
		OutputCriblHTTP: &criblHTTP,
		Type:            typ,
	}
}

func CreateCreateOutputRequestCriblSearchEngine(criblSearchEngine OutputCriblSearchEngine) CreateOutputRequest {
	typ := CreateOutputRequestTypeCriblSearchEngine

	typStr := TypeCriblSearchEngine(typ)
	criblSearchEngine.Type = typStr

	return CreateOutputRequest{
		OutputCriblSearchEngine: &criblSearchEngine,
		Type:                    typ,
	}
}

func CreateCreateOutputRequestHumioHec(humioHec OutputHumioHec) CreateOutputRequest {
	typ := CreateOutputRequestTypeHumioHec

	typStr := TypeHumioHec(typ)
	humioHec.Type = typStr

	return CreateOutputRequest{
		OutputHumioHec: &humioHec,
		Type:           typ,
	}
}

func CreateCreateOutputRequestCrowdstrikeNextGenSiem(crowdstrikeNextGenSiem OutputCrowdstrikeNextGenSiem) CreateOutputRequest {
	typ := CreateOutputRequestTypeCrowdstrikeNextGenSiem

	typStr := TypeCrowdstrikeNextGenSiem(typ)
	crowdstrikeNextGenSiem.Type = typStr

	return CreateOutputRequest{
		OutputCrowdstrikeNextGenSiem: &crowdstrikeNextGenSiem,
		Type:                         typ,
	}
}

func CreateCreateOutputRequestDlS3(dlS3 OutputDlS3) CreateOutputRequest {
	typ := CreateOutputRequestTypeDlS3

	typStr := TypeDlS3(typ)
	dlS3.Type = typStr

	return CreateOutputRequest{
		OutputDlS3: &dlS3,
		Type:       typ,
	}
}

func CreateCreateOutputRequestSecurityLake(securityLake OutputSecurityLake) CreateOutputRequest {
	typ := CreateOutputRequestTypeSecurityLake

	typStr := CreateOutputTypeSecurityLake(typ)
	securityLake.Type = typStr

	return CreateOutputRequest{
		OutputSecurityLake: &securityLake,
		Type:               typ,
	}
}

func CreateCreateOutputRequestCriblLake(criblLake OutputCriblLake) CreateOutputRequest {
	typ := CreateOutputRequestTypeCriblLake

	typStr := TypeCriblLake(typ)
	criblLake.Type = typStr

	return CreateOutputRequest{
		OutputCriblLake: &criblLake,
		Type:            typ,
	}
}

func CreateCreateOutputRequestDiskSpool(diskSpool OutputDiskSpool) CreateOutputRequest {
	typ := CreateOutputRequestTypeDiskSpool

	typStr := TypeDiskSpool(typ)
	diskSpool.Type = typStr

	return CreateOutputRequest{
		OutputDiskSpool: &diskSpool,
		Type:            typ,
	}
}

func CreateCreateOutputRequestClickHouse(clickHouse OutputClickHouse) CreateOutputRequest {
	typ := CreateOutputRequestTypeClickHouse

	typStr := TypeClickHouse(typ)
	clickHouse.Type = typStr

	return CreateOutputRequest{
		OutputClickHouse: &clickHouse,
		Type:             typ,
	}
}

func CreateCreateOutputRequestXsiam(xsiam OutputXsiam) CreateOutputRequest {
	typ := CreateOutputRequestTypeXsiam

	typStr := TypeXsiam(typ)
	xsiam.Type = typStr

	return CreateOutputRequest{
		OutputXsiam: &xsiam,
		Type:        typ,
	}
}

func CreateCreateOutputRequestNetflow(netflow OutputNetflow) CreateOutputRequest {
	typ := CreateOutputRequestTypeNetflow

	typStr := CreateOutputTypeNetflow(typ)
	netflow.Type = typStr

	return CreateOutputRequest{
		OutputNetflow: &netflow,
		Type:          typ,
	}
}

func CreateCreateOutputRequestDynatraceHTTP(dynatraceHTTP OutputDynatraceHTTP) CreateOutputRequest {
	typ := CreateOutputRequestTypeDynatraceHTTP

	typStr := TypeDynatraceHTTP(typ)
	dynatraceHTTP.Type = typStr

	return CreateOutputRequest{
		OutputDynatraceHTTP: &dynatraceHTTP,
		Type:                typ,
	}
}

func CreateCreateOutputRequestDynatraceOtlp(dynatraceOtlp OutputDynatraceOtlp) CreateOutputRequest {
	typ := CreateOutputRequestTypeDynatraceOtlp

	typStr := TypeDynatraceOtlp(typ)
	dynatraceOtlp.Type = typStr

	return CreateOutputRequest{
		OutputDynatraceOtlp: &dynatraceOtlp,
		Type:                typ,
	}
}

func CreateCreateOutputRequestSentinelOneAiSiem(sentinelOneAiSiem OutputSentinelOneAiSiem) CreateOutputRequest {
	typ := CreateOutputRequestTypeSentinelOneAiSiem

	typStr := TypeSentinelOneAiSiem(typ)
	sentinelOneAiSiem.Type = typStr

	return CreateOutputRequest{
		OutputSentinelOneAiSiem: &sentinelOneAiSiem,
		Type:                    typ,
	}
}

func CreateCreateOutputRequestChronicle(chronicle OutputChronicle) CreateOutputRequest {
	typ := CreateOutputRequestTypeChronicle

	typStr := TypeChronicle(typ)
	chronicle.Type = typStr

	return CreateOutputRequest{
		OutputChronicle: &chronicle,
		Type:            typ,
	}
}

func CreateCreateOutputRequestDatabricks(databricks OutputDatabricks) CreateOutputRequest {
	typ := CreateOutputRequestTypeDatabricks

	typStr := TypeDatabricks(typ)
	databricks.Type = typStr

	return CreateOutputRequest{
		OutputDatabricks: &databricks,
		Type:             typ,
	}
}

func CreateCreateOutputRequestMicrosoftFabric(microsoftFabric OutputMicrosoftFabric) CreateOutputRequest {
	typ := CreateOutputRequestTypeMicrosoftFabric

	typStr := TypeMicrosoftFabric(typ)
	microsoftFabric.Type = typStr

	return CreateOutputRequest{
		OutputMicrosoftFabric: &microsoftFabric,
		Type:                  typ,
	}
}

func CreateCreateOutputRequestCloudflareR2(cloudflareR2 OutputCloudflareR2) CreateOutputRequest {
	typ := CreateOutputRequestTypeCloudflareR2

	typStr := TypeCloudflareR2(typ)
	cloudflareR2.Type = typStr

	return CreateOutputRequest{
		OutputCloudflareR2: &cloudflareR2,
		Type:               typ,
	}
}

func (u *CreateOutputRequest) UnmarshalJSON(data []byte) error {

	type discriminator struct {
		Type string `json:"type"`
	}

	dis := new(discriminator)
	if err := json.Unmarshal(data, &dis); err != nil {
		return fmt.Errorf("could not unmarshal discriminator: %w", err)
	}

	switch dis.Type {
	case "default":
		outputDefault := new(OutputDefault)
		if err := utils.UnmarshalJSON(data, &outputDefault, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == default) type OutputDefault within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputDefault = outputDefault
		u.Type = CreateOutputRequestTypeDefault
		return nil
	case "webhook":
		outputWebhook := new(OutputWebhook)
		if err := utils.UnmarshalJSON(data, &outputWebhook, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == webhook) type OutputWebhook within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputWebhook = outputWebhook
		u.Type = CreateOutputRequestTypeWebhook
		return nil
	case "sentinel":
		outputSentinel := new(OutputSentinel)
		if err := utils.UnmarshalJSON(data, &outputSentinel, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == sentinel) type OutputSentinel within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSentinel = outputSentinel
		u.Type = CreateOutputRequestTypeSentinel
		return nil
	case "devnull":
		outputDevnull := new(OutputDevnull)
		if err := utils.UnmarshalJSON(data, &outputDevnull, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == devnull) type OutputDevnull within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputDevnull = outputDevnull
		u.Type = CreateOutputRequestTypeDevnull
		return nil
	case "syslog":
		outputSyslog := new(OutputSyslog)
		if err := utils.UnmarshalJSON(data, &outputSyslog, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == syslog) type OutputSyslog within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSyslog = outputSyslog
		u.Type = CreateOutputRequestTypeSyslog
		return nil
	case "splunk":
		outputSplunk := new(OutputSplunk)
		if err := utils.UnmarshalJSON(data, &outputSplunk, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == splunk) type OutputSplunk within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSplunk = outputSplunk
		u.Type = CreateOutputRequestTypeSplunk
		return nil
	case "splunk_lb":
		outputSplunkLb := new(OutputSplunkLb)
		if err := utils.UnmarshalJSON(data, &outputSplunkLb, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == splunk_lb) type OutputSplunkLb within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSplunkLb = outputSplunkLb
		u.Type = CreateOutputRequestTypeSplunkLb
		return nil
	case "splunk_hec":
		outputSplunkHec := new(OutputSplunkHec)
		if err := utils.UnmarshalJSON(data, &outputSplunkHec, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == splunk_hec) type OutputSplunkHec within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSplunkHec = outputSplunkHec
		u.Type = CreateOutputRequestTypeSplunkHec
		return nil
	case "tcpjson":
		outputTcpjson := new(OutputTcpjson)
		if err := utils.UnmarshalJSON(data, &outputTcpjson, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == tcpjson) type OutputTcpjson within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputTcpjson = outputTcpjson
		u.Type = CreateOutputRequestTypeTcpjson
		return nil
	case "wavefront":
		outputWavefront := new(OutputWavefront)
		if err := utils.UnmarshalJSON(data, &outputWavefront, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == wavefront) type OutputWavefront within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputWavefront = outputWavefront
		u.Type = CreateOutputRequestTypeWavefront
		return nil
	case "signalfx":
		outputSignalfx := new(OutputSignalfx)
		if err := utils.UnmarshalJSON(data, &outputSignalfx, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == signalfx) type OutputSignalfx within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSignalfx = outputSignalfx
		u.Type = CreateOutputRequestTypeSignalfx
		return nil
	case "filesystem":
		outputFilesystem := new(OutputFilesystem)
		if err := utils.UnmarshalJSON(data, &outputFilesystem, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == filesystem) type OutputFilesystem within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputFilesystem = outputFilesystem
		u.Type = CreateOutputRequestTypeFilesystem
		return nil
	case "s3":
		outputS3 := new(OutputS3)
		if err := utils.UnmarshalJSON(data, &outputS3, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == s3) type OutputS3 within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputS3 = outputS3
		u.Type = CreateOutputRequestTypeS3
		return nil
	case "azure_blob":
		outputAzureBlob := new(OutputAzureBlob)
		if err := utils.UnmarshalJSON(data, &outputAzureBlob, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == azure_blob) type OutputAzureBlob within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputAzureBlob = outputAzureBlob
		u.Type = CreateOutputRequestTypeAzureBlob
		return nil
	case "azure_data_explorer":
		outputAzureDataExplorer := new(OutputAzureDataExplorer)
		if err := utils.UnmarshalJSON(data, &outputAzureDataExplorer, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == azure_data_explorer) type OutputAzureDataExplorer within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputAzureDataExplorer = outputAzureDataExplorer
		u.Type = CreateOutputRequestTypeAzureDataExplorer
		return nil
	case "azure_logs":
		outputAzureLogs := new(OutputAzureLogs)
		if err := utils.UnmarshalJSON(data, &outputAzureLogs, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == azure_logs) type OutputAzureLogs within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputAzureLogs = outputAzureLogs
		u.Type = CreateOutputRequestTypeAzureLogs
		return nil
	case "kinesis":
		outputKinesis := new(OutputKinesis)
		if err := utils.UnmarshalJSON(data, &outputKinesis, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == kinesis) type OutputKinesis within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputKinesis = outputKinesis
		u.Type = CreateOutputRequestTypeKinesis
		return nil
	case "honeycomb":
		outputHoneycomb := new(OutputHoneycomb)
		if err := utils.UnmarshalJSON(data, &outputHoneycomb, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == honeycomb) type OutputHoneycomb within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputHoneycomb = outputHoneycomb
		u.Type = CreateOutputRequestTypeHoneycomb
		return nil
	case "azure_eventhub":
		outputAzureEventhub := new(OutputAzureEventhub)
		if err := utils.UnmarshalJSON(data, &outputAzureEventhub, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == azure_eventhub) type OutputAzureEventhub within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputAzureEventhub = outputAzureEventhub
		u.Type = CreateOutputRequestTypeAzureEventhub
		return nil
	case "google_chronicle":
		outputGoogleChronicle := new(OutputGoogleChronicle)
		if err := utils.UnmarshalJSON(data, &outputGoogleChronicle, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == google_chronicle) type OutputGoogleChronicle within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputGoogleChronicle = outputGoogleChronicle
		u.Type = CreateOutputRequestTypeGoogleChronicle
		return nil
	case "google_cloud_storage":
		outputGoogleCloudStorage := new(OutputGoogleCloudStorage)
		if err := utils.UnmarshalJSON(data, &outputGoogleCloudStorage, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == google_cloud_storage) type OutputGoogleCloudStorage within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputGoogleCloudStorage = outputGoogleCloudStorage
		u.Type = CreateOutputRequestTypeGoogleCloudStorage
		return nil
	case "google_cloud_logging":
		outputGoogleCloudLogging := new(OutputGoogleCloudLogging)
		if err := utils.UnmarshalJSON(data, &outputGoogleCloudLogging, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == google_cloud_logging) type OutputGoogleCloudLogging within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputGoogleCloudLogging = outputGoogleCloudLogging
		u.Type = CreateOutputRequestTypeGoogleCloudLogging
		return nil
	case "google_pubsub":
		outputGooglePubsub := new(OutputGooglePubsub)
		if err := utils.UnmarshalJSON(data, &outputGooglePubsub, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == google_pubsub) type OutputGooglePubsub within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputGooglePubsub = outputGooglePubsub
		u.Type = CreateOutputRequestTypeGooglePubsub
		return nil
	case "exabeam":
		outputExabeam := new(OutputExabeam)
		if err := utils.UnmarshalJSON(data, &outputExabeam, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == exabeam) type OutputExabeam within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputExabeam = outputExabeam
		u.Type = CreateOutputRequestTypeExabeam
		return nil
	case "kafka":
		outputKafka := new(OutputKafka)
		if err := utils.UnmarshalJSON(data, &outputKafka, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == kafka) type OutputKafka within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputKafka = outputKafka
		u.Type = CreateOutputRequestTypeKafka
		return nil
	case "confluent_cloud":
		outputConfluentCloud := new(OutputConfluentCloud)
		if err := utils.UnmarshalJSON(data, &outputConfluentCloud, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == confluent_cloud) type OutputConfluentCloud within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputConfluentCloud = outputConfluentCloud
		u.Type = CreateOutputRequestTypeConfluentCloud
		return nil
	case "msk":
		outputMsk := new(OutputMsk)
		if err := utils.UnmarshalJSON(data, &outputMsk, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == msk) type OutputMsk within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputMsk = outputMsk
		u.Type = CreateOutputRequestTypeMsk
		return nil
	case "elastic":
		outputElastic := new(OutputElastic)
		if err := utils.UnmarshalJSON(data, &outputElastic, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == elastic) type OutputElastic within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputElastic = outputElastic
		u.Type = CreateOutputRequestTypeElastic
		return nil
	case "elastic_cloud":
		outputElasticCloud := new(OutputElasticCloud)
		if err := utils.UnmarshalJSON(data, &outputElasticCloud, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == elastic_cloud) type OutputElasticCloud within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputElasticCloud = outputElasticCloud
		u.Type = CreateOutputRequestTypeElasticCloud
		return nil
	case "newrelic":
		outputNewrelic := new(OutputNewrelic)
		if err := utils.UnmarshalJSON(data, &outputNewrelic, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == newrelic) type OutputNewrelic within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputNewrelic = outputNewrelic
		u.Type = CreateOutputRequestTypeNewrelic
		return nil
	case "newrelic_events":
		outputNewrelicEvents := new(OutputNewrelicEvents)
		if err := utils.UnmarshalJSON(data, &outputNewrelicEvents, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == newrelic_events) type OutputNewrelicEvents within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputNewrelicEvents = outputNewrelicEvents
		u.Type = CreateOutputRequestTypeNewrelicEvents
		return nil
	case "influxdb":
		outputInfluxdb := new(OutputInfluxdb)
		if err := utils.UnmarshalJSON(data, &outputInfluxdb, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == influxdb) type OutputInfluxdb within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputInfluxdb = outputInfluxdb
		u.Type = CreateOutputRequestTypeInfluxdb
		return nil
	case "cloudwatch":
		outputCloudwatch := new(OutputCloudwatch)
		if err := utils.UnmarshalJSON(data, &outputCloudwatch, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == cloudwatch) type OutputCloudwatch within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputCloudwatch = outputCloudwatch
		u.Type = CreateOutputRequestTypeCloudwatch
		return nil
	case "minio":
		outputMinio := new(OutputMinio)
		if err := utils.UnmarshalJSON(data, &outputMinio, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == minio) type OutputMinio within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputMinio = outputMinio
		u.Type = CreateOutputRequestTypeMinio
		return nil
	case "statsd":
		outputStatsd := new(OutputStatsd)
		if err := utils.UnmarshalJSON(data, &outputStatsd, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == statsd) type OutputStatsd within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputStatsd = outputStatsd
		u.Type = CreateOutputRequestTypeStatsd
		return nil
	case "statsd_ext":
		outputStatsdExt := new(OutputStatsdExt)
		if err := utils.UnmarshalJSON(data, &outputStatsdExt, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == statsd_ext) type OutputStatsdExt within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputStatsdExt = outputStatsdExt
		u.Type = CreateOutputRequestTypeStatsdExt
		return nil
	case "graphite":
		outputGraphite := new(OutputGraphite)
		if err := utils.UnmarshalJSON(data, &outputGraphite, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == graphite) type OutputGraphite within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputGraphite = outputGraphite
		u.Type = CreateOutputRequestTypeGraphite
		return nil
	case "router":
		outputRouter := new(OutputRouter)
		if err := utils.UnmarshalJSON(data, &outputRouter, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == router) type OutputRouter within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputRouter = outputRouter
		u.Type = CreateOutputRequestTypeRouter
		return nil
	case "sns":
		outputSns := new(OutputSns)
		if err := utils.UnmarshalJSON(data, &outputSns, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == sns) type OutputSns within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSns = outputSns
		u.Type = CreateOutputRequestTypeSns
		return nil
	case "sqs":
		outputSqs := new(OutputSqs)
		if err := utils.UnmarshalJSON(data, &outputSqs, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == sqs) type OutputSqs within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSqs = outputSqs
		u.Type = CreateOutputRequestTypeSqs
		return nil
	case "snmp":
		outputSnmp := new(OutputSnmp)
		if err := utils.UnmarshalJSON(data, &outputSnmp, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == snmp) type OutputSnmp within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSnmp = outputSnmp
		u.Type = CreateOutputRequestTypeSnmp
		return nil
	case "sumo_logic":
		outputSumoLogic := new(OutputSumoLogic)
		if err := utils.UnmarshalJSON(data, &outputSumoLogic, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == sumo_logic) type OutputSumoLogic within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSumoLogic = outputSumoLogic
		u.Type = CreateOutputRequestTypeSumoLogic
		return nil
	case "datadog":
		outputDatadog := new(OutputDatadog)
		if err := utils.UnmarshalJSON(data, &outputDatadog, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == datadog) type OutputDatadog within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputDatadog = outputDatadog
		u.Type = CreateOutputRequestTypeDatadog
		return nil
	case "grafana_cloud":
		outputGrafanaCloud := new(OutputGrafanaCloud)
		if err := utils.UnmarshalJSON(data, &outputGrafanaCloud, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == grafana_cloud) type OutputGrafanaCloud within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputGrafanaCloud = outputGrafanaCloud
		u.Type = CreateOutputRequestTypeGrafanaCloud
		return nil
	case "loki":
		outputLoki := new(OutputLoki)
		if err := utils.UnmarshalJSON(data, &outputLoki, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == loki) type OutputLoki within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputLoki = outputLoki
		u.Type = CreateOutputRequestTypeLoki
		return nil
	case "prometheus":
		outputPrometheus := new(OutputPrometheus)
		if err := utils.UnmarshalJSON(data, &outputPrometheus, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == prometheus) type OutputPrometheus within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputPrometheus = outputPrometheus
		u.Type = CreateOutputRequestTypePrometheus
		return nil
	case "ring":
		outputRing := new(OutputRing)
		if err := utils.UnmarshalJSON(data, &outputRing, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == ring) type OutputRing within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputRing = outputRing
		u.Type = CreateOutputRequestTypeRing
		return nil
	case "open_telemetry":
		outputOpenTelemetry := new(OutputOpenTelemetry)
		if err := utils.UnmarshalJSON(data, &outputOpenTelemetry, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == open_telemetry) type OutputOpenTelemetry within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputOpenTelemetry = outputOpenTelemetry
		u.Type = CreateOutputRequestTypeOpenTelemetry
		return nil
	case "service_now":
		outputServiceNow := new(OutputServiceNow)
		if err := utils.UnmarshalJSON(data, &outputServiceNow, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == service_now) type OutputServiceNow within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputServiceNow = outputServiceNow
		u.Type = CreateOutputRequestTypeServiceNow
		return nil
	case "dataset":
		outputDataset := new(OutputDataset)
		if err := utils.UnmarshalJSON(data, &outputDataset, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == dataset) type OutputDataset within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputDataset = outputDataset
		u.Type = CreateOutputRequestTypeDataset
		return nil
	case "cribl_tcp":
		outputCriblTCP := new(OutputCriblTCP)
		if err := utils.UnmarshalJSON(data, &outputCriblTCP, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == cribl_tcp) type OutputCriblTCP within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputCriblTCP = outputCriblTCP
		u.Type = CreateOutputRequestTypeCriblTCP
		return nil
	case "cribl_http":
		outputCriblHTTP := new(OutputCriblHTTP)
		if err := utils.UnmarshalJSON(data, &outputCriblHTTP, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == cribl_http) type OutputCriblHTTP within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputCriblHTTP = outputCriblHTTP
		u.Type = CreateOutputRequestTypeCriblHTTP
		return nil
	case "cribl_search_engine":
		outputCriblSearchEngine := new(OutputCriblSearchEngine)
		if err := utils.UnmarshalJSON(data, &outputCriblSearchEngine, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == cribl_search_engine) type OutputCriblSearchEngine within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputCriblSearchEngine = outputCriblSearchEngine
		u.Type = CreateOutputRequestTypeCriblSearchEngine
		return nil
	case "humio_hec":
		outputHumioHec := new(OutputHumioHec)
		if err := utils.UnmarshalJSON(data, &outputHumioHec, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == humio_hec) type OutputHumioHec within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputHumioHec = outputHumioHec
		u.Type = CreateOutputRequestTypeHumioHec
		return nil
	case "crowdstrike_next_gen_siem":
		outputCrowdstrikeNextGenSiem := new(OutputCrowdstrikeNextGenSiem)
		if err := utils.UnmarshalJSON(data, &outputCrowdstrikeNextGenSiem, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == crowdstrike_next_gen_siem) type OutputCrowdstrikeNextGenSiem within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputCrowdstrikeNextGenSiem = outputCrowdstrikeNextGenSiem
		u.Type = CreateOutputRequestTypeCrowdstrikeNextGenSiem
		return nil
	case "dl_s3":
		outputDlS3 := new(OutputDlS3)
		if err := utils.UnmarshalJSON(data, &outputDlS3, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == dl_s3) type OutputDlS3 within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputDlS3 = outputDlS3
		u.Type = CreateOutputRequestTypeDlS3
		return nil
	case "security_lake":
		outputSecurityLake := new(OutputSecurityLake)
		if err := utils.UnmarshalJSON(data, &outputSecurityLake, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == security_lake) type OutputSecurityLake within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSecurityLake = outputSecurityLake
		u.Type = CreateOutputRequestTypeSecurityLake
		return nil
	case "cribl_lake":
		outputCriblLake := new(OutputCriblLake)
		if err := utils.UnmarshalJSON(data, &outputCriblLake, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == cribl_lake) type OutputCriblLake within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputCriblLake = outputCriblLake
		u.Type = CreateOutputRequestTypeCriblLake
		return nil
	case "disk_spool":
		outputDiskSpool := new(OutputDiskSpool)
		if err := utils.UnmarshalJSON(data, &outputDiskSpool, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == disk_spool) type OutputDiskSpool within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputDiskSpool = outputDiskSpool
		u.Type = CreateOutputRequestTypeDiskSpool
		return nil
	case "click_house":
		outputClickHouse := new(OutputClickHouse)
		if err := utils.UnmarshalJSON(data, &outputClickHouse, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == click_house) type OutputClickHouse within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputClickHouse = outputClickHouse
		u.Type = CreateOutputRequestTypeClickHouse
		return nil
	case "xsiam":
		outputXsiam := new(OutputXsiam)
		if err := utils.UnmarshalJSON(data, &outputXsiam, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == xsiam) type OutputXsiam within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputXsiam = outputXsiam
		u.Type = CreateOutputRequestTypeXsiam
		return nil
	case "netflow":
		outputNetflow := new(OutputNetflow)
		if err := utils.UnmarshalJSON(data, &outputNetflow, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == netflow) type OutputNetflow within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputNetflow = outputNetflow
		u.Type = CreateOutputRequestTypeNetflow
		return nil
	case "dynatrace_http":
		outputDynatraceHTTP := new(OutputDynatraceHTTP)
		if err := utils.UnmarshalJSON(data, &outputDynatraceHTTP, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == dynatrace_http) type OutputDynatraceHTTP within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputDynatraceHTTP = outputDynatraceHTTP
		u.Type = CreateOutputRequestTypeDynatraceHTTP
		return nil
	case "dynatrace_otlp":
		outputDynatraceOtlp := new(OutputDynatraceOtlp)
		if err := utils.UnmarshalJSON(data, &outputDynatraceOtlp, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == dynatrace_otlp) type OutputDynatraceOtlp within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputDynatraceOtlp = outputDynatraceOtlp
		u.Type = CreateOutputRequestTypeDynatraceOtlp
		return nil
	case "sentinel_one_ai_siem":
		outputSentinelOneAiSiem := new(OutputSentinelOneAiSiem)
		if err := utils.UnmarshalJSON(data, &outputSentinelOneAiSiem, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == sentinel_one_ai_siem) type OutputSentinelOneAiSiem within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputSentinelOneAiSiem = outputSentinelOneAiSiem
		u.Type = CreateOutputRequestTypeSentinelOneAiSiem
		return nil
	case "chronicle":
		outputChronicle := new(OutputChronicle)
		if err := utils.UnmarshalJSON(data, &outputChronicle, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == chronicle) type OutputChronicle within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputChronicle = outputChronicle
		u.Type = CreateOutputRequestTypeChronicle
		return nil
	case "databricks":
		outputDatabricks := new(OutputDatabricks)
		if err := utils.UnmarshalJSON(data, &outputDatabricks, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == databricks) type OutputDatabricks within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputDatabricks = outputDatabricks
		u.Type = CreateOutputRequestTypeDatabricks
		return nil
	case "microsoft_fabric":
		outputMicrosoftFabric := new(OutputMicrosoftFabric)
		if err := utils.UnmarshalJSON(data, &outputMicrosoftFabric, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == microsoft_fabric) type OutputMicrosoftFabric within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputMicrosoftFabric = outputMicrosoftFabric
		u.Type = CreateOutputRequestTypeMicrosoftFabric
		return nil
	case "cloudflare_r2":
		outputCloudflareR2 := new(OutputCloudflareR2)
		if err := utils.UnmarshalJSON(data, &outputCloudflareR2, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == cloudflare_r2) type OutputCloudflareR2 within CreateOutputRequest: %w", string(data), err)
		}

		u.OutputCloudflareR2 = outputCloudflareR2
		u.Type = CreateOutputRequestTypeCloudflareR2
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for CreateOutputRequest", string(data))
}

func (u CreateOutputRequest) MarshalJSON() ([]byte, error) {
	if u.OutputDefault != nil {
		return utils.MarshalJSON(u.OutputDefault, "", true)
	}

	if u.OutputWebhook != nil {
		return utils.MarshalJSON(u.OutputWebhook, "", true)
	}

	if u.OutputSentinel != nil {
		return utils.MarshalJSON(u.OutputSentinel, "", true)
	}

	if u.OutputDevnull != nil {
		return utils.MarshalJSON(u.OutputDevnull, "", true)
	}

	if u.OutputSyslog != nil {
		return utils.MarshalJSON(u.OutputSyslog, "", true)
	}

	if u.OutputSplunk != nil {
		return utils.MarshalJSON(u.OutputSplunk, "", true)
	}

	if u.OutputSplunkLb != nil {
		return utils.MarshalJSON(u.OutputSplunkLb, "", true)
	}

	if u.OutputSplunkHec != nil {
		return utils.MarshalJSON(u.OutputSplunkHec, "", true)
	}

	if u.OutputTcpjson != nil {
		return utils.MarshalJSON(u.OutputTcpjson, "", true)
	}

	if u.OutputWavefront != nil {
		return utils.MarshalJSON(u.OutputWavefront, "", true)
	}

	if u.OutputSignalfx != nil {
		return utils.MarshalJSON(u.OutputSignalfx, "", true)
	}

	if u.OutputFilesystem != nil {
		return utils.MarshalJSON(u.OutputFilesystem, "", true)
	}

	if u.OutputS3 != nil {
		return utils.MarshalJSON(u.OutputS3, "", true)
	}

	if u.OutputAzureBlob != nil {
		return utils.MarshalJSON(u.OutputAzureBlob, "", true)
	}

	if u.OutputAzureDataExplorer != nil {
		return utils.MarshalJSON(u.OutputAzureDataExplorer, "", true)
	}

	if u.OutputAzureLogs != nil {
		return utils.MarshalJSON(u.OutputAzureLogs, "", true)
	}

	if u.OutputKinesis != nil {
		return utils.MarshalJSON(u.OutputKinesis, "", true)
	}

	if u.OutputHoneycomb != nil {
		return utils.MarshalJSON(u.OutputHoneycomb, "", true)
	}

	if u.OutputAzureEventhub != nil {
		return utils.MarshalJSON(u.OutputAzureEventhub, "", true)
	}

	if u.OutputGoogleChronicle != nil {
		return utils.MarshalJSON(u.OutputGoogleChronicle, "", true)
	}

	if u.OutputGoogleCloudStorage != nil {
		return utils.MarshalJSON(u.OutputGoogleCloudStorage, "", true)
	}

	if u.OutputGoogleCloudLogging != nil {
		return utils.MarshalJSON(u.OutputGoogleCloudLogging, "", true)
	}

	if u.OutputGooglePubsub != nil {
		return utils.MarshalJSON(u.OutputGooglePubsub, "", true)
	}

	if u.OutputExabeam != nil {
		return utils.MarshalJSON(u.OutputExabeam, "", true)
	}

	if u.OutputKafka != nil {
		return utils.MarshalJSON(u.OutputKafka, "", true)
	}

	if u.OutputConfluentCloud != nil {
		return utils.MarshalJSON(u.OutputConfluentCloud, "", true)
	}

	if u.OutputMsk != nil {
		return utils.MarshalJSON(u.OutputMsk, "", true)
	}

	if u.OutputElastic != nil {
		return utils.MarshalJSON(u.OutputElastic, "", true)
	}

	if u.OutputElasticCloud != nil {
		return utils.MarshalJSON(u.OutputElasticCloud, "", true)
	}

	if u.OutputNewrelic != nil {
		return utils.MarshalJSON(u.OutputNewrelic, "", true)
	}

	if u.OutputNewrelicEvents != nil {
		return utils.MarshalJSON(u.OutputNewrelicEvents, "", true)
	}

	if u.OutputInfluxdb != nil {
		return utils.MarshalJSON(u.OutputInfluxdb, "", true)
	}

	if u.OutputCloudwatch != nil {
		return utils.MarshalJSON(u.OutputCloudwatch, "", true)
	}

	if u.OutputMinio != nil {
		return utils.MarshalJSON(u.OutputMinio, "", true)
	}

	if u.OutputStatsd != nil {
		return utils.MarshalJSON(u.OutputStatsd, "", true)
	}

	if u.OutputStatsdExt != nil {
		return utils.MarshalJSON(u.OutputStatsdExt, "", true)
	}

	if u.OutputGraphite != nil {
		return utils.MarshalJSON(u.OutputGraphite, "", true)
	}

	if u.OutputRouter != nil {
		return utils.MarshalJSON(u.OutputRouter, "", true)
	}

	if u.OutputSns != nil {
		return utils.MarshalJSON(u.OutputSns, "", true)
	}

	if u.OutputSqs != nil {
		return utils.MarshalJSON(u.OutputSqs, "", true)
	}

	if u.OutputSnmp != nil {
		return utils.MarshalJSON(u.OutputSnmp, "", true)
	}

	if u.OutputSumoLogic != nil {
		return utils.MarshalJSON(u.OutputSumoLogic, "", true)
	}

	if u.OutputDatadog != nil {
		return utils.MarshalJSON(u.OutputDatadog, "", true)
	}

	if u.OutputGrafanaCloud != nil {
		return utils.MarshalJSON(u.OutputGrafanaCloud, "", true)
	}

	if u.OutputLoki != nil {
		return utils.MarshalJSON(u.OutputLoki, "", true)
	}

	if u.OutputPrometheus != nil {
		return utils.MarshalJSON(u.OutputPrometheus, "", true)
	}

	if u.OutputRing != nil {
		return utils.MarshalJSON(u.OutputRing, "", true)
	}

	if u.OutputOpenTelemetry != nil {
		return utils.MarshalJSON(u.OutputOpenTelemetry, "", true)
	}

	if u.OutputServiceNow != nil {
		return utils.MarshalJSON(u.OutputServiceNow, "", true)
	}

	if u.OutputDataset != nil {
		return utils.MarshalJSON(u.OutputDataset, "", true)
	}

	if u.OutputCriblTCP != nil {
		return utils.MarshalJSON(u.OutputCriblTCP, "", true)
	}

	if u.OutputCriblHTTP != nil {
		return utils.MarshalJSON(u.OutputCriblHTTP, "", true)
	}

	if u.OutputCriblSearchEngine != nil {
		return utils.MarshalJSON(u.OutputCriblSearchEngine, "", true)
	}

	if u.OutputHumioHec != nil {
		return utils.MarshalJSON(u.OutputHumioHec, "", true)
	}

	if u.OutputCrowdstrikeNextGenSiem != nil {
		return utils.MarshalJSON(u.OutputCrowdstrikeNextGenSiem, "", true)
	}

	if u.OutputDlS3 != nil {
		return utils.MarshalJSON(u.OutputDlS3, "", true)
	}

	if u.OutputSecurityLake != nil {
		return utils.MarshalJSON(u.OutputSecurityLake, "", true)
	}

	if u.OutputCriblLake != nil {
		return utils.MarshalJSON(u.OutputCriblLake, "", true)
	}

	if u.OutputDiskSpool != nil {
		return utils.MarshalJSON(u.OutputDiskSpool, "", true)
	}

	if u.OutputClickHouse != nil {
		return utils.MarshalJSON(u.OutputClickHouse, "", true)
	}

	if u.OutputXsiam != nil {
		return utils.MarshalJSON(u.OutputXsiam, "", true)
	}

	if u.OutputNetflow != nil {
		return utils.MarshalJSON(u.OutputNetflow, "", true)
	}

	if u.OutputDynatraceHTTP != nil {
		return utils.MarshalJSON(u.OutputDynatraceHTTP, "", true)
	}

	if u.OutputDynatraceOtlp != nil {
		return utils.MarshalJSON(u.OutputDynatraceOtlp, "", true)
	}

	if u.OutputSentinelOneAiSiem != nil {
		return utils.MarshalJSON(u.OutputSentinelOneAiSiem, "", true)
	}

	if u.OutputChronicle != nil {
		return utils.MarshalJSON(u.OutputChronicle, "", true)
	}

	if u.OutputDatabricks != nil {
		return utils.MarshalJSON(u.OutputDatabricks, "", true)
	}

	if u.OutputMicrosoftFabric != nil {
		return utils.MarshalJSON(u.OutputMicrosoftFabric, "", true)
	}

	if u.OutputCloudflareR2 != nil {
		return utils.MarshalJSON(u.OutputCloudflareR2, "", true)
	}

	return nil, errors.New("could not marshal union type CreateOutputRequest: all fields are null")
}

type CreateOutputResponse struct {
	HTTPMeta components.HTTPMetadata `json:"-"`
	// a list of Destination objects
	CountedOutput *components.CountedOutput
}

func (c *CreateOutputResponse) GetHTTPMeta() components.HTTPMetadata {
	if c == nil {
		return components.HTTPMetadata{}
	}
	return c.HTTPMeta
}

func (c *CreateOutputResponse) GetCountedOutput() *components.CountedOutput {
	if c == nil {
		return nil
	}
	return c.CountedOutput
}
