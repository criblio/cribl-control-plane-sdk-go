// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package components

import (
	"encoding/json"
	"errors"
	"fmt"
	"github.com/criblio/cribl-control-plane-sdk-go/internal/utils"
)

type OutputDatabricksType6 string

const (
	OutputDatabricksType6Databricks OutputDatabricksType6 = "databricks"
)

func (e OutputDatabricksType6) ToPointer() *OutputDatabricksType6 {
	return &e
}
func (e *OutputDatabricksType6) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "databricks":
		*e = OutputDatabricksType6(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputDatabricksType6: %v", v)
	}
}

type OutputDatabricksDatabricks6 struct {
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Unique ID for this output
	ID   *string               `json:"id,omitempty"`
	Type OutputDatabricksType6 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Optional path to prepend to files before uploading.
	DestPath *string `default:"" json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *Format1Options `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnBackpressure *PqOnBackpressureOptions `default:"block" json:"onBackpressure"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnDiskFullBackpressure *PqOnBackpressureOptions `default:"block" json:"onDiskFullBackpressure"`
	// Databricks workspace ID
	WorkspaceID string `json:"workspaceId"`
	// OAuth scope for Unity Catalog authentication
	Scope *string `default:"all-apis" json:"scope"`
	// OAuth client ID for Unity Catalog authentication
	ClientID string `json:"clientId"`
	// Name of the catalog to use for the output
	Catalog *string `default:"main" json:"catalog"`
	// Name of the catalog schema to use for the output
	Schema *string `default:"external" json:"schema"`
	// Name of the events volume in Databricks
	EventsVolumeName *string `default:"events" json:"eventsVolumeName"`
	// OAuth client secret for Unity Catalog authentication
	ClientTextSecret string  `json:"clientTextSecret"`
	Description      *string `json:"description,omitempty"`
	// Codec to use to compress the persisted data
	Compress *PqCompressOptions `default:"none" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelOptions `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionOptions `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *ParquetDataPageVersionOptions `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []TagsType `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputDatabricksDatabricks6) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDatabricksDatabricks6) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "workspaceId", "clientId", "clientTextSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDatabricksDatabricks6) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDatabricksDatabricks6) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDatabricksDatabricks6) GetType() OutputDatabricksType6 {
	if o == nil {
		return OutputDatabricksType6("")
	}
	return o.Type
}

func (o *OutputDatabricksDatabricks6) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDatabricksDatabricks6) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDatabricksDatabricks6) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDatabricksDatabricks6) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDatabricksDatabricks6) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDatabricksDatabricks6) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDatabricksDatabricks6) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDatabricksDatabricks6) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDatabricksDatabricks6) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputDatabricksDatabricks6) GetFormat() *Format1Options {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDatabricksDatabricks6) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDatabricksDatabricks6) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDatabricksDatabricks6) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDatabricksDatabricks6) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDatabricksDatabricks6) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDatabricksDatabricks6) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDatabricksDatabricks6) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDatabricksDatabricks6) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDatabricksDatabricks6) GetOnBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDatabricksDatabricks6) GetOnDiskFullBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDatabricksDatabricks6) GetWorkspaceID() string {
	if o == nil {
		return ""
	}
	return o.WorkspaceID
}

func (o *OutputDatabricksDatabricks6) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputDatabricksDatabricks6) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputDatabricksDatabricks6) GetCatalog() *string {
	if o == nil {
		return nil
	}
	return o.Catalog
}

func (o *OutputDatabricksDatabricks6) GetSchema() *string {
	if o == nil {
		return nil
	}
	return o.Schema
}

func (o *OutputDatabricksDatabricks6) GetEventsVolumeName() *string {
	if o == nil {
		return nil
	}
	return o.EventsVolumeName
}

func (o *OutputDatabricksDatabricks6) GetClientTextSecret() string {
	if o == nil {
		return ""
	}
	return o.ClientTextSecret
}

func (o *OutputDatabricksDatabricks6) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDatabricksDatabricks6) GetCompress() *PqCompressOptions {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDatabricksDatabricks6) GetCompressionLevel() *CompressionLevelOptions {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDatabricksDatabricks6) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDatabricksDatabricks6) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputDatabricksDatabricks6) GetParquetVersion() *ParquetVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDatabricksDatabricks6) GetParquetDataPageVersion() *ParquetDataPageVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDatabricksDatabricks6) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDatabricksDatabricks6) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDatabricksDatabricks6) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDatabricksDatabricks6) GetKeyValueMetadata() []TagsType {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputDatabricksDatabricks6) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDatabricksDatabricks6) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDatabricksDatabricks6) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDatabricksDatabricks6) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDatabricksDatabricks6) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDatabricksDatabricks6) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type OutputDatabricksType5 string

const (
	OutputDatabricksType5Databricks OutputDatabricksType5 = "databricks"
)

func (e OutputDatabricksType5) ToPointer() *OutputDatabricksType5 {
	return &e
}
func (e *OutputDatabricksType5) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "databricks":
		*e = OutputDatabricksType5(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputDatabricksType5: %v", v)
	}
}

type OutputDatabricksDatabricks5 struct {
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Unique ID for this output
	ID   *string               `json:"id,omitempty"`
	Type OutputDatabricksType5 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Optional path to prepend to files before uploading.
	DestPath *string `default:"" json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *Format1Options `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnBackpressure *PqOnBackpressureOptions `default:"block" json:"onBackpressure"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnDiskFullBackpressure *PqOnBackpressureOptions `default:"block" json:"onDiskFullBackpressure"`
	// Databricks workspace ID
	WorkspaceID string `json:"workspaceId"`
	// OAuth scope for Unity Catalog authentication
	Scope *string `default:"all-apis" json:"scope"`
	// OAuth client ID for Unity Catalog authentication
	ClientID string `json:"clientId"`
	// Name of the catalog to use for the output
	Catalog *string `default:"main" json:"catalog"`
	// Name of the catalog schema to use for the output
	Schema *string `default:"external" json:"schema"`
	// Name of the events volume in Databricks
	EventsVolumeName *string `default:"events" json:"eventsVolumeName"`
	// OAuth client secret for Unity Catalog authentication
	ClientTextSecret string  `json:"clientTextSecret"`
	Description      *string `json:"description,omitempty"`
	// Codec to use to compress the persisted data
	Compress *PqCompressOptions `default:"none" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelOptions `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionOptions `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *ParquetDataPageVersionOptions `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []TagsType `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputDatabricksDatabricks5) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDatabricksDatabricks5) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "workspaceId", "clientId", "clientTextSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDatabricksDatabricks5) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDatabricksDatabricks5) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDatabricksDatabricks5) GetType() OutputDatabricksType5 {
	if o == nil {
		return OutputDatabricksType5("")
	}
	return o.Type
}

func (o *OutputDatabricksDatabricks5) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDatabricksDatabricks5) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDatabricksDatabricks5) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDatabricksDatabricks5) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDatabricksDatabricks5) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDatabricksDatabricks5) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDatabricksDatabricks5) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDatabricksDatabricks5) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDatabricksDatabricks5) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputDatabricksDatabricks5) GetFormat() *Format1Options {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDatabricksDatabricks5) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDatabricksDatabricks5) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDatabricksDatabricks5) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDatabricksDatabricks5) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDatabricksDatabricks5) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDatabricksDatabricks5) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDatabricksDatabricks5) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDatabricksDatabricks5) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDatabricksDatabricks5) GetOnBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDatabricksDatabricks5) GetOnDiskFullBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDatabricksDatabricks5) GetWorkspaceID() string {
	if o == nil {
		return ""
	}
	return o.WorkspaceID
}

func (o *OutputDatabricksDatabricks5) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputDatabricksDatabricks5) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputDatabricksDatabricks5) GetCatalog() *string {
	if o == nil {
		return nil
	}
	return o.Catalog
}

func (o *OutputDatabricksDatabricks5) GetSchema() *string {
	if o == nil {
		return nil
	}
	return o.Schema
}

func (o *OutputDatabricksDatabricks5) GetEventsVolumeName() *string {
	if o == nil {
		return nil
	}
	return o.EventsVolumeName
}

func (o *OutputDatabricksDatabricks5) GetClientTextSecret() string {
	if o == nil {
		return ""
	}
	return o.ClientTextSecret
}

func (o *OutputDatabricksDatabricks5) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDatabricksDatabricks5) GetCompress() *PqCompressOptions {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDatabricksDatabricks5) GetCompressionLevel() *CompressionLevelOptions {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDatabricksDatabricks5) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDatabricksDatabricks5) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputDatabricksDatabricks5) GetParquetVersion() *ParquetVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDatabricksDatabricks5) GetParquetDataPageVersion() *ParquetDataPageVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDatabricksDatabricks5) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDatabricksDatabricks5) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDatabricksDatabricks5) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDatabricksDatabricks5) GetKeyValueMetadata() []TagsType {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputDatabricksDatabricks5) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDatabricksDatabricks5) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDatabricksDatabricks5) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDatabricksDatabricks5) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDatabricksDatabricks5) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDatabricksDatabricks5) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type OutputDatabricksType4 string

const (
	OutputDatabricksType4Databricks OutputDatabricksType4 = "databricks"
)

func (e OutputDatabricksType4) ToPointer() *OutputDatabricksType4 {
	return &e
}
func (e *OutputDatabricksType4) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "databricks":
		*e = OutputDatabricksType4(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputDatabricksType4: %v", v)
	}
}

type OutputDatabricksDatabricks4 struct {
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// Unique ID for this output
	ID   *string               `json:"id,omitempty"`
	Type OutputDatabricksType4 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Optional path to prepend to files before uploading.
	DestPath *string `default:"" json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *Format1Options `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnBackpressure *PqOnBackpressureOptions `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnDiskFullBackpressure *PqOnBackpressureOptions `default:"block" json:"onDiskFullBackpressure"`
	// Databricks workspace ID
	WorkspaceID string `json:"workspaceId"`
	// OAuth scope for Unity Catalog authentication
	Scope *string `default:"all-apis" json:"scope"`
	// OAuth client ID for Unity Catalog authentication
	ClientID string `json:"clientId"`
	// Name of the catalog to use for the output
	Catalog *string `default:"main" json:"catalog"`
	// Name of the catalog schema to use for the output
	Schema *string `default:"external" json:"schema"`
	// Name of the events volume in Databricks
	EventsVolumeName *string `default:"events" json:"eventsVolumeName"`
	// OAuth client secret for Unity Catalog authentication
	ClientTextSecret string  `json:"clientTextSecret"`
	Description      *string `json:"description,omitempty"`
	// Codec to use to compress the persisted data
	Compress *PqCompressOptions `default:"none" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelOptions `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionOptions `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *ParquetDataPageVersionOptions `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []TagsType `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputDatabricksDatabricks4) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDatabricksDatabricks4) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "workspaceId", "clientId", "clientTextSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDatabricksDatabricks4) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDatabricksDatabricks4) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDatabricksDatabricks4) GetType() OutputDatabricksType4 {
	if o == nil {
		return OutputDatabricksType4("")
	}
	return o.Type
}

func (o *OutputDatabricksDatabricks4) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDatabricksDatabricks4) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDatabricksDatabricks4) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDatabricksDatabricks4) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDatabricksDatabricks4) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDatabricksDatabricks4) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDatabricksDatabricks4) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDatabricksDatabricks4) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputDatabricksDatabricks4) GetFormat() *Format1Options {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDatabricksDatabricks4) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDatabricksDatabricks4) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDatabricksDatabricks4) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDatabricksDatabricks4) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDatabricksDatabricks4) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDatabricksDatabricks4) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDatabricksDatabricks4) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDatabricksDatabricks4) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDatabricksDatabricks4) GetOnBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDatabricksDatabricks4) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDatabricksDatabricks4) GetOnDiskFullBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDatabricksDatabricks4) GetWorkspaceID() string {
	if o == nil {
		return ""
	}
	return o.WorkspaceID
}

func (o *OutputDatabricksDatabricks4) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputDatabricksDatabricks4) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputDatabricksDatabricks4) GetCatalog() *string {
	if o == nil {
		return nil
	}
	return o.Catalog
}

func (o *OutputDatabricksDatabricks4) GetSchema() *string {
	if o == nil {
		return nil
	}
	return o.Schema
}

func (o *OutputDatabricksDatabricks4) GetEventsVolumeName() *string {
	if o == nil {
		return nil
	}
	return o.EventsVolumeName
}

func (o *OutputDatabricksDatabricks4) GetClientTextSecret() string {
	if o == nil {
		return ""
	}
	return o.ClientTextSecret
}

func (o *OutputDatabricksDatabricks4) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDatabricksDatabricks4) GetCompress() *PqCompressOptions {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDatabricksDatabricks4) GetCompressionLevel() *CompressionLevelOptions {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDatabricksDatabricks4) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDatabricksDatabricks4) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputDatabricksDatabricks4) GetParquetVersion() *ParquetVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDatabricksDatabricks4) GetParquetDataPageVersion() *ParquetDataPageVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDatabricksDatabricks4) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDatabricksDatabricks4) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDatabricksDatabricks4) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDatabricksDatabricks4) GetKeyValueMetadata() []TagsType {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputDatabricksDatabricks4) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDatabricksDatabricks4) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDatabricksDatabricks4) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDatabricksDatabricks4) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDatabricksDatabricks4) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDatabricksDatabricks4) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type OutputDatabricksType3 string

const (
	OutputDatabricksType3Databricks OutputDatabricksType3 = "databricks"
)

func (e OutputDatabricksType3) ToPointer() *OutputDatabricksType3 {
	return &e
}
func (e *OutputDatabricksType3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "databricks":
		*e = OutputDatabricksType3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputDatabricksType3: %v", v)
	}
}

type OutputDatabricksDatabricks3 struct {
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// Unique ID for this output
	ID   *string               `json:"id,omitempty"`
	Type OutputDatabricksType3 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Optional path to prepend to files before uploading.
	DestPath *string `default:"" json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *Format1Options `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnBackpressure *PqOnBackpressureOptions `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnDiskFullBackpressure *PqOnBackpressureOptions `default:"block" json:"onDiskFullBackpressure"`
	// Databricks workspace ID
	WorkspaceID string `json:"workspaceId"`
	// OAuth scope for Unity Catalog authentication
	Scope *string `default:"all-apis" json:"scope"`
	// OAuth client ID for Unity Catalog authentication
	ClientID string `json:"clientId"`
	// Name of the catalog to use for the output
	Catalog *string `default:"main" json:"catalog"`
	// Name of the catalog schema to use for the output
	Schema *string `default:"external" json:"schema"`
	// Name of the events volume in Databricks
	EventsVolumeName *string `default:"events" json:"eventsVolumeName"`
	// OAuth client secret for Unity Catalog authentication
	ClientTextSecret string  `json:"clientTextSecret"`
	Description      *string `json:"description,omitempty"`
	// Codec to use to compress the persisted data
	Compress *PqCompressOptions `default:"none" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelOptions `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionOptions `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *ParquetDataPageVersionOptions `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []TagsType `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputDatabricksDatabricks3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDatabricksDatabricks3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "workspaceId", "clientId", "clientTextSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDatabricksDatabricks3) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDatabricksDatabricks3) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDatabricksDatabricks3) GetType() OutputDatabricksType3 {
	if o == nil {
		return OutputDatabricksType3("")
	}
	return o.Type
}

func (o *OutputDatabricksDatabricks3) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDatabricksDatabricks3) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDatabricksDatabricks3) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDatabricksDatabricks3) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDatabricksDatabricks3) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDatabricksDatabricks3) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDatabricksDatabricks3) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDatabricksDatabricks3) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputDatabricksDatabricks3) GetFormat() *Format1Options {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDatabricksDatabricks3) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDatabricksDatabricks3) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDatabricksDatabricks3) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDatabricksDatabricks3) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDatabricksDatabricks3) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDatabricksDatabricks3) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDatabricksDatabricks3) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDatabricksDatabricks3) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDatabricksDatabricks3) GetOnBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDatabricksDatabricks3) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDatabricksDatabricks3) GetOnDiskFullBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDatabricksDatabricks3) GetWorkspaceID() string {
	if o == nil {
		return ""
	}
	return o.WorkspaceID
}

func (o *OutputDatabricksDatabricks3) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputDatabricksDatabricks3) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputDatabricksDatabricks3) GetCatalog() *string {
	if o == nil {
		return nil
	}
	return o.Catalog
}

func (o *OutputDatabricksDatabricks3) GetSchema() *string {
	if o == nil {
		return nil
	}
	return o.Schema
}

func (o *OutputDatabricksDatabricks3) GetEventsVolumeName() *string {
	if o == nil {
		return nil
	}
	return o.EventsVolumeName
}

func (o *OutputDatabricksDatabricks3) GetClientTextSecret() string {
	if o == nil {
		return ""
	}
	return o.ClientTextSecret
}

func (o *OutputDatabricksDatabricks3) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDatabricksDatabricks3) GetCompress() *PqCompressOptions {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDatabricksDatabricks3) GetCompressionLevel() *CompressionLevelOptions {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDatabricksDatabricks3) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDatabricksDatabricks3) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputDatabricksDatabricks3) GetParquetVersion() *ParquetVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDatabricksDatabricks3) GetParquetDataPageVersion() *ParquetDataPageVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDatabricksDatabricks3) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDatabricksDatabricks3) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDatabricksDatabricks3) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDatabricksDatabricks3) GetKeyValueMetadata() []TagsType {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputDatabricksDatabricks3) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDatabricksDatabricks3) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDatabricksDatabricks3) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDatabricksDatabricks3) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDatabricksDatabricks3) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDatabricksDatabricks3) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type OutputDatabricksType2 string

const (
	OutputDatabricksType2Databricks OutputDatabricksType2 = "databricks"
)

func (e OutputDatabricksType2) ToPointer() *OutputDatabricksType2 {
	return &e
}
func (e *OutputDatabricksType2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "databricks":
		*e = OutputDatabricksType2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputDatabricksType2: %v", v)
	}
}

type OutputDatabricksDatabricks2 struct {
	// Format of the output data
	Format *Format1Options `default:"json" json:"format"`
	// Unique ID for this output
	ID   *string               `json:"id,omitempty"`
	Type OutputDatabricksType2 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Optional path to prepend to files before uploading.
	DestPath *string `default:"" json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnBackpressure *PqOnBackpressureOptions `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnDiskFullBackpressure *PqOnBackpressureOptions `default:"block" json:"onDiskFullBackpressure"`
	// Databricks workspace ID
	WorkspaceID string `json:"workspaceId"`
	// OAuth scope for Unity Catalog authentication
	Scope *string `default:"all-apis" json:"scope"`
	// OAuth client ID for Unity Catalog authentication
	ClientID string `json:"clientId"`
	// Name of the catalog to use for the output
	Catalog *string `default:"main" json:"catalog"`
	// Name of the catalog schema to use for the output
	Schema *string `default:"external" json:"schema"`
	// Name of the events volume in Databricks
	EventsVolumeName *string `default:"events" json:"eventsVolumeName"`
	// OAuth client secret for Unity Catalog authentication
	ClientTextSecret string  `json:"clientTextSecret"`
	Description      *string `json:"description,omitempty"`
	// Codec to use to compress the persisted data
	Compress *PqCompressOptions `default:"none" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelOptions `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionOptions `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *ParquetDataPageVersionOptions `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows bool `json:"shouldLogInvalidRows"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []TagsType `json:"keyValueMetadata"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputDatabricksDatabricks2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDatabricksDatabricks2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "workspaceId", "clientId", "clientTextSecret", "shouldLogInvalidRows", "keyValueMetadata"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDatabricksDatabricks2) GetFormat() *Format1Options {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDatabricksDatabricks2) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDatabricksDatabricks2) GetType() OutputDatabricksType2 {
	if o == nil {
		return OutputDatabricksType2("")
	}
	return o.Type
}

func (o *OutputDatabricksDatabricks2) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDatabricksDatabricks2) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDatabricksDatabricks2) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDatabricksDatabricks2) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDatabricksDatabricks2) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDatabricksDatabricks2) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDatabricksDatabricks2) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDatabricksDatabricks2) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDatabricksDatabricks2) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputDatabricksDatabricks2) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDatabricksDatabricks2) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDatabricksDatabricks2) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDatabricksDatabricks2) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDatabricksDatabricks2) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDatabricksDatabricks2) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDatabricksDatabricks2) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDatabricksDatabricks2) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDatabricksDatabricks2) GetOnBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDatabricksDatabricks2) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDatabricksDatabricks2) GetOnDiskFullBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDatabricksDatabricks2) GetWorkspaceID() string {
	if o == nil {
		return ""
	}
	return o.WorkspaceID
}

func (o *OutputDatabricksDatabricks2) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputDatabricksDatabricks2) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputDatabricksDatabricks2) GetCatalog() *string {
	if o == nil {
		return nil
	}
	return o.Catalog
}

func (o *OutputDatabricksDatabricks2) GetSchema() *string {
	if o == nil {
		return nil
	}
	return o.Schema
}

func (o *OutputDatabricksDatabricks2) GetEventsVolumeName() *string {
	if o == nil {
		return nil
	}
	return o.EventsVolumeName
}

func (o *OutputDatabricksDatabricks2) GetClientTextSecret() string {
	if o == nil {
		return ""
	}
	return o.ClientTextSecret
}

func (o *OutputDatabricksDatabricks2) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDatabricksDatabricks2) GetCompress() *PqCompressOptions {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDatabricksDatabricks2) GetCompressionLevel() *CompressionLevelOptions {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDatabricksDatabricks2) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDatabricksDatabricks2) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputDatabricksDatabricks2) GetParquetVersion() *ParquetVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDatabricksDatabricks2) GetParquetDataPageVersion() *ParquetDataPageVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDatabricksDatabricks2) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDatabricksDatabricks2) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDatabricksDatabricks2) GetShouldLogInvalidRows() bool {
	if o == nil {
		return false
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDatabricksDatabricks2) GetKeyValueMetadata() []TagsType {
	if o == nil {
		return []TagsType{}
	}
	return o.KeyValueMetadata
}

func (o *OutputDatabricksDatabricks2) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDatabricksDatabricks2) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDatabricksDatabricks2) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDatabricksDatabricks2) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDatabricksDatabricks2) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDatabricksDatabricks2) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type OutputDatabricksType1 string

const (
	OutputDatabricksType1Databricks OutputDatabricksType1 = "databricks"
)

func (e OutputDatabricksType1) ToPointer() *OutputDatabricksType1 {
	return &e
}
func (e *OutputDatabricksType1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "databricks":
		*e = OutputDatabricksType1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputDatabricksType1: %v", v)
	}
}

type OutputDatabricksDatabricks1 struct {
	// Format of the output data
	Format *Format1Options `default:"json" json:"format"`
	// Unique ID for this output
	ID   *string               `json:"id,omitempty"`
	Type OutputDatabricksType1 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Optional path to prepend to files before uploading.
	DestPath *string `default:"" json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnBackpressure *PqOnBackpressureOptions `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	OnDiskFullBackpressure *PqOnBackpressureOptions `default:"block" json:"onDiskFullBackpressure"`
	// Databricks workspace ID
	WorkspaceID string `json:"workspaceId"`
	// OAuth scope for Unity Catalog authentication
	Scope *string `default:"all-apis" json:"scope"`
	// OAuth client ID for Unity Catalog authentication
	ClientID string `json:"clientId"`
	// Name of the catalog to use for the output
	Catalog *string `default:"main" json:"catalog"`
	// Name of the catalog schema to use for the output
	Schema *string `default:"external" json:"schema"`
	// Name of the events volume in Databricks
	EventsVolumeName *string `default:"events" json:"eventsVolumeName"`
	// OAuth client secret for Unity Catalog authentication
	ClientTextSecret string  `json:"clientTextSecret"`
	Description      *string `json:"description,omitempty"`
	// Codec to use to compress the persisted data
	Compress *PqCompressOptions `default:"none" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelOptions `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionOptions `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *ParquetDataPageVersionOptions `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []TagsType `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
}

func (o OutputDatabricksDatabricks1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDatabricksDatabricks1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "workspaceId", "clientId", "clientTextSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDatabricksDatabricks1) GetFormat() *Format1Options {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDatabricksDatabricks1) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDatabricksDatabricks1) GetType() OutputDatabricksType1 {
	if o == nil {
		return OutputDatabricksType1("")
	}
	return o.Type
}

func (o *OutputDatabricksDatabricks1) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDatabricksDatabricks1) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDatabricksDatabricks1) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDatabricksDatabricks1) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDatabricksDatabricks1) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDatabricksDatabricks1) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDatabricksDatabricks1) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDatabricksDatabricks1) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDatabricksDatabricks1) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputDatabricksDatabricks1) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDatabricksDatabricks1) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDatabricksDatabricks1) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDatabricksDatabricks1) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDatabricksDatabricks1) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDatabricksDatabricks1) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDatabricksDatabricks1) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDatabricksDatabricks1) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDatabricksDatabricks1) GetOnBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDatabricksDatabricks1) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDatabricksDatabricks1) GetOnDiskFullBackpressure() *PqOnBackpressureOptions {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDatabricksDatabricks1) GetWorkspaceID() string {
	if o == nil {
		return ""
	}
	return o.WorkspaceID
}

func (o *OutputDatabricksDatabricks1) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputDatabricksDatabricks1) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputDatabricksDatabricks1) GetCatalog() *string {
	if o == nil {
		return nil
	}
	return o.Catalog
}

func (o *OutputDatabricksDatabricks1) GetSchema() *string {
	if o == nil {
		return nil
	}
	return o.Schema
}

func (o *OutputDatabricksDatabricks1) GetEventsVolumeName() *string {
	if o == nil {
		return nil
	}
	return o.EventsVolumeName
}

func (o *OutputDatabricksDatabricks1) GetClientTextSecret() string {
	if o == nil {
		return ""
	}
	return o.ClientTextSecret
}

func (o *OutputDatabricksDatabricks1) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDatabricksDatabricks1) GetCompress() *PqCompressOptions {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDatabricksDatabricks1) GetCompressionLevel() *CompressionLevelOptions {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDatabricksDatabricks1) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDatabricksDatabricks1) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputDatabricksDatabricks1) GetParquetVersion() *ParquetVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDatabricksDatabricks1) GetParquetDataPageVersion() *ParquetDataPageVersionOptions {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDatabricksDatabricks1) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDatabricksDatabricks1) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDatabricksDatabricks1) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDatabricksDatabricks1) GetKeyValueMetadata() []TagsType {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputDatabricksDatabricks1) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDatabricksDatabricks1) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDatabricksDatabricks1) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDatabricksDatabricks1) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDatabricksDatabricks1) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDatabricksDatabricks1) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

type OutputDatabricksType string

const (
	OutputDatabricksTypeOutputDatabricksDatabricks1 OutputDatabricksType = "OutputDatabricks_Databricks_1"
	OutputDatabricksTypeOutputDatabricksDatabricks2 OutputDatabricksType = "OutputDatabricks_Databricks_2"
	OutputDatabricksTypeOutputDatabricksDatabricks3 OutputDatabricksType = "OutputDatabricks_Databricks_3"
	OutputDatabricksTypeOutputDatabricksDatabricks4 OutputDatabricksType = "OutputDatabricks_Databricks_4"
	OutputDatabricksTypeOutputDatabricksDatabricks5 OutputDatabricksType = "OutputDatabricks_Databricks_5"
	OutputDatabricksTypeOutputDatabricksDatabricks6 OutputDatabricksType = "OutputDatabricks_Databricks_6"
)

type OutputDatabricks struct {
	OutputDatabricksDatabricks1 *OutputDatabricksDatabricks1 `queryParam:"inline,name=OutputDatabricks"`
	OutputDatabricksDatabricks2 *OutputDatabricksDatabricks2 `queryParam:"inline,name=OutputDatabricks"`
	OutputDatabricksDatabricks3 *OutputDatabricksDatabricks3 `queryParam:"inline,name=OutputDatabricks"`
	OutputDatabricksDatabricks4 *OutputDatabricksDatabricks4 `queryParam:"inline,name=OutputDatabricks"`
	OutputDatabricksDatabricks5 *OutputDatabricksDatabricks5 `queryParam:"inline,name=OutputDatabricks"`
	OutputDatabricksDatabricks6 *OutputDatabricksDatabricks6 `queryParam:"inline,name=OutputDatabricks"`

	Type OutputDatabricksType
}

func CreateOutputDatabricksOutputDatabricksDatabricks1(outputDatabricksDatabricks1 OutputDatabricksDatabricks1) OutputDatabricks {
	typ := OutputDatabricksTypeOutputDatabricksDatabricks1

	return OutputDatabricks{
		OutputDatabricksDatabricks1: &outputDatabricksDatabricks1,
		Type:                        typ,
	}
}

func CreateOutputDatabricksOutputDatabricksDatabricks2(outputDatabricksDatabricks2 OutputDatabricksDatabricks2) OutputDatabricks {
	typ := OutputDatabricksTypeOutputDatabricksDatabricks2

	return OutputDatabricks{
		OutputDatabricksDatabricks2: &outputDatabricksDatabricks2,
		Type:                        typ,
	}
}

func CreateOutputDatabricksOutputDatabricksDatabricks3(outputDatabricksDatabricks3 OutputDatabricksDatabricks3) OutputDatabricks {
	typ := OutputDatabricksTypeOutputDatabricksDatabricks3

	return OutputDatabricks{
		OutputDatabricksDatabricks3: &outputDatabricksDatabricks3,
		Type:                        typ,
	}
}

func CreateOutputDatabricksOutputDatabricksDatabricks4(outputDatabricksDatabricks4 OutputDatabricksDatabricks4) OutputDatabricks {
	typ := OutputDatabricksTypeOutputDatabricksDatabricks4

	return OutputDatabricks{
		OutputDatabricksDatabricks4: &outputDatabricksDatabricks4,
		Type:                        typ,
	}
}

func CreateOutputDatabricksOutputDatabricksDatabricks5(outputDatabricksDatabricks5 OutputDatabricksDatabricks5) OutputDatabricks {
	typ := OutputDatabricksTypeOutputDatabricksDatabricks5

	return OutputDatabricks{
		OutputDatabricksDatabricks5: &outputDatabricksDatabricks5,
		Type:                        typ,
	}
}

func CreateOutputDatabricksOutputDatabricksDatabricks6(outputDatabricksDatabricks6 OutputDatabricksDatabricks6) OutputDatabricks {
	typ := OutputDatabricksTypeOutputDatabricksDatabricks6

	return OutputDatabricks{
		OutputDatabricksDatabricks6: &outputDatabricksDatabricks6,
		Type:                        typ,
	}
}

func (u *OutputDatabricks) UnmarshalJSON(data []byte) error {

	var outputDatabricksDatabricks2 OutputDatabricksDatabricks2 = OutputDatabricksDatabricks2{}
	if err := utils.UnmarshalJSON(data, &outputDatabricksDatabricks2, "", true, nil); err == nil {
		u.OutputDatabricksDatabricks2 = &outputDatabricksDatabricks2
		u.Type = OutputDatabricksTypeOutputDatabricksDatabricks2
		return nil
	}

	var outputDatabricksDatabricks1 OutputDatabricksDatabricks1 = OutputDatabricksDatabricks1{}
	if err := utils.UnmarshalJSON(data, &outputDatabricksDatabricks1, "", true, nil); err == nil {
		u.OutputDatabricksDatabricks1 = &outputDatabricksDatabricks1
		u.Type = OutputDatabricksTypeOutputDatabricksDatabricks1
		return nil
	}

	var outputDatabricksDatabricks3 OutputDatabricksDatabricks3 = OutputDatabricksDatabricks3{}
	if err := utils.UnmarshalJSON(data, &outputDatabricksDatabricks3, "", true, nil); err == nil {
		u.OutputDatabricksDatabricks3 = &outputDatabricksDatabricks3
		u.Type = OutputDatabricksTypeOutputDatabricksDatabricks3
		return nil
	}

	var outputDatabricksDatabricks4 OutputDatabricksDatabricks4 = OutputDatabricksDatabricks4{}
	if err := utils.UnmarshalJSON(data, &outputDatabricksDatabricks4, "", true, nil); err == nil {
		u.OutputDatabricksDatabricks4 = &outputDatabricksDatabricks4
		u.Type = OutputDatabricksTypeOutputDatabricksDatabricks4
		return nil
	}

	var outputDatabricksDatabricks5 OutputDatabricksDatabricks5 = OutputDatabricksDatabricks5{}
	if err := utils.UnmarshalJSON(data, &outputDatabricksDatabricks5, "", true, nil); err == nil {
		u.OutputDatabricksDatabricks5 = &outputDatabricksDatabricks5
		u.Type = OutputDatabricksTypeOutputDatabricksDatabricks5
		return nil
	}

	var outputDatabricksDatabricks6 OutputDatabricksDatabricks6 = OutputDatabricksDatabricks6{}
	if err := utils.UnmarshalJSON(data, &outputDatabricksDatabricks6, "", true, nil); err == nil {
		u.OutputDatabricksDatabricks6 = &outputDatabricksDatabricks6
		u.Type = OutputDatabricksTypeOutputDatabricksDatabricks6
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for OutputDatabricks", string(data))
}

func (u OutputDatabricks) MarshalJSON() ([]byte, error) {
	if u.OutputDatabricksDatabricks1 != nil {
		return utils.MarshalJSON(u.OutputDatabricksDatabricks1, "", true)
	}

	if u.OutputDatabricksDatabricks2 != nil {
		return utils.MarshalJSON(u.OutputDatabricksDatabricks2, "", true)
	}

	if u.OutputDatabricksDatabricks3 != nil {
		return utils.MarshalJSON(u.OutputDatabricksDatabricks3, "", true)
	}

	if u.OutputDatabricksDatabricks4 != nil {
		return utils.MarshalJSON(u.OutputDatabricksDatabricks4, "", true)
	}

	if u.OutputDatabricksDatabricks5 != nil {
		return utils.MarshalJSON(u.OutputDatabricksDatabricks5, "", true)
	}

	if u.OutputDatabricksDatabricks6 != nil {
		return utils.MarshalJSON(u.OutputDatabricksDatabricks6, "", true)
	}

	return nil, errors.New("could not marshal union type OutputDatabricks: all fields are null")
}
