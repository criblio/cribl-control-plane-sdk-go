// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package components

import (
	"encoding/json"
	"errors"
	"fmt"
	"github.com/criblio/cribl-control-plane-sdk-go/internal/utils"
)

type TypeCloudflareR2 string

const (
	TypeCloudflareR2CloudflareR2 TypeCloudflareR2 = "cloudflare_r2"
)

func (e TypeCloudflareR2) ToPointer() *TypeCloudflareR2 {
	return &e
}
func (e *TypeCloudflareR2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cloudflare_r2":
		*e = TypeCloudflareR2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCloudflareR2: %v", v)
	}
}

// AuthenticationMethodCloudflareR2 - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodCloudflareR2 string

const (
	AuthenticationMethodCloudflareR2Auto   AuthenticationMethodCloudflareR2 = "auto"
	AuthenticationMethodCloudflareR2Secret AuthenticationMethodCloudflareR2 = "secret"
	AuthenticationMethodCloudflareR2Manual AuthenticationMethodCloudflareR2 = "manual"
)

func (e AuthenticationMethodCloudflareR2) ToPointer() *AuthenticationMethodCloudflareR2 {
	return &e
}

// SignatureVersionCloudflareR2 - Signature version to use for signing MinIO requests
type SignatureVersionCloudflareR2 string

const (
	SignatureVersionCloudflareR2V2 SignatureVersionCloudflareR2 = "v2"
	SignatureVersionCloudflareR2V4 SignatureVersionCloudflareR2 = "v4"
)

func (e SignatureVersionCloudflareR2) ToPointer() *SignatureVersionCloudflareR2 {
	return &e
}

// StorageClassCloudflareR2 - Storage class to select for uploaded objects
type StorageClassCloudflareR2 string

const (
	// StorageClassCloudflareR2Standard Standard
	StorageClassCloudflareR2Standard StorageClassCloudflareR2 = "STANDARD"
	// StorageClassCloudflareR2ReducedRedundancy Reduced Redundancy Storage
	StorageClassCloudflareR2ReducedRedundancy StorageClassCloudflareR2 = "REDUCED_REDUNDANCY"
)

func (e StorageClassCloudflareR2) ToPointer() *StorageClassCloudflareR2 {
	return &e
}

// ServerSideEncryptionCloudflareR2 - Server-side encryption for uploaded objects
type ServerSideEncryptionCloudflareR2 string

const (
	// ServerSideEncryptionCloudflareR2Aes256 Amazon S3 Managed Key
	ServerSideEncryptionCloudflareR2Aes256 ServerSideEncryptionCloudflareR2 = "AES256"
)

func (e ServerSideEncryptionCloudflareR2) ToPointer() *ServerSideEncryptionCloudflareR2 {
	return &e
}

// DataFormatCloudflareR2 - Format of the output data
type DataFormatCloudflareR2 string

const (
	// DataFormatCloudflareR2JSON JSON
	DataFormatCloudflareR2JSON DataFormatCloudflareR2 = "json"
	// DataFormatCloudflareR2Raw Raw
	DataFormatCloudflareR2Raw DataFormatCloudflareR2 = "raw"
	// DataFormatCloudflareR2Parquet Parquet
	DataFormatCloudflareR2Parquet DataFormatCloudflareR2 = "parquet"
)

func (e DataFormatCloudflareR2) ToPointer() *DataFormatCloudflareR2 {
	return &e
}

// BackpressureBehaviorCloudflareR2 - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCloudflareR2 string

const (
	// BackpressureBehaviorCloudflareR2Block Block
	BackpressureBehaviorCloudflareR2Block BackpressureBehaviorCloudflareR2 = "block"
	// BackpressureBehaviorCloudflareR2Drop Drop
	BackpressureBehaviorCloudflareR2Drop BackpressureBehaviorCloudflareR2 = "drop"
)

func (e BackpressureBehaviorCloudflareR2) ToPointer() *BackpressureBehaviorCloudflareR2 {
	return &e
}

// DiskSpaceProtectionCloudflareR2 - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionCloudflareR2 string

const (
	// DiskSpaceProtectionCloudflareR2Block Block
	DiskSpaceProtectionCloudflareR2Block DiskSpaceProtectionCloudflareR2 = "block"
	// DiskSpaceProtectionCloudflareR2Drop Drop
	DiskSpaceProtectionCloudflareR2Drop DiskSpaceProtectionCloudflareR2 = "drop"
)

func (e DiskSpaceProtectionCloudflareR2) ToPointer() *DiskSpaceProtectionCloudflareR2 {
	return &e
}

// CompressionCloudflareR2 - Data compression format to apply to HTTP content before it is delivered
type CompressionCloudflareR2 string

const (
	CompressionCloudflareR2None CompressionCloudflareR2 = "none"
	CompressionCloudflareR2Gzip CompressionCloudflareR2 = "gzip"
)

func (e CompressionCloudflareR2) ToPointer() *CompressionCloudflareR2 {
	return &e
}

// CompressionLevelCloudflareR2 - Compression level to apply before moving files to final destination
type CompressionLevelCloudflareR2 string

const (
	// CompressionLevelCloudflareR2BestSpeed Best Speed
	CompressionLevelCloudflareR2BestSpeed CompressionLevelCloudflareR2 = "best_speed"
	// CompressionLevelCloudflareR2Normal Normal
	CompressionLevelCloudflareR2Normal CompressionLevelCloudflareR2 = "normal"
	// CompressionLevelCloudflareR2BestCompression Best Compression
	CompressionLevelCloudflareR2BestCompression CompressionLevelCloudflareR2 = "best_compression"
)

func (e CompressionLevelCloudflareR2) ToPointer() *CompressionLevelCloudflareR2 {
	return &e
}

// ParquetVersionCloudflareR2 - Determines which data types are supported and how they are represented
type ParquetVersionCloudflareR2 string

const (
	// ParquetVersionCloudflareR2Parquet10 1.0
	ParquetVersionCloudflareR2Parquet10 ParquetVersionCloudflareR2 = "PARQUET_1_0"
	// ParquetVersionCloudflareR2Parquet24 2.4
	ParquetVersionCloudflareR2Parquet24 ParquetVersionCloudflareR2 = "PARQUET_2_4"
	// ParquetVersionCloudflareR2Parquet26 2.6
	ParquetVersionCloudflareR2Parquet26 ParquetVersionCloudflareR2 = "PARQUET_2_6"
)

func (e ParquetVersionCloudflareR2) ToPointer() *ParquetVersionCloudflareR2 {
	return &e
}

// DataPageVersionCloudflareR2 - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionCloudflareR2 string

const (
	// DataPageVersionCloudflareR2DataPageV1 V1
	DataPageVersionCloudflareR2DataPageV1 DataPageVersionCloudflareR2 = "DATA_PAGE_V1"
	// DataPageVersionCloudflareR2DataPageV2 V2
	DataPageVersionCloudflareR2DataPageV2 DataPageVersionCloudflareR2 = "DATA_PAGE_V2"
)

func (e DataPageVersionCloudflareR2) ToPointer() *DataPageVersionCloudflareR2 {
	return &e
}

type KeyValueMetadatumCloudflareR2 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumCloudflareR2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumCloudflareR2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumCloudflareR2) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumCloudflareR2) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputCloudflareR2 struct {
	// Unique ID for this output
	ID   *string          `json:"id,omitempty"`
	Type TypeCloudflareR2 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Cloudflare R2 service URL (example: https://<ACCOUNT_ID>.r2.cloudflarestorage.com)
	Endpoint string `json:"endpoint"`
	// Name of the destination R2 bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodCloudflareR2 `default:"auto" json:"awsAuthenticationMethod"`
	// Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	Region       any     `json:"region,omitempty"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.
	DestPath *string `json:"destPath,omitempty"`
	// Signature version to use for signing MinIO requests
	SignatureVersion *SignatureVersionCloudflareR2 `default:"v4" json:"signatureVersion"`
	ObjectACL        any                           `json:"objectACL,omitempty"`
	// Storage class to select for uploaded objects
	StorageClass *StorageClassCloudflareR2 `json:"storageClass,omitempty"`
	// Server-side encryption for uploaded objects
	ServerSideEncryption *ServerSideEncryptionCloudflareR2 `json:"serverSideEncryption,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatCloudflareR2 `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCloudflareR2 `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionCloudflareR2 `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	Description            *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionCloudflareR2 `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelCloudflareR2 `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionCloudflareR2 `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionCloudflareR2 `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumCloudflareR2 `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum          *float64       `default:"20" json:"maxRetryNum"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputCloudflareR2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCloudflareR2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "endpoint", "bucket"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCloudflareR2) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputCloudflareR2) GetType() TypeCloudflareR2 {
	if o == nil {
		return TypeCloudflareR2("")
	}
	return o.Type
}

func (o *OutputCloudflareR2) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCloudflareR2) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCloudflareR2) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCloudflareR2) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCloudflareR2) GetEndpoint() string {
	if o == nil {
		return ""
	}
	return o.Endpoint
}

func (o *OutputCloudflareR2) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputCloudflareR2) GetAwsAuthenticationMethod() *AuthenticationMethodCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputCloudflareR2) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputCloudflareR2) GetRegion() any {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputCloudflareR2) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputCloudflareR2) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputCloudflareR2) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputCloudflareR2) GetSignatureVersion() *SignatureVersionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputCloudflareR2) GetObjectACL() any {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputCloudflareR2) GetStorageClass() *StorageClassCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputCloudflareR2) GetServerSideEncryption() *ServerSideEncryptionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputCloudflareR2) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputCloudflareR2) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCloudflareR2) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputCloudflareR2) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputCloudflareR2) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputCloudflareR2) GetFormat() *DataFormatCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputCloudflareR2) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputCloudflareR2) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputCloudflareR2) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputCloudflareR2) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputCloudflareR2) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputCloudflareR2) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputCloudflareR2) GetOnBackpressure() *BackpressureBehaviorCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCloudflareR2) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputCloudflareR2) GetOnDiskFullBackpressure() *DiskSpaceProtectionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputCloudflareR2) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputCloudflareR2) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputCloudflareR2) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputCloudflareR2) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputCloudflareR2) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCloudflareR2) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputCloudflareR2) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputCloudflareR2) GetCompress() *CompressionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputCloudflareR2) GetCompressionLevel() *CompressionLevelCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputCloudflareR2) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputCloudflareR2) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputCloudflareR2) GetParquetVersion() *ParquetVersionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputCloudflareR2) GetParquetDataPageVersion() *DataPageVersionCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputCloudflareR2) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputCloudflareR2) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputCloudflareR2) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputCloudflareR2) GetKeyValueMetadata() []KeyValueMetadatumCloudflareR2 {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputCloudflareR2) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputCloudflareR2) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputCloudflareR2) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputCloudflareR2) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputCloudflareR2) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputCloudflareR2) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputCloudflareR2) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputCloudflareR2) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeMicrosoftFabric string

const (
	TypeMicrosoftFabricMicrosoftFabric TypeMicrosoftFabric = "microsoft_fabric"
)

func (e TypeMicrosoftFabric) ToPointer() *TypeMicrosoftFabric {
	return &e
}
func (e *TypeMicrosoftFabric) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "microsoft_fabric":
		*e = TypeMicrosoftFabric(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeMicrosoftFabric: %v", v)
	}
}

// AcknowledgmentsMicrosoftFabric - Control the number of required acknowledgments
type AcknowledgmentsMicrosoftFabric int64

const (
	// AcknowledgmentsMicrosoftFabricOne Leader
	AcknowledgmentsMicrosoftFabricOne AcknowledgmentsMicrosoftFabric = 1
	// AcknowledgmentsMicrosoftFabricZero None
	AcknowledgmentsMicrosoftFabricZero AcknowledgmentsMicrosoftFabric = 0
	// AcknowledgmentsMicrosoftFabricMinus1 All
	AcknowledgmentsMicrosoftFabricMinus1 AcknowledgmentsMicrosoftFabric = -1
)

func (e AcknowledgmentsMicrosoftFabric) ToPointer() *AcknowledgmentsMicrosoftFabric {
	return &e
}

// RecordDataFormatMicrosoftFabric - Format to use to serialize events before writing to the Event Hubs Kafka brokers
type RecordDataFormatMicrosoftFabric string

const (
	// RecordDataFormatMicrosoftFabricJSON JSON
	RecordDataFormatMicrosoftFabricJSON RecordDataFormatMicrosoftFabric = "json"
	// RecordDataFormatMicrosoftFabricRaw Field _raw
	RecordDataFormatMicrosoftFabricRaw RecordDataFormatMicrosoftFabric = "raw"
)

func (e RecordDataFormatMicrosoftFabric) ToPointer() *RecordDataFormatMicrosoftFabric {
	return &e
}

type SASLMechanismMicrosoftFabric string

const (
	// SASLMechanismMicrosoftFabricPlain PLAIN
	SASLMechanismMicrosoftFabricPlain SASLMechanismMicrosoftFabric = "plain"
	// SASLMechanismMicrosoftFabricOauthbearer OAUTHBEARER
	SASLMechanismMicrosoftFabricOauthbearer SASLMechanismMicrosoftFabric = "oauthbearer"
)

func (e SASLMechanismMicrosoftFabric) ToPointer() *SASLMechanismMicrosoftFabric {
	return &e
}

type AuthenticationMethodMicrosoftFabric string

const (
	AuthenticationMethodMicrosoftFabricSecret      AuthenticationMethodMicrosoftFabric = "secret"
	AuthenticationMethodMicrosoftFabricCertificate AuthenticationMethodMicrosoftFabric = "certificate"
)

func (e AuthenticationMethodMicrosoftFabric) ToPointer() *AuthenticationMethodMicrosoftFabric {
	return &e
}

// MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric - Endpoint used to acquire authentication tokens from Azure
type MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric string

const (
	MicrosoftEntraIDAuthenticationEndpointMicrosoftFabricHTTPSLoginMicrosoftonlineCom       MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric = "https://login.microsoftonline.com"
	MicrosoftEntraIDAuthenticationEndpointMicrosoftFabricHTTPSLoginMicrosoftonlineUs        MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric = "https://login.microsoftonline.us"
	MicrosoftEntraIDAuthenticationEndpointMicrosoftFabricHTTPSLoginPartnerMicrosoftonlineCn MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric = "https://login.partner.microsoftonline.cn"
)

func (e MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric) ToPointer() *MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric {
	return &e
}

// AuthenticationMicrosoftFabric - Authentication parameters to use when connecting to bootstrap server. Using TLS is highly recommended.
type AuthenticationMicrosoftFabric struct {
	Disabled  *bool                         `default:"false" json:"disabled"`
	Mechanism *SASLMechanismMicrosoftFabric `default:"plain" json:"mechanism"`
	// The username for authentication. This should always be $ConnectionString.
	Username *string `default:"$ConnectionString" json:"username"`
	// Select or create a stored text secret corresponding to the SASL JASS Password Primary or Password Secondary
	TextSecret           *string                              `json:"textSecret,omitempty"`
	ClientSecretAuthType *AuthenticationMethodMicrosoftFabric `default:"secret" json:"clientSecretAuthType"`
	// Select or create a stored text secret
	ClientTextSecret *string `json:"clientTextSecret,omitempty"`
	// Select or create a stored certificate
	CertificateName *string `json:"certificateName,omitempty"`
	CertPath        *string `json:"certPath,omitempty"`
	PrivKeyPath     *string `json:"privKeyPath,omitempty"`
	Passphrase      *string `json:"passphrase,omitempty"`
	// Endpoint used to acquire authentication tokens from Azure
	OauthEndpoint *MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric `default:"https://login.microsoftonline.com" json:"oauthEndpoint"`
	// client_id to pass in the OAuth request parameter
	ClientID *string `json:"clientId,omitempty"`
	// Directory ID (tenant identifier) in Azure Active Directory
	TenantID *string `json:"tenantId,omitempty"`
	// Scope to pass in the OAuth request parameter
	Scope *string `json:"scope,omitempty"`
}

func (a AuthenticationMicrosoftFabric) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthenticationMicrosoftFabric) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (a *AuthenticationMicrosoftFabric) GetDisabled() *bool {
	if a == nil {
		return nil
	}
	return a.Disabled
}

func (a *AuthenticationMicrosoftFabric) GetMechanism() *SASLMechanismMicrosoftFabric {
	if a == nil {
		return nil
	}
	return a.Mechanism
}

func (a *AuthenticationMicrosoftFabric) GetUsername() *string {
	if a == nil {
		return nil
	}
	return a.Username
}

func (a *AuthenticationMicrosoftFabric) GetTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.TextSecret
}

func (a *AuthenticationMicrosoftFabric) GetClientSecretAuthType() *AuthenticationMethodMicrosoftFabric {
	if a == nil {
		return nil
	}
	return a.ClientSecretAuthType
}

func (a *AuthenticationMicrosoftFabric) GetClientTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.ClientTextSecret
}

func (a *AuthenticationMicrosoftFabric) GetCertificateName() *string {
	if a == nil {
		return nil
	}
	return a.CertificateName
}

func (a *AuthenticationMicrosoftFabric) GetCertPath() *string {
	if a == nil {
		return nil
	}
	return a.CertPath
}

func (a *AuthenticationMicrosoftFabric) GetPrivKeyPath() *string {
	if a == nil {
		return nil
	}
	return a.PrivKeyPath
}

func (a *AuthenticationMicrosoftFabric) GetPassphrase() *string {
	if a == nil {
		return nil
	}
	return a.Passphrase
}

func (a *AuthenticationMicrosoftFabric) GetOauthEndpoint() *MicrosoftEntraIDAuthenticationEndpointMicrosoftFabric {
	if a == nil {
		return nil
	}
	return a.OauthEndpoint
}

func (a *AuthenticationMicrosoftFabric) GetClientID() *string {
	if a == nil {
		return nil
	}
	return a.ClientID
}

func (a *AuthenticationMicrosoftFabric) GetTenantID() *string {
	if a == nil {
		return nil
	}
	return a.TenantID
}

func (a *AuthenticationMicrosoftFabric) GetScope() *string {
	if a == nil {
		return nil
	}
	return a.Scope
}

type TLSSettingsClientSideMicrosoftFabric struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
}

func (t TLSSettingsClientSideMicrosoftFabric) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideMicrosoftFabric) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideMicrosoftFabric) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideMicrosoftFabric) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

// BackpressureBehaviorMicrosoftFabric - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorMicrosoftFabric string

const (
	// BackpressureBehaviorMicrosoftFabricBlock Block
	BackpressureBehaviorMicrosoftFabricBlock BackpressureBehaviorMicrosoftFabric = "block"
	// BackpressureBehaviorMicrosoftFabricDrop Drop
	BackpressureBehaviorMicrosoftFabricDrop BackpressureBehaviorMicrosoftFabric = "drop"
	// BackpressureBehaviorMicrosoftFabricQueue Persistent Queue
	BackpressureBehaviorMicrosoftFabricQueue BackpressureBehaviorMicrosoftFabric = "queue"
)

func (e BackpressureBehaviorMicrosoftFabric) ToPointer() *BackpressureBehaviorMicrosoftFabric {
	return &e
}

// ModeMicrosoftFabric - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeMicrosoftFabric string

const (
	// ModeMicrosoftFabricError Error
	ModeMicrosoftFabricError ModeMicrosoftFabric = "error"
	// ModeMicrosoftFabricAlways Backpressure
	ModeMicrosoftFabricAlways ModeMicrosoftFabric = "always"
	// ModeMicrosoftFabricBackpressure Always On
	ModeMicrosoftFabricBackpressure ModeMicrosoftFabric = "backpressure"
)

func (e ModeMicrosoftFabric) ToPointer() *ModeMicrosoftFabric {
	return &e
}

// CompressionMicrosoftFabric - Codec to use to compress the persisted data
type CompressionMicrosoftFabric string

const (
	// CompressionMicrosoftFabricNone None
	CompressionMicrosoftFabricNone CompressionMicrosoftFabric = "none"
	// CompressionMicrosoftFabricGzip Gzip
	CompressionMicrosoftFabricGzip CompressionMicrosoftFabric = "gzip"
)

func (e CompressionMicrosoftFabric) ToPointer() *CompressionMicrosoftFabric {
	return &e
}

// QueueFullBehaviorMicrosoftFabric - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorMicrosoftFabric string

const (
	// QueueFullBehaviorMicrosoftFabricBlock Block
	QueueFullBehaviorMicrosoftFabricBlock QueueFullBehaviorMicrosoftFabric = "block"
	// QueueFullBehaviorMicrosoftFabricDrop Drop new data
	QueueFullBehaviorMicrosoftFabricDrop QueueFullBehaviorMicrosoftFabric = "drop"
)

func (e QueueFullBehaviorMicrosoftFabric) ToPointer() *QueueFullBehaviorMicrosoftFabric {
	return &e
}

type PqControlsMicrosoftFabric struct {
}

func (p PqControlsMicrosoftFabric) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsMicrosoftFabric) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputMicrosoftFabric struct {
	// Unique ID for this output
	ID   *string             `json:"id,omitempty"`
	Type TypeMicrosoftFabric `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Topic name from Fabric Eventstream's endpoint
	Topic string `json:"topic"`
	// Control the number of required acknowledgments
	Ack *AcknowledgmentsMicrosoftFabric `default:"1" json:"ack"`
	// Format to use to serialize events before writing to the Event Hubs Kafka brokers
	Format *RecordDataFormatMicrosoftFabric `default:"json" json:"format"`
	// Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// Maximum number of events in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to bootstrap server. Using TLS is highly recommended.
	Sasl *AuthenticationMicrosoftFabric        `json:"sasl,omitempty"`
	TLS  *TLSSettingsClientSideMicrosoftFabric `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorMicrosoftFabric `default:"block" json:"onBackpressure"`
	// Bootstrap server from Fabric Eventstream's endpoint
	BootstrapServer string  `json:"bootstrap_server"`
	Description     *string `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeMicrosoftFabric `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionMicrosoftFabric `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorMicrosoftFabric `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsMicrosoftFabric        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                    `additionalProperties:"true" json:"-"`
}

func (o OutputMicrosoftFabric) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMicrosoftFabric) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "topic", "bootstrap_server"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputMicrosoftFabric) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputMicrosoftFabric) GetType() TypeMicrosoftFabric {
	if o == nil {
		return TypeMicrosoftFabric("")
	}
	return o.Type
}

func (o *OutputMicrosoftFabric) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputMicrosoftFabric) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputMicrosoftFabric) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputMicrosoftFabric) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputMicrosoftFabric) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputMicrosoftFabric) GetAck() *AcknowledgmentsMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputMicrosoftFabric) GetFormat() *RecordDataFormatMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputMicrosoftFabric) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputMicrosoftFabric) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputMicrosoftFabric) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputMicrosoftFabric) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputMicrosoftFabric) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputMicrosoftFabric) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputMicrosoftFabric) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputMicrosoftFabric) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputMicrosoftFabric) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputMicrosoftFabric) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputMicrosoftFabric) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputMicrosoftFabric) GetSasl() *AuthenticationMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputMicrosoftFabric) GetTLS() *TLSSettingsClientSideMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputMicrosoftFabric) GetOnBackpressure() *BackpressureBehaviorMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputMicrosoftFabric) GetBootstrapServer() string {
	if o == nil {
		return ""
	}
	return o.BootstrapServer
}

func (o *OutputMicrosoftFabric) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputMicrosoftFabric) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputMicrosoftFabric) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputMicrosoftFabric) GetPqMode() *ModeMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputMicrosoftFabric) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputMicrosoftFabric) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputMicrosoftFabric) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputMicrosoftFabric) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputMicrosoftFabric) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputMicrosoftFabric) GetPqCompress() *CompressionMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputMicrosoftFabric) GetPqOnBackpressure() *QueueFullBehaviorMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputMicrosoftFabric) GetPqControls() *PqControlsMicrosoftFabric {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputMicrosoftFabric) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeDatabricks string

const (
	TypeDatabricksDatabricks TypeDatabricks = "databricks"
)

func (e TypeDatabricks) ToPointer() *TypeDatabricks {
	return &e
}
func (e *TypeDatabricks) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "databricks":
		*e = TypeDatabricks(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDatabricks: %v", v)
	}
}

// DataFormatDatabricks - Format of the output data
type DataFormatDatabricks string

const (
	// DataFormatDatabricksJSON JSON
	DataFormatDatabricksJSON DataFormatDatabricks = "json"
	// DataFormatDatabricksRaw Raw
	DataFormatDatabricksRaw DataFormatDatabricks = "raw"
	// DataFormatDatabricksParquet Parquet
	DataFormatDatabricksParquet DataFormatDatabricks = "parquet"
)

func (e DataFormatDatabricks) ToPointer() *DataFormatDatabricks {
	return &e
}

// BackpressureBehaviorDatabricks - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDatabricks string

const (
	// BackpressureBehaviorDatabricksBlock Block
	BackpressureBehaviorDatabricksBlock BackpressureBehaviorDatabricks = "block"
	// BackpressureBehaviorDatabricksDrop Drop
	BackpressureBehaviorDatabricksDrop BackpressureBehaviorDatabricks = "drop"
)

func (e BackpressureBehaviorDatabricks) ToPointer() *BackpressureBehaviorDatabricks {
	return &e
}

// DiskSpaceProtectionDatabricks - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionDatabricks string

const (
	// DiskSpaceProtectionDatabricksBlock Block
	DiskSpaceProtectionDatabricksBlock DiskSpaceProtectionDatabricks = "block"
	// DiskSpaceProtectionDatabricksDrop Drop
	DiskSpaceProtectionDatabricksDrop DiskSpaceProtectionDatabricks = "drop"
)

func (e DiskSpaceProtectionDatabricks) ToPointer() *DiskSpaceProtectionDatabricks {
	return &e
}

// CompressionDatabricks - Data compression format to apply to HTTP content before it is delivered
type CompressionDatabricks string

const (
	CompressionDatabricksNone CompressionDatabricks = "none"
	CompressionDatabricksGzip CompressionDatabricks = "gzip"
)

func (e CompressionDatabricks) ToPointer() *CompressionDatabricks {
	return &e
}

// CompressionLevelDatabricks - Compression level to apply before moving files to final destination
type CompressionLevelDatabricks string

const (
	// CompressionLevelDatabricksBestSpeed Best Speed
	CompressionLevelDatabricksBestSpeed CompressionLevelDatabricks = "best_speed"
	// CompressionLevelDatabricksNormal Normal
	CompressionLevelDatabricksNormal CompressionLevelDatabricks = "normal"
	// CompressionLevelDatabricksBestCompression Best Compression
	CompressionLevelDatabricksBestCompression CompressionLevelDatabricks = "best_compression"
)

func (e CompressionLevelDatabricks) ToPointer() *CompressionLevelDatabricks {
	return &e
}

// ParquetVersionDatabricks - Determines which data types are supported and how they are represented
type ParquetVersionDatabricks string

const (
	// ParquetVersionDatabricksParquet10 1.0
	ParquetVersionDatabricksParquet10 ParquetVersionDatabricks = "PARQUET_1_0"
	// ParquetVersionDatabricksParquet24 2.4
	ParquetVersionDatabricksParquet24 ParquetVersionDatabricks = "PARQUET_2_4"
	// ParquetVersionDatabricksParquet26 2.6
	ParquetVersionDatabricksParquet26 ParquetVersionDatabricks = "PARQUET_2_6"
)

func (e ParquetVersionDatabricks) ToPointer() *ParquetVersionDatabricks {
	return &e
}

// DataPageVersionDatabricks - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionDatabricks string

const (
	// DataPageVersionDatabricksDataPageV1 V1
	DataPageVersionDatabricksDataPageV1 DataPageVersionDatabricks = "DATA_PAGE_V1"
	// DataPageVersionDatabricksDataPageV2 V2
	DataPageVersionDatabricksDataPageV2 DataPageVersionDatabricks = "DATA_PAGE_V2"
)

func (e DataPageVersionDatabricks) ToPointer() *DataPageVersionDatabricks {
	return &e
}

type KeyValueMetadatumDatabricks struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumDatabricks) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumDatabricks) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumDatabricks) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumDatabricks) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputDatabricks struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type TypeDatabricks `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Optional path to prepend to files before uploading.
	DestPath *string `default:"" json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatDatabricks `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDatabricks `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionDatabricks `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Databricks workspace ID
	WorkspaceID string `json:"workspaceId"`
	// OAuth scope for Unity Catalog authentication
	Scope *string `default:"all-apis" json:"scope"`
	// OAuth client ID for Unity Catalog authentication
	ClientID string `json:"clientId"`
	// Name of the catalog to use for the output
	Catalog *string `default:"main" json:"catalog"`
	// Name of the catalog schema to use for the output
	Schema *string `default:"external" json:"schema"`
	// Name of the events volume in Databricks
	EventsVolumeName *string `default:"events" json:"eventsVolumeName"`
	// OAuth client secret for Unity Catalog authentication
	ClientTextSecret string `json:"clientTextSecret"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec  *float64 `default:"60" json:"timeoutSec"`
	Description *string  `json:"description,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionDatabricks `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelDatabricks `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionDatabricks `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionDatabricks `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumDatabricks `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum          *float64       `default:"20" json:"maxRetryNum"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputDatabricks) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDatabricks) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "workspaceId", "clientId", "clientTextSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDatabricks) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDatabricks) GetType() TypeDatabricks {
	if o == nil {
		return TypeDatabricks("")
	}
	return o.Type
}

func (o *OutputDatabricks) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDatabricks) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDatabricks) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDatabricks) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDatabricks) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDatabricks) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDatabricks) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDatabricks) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDatabricks) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputDatabricks) GetFormat() *DataFormatDatabricks {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDatabricks) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDatabricks) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDatabricks) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDatabricks) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDatabricks) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDatabricks) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDatabricks) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDatabricks) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDatabricks) GetOnBackpressure() *BackpressureBehaviorDatabricks {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDatabricks) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDatabricks) GetOnDiskFullBackpressure() *DiskSpaceProtectionDatabricks {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDatabricks) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputDatabricks) GetWorkspaceID() string {
	if o == nil {
		return ""
	}
	return o.WorkspaceID
}

func (o *OutputDatabricks) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputDatabricks) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputDatabricks) GetCatalog() *string {
	if o == nil {
		return nil
	}
	return o.Catalog
}

func (o *OutputDatabricks) GetSchema() *string {
	if o == nil {
		return nil
	}
	return o.Schema
}

func (o *OutputDatabricks) GetEventsVolumeName() *string {
	if o == nil {
		return nil
	}
	return o.EventsVolumeName
}

func (o *OutputDatabricks) GetClientTextSecret() string {
	if o == nil {
		return ""
	}
	return o.ClientTextSecret
}

func (o *OutputDatabricks) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDatabricks) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDatabricks) GetCompress() *CompressionDatabricks {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDatabricks) GetCompressionLevel() *CompressionLevelDatabricks {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDatabricks) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDatabricks) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputDatabricks) GetParquetVersion() *ParquetVersionDatabricks {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDatabricks) GetParquetDataPageVersion() *DataPageVersionDatabricks {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDatabricks) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDatabricks) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDatabricks) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDatabricks) GetKeyValueMetadata() []KeyValueMetadatumDatabricks {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputDatabricks) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDatabricks) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDatabricks) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDatabricks) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDatabricks) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputDatabricks) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDatabricks) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputDatabricks) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeChronicle string

const (
	TypeChronicleChronicle TypeChronicle = "chronicle"
)

func (e TypeChronicle) ToPointer() *TypeChronicle {
	return &e
}
func (e *TypeChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "chronicle":
		*e = TypeChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeChronicle: %v", v)
	}
}

type AuthenticationMethodChronicle string

const (
	AuthenticationMethodChronicleServiceAccount       AuthenticationMethodChronicle = "serviceAccount"
	AuthenticationMethodChronicleServiceAccountSecret AuthenticationMethodChronicle = "serviceAccountSecret"
)

func (e AuthenticationMethodChronicle) ToPointer() *AuthenticationMethodChronicle {
	return &e
}

type ResponseRetrySettingChronicle struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingChronicle) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingChronicle) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingChronicle) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingChronicle) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsChronicle struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsChronicle) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsChronicle) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsChronicle) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsChronicle) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type ExtraHTTPHeaderChronicle struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderChronicle) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderChronicle) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeChronicle - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeChronicle string

const (
	// FailedRequestLoggingModeChroniclePayload Payload
	FailedRequestLoggingModeChroniclePayload FailedRequestLoggingModeChronicle = "payload"
	// FailedRequestLoggingModeChroniclePayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeChroniclePayloadAndHeaders FailedRequestLoggingModeChronicle = "payloadAndHeaders"
	// FailedRequestLoggingModeChronicleNone None
	FailedRequestLoggingModeChronicleNone FailedRequestLoggingModeChronicle = "none"
)

func (e FailedRequestLoggingModeChronicle) ToPointer() *FailedRequestLoggingModeChronicle {
	return &e
}

// BackpressureBehaviorChronicle - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorChronicle string

const (
	// BackpressureBehaviorChronicleBlock Block
	BackpressureBehaviorChronicleBlock BackpressureBehaviorChronicle = "block"
	// BackpressureBehaviorChronicleDrop Drop
	BackpressureBehaviorChronicleDrop BackpressureBehaviorChronicle = "drop"
	// BackpressureBehaviorChronicleQueue Persistent Queue
	BackpressureBehaviorChronicleQueue BackpressureBehaviorChronicle = "queue"
)

func (e BackpressureBehaviorChronicle) ToPointer() *BackpressureBehaviorChronicle {
	return &e
}

type CustomLabelChronicle struct {
	Key   string `json:"key"`
	Value string `json:"value"`
	// Designate this label for role-based access control and filtering
	RbacEnabled *bool `default:"false" json:"rbacEnabled"`
}

func (c CustomLabelChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CustomLabelChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"key", "value"}); err != nil {
		return err
	}
	return nil
}

func (c *CustomLabelChronicle) GetKey() string {
	if c == nil {
		return ""
	}
	return c.Key
}

func (c *CustomLabelChronicle) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

func (c *CustomLabelChronicle) GetRbacEnabled() *bool {
	if c == nil {
		return nil
	}
	return c.RbacEnabled
}

// ModeChronicle - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeChronicle string

const (
	// ModeChronicleError Error
	ModeChronicleError ModeChronicle = "error"
	// ModeChronicleAlways Backpressure
	ModeChronicleAlways ModeChronicle = "always"
	// ModeChronicleBackpressure Always On
	ModeChronicleBackpressure ModeChronicle = "backpressure"
)

func (e ModeChronicle) ToPointer() *ModeChronicle {
	return &e
}

// CompressionChronicle - Codec to use to compress the persisted data
type CompressionChronicle string

const (
	// CompressionChronicleNone None
	CompressionChronicleNone CompressionChronicle = "none"
	// CompressionChronicleGzip Gzip
	CompressionChronicleGzip CompressionChronicle = "gzip"
)

func (e CompressionChronicle) ToPointer() *CompressionChronicle {
	return &e
}

// QueueFullBehaviorChronicle - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorChronicle string

const (
	// QueueFullBehaviorChronicleBlock Block
	QueueFullBehaviorChronicleBlock QueueFullBehaviorChronicle = "block"
	// QueueFullBehaviorChronicleDrop Drop new data
	QueueFullBehaviorChronicleDrop QueueFullBehaviorChronicle = "drop"
)

func (e QueueFullBehaviorChronicle) ToPointer() *QueueFullBehaviorChronicle {
	return &e
}

type PqControlsChronicle struct {
}

func (p PqControlsChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputChronicle struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeChronicle `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags           []string                       `json:"streamtags,omitempty"`
	APIVersion           *string                        `default:"v1alpha" json:"apiVersion"`
	AuthenticationMethod *AuthenticationMethodChronicle `default:"serviceAccount" json:"authenticationMethod"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingChronicle `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsChronicle  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Regional endpoint to send events to
	Region string `json:"region"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"90" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderChronicle `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeChronicle `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorChronicle `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	IngestionMethod    *string  `default:"ImportLogs" json:"ingestionMethod"`
	// User-configured environment namespace to identify the data domain the logs originated from. This namespace is used as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.
	Namespace *string `json:"namespace,omitempty"`
	// Default log type value to send to SecOps. Can be overwritten by event field __logType.
	LogType string `json:"logType"`
	// Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.
	LogTextField *string `json:"logTextField,omitempty"`
	// The Google Cloud Platform (GCP) project ID to send events to
	GcpProjectID string `json:"gcpProjectId"`
	// The Google Cloud Platform (GCP) instance to send events to. This is the Chronicle customer uuid.
	GcpInstance string `json:"gcpInstance"`
	// Custom labels to be added to every event
	CustomLabels []CustomLabelChronicle `json:"customLabels,omitempty"`
	Description  *string                `json:"description,omitempty"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	ServiceAccountCredentialsSecret *string `json:"serviceAccountCredentialsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeChronicle `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionChronicle `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorChronicle `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsChronicle        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any              `additionalProperties:"true" json:"-"`
}

func (o OutputChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "region", "logType", "gcpProjectId", "gcpInstance"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputChronicle) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputChronicle) GetType() TypeChronicle {
	if o == nil {
		return TypeChronicle("")
	}
	return o.Type
}

func (o *OutputChronicle) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputChronicle) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputChronicle) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputChronicle) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputChronicle) GetAPIVersion() *string {
	if o == nil {
		return nil
	}
	return o.APIVersion
}

func (o *OutputChronicle) GetAuthenticationMethod() *AuthenticationMethodChronicle {
	if o == nil {
		return nil
	}
	return o.AuthenticationMethod
}

func (o *OutputChronicle) GetResponseRetrySettings() []ResponseRetrySettingChronicle {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputChronicle) GetTimeoutRetrySettings() *TimeoutRetrySettingsChronicle {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputChronicle) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputChronicle) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputChronicle) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputChronicle) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputChronicle) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputChronicle) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputChronicle) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputChronicle) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputChronicle) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputChronicle) GetExtraHTTPHeaders() []ExtraHTTPHeaderChronicle {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputChronicle) GetFailedRequestLoggingMode() *FailedRequestLoggingModeChronicle {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputChronicle) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputChronicle) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputChronicle) GetOnBackpressure() *BackpressureBehaviorChronicle {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputChronicle) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputChronicle) GetIngestionMethod() *string {
	if o == nil {
		return nil
	}
	return o.IngestionMethod
}

func (o *OutputChronicle) GetNamespace() *string {
	if o == nil {
		return nil
	}
	return o.Namespace
}

func (o *OutputChronicle) GetLogType() string {
	if o == nil {
		return ""
	}
	return o.LogType
}

func (o *OutputChronicle) GetLogTextField() *string {
	if o == nil {
		return nil
	}
	return o.LogTextField
}

func (o *OutputChronicle) GetGcpProjectID() string {
	if o == nil {
		return ""
	}
	return o.GcpProjectID
}

func (o *OutputChronicle) GetGcpInstance() string {
	if o == nil {
		return ""
	}
	return o.GcpInstance
}

func (o *OutputChronicle) GetCustomLabels() []CustomLabelChronicle {
	if o == nil {
		return nil
	}
	return o.CustomLabels
}

func (o *OutputChronicle) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputChronicle) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputChronicle) GetServiceAccountCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentialsSecret
}

func (o *OutputChronicle) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputChronicle) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputChronicle) GetPqMode() *ModeChronicle {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputChronicle) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputChronicle) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputChronicle) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputChronicle) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputChronicle) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputChronicle) GetPqCompress() *CompressionChronicle {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputChronicle) GetPqOnBackpressure() *QueueFullBehaviorChronicle {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputChronicle) GetPqControls() *PqControlsChronicle {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputChronicle) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeSentinelOneAiSiem string

const (
	TypeSentinelOneAiSiemSentinelOneAiSiem TypeSentinelOneAiSiem = "sentinel_one_ai_siem"
)

func (e TypeSentinelOneAiSiem) ToPointer() *TypeSentinelOneAiSiem {
	return &e
}
func (e *TypeSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sentinel_one_ai_siem":
		*e = TypeSentinelOneAiSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSentinelOneAiSiem: %v", v)
	}
}

// RegionSentinelOneAiSiem - The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.
type RegionSentinelOneAiSiem string

const (
	RegionSentinelOneAiSiemUs     RegionSentinelOneAiSiem = "US"
	RegionSentinelOneAiSiemCa     RegionSentinelOneAiSiem = "CA"
	RegionSentinelOneAiSiemEmea   RegionSentinelOneAiSiem = "EMEA"
	RegionSentinelOneAiSiemAp     RegionSentinelOneAiSiem = "AP"
	RegionSentinelOneAiSiemAps    RegionSentinelOneAiSiem = "APS"
	RegionSentinelOneAiSiemAu     RegionSentinelOneAiSiem = "AU"
	RegionSentinelOneAiSiemCustom RegionSentinelOneAiSiem = "Custom"
)

func (e RegionSentinelOneAiSiem) ToPointer() *RegionSentinelOneAiSiem {
	return &e
}

// AISIEMEndpointPath - Endpoint to send events to. Use /services/collector/event for structured JSON payloads with standard HEC top-level fields. Use /services/collector/raw for unstructured log lines (plain text).
type AISIEMEndpointPath string

const (
	AISIEMEndpointPathRootServicesCollectorEvent AISIEMEndpointPath = "/services/collector/event"
	AISIEMEndpointPathRootServicesCollectorRaw   AISIEMEndpointPath = "/services/collector/raw"
)

func (e AISIEMEndpointPath) ToPointer() *AISIEMEndpointPath {
	return &e
}

type ExtraHTTPHeaderSentinelOneAiSiem struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderSentinelOneAiSiem) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderSentinelOneAiSiem) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeSentinelOneAiSiem - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSentinelOneAiSiem string

const (
	// FailedRequestLoggingModeSentinelOneAiSiemPayload Payload
	FailedRequestLoggingModeSentinelOneAiSiemPayload FailedRequestLoggingModeSentinelOneAiSiem = "payload"
	// FailedRequestLoggingModeSentinelOneAiSiemPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeSentinelOneAiSiemPayloadAndHeaders FailedRequestLoggingModeSentinelOneAiSiem = "payloadAndHeaders"
	// FailedRequestLoggingModeSentinelOneAiSiemNone None
	FailedRequestLoggingModeSentinelOneAiSiemNone FailedRequestLoggingModeSentinelOneAiSiem = "none"
)

func (e FailedRequestLoggingModeSentinelOneAiSiem) ToPointer() *FailedRequestLoggingModeSentinelOneAiSiem {
	return &e
}

// AuthenticationMethodSentinelOneAiSiem - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSentinelOneAiSiem string

const (
	AuthenticationMethodSentinelOneAiSiemManual AuthenticationMethodSentinelOneAiSiem = "manual"
	AuthenticationMethodSentinelOneAiSiemSecret AuthenticationMethodSentinelOneAiSiem = "secret"
)

func (e AuthenticationMethodSentinelOneAiSiem) ToPointer() *AuthenticationMethodSentinelOneAiSiem {
	return &e
}

type ResponseRetrySettingSentinelOneAiSiem struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingSentinelOneAiSiem) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingSentinelOneAiSiem) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingSentinelOneAiSiem) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingSentinelOneAiSiem) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsSentinelOneAiSiem struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsSentinelOneAiSiem) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsSentinelOneAiSiem) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsSentinelOneAiSiem) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsSentinelOneAiSiem) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorSentinelOneAiSiem - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSentinelOneAiSiem string

const (
	// BackpressureBehaviorSentinelOneAiSiemBlock Block
	BackpressureBehaviorSentinelOneAiSiemBlock BackpressureBehaviorSentinelOneAiSiem = "block"
	// BackpressureBehaviorSentinelOneAiSiemDrop Drop
	BackpressureBehaviorSentinelOneAiSiemDrop BackpressureBehaviorSentinelOneAiSiem = "drop"
	// BackpressureBehaviorSentinelOneAiSiemQueue Persistent Queue
	BackpressureBehaviorSentinelOneAiSiemQueue BackpressureBehaviorSentinelOneAiSiem = "queue"
)

func (e BackpressureBehaviorSentinelOneAiSiem) ToPointer() *BackpressureBehaviorSentinelOneAiSiem {
	return &e
}

// ModeSentinelOneAiSiem - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSentinelOneAiSiem string

const (
	// ModeSentinelOneAiSiemError Error
	ModeSentinelOneAiSiemError ModeSentinelOneAiSiem = "error"
	// ModeSentinelOneAiSiemAlways Backpressure
	ModeSentinelOneAiSiemAlways ModeSentinelOneAiSiem = "always"
	// ModeSentinelOneAiSiemBackpressure Always On
	ModeSentinelOneAiSiemBackpressure ModeSentinelOneAiSiem = "backpressure"
)

func (e ModeSentinelOneAiSiem) ToPointer() *ModeSentinelOneAiSiem {
	return &e
}

// CompressionSentinelOneAiSiem - Codec to use to compress the persisted data
type CompressionSentinelOneAiSiem string

const (
	// CompressionSentinelOneAiSiemNone None
	CompressionSentinelOneAiSiemNone CompressionSentinelOneAiSiem = "none"
	// CompressionSentinelOneAiSiemGzip Gzip
	CompressionSentinelOneAiSiemGzip CompressionSentinelOneAiSiem = "gzip"
)

func (e CompressionSentinelOneAiSiem) ToPointer() *CompressionSentinelOneAiSiem {
	return &e
}

// QueueFullBehaviorSentinelOneAiSiem - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSentinelOneAiSiem string

const (
	// QueueFullBehaviorSentinelOneAiSiemBlock Block
	QueueFullBehaviorSentinelOneAiSiemBlock QueueFullBehaviorSentinelOneAiSiem = "block"
	// QueueFullBehaviorSentinelOneAiSiemDrop Drop new data
	QueueFullBehaviorSentinelOneAiSiemDrop QueueFullBehaviorSentinelOneAiSiem = "drop"
)

func (e QueueFullBehaviorSentinelOneAiSiem) ToPointer() *QueueFullBehaviorSentinelOneAiSiem {
	return &e
}

type PqControlsSentinelOneAiSiem struct {
}

func (p PqControlsSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSentinelOneAiSiem struct {
	// Unique ID for this output
	ID   *string               `json:"id,omitempty"`
	Type TypeSentinelOneAiSiem `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.
	Region *RegionSentinelOneAiSiem `default:"US" json:"region"`
	// Endpoint to send events to. Use /services/collector/event for structured JSON payloads with standard HEC top-level fields. Use /services/collector/raw for unstructured log lines (plain text).
	Endpoint *AISIEMEndpointPath `default:"/services/collector/event" json:"endpoint"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"5120" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"5" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderSentinelOneAiSiem `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSentinelOneAiSiem `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodSentinelOneAiSiem `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSentinelOneAiSiem `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSentinelOneAiSiem  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSentinelOneAiSiem `default:"block" json:"onBackpressure"`
	Description    *string                                `json:"description,omitempty"`
	// In the SentinelOne Console select Policy & Settings then select the Singularity AI SIEM section, API Keys will be at the bottom. Under Log Access Keys select a Write token and copy it here
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Base URL of the endpoint used to send events to, such as https://<Your-S1-Tenant>.sentinelone.net. Must begin with http:// or https://, can include a port number, and no trailing slashes. Matches pattern: ^https?://[a-zA-Z0-9.-]+(:[0-9]+)?$.
	BaseURL *string `default:"https://<Your-S1-Tenant>.sentinelone.net" json:"baseUrl"`
	// Define serverHost for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myServer').
	HostExpression *string `default:"__e.host || C.os.hostname()" json:"hostExpression"`
	// Define logFile for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myLogFile.txt').
	SourceExpression *string `default:"__e.source || (__e.__criblMetrics ? 'metrics' : 'cribl')" json:"sourceExpression"`
	// Define the parser for events using a JavaScript expression. This value helps parse data into AI SIEM. You must enclose text constants in quotes (such as, 'dottedJson'). For custom parsers, substitute 'dottedJson' with your parser's name.
	SourceTypeExpression *string `default:"__e.sourcetype || 'dottedJson'" json:"sourceTypeExpression"`
	// Define the dataSource.category for events using a JavaScript expression. This value helps categorize data and helps enable extra features in SentinelOne AI SIEM. You must enclose text constants in quotes. The default value is 'security'.
	DataSourceCategoryExpression *string `default:"'security'" json:"dataSourceCategoryExpression"`
	// Define the dataSource.name for events using a JavaScript expression. This value should reflect the type of data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'networkActivity' or 'authLogs').
	DataSourceNameExpression *string `default:"__e.__dataSourceName || 'cribl'" json:"dataSourceNameExpression"`
	// Define the dataSource.vendor for events using a JavaScript expression. This value should reflect the vendor of the data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'Cisco' or 'Microsoft').
	DataSourceVendorExpression *string `default:"__e.__dataSourceVendor || 'cribl'" json:"dataSourceVendorExpression"`
	// Optionally, define the event.type for events using a JavaScript expression. This value acts as a label, grouping events into meaningful categories. You must enclose text constants in quotes (such as, 'Process Creation' or 'Network Connection').
	EventTypeExpression *string `default:"" json:"eventTypeExpression"`
	// Define the serverHost for events using a JavaScript expression. This value will be passed to AI SIEM. You must enclose text constants in quotes (such as, 'myServerName').
	Host *string `default:"C.os.hostname()" json:"host"`
	// Specify the logFile value to pass as a parameter to SentinelOne AI SIEM. Don't quote this value. The default is cribl.
	Source *string `default:"cribl" json:"source"`
	// Specify the sourcetype parameter for SentinelOne AI SIEM, which determines the parser. Don't quote this value. For custom parsers, substitute hecRawParser with your parser's name. The default is hecRawParser.
	SourceType *string `default:"hecRawParser" json:"sourceType"`
	// Specify the dataSource.category value to pass as a parameter to SentinelOne AI SIEM. This value helps categorize data and enables additional features. Don't quote this value. The default is security.
	DataSourceCategory *string `default:"security" json:"dataSourceCategory"`
	// Specify the dataSource.name value to pass as a parameter to AI SIEM. This value should reflect the type of data being inserted. Don't quote this value. The default is cribl.
	DataSourceName *string `default:"cribl" json:"dataSourceName"`
	// Specify the dataSource.vendorvalue to pass as a parameter to AI SIEM. This value should reflect the vendor of the data being inserted. Don't quote this value. The default is cribl.
	DataSourceVendor *string `default:"cribl" json:"dataSourceVendor"`
	// Specify the event.type value to pass as an optional parameter to AI SIEM. This value acts as a label, grouping events into meaningful categories like Process Creation, File Modification, or Network Connection. Don't quote this value. By default, this field is empty.
	EventType *string `default:"" json:"eventType"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSentinelOneAiSiem `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSentinelOneAiSiem `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorSentinelOneAiSiem `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsSentinelOneAiSiem        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                      `additionalProperties:"true" json:"-"`
}

func (o OutputSentinelOneAiSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSentinelOneAiSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSentinelOneAiSiem) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSentinelOneAiSiem) GetType() TypeSentinelOneAiSiem {
	if o == nil {
		return TypeSentinelOneAiSiem("")
	}
	return o.Type
}

func (o *OutputSentinelOneAiSiem) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSentinelOneAiSiem) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSentinelOneAiSiem) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSentinelOneAiSiem) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSentinelOneAiSiem) GetRegion() *RegionSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputSentinelOneAiSiem) GetEndpoint() *AISIEMEndpointPath {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSentinelOneAiSiem) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSentinelOneAiSiem) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSentinelOneAiSiem) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSentinelOneAiSiem) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSentinelOneAiSiem) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSentinelOneAiSiem) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSentinelOneAiSiem) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSentinelOneAiSiem) GetExtraHTTPHeaders() []ExtraHTTPHeaderSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSentinelOneAiSiem) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSentinelOneAiSiem) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSentinelOneAiSiem) GetAuthType() *AuthenticationMethodSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSentinelOneAiSiem) GetResponseRetrySettings() []ResponseRetrySettingSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSentinelOneAiSiem) GetTimeoutRetrySettings() *TimeoutRetrySettingsSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSentinelOneAiSiem) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSentinelOneAiSiem) GetOnBackpressure() *BackpressureBehaviorSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSentinelOneAiSiem) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSentinelOneAiSiem) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputSentinelOneAiSiem) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSentinelOneAiSiem) GetBaseURL() *string {
	if o == nil {
		return nil
	}
	return o.BaseURL
}

func (o *OutputSentinelOneAiSiem) GetHostExpression() *string {
	if o == nil {
		return nil
	}
	return o.HostExpression
}

func (o *OutputSentinelOneAiSiem) GetSourceExpression() *string {
	if o == nil {
		return nil
	}
	return o.SourceExpression
}

func (o *OutputSentinelOneAiSiem) GetSourceTypeExpression() *string {
	if o == nil {
		return nil
	}
	return o.SourceTypeExpression
}

func (o *OutputSentinelOneAiSiem) GetDataSourceCategoryExpression() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceCategoryExpression
}

func (o *OutputSentinelOneAiSiem) GetDataSourceNameExpression() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceNameExpression
}

func (o *OutputSentinelOneAiSiem) GetDataSourceVendorExpression() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceVendorExpression
}

func (o *OutputSentinelOneAiSiem) GetEventTypeExpression() *string {
	if o == nil {
		return nil
	}
	return o.EventTypeExpression
}

func (o *OutputSentinelOneAiSiem) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputSentinelOneAiSiem) GetSource() *string {
	if o == nil {
		return nil
	}
	return o.Source
}

func (o *OutputSentinelOneAiSiem) GetSourceType() *string {
	if o == nil {
		return nil
	}
	return o.SourceType
}

func (o *OutputSentinelOneAiSiem) GetDataSourceCategory() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceCategory
}

func (o *OutputSentinelOneAiSiem) GetDataSourceName() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceName
}

func (o *OutputSentinelOneAiSiem) GetDataSourceVendor() *string {
	if o == nil {
		return nil
	}
	return o.DataSourceVendor
}

func (o *OutputSentinelOneAiSiem) GetEventType() *string {
	if o == nil {
		return nil
	}
	return o.EventType
}

func (o *OutputSentinelOneAiSiem) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSentinelOneAiSiem) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSentinelOneAiSiem) GetPqMode() *ModeSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSentinelOneAiSiem) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSentinelOneAiSiem) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSentinelOneAiSiem) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSentinelOneAiSiem) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSentinelOneAiSiem) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSentinelOneAiSiem) GetPqCompress() *CompressionSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSentinelOneAiSiem) GetPqOnBackpressure() *QueueFullBehaviorSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSentinelOneAiSiem) GetPqControls() *PqControlsSentinelOneAiSiem {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSentinelOneAiSiem) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeDynatraceOtlp string

const (
	TypeDynatraceOtlpDynatraceOtlp TypeDynatraceOtlp = "dynatrace_otlp"
)

func (e TypeDynatraceOtlp) ToPointer() *TypeDynatraceOtlp {
	return &e
}
func (e *TypeDynatraceOtlp) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dynatrace_otlp":
		*e = TypeDynatraceOtlp(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDynatraceOtlp: %v", v)
	}
}

// ProtocolDynatraceOtlp - Select a transport option for Dynatrace
type ProtocolDynatraceOtlp string

const (
	// ProtocolDynatraceOtlpHTTP HTTP
	ProtocolDynatraceOtlpHTTP ProtocolDynatraceOtlp = "http"
)

func (e ProtocolDynatraceOtlp) ToPointer() *ProtocolDynatraceOtlp {
	return &e
}

// OTLPVersionDynatraceOTLP - The version of OTLP Protobuf definitions to use when structuring data to send
type OTLPVersionDynatraceOTLP string

const (
	// OTLPVersionDynatraceOTLPOneDot3Dot1 1.3.1
	OTLPVersionDynatraceOTLPOneDot3Dot1 OTLPVersionDynatraceOTLP = "1.3.1"
)

func (e OTLPVersionDynatraceOTLP) ToPointer() *OTLPVersionDynatraceOTLP {
	return &e
}

// CompressCompressionDynatraceOtlp - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type CompressCompressionDynatraceOtlp string

const (
	// CompressCompressionDynatraceOtlpNone None
	CompressCompressionDynatraceOtlpNone CompressCompressionDynatraceOtlp = "none"
	// CompressCompressionDynatraceOtlpDeflate Deflate
	CompressCompressionDynatraceOtlpDeflate CompressCompressionDynatraceOtlp = "deflate"
	// CompressCompressionDynatraceOtlpGzip Gzip
	CompressCompressionDynatraceOtlpGzip CompressCompressionDynatraceOtlp = "gzip"
)

func (e CompressCompressionDynatraceOtlp) ToPointer() *CompressCompressionDynatraceOtlp {
	return &e
}

// HTTPCompressCompressionDynatraceOtlp - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompressionDynatraceOtlp string

const (
	// HTTPCompressCompressionDynatraceOtlpNone None
	HTTPCompressCompressionDynatraceOtlpNone HTTPCompressCompressionDynatraceOtlp = "none"
	// HTTPCompressCompressionDynatraceOtlpGzip Gzip
	HTTPCompressCompressionDynatraceOtlpGzip HTTPCompressCompressionDynatraceOtlp = "gzip"
)

func (e HTTPCompressCompressionDynatraceOtlp) ToPointer() *HTTPCompressCompressionDynatraceOtlp {
	return &e
}

type MetadatumDynatraceOtlp struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (m MetadatumDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(m, "", false)
}

func (m *MetadatumDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &m, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (m *MetadatumDynatraceOtlp) GetKey() *string {
	if m == nil {
		return nil
	}
	return m.Key
}

func (m *MetadatumDynatraceOtlp) GetValue() string {
	if m == nil {
		return ""
	}
	return m.Value
}

// FailedRequestLoggingModeDynatraceOtlp - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDynatraceOtlp string

const (
	// FailedRequestLoggingModeDynatraceOtlpPayload Payload
	FailedRequestLoggingModeDynatraceOtlpPayload FailedRequestLoggingModeDynatraceOtlp = "payload"
	// FailedRequestLoggingModeDynatraceOtlpPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeDynatraceOtlpPayloadAndHeaders FailedRequestLoggingModeDynatraceOtlp = "payloadAndHeaders"
	// FailedRequestLoggingModeDynatraceOtlpNone None
	FailedRequestLoggingModeDynatraceOtlpNone FailedRequestLoggingModeDynatraceOtlp = "none"
)

func (e FailedRequestLoggingModeDynatraceOtlp) ToPointer() *FailedRequestLoggingModeDynatraceOtlp {
	return &e
}

// EndpointType - Select the type of Dynatrace endpoint configured
type EndpointType string

const (
	// EndpointTypeSaas SaaS
	EndpointTypeSaas EndpointType = "saas"
	// EndpointTypeAg ActiveGate
	EndpointTypeAg EndpointType = "ag"
)

func (e EndpointType) ToPointer() *EndpointType {
	return &e
}

// BackpressureBehaviorDynatraceOtlp - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDynatraceOtlp string

const (
	// BackpressureBehaviorDynatraceOtlpBlock Block
	BackpressureBehaviorDynatraceOtlpBlock BackpressureBehaviorDynatraceOtlp = "block"
	// BackpressureBehaviorDynatraceOtlpDrop Drop
	BackpressureBehaviorDynatraceOtlpDrop BackpressureBehaviorDynatraceOtlp = "drop"
	// BackpressureBehaviorDynatraceOtlpQueue Persistent Queue
	BackpressureBehaviorDynatraceOtlpQueue BackpressureBehaviorDynatraceOtlp = "queue"
)

func (e BackpressureBehaviorDynatraceOtlp) ToPointer() *BackpressureBehaviorDynatraceOtlp {
	return &e
}

type ExtraHTTPHeaderDynatraceOtlp struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderDynatraceOtlp) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderDynatraceOtlp) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

type ResponseRetrySettingDynatraceOtlp struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingDynatraceOtlp) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingDynatraceOtlp) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingDynatraceOtlp) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingDynatraceOtlp) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsDynatraceOtlp struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsDynatraceOtlp) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsDynatraceOtlp) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsDynatraceOtlp) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsDynatraceOtlp) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// ModeDynatraceOtlp - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeDynatraceOtlp string

const (
	// ModeDynatraceOtlpError Error
	ModeDynatraceOtlpError ModeDynatraceOtlp = "error"
	// ModeDynatraceOtlpAlways Backpressure
	ModeDynatraceOtlpAlways ModeDynatraceOtlp = "always"
	// ModeDynatraceOtlpBackpressure Always On
	ModeDynatraceOtlpBackpressure ModeDynatraceOtlp = "backpressure"
)

func (e ModeDynatraceOtlp) ToPointer() *ModeDynatraceOtlp {
	return &e
}

// PqCompressCompressionDynatraceOtlp - Codec to use to compress the persisted data
type PqCompressCompressionDynatraceOtlp string

const (
	// PqCompressCompressionDynatraceOtlpNone None
	PqCompressCompressionDynatraceOtlpNone PqCompressCompressionDynatraceOtlp = "none"
	// PqCompressCompressionDynatraceOtlpGzip Gzip
	PqCompressCompressionDynatraceOtlpGzip PqCompressCompressionDynatraceOtlp = "gzip"
)

func (e PqCompressCompressionDynatraceOtlp) ToPointer() *PqCompressCompressionDynatraceOtlp {
	return &e
}

// QueueFullBehaviorDynatraceOtlp - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDynatraceOtlp string

const (
	// QueueFullBehaviorDynatraceOtlpBlock Block
	QueueFullBehaviorDynatraceOtlpBlock QueueFullBehaviorDynatraceOtlp = "block"
	// QueueFullBehaviorDynatraceOtlpDrop Drop new data
	QueueFullBehaviorDynatraceOtlpDrop QueueFullBehaviorDynatraceOtlp = "drop"
)

func (e QueueFullBehaviorDynatraceOtlp) ToPointer() *QueueFullBehaviorDynatraceOtlp {
	return &e
}

type PqControlsDynatraceOtlp struct {
}

func (p PqControlsDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputDynatraceOtlp struct {
	// Unique ID for this output
	ID   *string           `json:"id,omitempty"`
	Type TypeDynatraceOtlp `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select a transport option for Dynatrace
	Protocol *ProtocolDynatraceOtlp `default:"http" json:"protocol"`
	// The endpoint where Dynatrace events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)
	Endpoint *string `default:"https://{your-environment-id}.live.dynatrace.com/api/v2/otlp" json:"endpoint"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *OTLPVersionDynatraceOTLP `default:"1.3.1" json:"otlpVersion"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *CompressCompressionDynatraceOtlp `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompressionDynatraceOtlp `default:"gzip" json:"httpCompress"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []MetadatumDynatraceOtlp `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (in KB) of the request body. The maximum payload size is 4 MB. If this limit is exceeded, the entire OTLP message is dropped
	MaxPayloadSizeKB *float64 `default:"2048" json:"maxPayloadSizeKB"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDynatraceOtlp `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Select the type of Dynatrace endpoint configured
	EndpointType *EndpointType `default:"saas" json:"endpointType"`
	// Select or create a stored text secret
	TokenSecret   string  `json:"tokenSecret"`
	AuthTokenName *string `default:"Authorization" json:"authTokenName"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDynatraceOtlp `default:"block" json:"onBackpressure"`
	Description    *string                            `json:"description,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderDynatraceOtlp `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingDynatraceOtlp `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDynatraceOtlp  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeDynatraceOtlp `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionDynatraceOtlp `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorDynatraceOtlp `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsDynatraceOtlp        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                  `additionalProperties:"true" json:"-"`
}

func (o OutputDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "tokenSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDynatraceOtlp) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDynatraceOtlp) GetType() TypeDynatraceOtlp {
	if o == nil {
		return TypeDynatraceOtlp("")
	}
	return o.Type
}

func (o *OutputDynatraceOtlp) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDynatraceOtlp) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDynatraceOtlp) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDynatraceOtlp) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDynatraceOtlp) GetProtocol() *ProtocolDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputDynatraceOtlp) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDynatraceOtlp) GetOtlpVersion() *OTLPVersionDynatraceOTLP {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputDynatraceOtlp) GetCompress() *CompressCompressionDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDynatraceOtlp) GetHTTPCompress() *HTTPCompressCompressionDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputDynatraceOtlp) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputDynatraceOtlp) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputDynatraceOtlp) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputDynatraceOtlp) GetMetadata() []MetadatumDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputDynatraceOtlp) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDynatraceOtlp) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDynatraceOtlp) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDynatraceOtlp) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDynatraceOtlp) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDynatraceOtlp) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputDynatraceOtlp) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputDynatraceOtlp) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputDynatraceOtlp) GetEndpointType() *EndpointType {
	if o == nil {
		return nil
	}
	return o.EndpointType
}

func (o *OutputDynatraceOtlp) GetTokenSecret() string {
	if o == nil {
		return ""
	}
	return o.TokenSecret
}

func (o *OutputDynatraceOtlp) GetAuthTokenName() *string {
	if o == nil {
		return nil
	}
	return o.AuthTokenName
}

func (o *OutputDynatraceOtlp) GetOnBackpressure() *BackpressureBehaviorDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDynatraceOtlp) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDynatraceOtlp) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDynatraceOtlp) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDynatraceOtlp) GetExtraHTTPHeaders() []ExtraHTTPHeaderDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDynatraceOtlp) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDynatraceOtlp) GetResponseRetrySettings() []ResponseRetrySettingDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDynatraceOtlp) GetTimeoutRetrySettings() *TimeoutRetrySettingsDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDynatraceOtlp) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDynatraceOtlp) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputDynatraceOtlp) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputDynatraceOtlp) GetPqMode() *ModeDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDynatraceOtlp) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputDynatraceOtlp) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputDynatraceOtlp) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDynatraceOtlp) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDynatraceOtlp) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDynatraceOtlp) GetPqCompress() *PqCompressCompressionDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDynatraceOtlp) GetPqOnBackpressure() *QueueFullBehaviorDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDynatraceOtlp) GetPqControls() *PqControlsDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDynatraceOtlp) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeDynatraceHTTP string

const (
	TypeDynatraceHTTPDynatraceHTTP TypeDynatraceHTTP = "dynatrace_http"
)

func (e TypeDynatraceHTTP) ToPointer() *TypeDynatraceHTTP {
	return &e
}
func (e *TypeDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dynatrace_http":
		*e = TypeDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDynatraceHTTP: %v", v)
	}
}

// MethodDynatraceHTTP - The method to use when sending events
type MethodDynatraceHTTP string

const (
	MethodDynatraceHTTPPost  MethodDynatraceHTTP = "POST"
	MethodDynatraceHTTPPut   MethodDynatraceHTTP = "PUT"
	MethodDynatraceHTTPPatch MethodDynatraceHTTP = "PATCH"
)

func (e MethodDynatraceHTTP) ToPointer() *MethodDynatraceHTTP {
	return &e
}

type ExtraHTTPHeaderDynatraceHTTP struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderDynatraceHTTP) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderDynatraceHTTP) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeDynatraceHTTP - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDynatraceHTTP string

const (
	// FailedRequestLoggingModeDynatraceHTTPPayload Payload
	FailedRequestLoggingModeDynatraceHTTPPayload FailedRequestLoggingModeDynatraceHTTP = "payload"
	// FailedRequestLoggingModeDynatraceHTTPPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeDynatraceHTTPPayloadAndHeaders FailedRequestLoggingModeDynatraceHTTP = "payloadAndHeaders"
	// FailedRequestLoggingModeDynatraceHTTPNone None
	FailedRequestLoggingModeDynatraceHTTPNone FailedRequestLoggingModeDynatraceHTTP = "none"
)

func (e FailedRequestLoggingModeDynatraceHTTP) ToPointer() *FailedRequestLoggingModeDynatraceHTTP {
	return &e
}

type ResponseRetrySettingDynatraceHTTP struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingDynatraceHTTP) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingDynatraceHTTP) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingDynatraceHTTP) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingDynatraceHTTP) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsDynatraceHTTP struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsDynatraceHTTP) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsDynatraceHTTP) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsDynatraceHTTP) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsDynatraceHTTP) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorDynatraceHTTP - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDynatraceHTTP string

const (
	// BackpressureBehaviorDynatraceHTTPBlock Block
	BackpressureBehaviorDynatraceHTTPBlock BackpressureBehaviorDynatraceHTTP = "block"
	// BackpressureBehaviorDynatraceHTTPDrop Drop
	BackpressureBehaviorDynatraceHTTPDrop BackpressureBehaviorDynatraceHTTP = "drop"
	// BackpressureBehaviorDynatraceHTTPQueue Persistent Queue
	BackpressureBehaviorDynatraceHTTPQueue BackpressureBehaviorDynatraceHTTP = "queue"
)

func (e BackpressureBehaviorDynatraceHTTP) ToPointer() *BackpressureBehaviorDynatraceHTTP {
	return &e
}

type AuthenticationTypeDynatraceHTTP string

const (
	// AuthenticationTypeDynatraceHTTPToken Auth token
	AuthenticationTypeDynatraceHTTPToken AuthenticationTypeDynatraceHTTP = "token"
	// AuthenticationTypeDynatraceHTTPTextSecret Token (text secret)
	AuthenticationTypeDynatraceHTTPTextSecret AuthenticationTypeDynatraceHTTP = "textSecret"
)

func (e AuthenticationTypeDynatraceHTTP) ToPointer() *AuthenticationTypeDynatraceHTTP {
	return &e
}

// FormatDynatraceHTTP - How to format events before sending. Defaults to JSON. Plaintext is not currently supported.
type FormatDynatraceHTTP string

const (
	// FormatDynatraceHTTPJSONArray JSON
	FormatDynatraceHTTPJSONArray FormatDynatraceHTTP = "json_array"
	// FormatDynatraceHTTPPlaintext Plaintext
	FormatDynatraceHTTPPlaintext FormatDynatraceHTTP = "plaintext"
)

func (e FormatDynatraceHTTP) ToPointer() *FormatDynatraceHTTP {
	return &e
}

type Endpoint string

const (
	// EndpointCloud Cloud
	EndpointCloud Endpoint = "cloud"
	// EndpointActiveGate ActiveGate
	EndpointActiveGate Endpoint = "activeGate"
	// EndpointManual Manual
	EndpointManual Endpoint = "manual"
)

func (e Endpoint) ToPointer() *Endpoint {
	return &e
}

type TelemetryType string

const (
	// TelemetryTypeLogs Logs
	TelemetryTypeLogs TelemetryType = "logs"
	// TelemetryTypeMetrics Metrics
	TelemetryTypeMetrics TelemetryType = "metrics"
)

func (e TelemetryType) ToPointer() *TelemetryType {
	return &e
}

// ModeDynatraceHTTP - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeDynatraceHTTP string

const (
	// ModeDynatraceHTTPError Error
	ModeDynatraceHTTPError ModeDynatraceHTTP = "error"
	// ModeDynatraceHTTPAlways Backpressure
	ModeDynatraceHTTPAlways ModeDynatraceHTTP = "always"
	// ModeDynatraceHTTPBackpressure Always On
	ModeDynatraceHTTPBackpressure ModeDynatraceHTTP = "backpressure"
)

func (e ModeDynatraceHTTP) ToPointer() *ModeDynatraceHTTP {
	return &e
}

// CompressionDynatraceHTTP - Codec to use to compress the persisted data
type CompressionDynatraceHTTP string

const (
	// CompressionDynatraceHTTPNone None
	CompressionDynatraceHTTPNone CompressionDynatraceHTTP = "none"
	// CompressionDynatraceHTTPGzip Gzip
	CompressionDynatraceHTTPGzip CompressionDynatraceHTTP = "gzip"
)

func (e CompressionDynatraceHTTP) ToPointer() *CompressionDynatraceHTTP {
	return &e
}

// QueueFullBehaviorDynatraceHTTP - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDynatraceHTTP string

const (
	// QueueFullBehaviorDynatraceHTTPBlock Block
	QueueFullBehaviorDynatraceHTTPBlock QueueFullBehaviorDynatraceHTTP = "block"
	// QueueFullBehaviorDynatraceHTTPDrop Drop new data
	QueueFullBehaviorDynatraceHTTPDrop QueueFullBehaviorDynatraceHTTP = "drop"
)

func (e QueueFullBehaviorDynatraceHTTP) ToPointer() *QueueFullBehaviorDynatraceHTTP {
	return &e
}

type PqControlsDynatraceHTTP struct {
}

func (p PqControlsDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputDynatraceHTTP struct {
	// Unique ID for this output
	ID   *string           `json:"id,omitempty"`
	Type TypeDynatraceHTTP `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The method to use when sending events
	Method *MethodDynatraceHTTP `default:"POST" json:"method"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
	ExtraHTTPHeaders []ExtraHTTPHeaderDynatraceHTTP `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDynatraceHTTP `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingDynatraceHTTP `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDynatraceHTTP  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDynatraceHTTP `default:"block" json:"onBackpressure"`
	AuthType       *AuthenticationTypeDynatraceHTTP   `default:"token" json:"authType"`
	// How to format events before sending. Defaults to JSON. Plaintext is not currently supported.
	Format        *FormatDynatraceHTTP `default:"json_array" json:"format"`
	Endpoint      *Endpoint            `default:"cloud" json:"endpoint"`
	TelemetryType *TelemetryType       `default:"logs" json:"telemetryType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeDynatraceHTTP `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionDynatraceHTTP `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorDynatraceHTTP `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsDynatraceHTTP        `json:"pqControls,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// ID of the environment to send to
	EnvironmentID *string `json:"environmentId,omitempty"`
	// ActiveGate domain with Log analytics collector module enabled. For example https://{activeGate-domain}:9999/e/{environment-id}/api/v2/logs/ingest.
	ActiveGateDomain *string `json:"activeGateDomain,omitempty"`
	// URL to send events to. Can be overwritten by an event's __url field.
	URL                  *string        `json:"url,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDynatraceHTTP) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDynatraceHTTP) GetType() TypeDynatraceHTTP {
	if o == nil {
		return TypeDynatraceHTTP("")
	}
	return o.Type
}

func (o *OutputDynatraceHTTP) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDynatraceHTTP) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDynatraceHTTP) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDynatraceHTTP) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDynatraceHTTP) GetMethod() *MethodDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.Method
}

func (o *OutputDynatraceHTTP) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputDynatraceHTTP) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDynatraceHTTP) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDynatraceHTTP) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputDynatraceHTTP) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDynatraceHTTP) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDynatraceHTTP) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDynatraceHTTP) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDynatraceHTTP) GetExtraHTTPHeaders() []ExtraHTTPHeaderDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDynatraceHTTP) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDynatraceHTTP) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDynatraceHTTP) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDynatraceHTTP) GetResponseRetrySettings() []ResponseRetrySettingDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDynatraceHTTP) GetTimeoutRetrySettings() *TimeoutRetrySettingsDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDynatraceHTTP) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDynatraceHTTP) GetOnBackpressure() *BackpressureBehaviorDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDynatraceHTTP) GetAuthType() *AuthenticationTypeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputDynatraceHTTP) GetFormat() *FormatDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDynatraceHTTP) GetEndpoint() *Endpoint {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDynatraceHTTP) GetTelemetryType() *TelemetryType {
	if o == nil {
		return nil
	}
	return o.TelemetryType
}

func (o *OutputDynatraceHTTP) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputDynatraceHTTP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDynatraceHTTP) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputDynatraceHTTP) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputDynatraceHTTP) GetPqMode() *ModeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDynatraceHTTP) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputDynatraceHTTP) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputDynatraceHTTP) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDynatraceHTTP) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDynatraceHTTP) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDynatraceHTTP) GetPqCompress() *CompressionDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDynatraceHTTP) GetPqOnBackpressure() *QueueFullBehaviorDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDynatraceHTTP) GetPqControls() *PqControlsDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDynatraceHTTP) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputDynatraceHTTP) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputDynatraceHTTP) GetEnvironmentID() *string {
	if o == nil {
		return nil
	}
	return o.EnvironmentID
}

func (o *OutputDynatraceHTTP) GetActiveGateDomain() *string {
	if o == nil {
		return nil
	}
	return o.ActiveGateDomain
}

func (o *OutputDynatraceHTTP) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputDynatraceHTTP) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeNetflow string

const (
	OutputTypeNetflowNetflow OutputTypeNetflow = "netflow"
)

func (e OutputTypeNetflow) ToPointer() *OutputTypeNetflow {
	return &e
}
func (e *OutputTypeNetflow) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "netflow":
		*e = OutputTypeNetflow(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeNetflow: %v", v)
	}
}

type HostNetflow struct {
	// Destination host
	Host string `json:"host"`
	// Destination port, default is 2055
	Port *float64 `default:"2055" json:"port"`
}

func (h HostNetflow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostNetflow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host"}); err != nil {
		return err
	}
	return nil
}

func (h *HostNetflow) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostNetflow) GetPort() *float64 {
	if h == nil {
		return nil
	}
	return h.Port
}

type OutputNetflow struct {
	// Unique ID for this output
	ID   *string           `json:"id,omitempty"`
	Type OutputTypeNetflow `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// One or more NetFlow destinations to forward events to
	Hosts []HostNetflow `json:"hosts"`
	// How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every datagram sent will incur a DNS lookup.
	DNSResolvePeriodSec  *float64       `default:"0" json:"dnsResolvePeriodSec"`
	Description          *string        `json:"description,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputNetflow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputNetflow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "hosts"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputNetflow) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputNetflow) GetType() OutputTypeNetflow {
	if o == nil {
		return OutputTypeNetflow("")
	}
	return o.Type
}

func (o *OutputNetflow) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputNetflow) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputNetflow) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputNetflow) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputNetflow) GetHosts() []HostNetflow {
	if o == nil {
		return []HostNetflow{}
	}
	return o.Hosts
}

func (o *OutputNetflow) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputNetflow) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputNetflow) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeXsiam string

const (
	TypeXsiamXsiam TypeXsiam = "xsiam"
)

func (e TypeXsiam) ToPointer() *TypeXsiam {
	return &e
}
func (e *TypeXsiam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "xsiam":
		*e = TypeXsiam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeXsiam: %v", v)
	}
}

type ExtraHTTPHeaderXsiam struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderXsiam) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderXsiam) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeXsiam - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeXsiam string

const (
	// FailedRequestLoggingModeXsiamPayload Payload
	FailedRequestLoggingModeXsiamPayload FailedRequestLoggingModeXsiam = "payload"
	// FailedRequestLoggingModeXsiamPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeXsiamPayloadAndHeaders FailedRequestLoggingModeXsiam = "payloadAndHeaders"
	// FailedRequestLoggingModeXsiamNone None
	FailedRequestLoggingModeXsiamNone FailedRequestLoggingModeXsiam = "none"
)

func (e FailedRequestLoggingModeXsiam) ToPointer() *FailedRequestLoggingModeXsiam {
	return &e
}

// AuthenticationMethodXsiam - Enter a token directly, or provide a secret referencing a token
type AuthenticationMethodXsiam string

const (
	AuthenticationMethodXsiamToken  AuthenticationMethodXsiam = "token"
	AuthenticationMethodXsiamSecret AuthenticationMethodXsiam = "secret"
)

func (e AuthenticationMethodXsiam) ToPointer() *AuthenticationMethodXsiam {
	return &e
}

type ResponseRetrySettingXsiam struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingXsiam) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingXsiam) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingXsiam) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingXsiam) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsXsiam struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsXsiam) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsXsiam) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsXsiam) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsXsiam) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorXsiam - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorXsiam string

const (
	// BackpressureBehaviorXsiamBlock Block
	BackpressureBehaviorXsiamBlock BackpressureBehaviorXsiam = "block"
	// BackpressureBehaviorXsiamDrop Drop
	BackpressureBehaviorXsiamDrop BackpressureBehaviorXsiam = "drop"
	// BackpressureBehaviorXsiamQueue Persistent Queue
	BackpressureBehaviorXsiamQueue BackpressureBehaviorXsiam = "queue"
)

func (e BackpressureBehaviorXsiam) ToPointer() *BackpressureBehaviorXsiam {
	return &e
}

type URLXsiam struct {
	URL any `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, []string{"url"}); err != nil {
		return err
	}
	return nil
}

func (u *URLXsiam) GetURL() any {
	if u == nil {
		return nil
	}
	return u.URL
}

func (u *URLXsiam) GetWeight() *float64 {
	if u == nil {
		return nil
	}
	return u.Weight
}

// ModeXsiam - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeXsiam string

const (
	// ModeXsiamError Error
	ModeXsiamError ModeXsiam = "error"
	// ModeXsiamAlways Backpressure
	ModeXsiamAlways ModeXsiam = "always"
	// ModeXsiamBackpressure Always On
	ModeXsiamBackpressure ModeXsiam = "backpressure"
)

func (e ModeXsiam) ToPointer() *ModeXsiam {
	return &e
}

// CompressionXsiam - Codec to use to compress the persisted data
type CompressionXsiam string

const (
	// CompressionXsiamNone None
	CompressionXsiamNone CompressionXsiam = "none"
	// CompressionXsiamGzip Gzip
	CompressionXsiamGzip CompressionXsiam = "gzip"
)

func (e CompressionXsiam) ToPointer() *CompressionXsiam {
	return &e
}

// QueueFullBehaviorXsiam - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorXsiam string

const (
	// QueueFullBehaviorXsiamBlock Block
	QueueFullBehaviorXsiamBlock QueueFullBehaviorXsiam = "block"
	// QueueFullBehaviorXsiamDrop Drop new data
	QueueFullBehaviorXsiamDrop QueueFullBehaviorXsiam = "drop"
)

func (e QueueFullBehaviorXsiam) ToPointer() *QueueFullBehaviorXsiam {
	return &e
}

type PqControlsXsiam struct {
}

func (p PqControlsXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputXsiam struct {
	// Unique ID for this output
	ID   *string   `json:"id,omitempty"`
	Type TypeXsiam `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"false" json:"loadBalanced"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"10000" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderXsiam `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeXsiam `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Enter a token directly, or provide a secret referencing a token
	AuthType *AuthenticationMethodXsiam `default:"token" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingXsiam `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsXsiam  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Maximum number of requests to limit to per second
	ThrottleRateReqPerSec *int64 `default:"400" json:"throttleRateReqPerSec"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorXsiam `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// XSIAM endpoint URL to send events to, such as https://api-{tenant external URL}/logs/v1/event
	URL *string `default:"http://localhost:8088/logs/v1/event" json:"url"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool      `default:"false" json:"excludeSelf"`
	Urls        []URLXsiam `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// XSIAM authentication token
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeXsiam `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionXsiam `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorXsiam `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsXsiam        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any          `additionalProperties:"true" json:"-"`
}

func (o OutputXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputXsiam) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputXsiam) GetType() TypeXsiam {
	if o == nil {
		return TypeXsiam("")
	}
	return o.Type
}

func (o *OutputXsiam) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputXsiam) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputXsiam) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputXsiam) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputXsiam) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputXsiam) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputXsiam) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputXsiam) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputXsiam) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputXsiam) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputXsiam) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputXsiam) GetExtraHTTPHeaders() []ExtraHTTPHeaderXsiam {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputXsiam) GetFailedRequestLoggingMode() *FailedRequestLoggingModeXsiam {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputXsiam) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputXsiam) GetAuthType() *AuthenticationMethodXsiam {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputXsiam) GetResponseRetrySettings() []ResponseRetrySettingXsiam {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputXsiam) GetTimeoutRetrySettings() *TimeoutRetrySettingsXsiam {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputXsiam) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputXsiam) GetThrottleRateReqPerSec() *int64 {
	if o == nil {
		return nil
	}
	return o.ThrottleRateReqPerSec
}

func (o *OutputXsiam) GetOnBackpressure() *BackpressureBehaviorXsiam {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputXsiam) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputXsiam) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputXsiam) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputXsiam) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputXsiam) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputXsiam) GetUrls() []URLXsiam {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputXsiam) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputXsiam) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputXsiam) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputXsiam) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputXsiam) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputXsiam) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputXsiam) GetPqMode() *ModeXsiam {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputXsiam) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputXsiam) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputXsiam) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputXsiam) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputXsiam) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputXsiam) GetPqCompress() *CompressionXsiam {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputXsiam) GetPqOnBackpressure() *QueueFullBehaviorXsiam {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputXsiam) GetPqControls() *PqControlsXsiam {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputXsiam) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeClickHouse string

const (
	TypeClickHouseClickHouse TypeClickHouse = "click_house"
)

func (e TypeClickHouse) ToPointer() *TypeClickHouse {
	return &e
}
func (e *TypeClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "click_house":
		*e = TypeClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeClickHouse: %v", v)
	}
}

type AuthenticationTypeClickHouse string

const (
	AuthenticationTypeClickHouseNone               AuthenticationTypeClickHouse = "none"
	AuthenticationTypeClickHouseBasic              AuthenticationTypeClickHouse = "basic"
	AuthenticationTypeClickHouseCredentialsSecret  AuthenticationTypeClickHouse = "credentialsSecret"
	AuthenticationTypeClickHouseSslUserCertificate AuthenticationTypeClickHouse = "sslUserCertificate"
	AuthenticationTypeClickHouseToken              AuthenticationTypeClickHouse = "token"
	AuthenticationTypeClickHouseTextSecret         AuthenticationTypeClickHouse = "textSecret"
	AuthenticationTypeClickHouseOauth              AuthenticationTypeClickHouse = "oauth"
)

func (e AuthenticationTypeClickHouse) ToPointer() *AuthenticationTypeClickHouse {
	return &e
}

// FormatClickHouse - Data format to use when sending data to ClickHouse. Defaults to JSON Compact.
type FormatClickHouse string

const (
	// FormatClickHouseJSONCompactEachRowWithNames JSONCompactEachRowWithNames
	FormatClickHouseJSONCompactEachRowWithNames FormatClickHouse = "json-compact-each-row-with-names"
	// FormatClickHouseJSONEachRow JSONEachRow
	FormatClickHouseJSONEachRow FormatClickHouse = "json-each-row"
)

func (e FormatClickHouse) ToPointer() *FormatClickHouse {
	return &e
}

// MappingType - How event fields are mapped to ClickHouse columns.
type MappingType string

const (
	// MappingTypeAutomatic Automatic
	MappingTypeAutomatic MappingType = "automatic"
	// MappingTypeCustom Custom
	MappingTypeCustom MappingType = "custom"
)

func (e MappingType) ToPointer() *MappingType {
	return &e
}

type MinimumTLSVersionClickHouse string

const (
	MinimumTLSVersionClickHouseTlSv1  MinimumTLSVersionClickHouse = "TLSv1"
	MinimumTLSVersionClickHouseTlSv11 MinimumTLSVersionClickHouse = "TLSv1.1"
	MinimumTLSVersionClickHouseTlSv12 MinimumTLSVersionClickHouse = "TLSv1.2"
	MinimumTLSVersionClickHouseTlSv13 MinimumTLSVersionClickHouse = "TLSv1.3"
)

func (e MinimumTLSVersionClickHouse) ToPointer() *MinimumTLSVersionClickHouse {
	return &e
}

type MaximumTLSVersionClickHouse string

const (
	MaximumTLSVersionClickHouseTlSv1  MaximumTLSVersionClickHouse = "TLSv1"
	MaximumTLSVersionClickHouseTlSv11 MaximumTLSVersionClickHouse = "TLSv1.1"
	MaximumTLSVersionClickHouseTlSv12 MaximumTLSVersionClickHouse = "TLSv1.2"
	MaximumTLSVersionClickHouseTlSv13 MaximumTLSVersionClickHouse = "TLSv1.3"
)

func (e MaximumTLSVersionClickHouse) ToPointer() *MaximumTLSVersionClickHouse {
	return &e
}

type TLSSettingsClientSideClickHouse struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                      `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionClickHouse `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionClickHouse `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideClickHouse) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideClickHouse) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideClickHouse) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideClickHouse) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideClickHouse) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideClickHouse) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideClickHouse) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideClickHouse) GetMinVersion() *MinimumTLSVersionClickHouse {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideClickHouse) GetMaxVersion() *MaximumTLSVersionClickHouse {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

type ExtraHTTPHeaderClickHouse struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderClickHouse) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderClickHouse) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeClickHouse - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeClickHouse string

const (
	// FailedRequestLoggingModeClickHousePayload Payload
	FailedRequestLoggingModeClickHousePayload FailedRequestLoggingModeClickHouse = "payload"
	// FailedRequestLoggingModeClickHousePayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeClickHousePayloadAndHeaders FailedRequestLoggingModeClickHouse = "payloadAndHeaders"
	// FailedRequestLoggingModeClickHouseNone None
	FailedRequestLoggingModeClickHouseNone FailedRequestLoggingModeClickHouse = "none"
)

func (e FailedRequestLoggingModeClickHouse) ToPointer() *FailedRequestLoggingModeClickHouse {
	return &e
}

type ResponseRetrySettingClickHouse struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingClickHouse) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingClickHouse) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingClickHouse) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingClickHouse) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsClickHouse struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsClickHouse) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsClickHouse) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsClickHouse) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsClickHouse) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorClickHouse - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorClickHouse string

const (
	// BackpressureBehaviorClickHouseBlock Block
	BackpressureBehaviorClickHouseBlock BackpressureBehaviorClickHouse = "block"
	// BackpressureBehaviorClickHouseDrop Drop
	BackpressureBehaviorClickHouseDrop BackpressureBehaviorClickHouse = "drop"
	// BackpressureBehaviorClickHouseQueue Persistent Queue
	BackpressureBehaviorClickHouseQueue BackpressureBehaviorClickHouse = "queue"
)

func (e BackpressureBehaviorClickHouse) ToPointer() *BackpressureBehaviorClickHouse {
	return &e
}

type OauthParamClickHouse struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o OauthParamClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthParamClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthParamClickHouse) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamClickHouse) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderClickHouse struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o OauthHeaderClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthHeaderClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthHeaderClickHouse) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderClickHouse) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ColumnMapping struct {
	// Name of the column in ClickHouse that will store field value
	ColumnName string `json:"columnName"`
	// Type of the column in the ClickHouse database
	ColumnType *string `json:"columnType,omitempty"`
	// JavaScript expression to compute value to be inserted into ClickHouse table
	ColumnValueExpression string `json:"columnValueExpression"`
}

func (c ColumnMapping) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *ColumnMapping) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"columnName", "columnValueExpression"}); err != nil {
		return err
	}
	return nil
}

func (c *ColumnMapping) GetColumnName() string {
	if c == nil {
		return ""
	}
	return c.ColumnName
}

func (c *ColumnMapping) GetColumnType() *string {
	if c == nil {
		return nil
	}
	return c.ColumnType
}

func (c *ColumnMapping) GetColumnValueExpression() string {
	if c == nil {
		return ""
	}
	return c.ColumnValueExpression
}

// ModeClickHouse - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeClickHouse string

const (
	// ModeClickHouseError Error
	ModeClickHouseError ModeClickHouse = "error"
	// ModeClickHouseAlways Backpressure
	ModeClickHouseAlways ModeClickHouse = "always"
	// ModeClickHouseBackpressure Always On
	ModeClickHouseBackpressure ModeClickHouse = "backpressure"
)

func (e ModeClickHouse) ToPointer() *ModeClickHouse {
	return &e
}

// CompressionClickHouse - Codec to use to compress the persisted data
type CompressionClickHouse string

const (
	// CompressionClickHouseNone None
	CompressionClickHouseNone CompressionClickHouse = "none"
	// CompressionClickHouseGzip Gzip
	CompressionClickHouseGzip CompressionClickHouse = "gzip"
)

func (e CompressionClickHouse) ToPointer() *CompressionClickHouse {
	return &e
}

// QueueFullBehaviorClickHouse - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorClickHouse string

const (
	// QueueFullBehaviorClickHouseBlock Block
	QueueFullBehaviorClickHouseBlock QueueFullBehaviorClickHouse = "block"
	// QueueFullBehaviorClickHouseDrop Drop new data
	QueueFullBehaviorClickHouseDrop QueueFullBehaviorClickHouse = "drop"
)

func (e QueueFullBehaviorClickHouse) ToPointer() *QueueFullBehaviorClickHouse {
	return &e
}

type PqControlsClickHouse struct {
}

func (p PqControlsClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputClickHouse struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type TypeClickHouse `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL of the ClickHouse instance. Example: http://localhost:8123/
	URL      string                        `json:"url"`
	AuthType *AuthenticationTypeClickHouse `default:"none" json:"authType"`
	Database string                        `json:"database"`
	// Name of the ClickHouse table where data will be inserted. Name can contain letters (A-Z, a-z), numbers (0-9), and the character "_", and must start with either a letter or the character "_".
	TableName string `json:"tableName"`
	// Data format to use when sending data to ClickHouse. Defaults to JSON Compact.
	Format *FormatClickHouse `default:"json-compact-each-row-with-names" json:"format"`
	// How event fields are mapped to ClickHouse columns.
	MappingType *MappingType `default:"automatic" json:"mappingType"`
	// Collect data into batches for later processing. Disable to write to a ClickHouse table immediately.
	AsyncInserts *bool                            `default:"false" json:"asyncInserts"`
	TLS          *TLSSettingsClientSideClickHouse `json:"tls,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderClickHouse `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeClickHouse `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingClickHouse `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsClickHouse  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Log the most recent event that fails to match the table schema
	DumpFormatErrorsToDisk *bool `default:"false" json:"dumpFormatErrorsToDisk"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorClickHouse `default:"block" json:"onBackpressure"`
	Description    *string                         `json:"description,omitempty"`
	Username       *string                         `json:"username,omitempty"`
	Password       *string                         `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamClickHouse `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderClickHouse `json:"oauthHeaders,omitempty"`
	// Username for certificate authentication
	SQLUsername *string `json:"sqlUsername,omitempty"`
	// Cribl will wait for confirmation that data has been fully inserted into the ClickHouse database before proceeding. Disabling this option can increase throughput, but Cribl won’t be able to verify data has been completely inserted.
	WaitForAsyncInserts *bool `default:"true" json:"waitForAsyncInserts"`
	// Fields to exclude from sending to ClickHouse
	ExcludeMappingFields []string `json:"excludeMappingFields,omitempty"`
	// Retrieves the table schema from ClickHouse and populates the Column Mapping table
	DescribeTable  *string         `json:"describeTable,omitempty"`
	ColumnMappings []ColumnMapping `json:"columnMappings,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeClickHouse `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionClickHouse `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorClickHouse `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsClickHouse        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any               `additionalProperties:"true" json:"-"`
}

func (o OutputClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "url", "database", "tableName"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputClickHouse) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputClickHouse) GetType() TypeClickHouse {
	if o == nil {
		return TypeClickHouse("")
	}
	return o.Type
}

func (o *OutputClickHouse) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputClickHouse) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputClickHouse) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputClickHouse) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputClickHouse) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputClickHouse) GetAuthType() *AuthenticationTypeClickHouse {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputClickHouse) GetDatabase() string {
	if o == nil {
		return ""
	}
	return o.Database
}

func (o *OutputClickHouse) GetTableName() string {
	if o == nil {
		return ""
	}
	return o.TableName
}

func (o *OutputClickHouse) GetFormat() *FormatClickHouse {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputClickHouse) GetMappingType() *MappingType {
	if o == nil {
		return nil
	}
	return o.MappingType
}

func (o *OutputClickHouse) GetAsyncInserts() *bool {
	if o == nil {
		return nil
	}
	return o.AsyncInserts
}

func (o *OutputClickHouse) GetTLS() *TLSSettingsClientSideClickHouse {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputClickHouse) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputClickHouse) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputClickHouse) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputClickHouse) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputClickHouse) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputClickHouse) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputClickHouse) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputClickHouse) GetExtraHTTPHeaders() []ExtraHTTPHeaderClickHouse {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputClickHouse) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputClickHouse) GetFailedRequestLoggingMode() *FailedRequestLoggingModeClickHouse {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputClickHouse) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputClickHouse) GetResponseRetrySettings() []ResponseRetrySettingClickHouse {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputClickHouse) GetTimeoutRetrySettings() *TimeoutRetrySettingsClickHouse {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputClickHouse) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputClickHouse) GetDumpFormatErrorsToDisk() *bool {
	if o == nil {
		return nil
	}
	return o.DumpFormatErrorsToDisk
}

func (o *OutputClickHouse) GetOnBackpressure() *BackpressureBehaviorClickHouse {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputClickHouse) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputClickHouse) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputClickHouse) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputClickHouse) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputClickHouse) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputClickHouse) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputClickHouse) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputClickHouse) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputClickHouse) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputClickHouse) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputClickHouse) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputClickHouse) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputClickHouse) GetOauthParams() []OauthParamClickHouse {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputClickHouse) GetOauthHeaders() []OauthHeaderClickHouse {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputClickHouse) GetSQLUsername() *string {
	if o == nil {
		return nil
	}
	return o.SQLUsername
}

func (o *OutputClickHouse) GetWaitForAsyncInserts() *bool {
	if o == nil {
		return nil
	}
	return o.WaitForAsyncInserts
}

func (o *OutputClickHouse) GetExcludeMappingFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeMappingFields
}

func (o *OutputClickHouse) GetDescribeTable() *string {
	if o == nil {
		return nil
	}
	return o.DescribeTable
}

func (o *OutputClickHouse) GetColumnMappings() []ColumnMapping {
	if o == nil {
		return nil
	}
	return o.ColumnMappings
}

func (o *OutputClickHouse) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputClickHouse) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputClickHouse) GetPqMode() *ModeClickHouse {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputClickHouse) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputClickHouse) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputClickHouse) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputClickHouse) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputClickHouse) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputClickHouse) GetPqCompress() *CompressionClickHouse {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputClickHouse) GetPqOnBackpressure() *QueueFullBehaviorClickHouse {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputClickHouse) GetPqControls() *PqControlsClickHouse {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputClickHouse) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeDiskSpool string

const (
	TypeDiskSpoolDiskSpool TypeDiskSpool = "disk_spool"
)

func (e TypeDiskSpool) ToPointer() *TypeDiskSpool {
	return &e
}
func (e *TypeDiskSpool) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "disk_spool":
		*e = TypeDiskSpool(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDiskSpool: %v", v)
	}
}

// CompressionDiskSpool - Data compression format. Default is gzip.
type CompressionDiskSpool string

const (
	CompressionDiskSpoolNone CompressionDiskSpool = "none"
	CompressionDiskSpoolGzip CompressionDiskSpool = "gzip"
)

func (e CompressionDiskSpool) ToPointer() *CompressionDiskSpool {
	return &e
}

type OutputDiskSpool struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeDiskSpool `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Time period for grouping spooled events. Default is 10m.
	TimeWindow *string `default:"10m" json:"timeWindow"`
	// Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.
	MaxDataSize *string `default:"1GB" json:"maxDataSize"`
	// Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.
	MaxDataTime *string `default:"24h" json:"maxDataTime"`
	// Data compression format. Default is gzip.
	Compress *CompressionDiskSpool `default:"gzip" json:"compress"`
	// JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory.
	PartitionExpr        *string        `json:"partitionExpr,omitempty"`
	Description          *string        `json:"description,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputDiskSpool) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDiskSpool) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDiskSpool) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDiskSpool) GetType() TypeDiskSpool {
	if o == nil {
		return TypeDiskSpool("")
	}
	return o.Type
}

func (o *OutputDiskSpool) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDiskSpool) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDiskSpool) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDiskSpool) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDiskSpool) GetTimeWindow() *string {
	if o == nil {
		return nil
	}
	return o.TimeWindow
}

func (o *OutputDiskSpool) GetMaxDataSize() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataSize
}

func (o *OutputDiskSpool) GetMaxDataTime() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataTime
}

func (o *OutputDiskSpool) GetCompress() *CompressionDiskSpool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDiskSpool) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputDiskSpool) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDiskSpool) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeCriblLake string

const (
	TypeCriblLakeCriblLake TypeCriblLake = "cribl_lake"
)

func (e TypeCriblLake) ToPointer() *TypeCriblLake {
	return &e
}
func (e *TypeCriblLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cribl_lake":
		*e = TypeCriblLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCriblLake: %v", v)
	}
}

// SignatureVersionCriblLake - Signature version to use for signing S3 requests
type SignatureVersionCriblLake string

const (
	SignatureVersionCriblLakeV2 SignatureVersionCriblLake = "v2"
	SignatureVersionCriblLakeV4 SignatureVersionCriblLake = "v4"
)

func (e SignatureVersionCriblLake) ToPointer() *SignatureVersionCriblLake {
	return &e
}

// ObjectACLCriblLake - Object ACL to assign to uploaded objects
type ObjectACLCriblLake string

const (
	// ObjectACLCriblLakePrivate Private
	ObjectACLCriblLakePrivate ObjectACLCriblLake = "private"
	// ObjectACLCriblLakePublicRead Public Read Only
	ObjectACLCriblLakePublicRead ObjectACLCriblLake = "public-read"
	// ObjectACLCriblLakePublicReadWrite Public Read/Write
	ObjectACLCriblLakePublicReadWrite ObjectACLCriblLake = "public-read-write"
	// ObjectACLCriblLakeAuthenticatedRead Authenticated Read Only
	ObjectACLCriblLakeAuthenticatedRead ObjectACLCriblLake = "authenticated-read"
	// ObjectACLCriblLakeAwsExecRead AWS EC2 AMI Read Only
	ObjectACLCriblLakeAwsExecRead ObjectACLCriblLake = "aws-exec-read"
	// ObjectACLCriblLakeBucketOwnerRead Bucket Owner Read Only
	ObjectACLCriblLakeBucketOwnerRead ObjectACLCriblLake = "bucket-owner-read"
	// ObjectACLCriblLakeBucketOwnerFullControl Bucket Owner Full Control
	ObjectACLCriblLakeBucketOwnerFullControl ObjectACLCriblLake = "bucket-owner-full-control"
)

func (e ObjectACLCriblLake) ToPointer() *ObjectACLCriblLake {
	return &e
}

// StorageClassCriblLake - Storage class to select for uploaded objects
type StorageClassCriblLake string

const (
	// StorageClassCriblLakeStandard Standard
	StorageClassCriblLakeStandard StorageClassCriblLake = "STANDARD"
	// StorageClassCriblLakeReducedRedundancy Reduced Redundancy Storage
	StorageClassCriblLakeReducedRedundancy StorageClassCriblLake = "REDUCED_REDUNDANCY"
	// StorageClassCriblLakeStandardIa Standard, Infrequent Access
	StorageClassCriblLakeStandardIa StorageClassCriblLake = "STANDARD_IA"
	// StorageClassCriblLakeOnezoneIa One Zone, Infrequent Access
	StorageClassCriblLakeOnezoneIa StorageClassCriblLake = "ONEZONE_IA"
	// StorageClassCriblLakeIntelligentTiering Intelligent Tiering
	StorageClassCriblLakeIntelligentTiering StorageClassCriblLake = "INTELLIGENT_TIERING"
	// StorageClassCriblLakeGlacier Glacier Flexible Retrieval
	StorageClassCriblLakeGlacier StorageClassCriblLake = "GLACIER"
	// StorageClassCriblLakeGlacierIr Glacier Instant Retrieval
	StorageClassCriblLakeGlacierIr StorageClassCriblLake = "GLACIER_IR"
	// StorageClassCriblLakeDeepArchive Glacier Deep Archive
	StorageClassCriblLakeDeepArchive StorageClassCriblLake = "DEEP_ARCHIVE"
)

func (e StorageClassCriblLake) ToPointer() *StorageClassCriblLake {
	return &e
}

type ServerSideEncryptionForUploadedObjectsCriblLake string

const (
	// ServerSideEncryptionForUploadedObjectsCriblLakeAes256 Amazon S3 Managed Key
	ServerSideEncryptionForUploadedObjectsCriblLakeAes256 ServerSideEncryptionForUploadedObjectsCriblLake = "AES256"
	// ServerSideEncryptionForUploadedObjectsCriblLakeAwsKms AWS KMS Managed Key
	ServerSideEncryptionForUploadedObjectsCriblLakeAwsKms ServerSideEncryptionForUploadedObjectsCriblLake = "aws:kms"
)

func (e ServerSideEncryptionForUploadedObjectsCriblLake) ToPointer() *ServerSideEncryptionForUploadedObjectsCriblLake {
	return &e
}

// BackpressureBehaviorCriblLake - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCriblLake string

const (
	// BackpressureBehaviorCriblLakeBlock Block
	BackpressureBehaviorCriblLakeBlock BackpressureBehaviorCriblLake = "block"
	// BackpressureBehaviorCriblLakeDrop Drop
	BackpressureBehaviorCriblLakeDrop BackpressureBehaviorCriblLake = "drop"
)

func (e BackpressureBehaviorCriblLake) ToPointer() *BackpressureBehaviorCriblLake {
	return &e
}

// DiskSpaceProtectionCriblLake - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionCriblLake string

const (
	// DiskSpaceProtectionCriblLakeBlock Block
	DiskSpaceProtectionCriblLakeBlock DiskSpaceProtectionCriblLake = "block"
	// DiskSpaceProtectionCriblLakeDrop Drop
	DiskSpaceProtectionCriblLakeDrop DiskSpaceProtectionCriblLake = "drop"
)

func (e DiskSpaceProtectionCriblLake) ToPointer() *DiskSpaceProtectionCriblLake {
	return &e
}

type AwsAuthenticationMethod string

const (
	AwsAuthenticationMethodAuto    AwsAuthenticationMethod = "auto"
	AwsAuthenticationMethodAutoRPC AwsAuthenticationMethod = "auto_rpc"
	AwsAuthenticationMethodManual  AwsAuthenticationMethod = "manual"
)

func (e AwsAuthenticationMethod) ToPointer() *AwsAuthenticationMethod {
	return &e
}

type FormatCriblLake string

const (
	FormatCriblLakeJSON    FormatCriblLake = "json"
	FormatCriblLakeParquet FormatCriblLake = "parquet"
	FormatCriblLakeDdss    FormatCriblLake = "ddss"
)

func (e FormatCriblLake) ToPointer() *FormatCriblLake {
	return &e
}

type OutputCriblLake struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeCriblLake `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket *string `json:"bucket,omitempty"`
	// Region where the S3 bucket is located
	Region *string `json:"region,omitempty"`
	// Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing S3 requests
	SignatureVersion *SignatureVersionCriblLake `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Lake dataset to send the data to.
	DestPath *string `json:"destPath,omitempty"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLCriblLake `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass         *StorageClassCriblLake                           `json:"storageClass,omitempty"`
	ServerSideEncryption *ServerSideEncryptionForUploadedObjectsCriblLake `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"64" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCriblLake `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionCriblLake `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"300" json:"maxFileIdleTimeSec"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64                 `default:"100" json:"maxClosingFilesToBackpressure"`
	AwsAuthenticationMethod       *AwsAuthenticationMethod `default:"auto" json:"awsAuthenticationMethod"`
	Format                        *FormatCriblLake         `json:"format,omitempty"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	Description            *string  `json:"description,omitempty"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum          *float64       `default:"20" json:"maxRetryNum"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputCriblLake) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCriblLake) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCriblLake) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputCriblLake) GetType() TypeCriblLake {
	if o == nil {
		return TypeCriblLake("")
	}
	return o.Type
}

func (o *OutputCriblLake) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCriblLake) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCriblLake) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCriblLake) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCriblLake) GetBucket() *string {
	if o == nil {
		return nil
	}
	return o.Bucket
}

func (o *OutputCriblLake) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputCriblLake) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputCriblLake) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputCriblLake) GetSignatureVersion() *SignatureVersionCriblLake {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputCriblLake) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputCriblLake) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCriblLake) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputCriblLake) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputCriblLake) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputCriblLake) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputCriblLake) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputCriblLake) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputCriblLake) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputCriblLake) GetObjectACL() *ObjectACLCriblLake {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputCriblLake) GetStorageClass() *StorageClassCriblLake {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputCriblLake) GetServerSideEncryption() *ServerSideEncryptionForUploadedObjectsCriblLake {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputCriblLake) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputCriblLake) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputCriblLake) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputCriblLake) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputCriblLake) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputCriblLake) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputCriblLake) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputCriblLake) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputCriblLake) GetOnBackpressure() *BackpressureBehaviorCriblLake {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCriblLake) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputCriblLake) GetOnDiskFullBackpressure() *DiskSpaceProtectionCriblLake {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputCriblLake) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputCriblLake) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputCriblLake) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputCriblLake) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputCriblLake) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputCriblLake) GetAwsAuthenticationMethod() *AwsAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputCriblLake) GetFormat() *FormatCriblLake {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputCriblLake) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputCriblLake) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCriblLake) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputCriblLake) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputCriblLake) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputCriblLake) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputCriblLake) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeSecurityLake string

const (
	OutputTypeSecurityLakeSecurityLake OutputTypeSecurityLake = "security_lake"
)

func (e OutputTypeSecurityLake) ToPointer() *OutputTypeSecurityLake {
	return &e
}
func (e *OutputTypeSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "security_lake":
		*e = OutputTypeSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSecurityLake: %v", v)
	}
}

// OutputAuthenticationMethodSecurityLake - AWS authentication method. Choose Auto to use IAM roles.
type OutputAuthenticationMethodSecurityLake string

const (
	// OutputAuthenticationMethodSecurityLakeAuto Auto
	OutputAuthenticationMethodSecurityLakeAuto OutputAuthenticationMethodSecurityLake = "auto"
	// OutputAuthenticationMethodSecurityLakeManual Manual
	OutputAuthenticationMethodSecurityLakeManual OutputAuthenticationMethodSecurityLake = "manual"
	// OutputAuthenticationMethodSecurityLakeSecret Secret Key pair
	OutputAuthenticationMethodSecurityLakeSecret OutputAuthenticationMethodSecurityLake = "secret"
)

func (e OutputAuthenticationMethodSecurityLake) ToPointer() *OutputAuthenticationMethodSecurityLake {
	return &e
}

// OutputSignatureVersionSecurityLake - Signature version to use for signing Amazon Security Lake requests
type OutputSignatureVersionSecurityLake string

const (
	OutputSignatureVersionSecurityLakeV2 OutputSignatureVersionSecurityLake = "v2"
	OutputSignatureVersionSecurityLakeV4 OutputSignatureVersionSecurityLake = "v4"
)

func (e OutputSignatureVersionSecurityLake) ToPointer() *OutputSignatureVersionSecurityLake {
	return &e
}

// ObjectACLSecurityLake - Object ACL to assign to uploaded objects
type ObjectACLSecurityLake string

const (
	// ObjectACLSecurityLakePrivate Private
	ObjectACLSecurityLakePrivate ObjectACLSecurityLake = "private"
	// ObjectACLSecurityLakePublicRead Public Read Only
	ObjectACLSecurityLakePublicRead ObjectACLSecurityLake = "public-read"
	// ObjectACLSecurityLakePublicReadWrite Public Read/Write
	ObjectACLSecurityLakePublicReadWrite ObjectACLSecurityLake = "public-read-write"
	// ObjectACLSecurityLakeAuthenticatedRead Authenticated Read Only
	ObjectACLSecurityLakeAuthenticatedRead ObjectACLSecurityLake = "authenticated-read"
	// ObjectACLSecurityLakeAwsExecRead AWS EC2 AMI Read Only
	ObjectACLSecurityLakeAwsExecRead ObjectACLSecurityLake = "aws-exec-read"
	// ObjectACLSecurityLakeBucketOwnerRead Bucket Owner Read Only
	ObjectACLSecurityLakeBucketOwnerRead ObjectACLSecurityLake = "bucket-owner-read"
	// ObjectACLSecurityLakeBucketOwnerFullControl Bucket Owner Full Control
	ObjectACLSecurityLakeBucketOwnerFullControl ObjectACLSecurityLake = "bucket-owner-full-control"
)

func (e ObjectACLSecurityLake) ToPointer() *ObjectACLSecurityLake {
	return &e
}

// StorageClassSecurityLake - Storage class to select for uploaded objects
type StorageClassSecurityLake string

const (
	// StorageClassSecurityLakeStandard Standard
	StorageClassSecurityLakeStandard StorageClassSecurityLake = "STANDARD"
	// StorageClassSecurityLakeReducedRedundancy Reduced Redundancy Storage
	StorageClassSecurityLakeReducedRedundancy StorageClassSecurityLake = "REDUCED_REDUNDANCY"
	// StorageClassSecurityLakeStandardIa Standard, Infrequent Access
	StorageClassSecurityLakeStandardIa StorageClassSecurityLake = "STANDARD_IA"
	// StorageClassSecurityLakeOnezoneIa One Zone, Infrequent Access
	StorageClassSecurityLakeOnezoneIa StorageClassSecurityLake = "ONEZONE_IA"
	// StorageClassSecurityLakeIntelligentTiering Intelligent Tiering
	StorageClassSecurityLakeIntelligentTiering StorageClassSecurityLake = "INTELLIGENT_TIERING"
	// StorageClassSecurityLakeGlacier Glacier Flexible Retrieval
	StorageClassSecurityLakeGlacier StorageClassSecurityLake = "GLACIER"
	// StorageClassSecurityLakeGlacierIr Glacier Instant Retrieval
	StorageClassSecurityLakeGlacierIr StorageClassSecurityLake = "GLACIER_IR"
	// StorageClassSecurityLakeDeepArchive Glacier Deep Archive
	StorageClassSecurityLakeDeepArchive StorageClassSecurityLake = "DEEP_ARCHIVE"
)

func (e StorageClassSecurityLake) ToPointer() *StorageClassSecurityLake {
	return &e
}

type ServerSideEncryptionForUploadedObjectsSecurityLake string

const (
	// ServerSideEncryptionForUploadedObjectsSecurityLakeAes256 Amazon S3 Managed Key
	ServerSideEncryptionForUploadedObjectsSecurityLakeAes256 ServerSideEncryptionForUploadedObjectsSecurityLake = "AES256"
	// ServerSideEncryptionForUploadedObjectsSecurityLakeAwsKms AWS KMS Managed Key
	ServerSideEncryptionForUploadedObjectsSecurityLakeAwsKms ServerSideEncryptionForUploadedObjectsSecurityLake = "aws:kms"
)

func (e ServerSideEncryptionForUploadedObjectsSecurityLake) ToPointer() *ServerSideEncryptionForUploadedObjectsSecurityLake {
	return &e
}

// BackpressureBehaviorSecurityLake - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSecurityLake string

const (
	// BackpressureBehaviorSecurityLakeBlock Block
	BackpressureBehaviorSecurityLakeBlock BackpressureBehaviorSecurityLake = "block"
	// BackpressureBehaviorSecurityLakeDrop Drop
	BackpressureBehaviorSecurityLakeDrop BackpressureBehaviorSecurityLake = "drop"
)

func (e BackpressureBehaviorSecurityLake) ToPointer() *BackpressureBehaviorSecurityLake {
	return &e
}

// DiskSpaceProtectionSecurityLake - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionSecurityLake string

const (
	// DiskSpaceProtectionSecurityLakeBlock Block
	DiskSpaceProtectionSecurityLakeBlock DiskSpaceProtectionSecurityLake = "block"
	// DiskSpaceProtectionSecurityLakeDrop Drop
	DiskSpaceProtectionSecurityLakeDrop DiskSpaceProtectionSecurityLake = "drop"
)

func (e DiskSpaceProtectionSecurityLake) ToPointer() *DiskSpaceProtectionSecurityLake {
	return &e
}

// ParquetVersionSecurityLake - Determines which data types are supported and how they are represented
type ParquetVersionSecurityLake string

const (
	// ParquetVersionSecurityLakeParquet10 1.0
	ParquetVersionSecurityLakeParquet10 ParquetVersionSecurityLake = "PARQUET_1_0"
	// ParquetVersionSecurityLakeParquet24 2.4
	ParquetVersionSecurityLakeParquet24 ParquetVersionSecurityLake = "PARQUET_2_4"
	// ParquetVersionSecurityLakeParquet26 2.6
	ParquetVersionSecurityLakeParquet26 ParquetVersionSecurityLake = "PARQUET_2_6"
)

func (e ParquetVersionSecurityLake) ToPointer() *ParquetVersionSecurityLake {
	return &e
}

// DataPageVersionSecurityLake - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionSecurityLake string

const (
	// DataPageVersionSecurityLakeDataPageV1 V1
	DataPageVersionSecurityLakeDataPageV1 DataPageVersionSecurityLake = "DATA_PAGE_V1"
	// DataPageVersionSecurityLakeDataPageV2 V2
	DataPageVersionSecurityLakeDataPageV2 DataPageVersionSecurityLake = "DATA_PAGE_V2"
)

func (e DataPageVersionSecurityLake) ToPointer() *DataPageVersionSecurityLake {
	return &e
}

type KeyValueMetadatumSecurityLake struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumSecurityLake) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumSecurityLake) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumSecurityLake) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumSecurityLake) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputSecurityLake struct {
	// Unique ID for this output
	ID   *string                `json:"id,omitempty"`
	Type OutputTypeSecurityLake `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// Region where the Amazon Security Lake is located.
	Region       string  `json:"region"`
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputAuthenticationMethodSecurityLake `default:"auto" json:"awsAuthenticationMethod"`
	// Amazon Security Lake service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Amazon Security Lake-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing Amazon Security Lake requests
	SignatureVersion *OutputSignatureVersionSecurityLake `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn string `json:"assumeRoleArn"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLSecurityLake `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass         *StorageClassSecurityLake                           `json:"storageClass,omitempty"`
	ServerSideEncryption *ServerSideEncryptionForUploadedObjectsSecurityLake `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSecurityLake `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionSecurityLake `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	// ID of the AWS account whose data the Destination will write to Security Lake. This should have been configured when creating the Amazon Security Lake custom source.
	AccountID string `json:"accountId"`
	// Name of the custom source configured in Amazon Security Lake
	CustomSource string `json:"customSource"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionSecurityLake `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionSecurityLake `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumSecurityLake `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool   `default:"false" json:"enablePageChecksum"`
	Description        *string `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum          *float64       `default:"20" json:"maxRetryNum"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputSecurityLake) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSecurityLake) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "bucket", "region", "assumeRoleArn", "accountId", "customSource"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSecurityLake) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSecurityLake) GetType() OutputTypeSecurityLake {
	if o == nil {
		return OutputTypeSecurityLake("")
	}
	return o.Type
}

func (o *OutputSecurityLake) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSecurityLake) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSecurityLake) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSecurityLake) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSecurityLake) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputSecurityLake) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputSecurityLake) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSecurityLake) GetAwsAuthenticationMethod() *OutputAuthenticationMethodSecurityLake {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSecurityLake) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSecurityLake) GetSignatureVersion() *OutputSignatureVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSecurityLake) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSecurityLake) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSecurityLake) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSecurityLake) GetAssumeRoleArn() string {
	if o == nil {
		return ""
	}
	return o.AssumeRoleArn
}

func (o *OutputSecurityLake) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSecurityLake) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSecurityLake) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputSecurityLake) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputSecurityLake) GetObjectACL() *ObjectACLSecurityLake {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputSecurityLake) GetStorageClass() *StorageClassSecurityLake {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputSecurityLake) GetServerSideEncryption() *ServerSideEncryptionForUploadedObjectsSecurityLake {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputSecurityLake) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputSecurityLake) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputSecurityLake) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputSecurityLake) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputSecurityLake) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputSecurityLake) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputSecurityLake) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputSecurityLake) GetOnBackpressure() *BackpressureBehaviorSecurityLake {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSecurityLake) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputSecurityLake) GetOnDiskFullBackpressure() *DiskSpaceProtectionSecurityLake {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputSecurityLake) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputSecurityLake) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputSecurityLake) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputSecurityLake) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputSecurityLake) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputSecurityLake) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputSecurityLake) GetAccountID() string {
	if o == nil {
		return ""
	}
	return o.AccountID
}

func (o *OutputSecurityLake) GetCustomSource() string {
	if o == nil {
		return ""
	}
	return o.CustomSource
}

func (o *OutputSecurityLake) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputSecurityLake) GetParquetVersion() *ParquetVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputSecurityLake) GetParquetDataPageVersion() *DataPageVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputSecurityLake) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputSecurityLake) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputSecurityLake) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputSecurityLake) GetKeyValueMetadata() []KeyValueMetadatumSecurityLake {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputSecurityLake) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputSecurityLake) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputSecurityLake) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputSecurityLake) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSecurityLake) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSecurityLake) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSecurityLake) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputSecurityLake) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputSecurityLake) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputSecurityLake) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputSecurityLake) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputSecurityLake) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeDlS3 string

const (
	TypeDlS3DlS3 TypeDlS3 = "dl_s3"
)

func (e TypeDlS3) ToPointer() *TypeDlS3 {
	return &e
}
func (e *TypeDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dl_s3":
		*e = TypeDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDlS3: %v", v)
	}
}

// AuthenticationMethodDlS3 - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodDlS3 string

const (
	// AuthenticationMethodDlS3Auto Auto
	AuthenticationMethodDlS3Auto AuthenticationMethodDlS3 = "auto"
	// AuthenticationMethodDlS3Manual Manual
	AuthenticationMethodDlS3Manual AuthenticationMethodDlS3 = "manual"
	// AuthenticationMethodDlS3Secret Secret Key pair
	AuthenticationMethodDlS3Secret AuthenticationMethodDlS3 = "secret"
)

func (e AuthenticationMethodDlS3) ToPointer() *AuthenticationMethodDlS3 {
	return &e
}

// SignatureVersionDlS3 - Signature version to use for signing S3 requests
type SignatureVersionDlS3 string

const (
	SignatureVersionDlS3V2 SignatureVersionDlS3 = "v2"
	SignatureVersionDlS3V4 SignatureVersionDlS3 = "v4"
)

func (e SignatureVersionDlS3) ToPointer() *SignatureVersionDlS3 {
	return &e
}

// ObjectACLDlS3 - Object ACL to assign to uploaded objects
type ObjectACLDlS3 string

const (
	// ObjectACLDlS3Private Private
	ObjectACLDlS3Private ObjectACLDlS3 = "private"
	// ObjectACLDlS3PublicRead Public Read Only
	ObjectACLDlS3PublicRead ObjectACLDlS3 = "public-read"
	// ObjectACLDlS3PublicReadWrite Public Read/Write
	ObjectACLDlS3PublicReadWrite ObjectACLDlS3 = "public-read-write"
	// ObjectACLDlS3AuthenticatedRead Authenticated Read Only
	ObjectACLDlS3AuthenticatedRead ObjectACLDlS3 = "authenticated-read"
	// ObjectACLDlS3AwsExecRead AWS EC2 AMI Read Only
	ObjectACLDlS3AwsExecRead ObjectACLDlS3 = "aws-exec-read"
	// ObjectACLDlS3BucketOwnerRead Bucket Owner Read Only
	ObjectACLDlS3BucketOwnerRead ObjectACLDlS3 = "bucket-owner-read"
	// ObjectACLDlS3BucketOwnerFullControl Bucket Owner Full Control
	ObjectACLDlS3BucketOwnerFullControl ObjectACLDlS3 = "bucket-owner-full-control"
)

func (e ObjectACLDlS3) ToPointer() *ObjectACLDlS3 {
	return &e
}

// StorageClassDlS3 - Storage class to select for uploaded objects
type StorageClassDlS3 string

const (
	// StorageClassDlS3Standard Standard
	StorageClassDlS3Standard StorageClassDlS3 = "STANDARD"
	// StorageClassDlS3ReducedRedundancy Reduced Redundancy Storage
	StorageClassDlS3ReducedRedundancy StorageClassDlS3 = "REDUCED_REDUNDANCY"
	// StorageClassDlS3StandardIa Standard, Infrequent Access
	StorageClassDlS3StandardIa StorageClassDlS3 = "STANDARD_IA"
	// StorageClassDlS3OnezoneIa One Zone, Infrequent Access
	StorageClassDlS3OnezoneIa StorageClassDlS3 = "ONEZONE_IA"
	// StorageClassDlS3IntelligentTiering Intelligent Tiering
	StorageClassDlS3IntelligentTiering StorageClassDlS3 = "INTELLIGENT_TIERING"
	// StorageClassDlS3Glacier Glacier Flexible Retrieval
	StorageClassDlS3Glacier StorageClassDlS3 = "GLACIER"
	// StorageClassDlS3GlacierIr Glacier Instant Retrieval
	StorageClassDlS3GlacierIr StorageClassDlS3 = "GLACIER_IR"
	// StorageClassDlS3DeepArchive Glacier Deep Archive
	StorageClassDlS3DeepArchive StorageClassDlS3 = "DEEP_ARCHIVE"
)

func (e StorageClassDlS3) ToPointer() *StorageClassDlS3 {
	return &e
}

type ServerSideEncryptionForUploadedObjectsDlS3 string

const (
	// ServerSideEncryptionForUploadedObjectsDlS3Aes256 Amazon S3 Managed Key
	ServerSideEncryptionForUploadedObjectsDlS3Aes256 ServerSideEncryptionForUploadedObjectsDlS3 = "AES256"
	// ServerSideEncryptionForUploadedObjectsDlS3AwsKms AWS KMS Managed Key
	ServerSideEncryptionForUploadedObjectsDlS3AwsKms ServerSideEncryptionForUploadedObjectsDlS3 = "aws:kms"
)

func (e ServerSideEncryptionForUploadedObjectsDlS3) ToPointer() *ServerSideEncryptionForUploadedObjectsDlS3 {
	return &e
}

// DataFormatDlS3 - Format of the output data
type DataFormatDlS3 string

const (
	// DataFormatDlS3JSON JSON
	DataFormatDlS3JSON DataFormatDlS3 = "json"
	// DataFormatDlS3Raw Raw
	DataFormatDlS3Raw DataFormatDlS3 = "raw"
	// DataFormatDlS3Parquet Parquet
	DataFormatDlS3Parquet DataFormatDlS3 = "parquet"
)

func (e DataFormatDlS3) ToPointer() *DataFormatDlS3 {
	return &e
}

// BackpressureBehaviorDlS3 - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDlS3 string

const (
	// BackpressureBehaviorDlS3Block Block
	BackpressureBehaviorDlS3Block BackpressureBehaviorDlS3 = "block"
	// BackpressureBehaviorDlS3Drop Drop
	BackpressureBehaviorDlS3Drop BackpressureBehaviorDlS3 = "drop"
)

func (e BackpressureBehaviorDlS3) ToPointer() *BackpressureBehaviorDlS3 {
	return &e
}

// DiskSpaceProtectionDlS3 - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionDlS3 string

const (
	// DiskSpaceProtectionDlS3Block Block
	DiskSpaceProtectionDlS3Block DiskSpaceProtectionDlS3 = "block"
	// DiskSpaceProtectionDlS3Drop Drop
	DiskSpaceProtectionDlS3Drop DiskSpaceProtectionDlS3 = "drop"
)

func (e DiskSpaceProtectionDlS3) ToPointer() *DiskSpaceProtectionDlS3 {
	return &e
}

// CompressionDlS3 - Data compression format to apply to HTTP content before it is delivered
type CompressionDlS3 string

const (
	CompressionDlS3None CompressionDlS3 = "none"
	CompressionDlS3Gzip CompressionDlS3 = "gzip"
)

func (e CompressionDlS3) ToPointer() *CompressionDlS3 {
	return &e
}

// CompressionLevelDlS3 - Compression level to apply before moving files to final destination
type CompressionLevelDlS3 string

const (
	// CompressionLevelDlS3BestSpeed Best Speed
	CompressionLevelDlS3BestSpeed CompressionLevelDlS3 = "best_speed"
	// CompressionLevelDlS3Normal Normal
	CompressionLevelDlS3Normal CompressionLevelDlS3 = "normal"
	// CompressionLevelDlS3BestCompression Best Compression
	CompressionLevelDlS3BestCompression CompressionLevelDlS3 = "best_compression"
)

func (e CompressionLevelDlS3) ToPointer() *CompressionLevelDlS3 {
	return &e
}

// ParquetVersionDlS3 - Determines which data types are supported and how they are represented
type ParquetVersionDlS3 string

const (
	// ParquetVersionDlS3Parquet10 1.0
	ParquetVersionDlS3Parquet10 ParquetVersionDlS3 = "PARQUET_1_0"
	// ParquetVersionDlS3Parquet24 2.4
	ParquetVersionDlS3Parquet24 ParquetVersionDlS3 = "PARQUET_2_4"
	// ParquetVersionDlS3Parquet26 2.6
	ParquetVersionDlS3Parquet26 ParquetVersionDlS3 = "PARQUET_2_6"
)

func (e ParquetVersionDlS3) ToPointer() *ParquetVersionDlS3 {
	return &e
}

// DataPageVersionDlS3 - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionDlS3 string

const (
	// DataPageVersionDlS3DataPageV1 V1
	DataPageVersionDlS3DataPageV1 DataPageVersionDlS3 = "DATA_PAGE_V1"
	// DataPageVersionDlS3DataPageV2 V2
	DataPageVersionDlS3DataPageV2 DataPageVersionDlS3 = "DATA_PAGE_V2"
)

func (e DataPageVersionDlS3) ToPointer() *DataPageVersionDlS3 {
	return &e
}

type KeyValueMetadatumDlS3 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumDlS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumDlS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumDlS3) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumDlS3) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputDlS3 struct {
	// Unique ID for this output
	ID   *string  `json:"id,omitempty"`
	Type TypeDlS3 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// Region where the S3 bucket is located
	Region *string `json:"region,omitempty"`
	// Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodDlS3 `default:"auto" json:"awsAuthenticationMethod"`
	// S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing S3 requests
	SignatureVersion *SignatureVersionDlS3 `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`
	DestPath *string `default:"" json:"destPath"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLDlS3 `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass         *StorageClassDlS3                           `json:"storageClass,omitempty"`
	ServerSideEncryption *ServerSideEncryptionForUploadedObjectsDlS3 `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// Format of the output data
	Format *DataFormatDlS3 `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDlS3 `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionDlS3 `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	// List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>.
	PartitioningFields []string `json:"partitioningFields,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionDlS3 `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelDlS3 `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionDlS3 `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionDlS3 `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumDlS3 `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum          *float64       `default:"20" json:"maxRetryNum"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputDlS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDlS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "bucket"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDlS3) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDlS3) GetType() TypeDlS3 {
	if o == nil {
		return TypeDlS3("")
	}
	return o.Type
}

func (o *OutputDlS3) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDlS3) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDlS3) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDlS3) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDlS3) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputDlS3) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputDlS3) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputDlS3) GetAwsAuthenticationMethod() *AuthenticationMethodDlS3 {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputDlS3) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDlS3) GetSignatureVersion() *SignatureVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputDlS3) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputDlS3) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDlS3) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputDlS3) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputDlS3) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputDlS3) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputDlS3) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDlS3) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDlS3) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDlS3) GetObjectACL() *ObjectACLDlS3 {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputDlS3) GetStorageClass() *StorageClassDlS3 {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputDlS3) GetServerSideEncryption() *ServerSideEncryptionForUploadedObjectsDlS3 {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputDlS3) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputDlS3) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDlS3) GetFormat() *DataFormatDlS3 {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDlS3) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDlS3) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDlS3) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDlS3) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDlS3) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDlS3) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDlS3) GetOnBackpressure() *BackpressureBehaviorDlS3 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDlS3) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDlS3) GetOnDiskFullBackpressure() *DiskSpaceProtectionDlS3 {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDlS3) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputDlS3) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDlS3) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDlS3) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputDlS3) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputDlS3) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputDlS3) GetPartitioningFields() []string {
	if o == nil {
		return nil
	}
	return o.PartitioningFields
}

func (o *OutputDlS3) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDlS3) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputDlS3) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputDlS3) GetCompress() *CompressionDlS3 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDlS3) GetCompressionLevel() *CompressionLevelDlS3 {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDlS3) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDlS3) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputDlS3) GetParquetVersion() *ParquetVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDlS3) GetParquetDataPageVersion() *DataPageVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDlS3) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDlS3) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDlS3) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDlS3) GetKeyValueMetadata() []KeyValueMetadatumDlS3 {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputDlS3) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDlS3) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDlS3) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDlS3) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDlS3) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputDlS3) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDlS3) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputDlS3) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeCrowdstrikeNextGenSiem string

const (
	TypeCrowdstrikeNextGenSiemCrowdstrikeNextGenSiem TypeCrowdstrikeNextGenSiem = "crowdstrike_next_gen_siem"
)

func (e TypeCrowdstrikeNextGenSiem) ToPointer() *TypeCrowdstrikeNextGenSiem {
	return &e
}
func (e *TypeCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "crowdstrike_next_gen_siem":
		*e = TypeCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCrowdstrikeNextGenSiem: %v", v)
	}
}

type ExtraHTTPHeaderCrowdstrikeNextGenSiem struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderCrowdstrikeNextGenSiem) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderCrowdstrikeNextGenSiem) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeCrowdstrikeNextGenSiem - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeCrowdstrikeNextGenSiem string

const (
	// FailedRequestLoggingModeCrowdstrikeNextGenSiemPayload Payload
	FailedRequestLoggingModeCrowdstrikeNextGenSiemPayload FailedRequestLoggingModeCrowdstrikeNextGenSiem = "payload"
	// FailedRequestLoggingModeCrowdstrikeNextGenSiemPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeCrowdstrikeNextGenSiemPayloadAndHeaders FailedRequestLoggingModeCrowdstrikeNextGenSiem = "payloadAndHeaders"
	// FailedRequestLoggingModeCrowdstrikeNextGenSiemNone None
	FailedRequestLoggingModeCrowdstrikeNextGenSiemNone FailedRequestLoggingModeCrowdstrikeNextGenSiem = "none"
)

func (e FailedRequestLoggingModeCrowdstrikeNextGenSiem) ToPointer() *FailedRequestLoggingModeCrowdstrikeNextGenSiem {
	return &e
}

// RequestFormatCrowdstrikeNextGenSiem - When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
type RequestFormatCrowdstrikeNextGenSiem string

const (
	// RequestFormatCrowdstrikeNextGenSiemJSON JSON
	RequestFormatCrowdstrikeNextGenSiemJSON RequestFormatCrowdstrikeNextGenSiem = "JSON"
	// RequestFormatCrowdstrikeNextGenSiemRaw Raw
	RequestFormatCrowdstrikeNextGenSiemRaw RequestFormatCrowdstrikeNextGenSiem = "raw"
)

func (e RequestFormatCrowdstrikeNextGenSiem) ToPointer() *RequestFormatCrowdstrikeNextGenSiem {
	return &e
}

// AuthenticationMethodCrowdstrikeNextGenSiem - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodCrowdstrikeNextGenSiem string

const (
	AuthenticationMethodCrowdstrikeNextGenSiemManual AuthenticationMethodCrowdstrikeNextGenSiem = "manual"
	AuthenticationMethodCrowdstrikeNextGenSiemSecret AuthenticationMethodCrowdstrikeNextGenSiem = "secret"
)

func (e AuthenticationMethodCrowdstrikeNextGenSiem) ToPointer() *AuthenticationMethodCrowdstrikeNextGenSiem {
	return &e
}

type ResponseRetrySettingCrowdstrikeNextGenSiem struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsCrowdstrikeNextGenSiem struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorCrowdstrikeNextGenSiem - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCrowdstrikeNextGenSiem string

const (
	// BackpressureBehaviorCrowdstrikeNextGenSiemBlock Block
	BackpressureBehaviorCrowdstrikeNextGenSiemBlock BackpressureBehaviorCrowdstrikeNextGenSiem = "block"
	// BackpressureBehaviorCrowdstrikeNextGenSiemDrop Drop
	BackpressureBehaviorCrowdstrikeNextGenSiemDrop BackpressureBehaviorCrowdstrikeNextGenSiem = "drop"
	// BackpressureBehaviorCrowdstrikeNextGenSiemQueue Persistent Queue
	BackpressureBehaviorCrowdstrikeNextGenSiemQueue BackpressureBehaviorCrowdstrikeNextGenSiem = "queue"
)

func (e BackpressureBehaviorCrowdstrikeNextGenSiem) ToPointer() *BackpressureBehaviorCrowdstrikeNextGenSiem {
	return &e
}

// ModeCrowdstrikeNextGenSiem - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeCrowdstrikeNextGenSiem string

const (
	// ModeCrowdstrikeNextGenSiemError Error
	ModeCrowdstrikeNextGenSiemError ModeCrowdstrikeNextGenSiem = "error"
	// ModeCrowdstrikeNextGenSiemAlways Backpressure
	ModeCrowdstrikeNextGenSiemAlways ModeCrowdstrikeNextGenSiem = "always"
	// ModeCrowdstrikeNextGenSiemBackpressure Always On
	ModeCrowdstrikeNextGenSiemBackpressure ModeCrowdstrikeNextGenSiem = "backpressure"
)

func (e ModeCrowdstrikeNextGenSiem) ToPointer() *ModeCrowdstrikeNextGenSiem {
	return &e
}

// CompressionCrowdstrikeNextGenSiem - Codec to use to compress the persisted data
type CompressionCrowdstrikeNextGenSiem string

const (
	// CompressionCrowdstrikeNextGenSiemNone None
	CompressionCrowdstrikeNextGenSiemNone CompressionCrowdstrikeNextGenSiem = "none"
	// CompressionCrowdstrikeNextGenSiemGzip Gzip
	CompressionCrowdstrikeNextGenSiemGzip CompressionCrowdstrikeNextGenSiem = "gzip"
)

func (e CompressionCrowdstrikeNextGenSiem) ToPointer() *CompressionCrowdstrikeNextGenSiem {
	return &e
}

// QueueFullBehaviorCrowdstrikeNextGenSiem - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCrowdstrikeNextGenSiem string

const (
	// QueueFullBehaviorCrowdstrikeNextGenSiemBlock Block
	QueueFullBehaviorCrowdstrikeNextGenSiemBlock QueueFullBehaviorCrowdstrikeNextGenSiem = "block"
	// QueueFullBehaviorCrowdstrikeNextGenSiemDrop Drop new data
	QueueFullBehaviorCrowdstrikeNextGenSiemDrop QueueFullBehaviorCrowdstrikeNextGenSiem = "drop"
)

func (e QueueFullBehaviorCrowdstrikeNextGenSiem) ToPointer() *QueueFullBehaviorCrowdstrikeNextGenSiem {
	return &e
}

type PqControlsCrowdstrikeNextGenSiem struct {
}

func (p PqControlsCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputCrowdstrikeNextGenSiem struct {
	// Unique ID for this output
	ID   *string                    `json:"id,omitempty"`
	Type TypeCrowdstrikeNextGenSiem `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL provided from a CrowdStrike data connector.
	// Example: https://ingest.<region>.crowdstrike.com/api/ingest/hec/<connection-id>/v1/services/collector
	URL string `json:"url"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderCrowdstrikeNextGenSiem `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"true" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeCrowdstrikeNextGenSiem `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
	Format *RequestFormatCrowdstrikeNextGenSiem `default:"JSON" json:"format"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodCrowdstrikeNextGenSiem `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingCrowdstrikeNextGenSiem `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsCrowdstrikeNextGenSiem  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCrowdstrikeNextGenSiem `default:"block" json:"onBackpressure"`
	Description    *string                                     `json:"description,omitempty"`
	Token          *string                                     `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeCrowdstrikeNextGenSiem `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionCrowdstrikeNextGenSiem `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorCrowdstrikeNextGenSiem `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsCrowdstrikeNextGenSiem        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                           `additionalProperties:"true" json:"-"`
}

func (o OutputCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "url"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCrowdstrikeNextGenSiem) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputCrowdstrikeNextGenSiem) GetType() TypeCrowdstrikeNextGenSiem {
	if o == nil {
		return TypeCrowdstrikeNextGenSiem("")
	}
	return o.Type
}

func (o *OutputCrowdstrikeNextGenSiem) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCrowdstrikeNextGenSiem) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCrowdstrikeNextGenSiem) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCrowdstrikeNextGenSiem) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCrowdstrikeNextGenSiem) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputCrowdstrikeNextGenSiem) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputCrowdstrikeNextGenSiem) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputCrowdstrikeNextGenSiem) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputCrowdstrikeNextGenSiem) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputCrowdstrikeNextGenSiem) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCrowdstrikeNextGenSiem) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetExtraHTTPHeaders() []ExtraHTTPHeaderCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputCrowdstrikeNextGenSiem) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputCrowdstrikeNextGenSiem) GetFailedRequestLoggingMode() *FailedRequestLoggingModeCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputCrowdstrikeNextGenSiem) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputCrowdstrikeNextGenSiem) GetFormat() *RequestFormatCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputCrowdstrikeNextGenSiem) GetAuthType() *AuthenticationMethodCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputCrowdstrikeNextGenSiem) GetResponseRetrySettings() []ResponseRetrySettingCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputCrowdstrikeNextGenSiem) GetTimeoutRetrySettings() *TimeoutRetrySettingsCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputCrowdstrikeNextGenSiem) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputCrowdstrikeNextGenSiem) GetOnBackpressure() *BackpressureBehaviorCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCrowdstrikeNextGenSiem) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCrowdstrikeNextGenSiem) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputCrowdstrikeNextGenSiem) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMode() *ModeCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqCompress() *CompressionCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqOnBackpressure() *QueueFullBehaviorCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqControls() *PqControlsCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputCrowdstrikeNextGenSiem) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeHumioHec string

const (
	TypeHumioHecHumioHec TypeHumioHec = "humio_hec"
)

func (e TypeHumioHec) ToPointer() *TypeHumioHec {
	return &e
}
func (e *TypeHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "humio_hec":
		*e = TypeHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeHumioHec: %v", v)
	}
}

type ExtraHTTPHeaderHumioHec struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderHumioHec) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderHumioHec) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeHumioHec - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeHumioHec string

const (
	// FailedRequestLoggingModeHumioHecPayload Payload
	FailedRequestLoggingModeHumioHecPayload FailedRequestLoggingModeHumioHec = "payload"
	// FailedRequestLoggingModeHumioHecPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeHumioHecPayloadAndHeaders FailedRequestLoggingModeHumioHec = "payloadAndHeaders"
	// FailedRequestLoggingModeHumioHecNone None
	FailedRequestLoggingModeHumioHecNone FailedRequestLoggingModeHumioHec = "none"
)

func (e FailedRequestLoggingModeHumioHec) ToPointer() *FailedRequestLoggingModeHumioHec {
	return &e
}

// RequestFormatHumioHec - When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
type RequestFormatHumioHec string

const (
	// RequestFormatHumioHecJSON JSON
	RequestFormatHumioHecJSON RequestFormatHumioHec = "JSON"
	// RequestFormatHumioHecRaw Raw
	RequestFormatHumioHecRaw RequestFormatHumioHec = "raw"
)

func (e RequestFormatHumioHec) ToPointer() *RequestFormatHumioHec {
	return &e
}

// AuthenticationMethodHumioHec - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodHumioHec string

const (
	AuthenticationMethodHumioHecManual AuthenticationMethodHumioHec = "manual"
	AuthenticationMethodHumioHecSecret AuthenticationMethodHumioHec = "secret"
)

func (e AuthenticationMethodHumioHec) ToPointer() *AuthenticationMethodHumioHec {
	return &e
}

type ResponseRetrySettingHumioHec struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingHumioHec) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingHumioHec) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingHumioHec) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingHumioHec) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsHumioHec struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsHumioHec) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsHumioHec) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsHumioHec) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsHumioHec) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorHumioHec - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorHumioHec string

const (
	// BackpressureBehaviorHumioHecBlock Block
	BackpressureBehaviorHumioHecBlock BackpressureBehaviorHumioHec = "block"
	// BackpressureBehaviorHumioHecDrop Drop
	BackpressureBehaviorHumioHecDrop BackpressureBehaviorHumioHec = "drop"
	// BackpressureBehaviorHumioHecQueue Persistent Queue
	BackpressureBehaviorHumioHecQueue BackpressureBehaviorHumioHec = "queue"
)

func (e BackpressureBehaviorHumioHec) ToPointer() *BackpressureBehaviorHumioHec {
	return &e
}

// ModeHumioHec - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeHumioHec string

const (
	// ModeHumioHecError Error
	ModeHumioHecError ModeHumioHec = "error"
	// ModeHumioHecAlways Backpressure
	ModeHumioHecAlways ModeHumioHec = "always"
	// ModeHumioHecBackpressure Always On
	ModeHumioHecBackpressure ModeHumioHec = "backpressure"
)

func (e ModeHumioHec) ToPointer() *ModeHumioHec {
	return &e
}

// CompressionHumioHec - Codec to use to compress the persisted data
type CompressionHumioHec string

const (
	// CompressionHumioHecNone None
	CompressionHumioHecNone CompressionHumioHec = "none"
	// CompressionHumioHecGzip Gzip
	CompressionHumioHecGzip CompressionHumioHec = "gzip"
)

func (e CompressionHumioHec) ToPointer() *CompressionHumioHec {
	return &e
}

// QueueFullBehaviorHumioHec - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorHumioHec string

const (
	// QueueFullBehaviorHumioHecBlock Block
	QueueFullBehaviorHumioHecBlock QueueFullBehaviorHumioHec = "block"
	// QueueFullBehaviorHumioHecDrop Drop new data
	QueueFullBehaviorHumioHecDrop QueueFullBehaviorHumioHec = "drop"
)

func (e QueueFullBehaviorHumioHec) ToPointer() *QueueFullBehaviorHumioHec {
	return &e
}

type PqControlsHumioHec struct {
}

func (p PqControlsHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputHumioHec struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type TypeHumioHec `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL to a CrowdStrike Falcon LogScale endpoint to send events to. Examples: https://cloud.us.humio.com/api/v1/ingest/hec for JSON and https://cloud.us.humio.com/api/v1/ingest/hec/raw for raw
	URL *string `default:"https://cloud.us.humio.com/api/v1/ingest/hec" json:"url"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderHumioHec `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"true" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeHumioHec `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
	Format *RequestFormatHumioHec `default:"JSON" json:"format"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodHumioHec `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingHumioHec `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsHumioHec  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorHumioHec `default:"block" json:"onBackpressure"`
	Description    *string                       `json:"description,omitempty"`
	// CrowdStrike Falcon LogScale authentication token
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeHumioHec `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionHumioHec `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorHumioHec `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsHumioHec        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any             `additionalProperties:"true" json:"-"`
}

func (o OutputHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputHumioHec) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputHumioHec) GetType() TypeHumioHec {
	if o == nil {
		return TypeHumioHec("")
	}
	return o.Type
}

func (o *OutputHumioHec) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputHumioHec) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputHumioHec) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputHumioHec) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputHumioHec) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputHumioHec) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputHumioHec) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputHumioHec) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputHumioHec) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputHumioHec) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputHumioHec) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputHumioHec) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputHumioHec) GetExtraHTTPHeaders() []ExtraHTTPHeaderHumioHec {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputHumioHec) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputHumioHec) GetFailedRequestLoggingMode() *FailedRequestLoggingModeHumioHec {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputHumioHec) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputHumioHec) GetFormat() *RequestFormatHumioHec {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputHumioHec) GetAuthType() *AuthenticationMethodHumioHec {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputHumioHec) GetResponseRetrySettings() []ResponseRetrySettingHumioHec {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputHumioHec) GetTimeoutRetrySettings() *TimeoutRetrySettingsHumioHec {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputHumioHec) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputHumioHec) GetOnBackpressure() *BackpressureBehaviorHumioHec {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputHumioHec) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputHumioHec) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputHumioHec) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputHumioHec) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputHumioHec) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputHumioHec) GetPqMode() *ModeHumioHec {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputHumioHec) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputHumioHec) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputHumioHec) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputHumioHec) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputHumioHec) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputHumioHec) GetPqCompress() *CompressionHumioHec {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputHumioHec) GetPqOnBackpressure() *QueueFullBehaviorHumioHec {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputHumioHec) GetPqControls() *PqControlsHumioHec {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputHumioHec) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeCriblHTTP string

const (
	OutputTypeCriblHTTPCriblHTTP OutputTypeCriblHTTP = "cribl_http"
)

func (e OutputTypeCriblHTTP) ToPointer() *OutputTypeCriblHTTP {
	return &e
}
func (e *OutputTypeCriblHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cribl_http":
		*e = OutputTypeCriblHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeCriblHTTP: %v", v)
	}
}

type OutputMinimumTLSVersionCriblHTTP string

const (
	OutputMinimumTLSVersionCriblHTTPTlSv1  OutputMinimumTLSVersionCriblHTTP = "TLSv1"
	OutputMinimumTLSVersionCriblHTTPTlSv11 OutputMinimumTLSVersionCriblHTTP = "TLSv1.1"
	OutputMinimumTLSVersionCriblHTTPTlSv12 OutputMinimumTLSVersionCriblHTTP = "TLSv1.2"
	OutputMinimumTLSVersionCriblHTTPTlSv13 OutputMinimumTLSVersionCriblHTTP = "TLSv1.3"
)

func (e OutputMinimumTLSVersionCriblHTTP) ToPointer() *OutputMinimumTLSVersionCriblHTTP {
	return &e
}

type OutputMaximumTLSVersionCriblHTTP string

const (
	OutputMaximumTLSVersionCriblHTTPTlSv1  OutputMaximumTLSVersionCriblHTTP = "TLSv1"
	OutputMaximumTLSVersionCriblHTTPTlSv11 OutputMaximumTLSVersionCriblHTTP = "TLSv1.1"
	OutputMaximumTLSVersionCriblHTTPTlSv12 OutputMaximumTLSVersionCriblHTTP = "TLSv1.2"
	OutputMaximumTLSVersionCriblHTTPTlSv13 OutputMaximumTLSVersionCriblHTTP = "TLSv1.3"
)

func (e OutputMaximumTLSVersionCriblHTTP) ToPointer() *OutputMaximumTLSVersionCriblHTTP {
	return &e
}

type TLSSettingsClientSideCriblHTTP struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                           `json:"passphrase,omitempty"`
	MinVersion *OutputMinimumTLSVersionCriblHTTP `json:"minVersion,omitempty"`
	MaxVersion *OutputMaximumTLSVersionCriblHTTP `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideCriblHTTP) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideCriblHTTP) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideCriblHTTP) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideCriblHTTP) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideCriblHTTP) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideCriblHTTP) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideCriblHTTP) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideCriblHTTP) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideCriblHTTP) GetMinVersion() *OutputMinimumTLSVersionCriblHTTP {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideCriblHTTP) GetMaxVersion() *OutputMaximumTLSVersionCriblHTTP {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// OutputCompressionCriblHTTP - Codec to use to compress the data before sending
type OutputCompressionCriblHTTP string

const (
	// OutputCompressionCriblHTTPNone None
	OutputCompressionCriblHTTPNone OutputCompressionCriblHTTP = "none"
	// OutputCompressionCriblHTTPGzip Gzip
	OutputCompressionCriblHTTPGzip OutputCompressionCriblHTTP = "gzip"
)

func (e OutputCompressionCriblHTTP) ToPointer() *OutputCompressionCriblHTTP {
	return &e
}

type ExtraHTTPHeaderCriblHTTP struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderCriblHTTP) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderCriblHTTP) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeCriblHTTP - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeCriblHTTP string

const (
	// FailedRequestLoggingModeCriblHTTPPayload Payload
	FailedRequestLoggingModeCriblHTTPPayload FailedRequestLoggingModeCriblHTTP = "payload"
	// FailedRequestLoggingModeCriblHTTPPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeCriblHTTPPayloadAndHeaders FailedRequestLoggingModeCriblHTTP = "payloadAndHeaders"
	// FailedRequestLoggingModeCriblHTTPNone None
	FailedRequestLoggingModeCriblHTTPNone FailedRequestLoggingModeCriblHTTP = "none"
)

func (e FailedRequestLoggingModeCriblHTTP) ToPointer() *FailedRequestLoggingModeCriblHTTP {
	return &e
}

type ResponseRetrySettingCriblHTTP struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingCriblHTTP) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingCriblHTTP) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingCriblHTTP) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingCriblHTTP) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsCriblHTTP struct {
	TimeoutRetry *bool `default:"true" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsCriblHTTP) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsCriblHTTP) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsCriblHTTP) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsCriblHTTP) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type OutputAuthTokenCriblHTTP struct {
	// Select or create a stored text secret
	TokenSecret string  `json:"tokenSecret"`
	Enabled     *bool   `default:"true" json:"enabled"`
	Description *string `json:"description,omitempty"`
}

func (o OutputAuthTokenCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthTokenCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"tokenSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthTokenCriblHTTP) GetTokenSecret() string {
	if o == nil {
		return ""
	}
	return o.TokenSecret
}

func (o *OutputAuthTokenCriblHTTP) GetEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.Enabled
}

func (o *OutputAuthTokenCriblHTTP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

// BackpressureBehaviorCriblHTTP - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCriblHTTP string

const (
	// BackpressureBehaviorCriblHTTPBlock Block
	BackpressureBehaviorCriblHTTPBlock BackpressureBehaviorCriblHTTP = "block"
	// BackpressureBehaviorCriblHTTPDrop Drop
	BackpressureBehaviorCriblHTTPDrop BackpressureBehaviorCriblHTTP = "drop"
	// BackpressureBehaviorCriblHTTPQueue Persistent Queue
	BackpressureBehaviorCriblHTTPQueue BackpressureBehaviorCriblHTTP = "queue"
)

func (e BackpressureBehaviorCriblHTTP) ToPointer() *BackpressureBehaviorCriblHTTP {
	return &e
}

type URLCriblHTTP struct {
	// URL of a Cribl Worker to send events to, such as http://localhost:10200
	URL string `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, []string{"url"}); err != nil {
		return err
	}
	return nil
}

func (u *URLCriblHTTP) GetURL() string {
	if u == nil {
		return ""
	}
	return u.URL
}

func (u *URLCriblHTTP) GetWeight() *float64 {
	if u == nil {
		return nil
	}
	return u.Weight
}

// OutputModeCriblHTTP - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeCriblHTTP string

const (
	// OutputModeCriblHTTPError Error
	OutputModeCriblHTTPError OutputModeCriblHTTP = "error"
	// OutputModeCriblHTTPAlways Backpressure
	OutputModeCriblHTTPAlways OutputModeCriblHTTP = "always"
	// OutputModeCriblHTTPBackpressure Always On
	OutputModeCriblHTTPBackpressure OutputModeCriblHTTP = "backpressure"
)

func (e OutputModeCriblHTTP) ToPointer() *OutputModeCriblHTTP {
	return &e
}

// PqCompressCompressionCriblHTTP - Codec to use to compress the persisted data
type PqCompressCompressionCriblHTTP string

const (
	// PqCompressCompressionCriblHTTPNone None
	PqCompressCompressionCriblHTTPNone PqCompressCompressionCriblHTTP = "none"
	// PqCompressCompressionCriblHTTPGzip Gzip
	PqCompressCompressionCriblHTTPGzip PqCompressCompressionCriblHTTP = "gzip"
)

func (e PqCompressCompressionCriblHTTP) ToPointer() *PqCompressCompressionCriblHTTP {
	return &e
}

// QueueFullBehaviorCriblHTTP - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCriblHTTP string

const (
	// QueueFullBehaviorCriblHTTPBlock Block
	QueueFullBehaviorCriblHTTPBlock QueueFullBehaviorCriblHTTP = "block"
	// QueueFullBehaviorCriblHTTPDrop Drop new data
	QueueFullBehaviorCriblHTTPDrop QueueFullBehaviorCriblHTTP = "drop"
)

func (e QueueFullBehaviorCriblHTTP) ToPointer() *QueueFullBehaviorCriblHTTP {
	return &e
}

type OutputPqControlsCriblHTTP struct {
}

func (o OutputPqControlsCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputCriblHTTP struct {
	// Unique ID for this output
	ID   *string             `json:"id,omitempty"`
	Type OutputTypeCriblHTTP `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs. If this setting is disabled, consider enabling round-robin DNS.
	LoadBalanced *bool                           `default:"true" json:"loadBalanced"`
	TLS          *TLSSettingsClientSideCriblHTTP `json:"tls,omitempty"`
	// The number of minutes before the internally generated authentication token expires. Valid values are between 1 and 60.
	TokenTTLMinutes *float64 `default:"60" json:"tokenTTLMinutes"`
	// Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.
	ExcludeFields []string `json:"excludeFields,omitempty"`
	// Codec to use to compress the data before sending
	Compression *OutputCompressionCriblHTTP `default:"gzip" json:"compression"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderCriblHTTP `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeCriblHTTP `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingCriblHTTP `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsCriblHTTP  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl HTTP Source in Cribl.Cloud.
	AuthTokens []OutputAuthTokenCriblHTTP `json:"authTokens,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCriblHTTP `default:"block" json:"onBackpressure"`
	Description    *string                        `json:"description,omitempty"`
	// URL of a Cribl Worker to send events to, such as http://localhost:10200
	URL *string `json:"url,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool          `default:"false" json:"excludeSelf"`
	Urls        []URLCriblHTTP `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeCriblHTTP `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionCriblHTTP `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorCriblHTTP `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsCriblHTTP  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any              `additionalProperties:"true" json:"-"`
}

func (o OutputCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCriblHTTP) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputCriblHTTP) GetType() OutputTypeCriblHTTP {
	if o == nil {
		return OutputTypeCriblHTTP("")
	}
	return o.Type
}

func (o *OutputCriblHTTP) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCriblHTTP) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCriblHTTP) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCriblHTTP) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCriblHTTP) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputCriblHTTP) GetTLS() *TLSSettingsClientSideCriblHTTP {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputCriblHTTP) GetTokenTTLMinutes() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTTLMinutes
}

func (o *OutputCriblHTTP) GetExcludeFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeFields
}

func (o *OutputCriblHTTP) GetCompression() *OutputCompressionCriblHTTP {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputCriblHTTP) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputCriblHTTP) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputCriblHTTP) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputCriblHTTP) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCriblHTTP) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputCriblHTTP) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCriblHTTP) GetExtraHTTPHeaders() []ExtraHTTPHeaderCriblHTTP {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputCriblHTTP) GetFailedRequestLoggingMode() *FailedRequestLoggingModeCriblHTTP {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputCriblHTTP) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputCriblHTTP) GetResponseRetrySettings() []ResponseRetrySettingCriblHTTP {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputCriblHTTP) GetTimeoutRetrySettings() *TimeoutRetrySettingsCriblHTTP {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputCriblHTTP) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputCriblHTTP) GetAuthTokens() []OutputAuthTokenCriblHTTP {
	if o == nil {
		return nil
	}
	return o.AuthTokens
}

func (o *OutputCriblHTTP) GetOnBackpressure() *BackpressureBehaviorCriblHTTP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCriblHTTP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCriblHTTP) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputCriblHTTP) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputCriblHTTP) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputCriblHTTP) GetUrls() []URLCriblHTTP {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputCriblHTTP) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputCriblHTTP) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputCriblHTTP) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputCriblHTTP) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputCriblHTTP) GetPqMode() *OutputModeCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCriblHTTP) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputCriblHTTP) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputCriblHTTP) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCriblHTTP) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCriblHTTP) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCriblHTTP) GetPqCompress() *PqCompressCompressionCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCriblHTTP) GetPqOnBackpressure() *QueueFullBehaviorCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCriblHTTP) GetPqControls() *OutputPqControlsCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputCriblHTTP) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeCriblTCP string

const (
	OutputTypeCriblTCPCriblTCP OutputTypeCriblTCP = "cribl_tcp"
)

func (e OutputTypeCriblTCP) ToPointer() *OutputTypeCriblTCP {
	return &e
}
func (e *OutputTypeCriblTCP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cribl_tcp":
		*e = OutputTypeCriblTCP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeCriblTCP: %v", v)
	}
}

// OutputCompressionCriblTCP - Codec to use to compress the data before sending
type OutputCompressionCriblTCP string

const (
	// OutputCompressionCriblTCPNone None
	OutputCompressionCriblTCPNone OutputCompressionCriblTCP = "none"
	// OutputCompressionCriblTCPGzip Gzip
	OutputCompressionCriblTCPGzip OutputCompressionCriblTCP = "gzip"
)

func (e OutputCompressionCriblTCP) ToPointer() *OutputCompressionCriblTCP {
	return &e
}

type OutputMinimumTLSVersionCriblTCP string

const (
	OutputMinimumTLSVersionCriblTCPTlSv1  OutputMinimumTLSVersionCriblTCP = "TLSv1"
	OutputMinimumTLSVersionCriblTCPTlSv11 OutputMinimumTLSVersionCriblTCP = "TLSv1.1"
	OutputMinimumTLSVersionCriblTCPTlSv12 OutputMinimumTLSVersionCriblTCP = "TLSv1.2"
	OutputMinimumTLSVersionCriblTCPTlSv13 OutputMinimumTLSVersionCriblTCP = "TLSv1.3"
)

func (e OutputMinimumTLSVersionCriblTCP) ToPointer() *OutputMinimumTLSVersionCriblTCP {
	return &e
}

type OutputMaximumTLSVersionCriblTCP string

const (
	OutputMaximumTLSVersionCriblTCPTlSv1  OutputMaximumTLSVersionCriblTCP = "TLSv1"
	OutputMaximumTLSVersionCriblTCPTlSv11 OutputMaximumTLSVersionCriblTCP = "TLSv1.1"
	OutputMaximumTLSVersionCriblTCPTlSv12 OutputMaximumTLSVersionCriblTCP = "TLSv1.2"
	OutputMaximumTLSVersionCriblTCPTlSv13 OutputMaximumTLSVersionCriblTCP = "TLSv1.3"
)

func (e OutputMaximumTLSVersionCriblTCP) ToPointer() *OutputMaximumTLSVersionCriblTCP {
	return &e
}

type TLSSettingsClientSideCriblTCP struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                          `json:"passphrase,omitempty"`
	MinVersion *OutputMinimumTLSVersionCriblTCP `json:"minVersion,omitempty"`
	MaxVersion *OutputMaximumTLSVersionCriblTCP `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideCriblTCP) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideCriblTCP) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideCriblTCP) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideCriblTCP) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideCriblTCP) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideCriblTCP) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideCriblTCP) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideCriblTCP) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideCriblTCP) GetMinVersion() *OutputMinimumTLSVersionCriblTCP {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideCriblTCP) GetMaxVersion() *OutputMaximumTLSVersionCriblTCP {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

type OutputAuthTokenCriblTCP struct {
	// Select or create a stored text secret
	TokenSecret string `json:"tokenSecret"`
	Enabled     *bool  `default:"true" json:"enabled"`
	// Optional token description
	Description *string `json:"description,omitempty"`
}

func (o OutputAuthTokenCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthTokenCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"tokenSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthTokenCriblTCP) GetTokenSecret() string {
	if o == nil {
		return ""
	}
	return o.TokenSecret
}

func (o *OutputAuthTokenCriblTCP) GetEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.Enabled
}

func (o *OutputAuthTokenCriblTCP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

// BackpressureBehaviorCriblTCP - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCriblTCP string

const (
	// BackpressureBehaviorCriblTCPBlock Block
	BackpressureBehaviorCriblTCPBlock BackpressureBehaviorCriblTCP = "block"
	// BackpressureBehaviorCriblTCPDrop Drop
	BackpressureBehaviorCriblTCPDrop BackpressureBehaviorCriblTCP = "drop"
	// BackpressureBehaviorCriblTCPQueue Persistent Queue
	BackpressureBehaviorCriblTCPQueue BackpressureBehaviorCriblTCP = "queue"
)

func (e BackpressureBehaviorCriblTCP) ToPointer() *BackpressureBehaviorCriblTCP {
	return &e
}

// TLSCriblTCP - Whether to inherit TLS configs from group setting or disable TLS
type TLSCriblTCP string

const (
	TLSCriblTCPInherit TLSCriblTCP = "inherit"
	TLSCriblTCPOff     TLSCriblTCP = "off"
)

func (e TLSCriblTCP) ToPointer() *TLSCriblTCP {
	return &e
}

type HostCriblTCP struct {
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port *float64 `default:"10300" json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS
	TLS *TLSCriblTCP `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host"}); err != nil {
		return err
	}
	return nil
}

func (h *HostCriblTCP) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostCriblTCP) GetPort() *float64 {
	if h == nil {
		return nil
	}
	return h.Port
}

func (h *HostCriblTCP) GetTLS() *TLSCriblTCP {
	if h == nil {
		return nil
	}
	return h.TLS
}

func (h *HostCriblTCP) GetServername() *string {
	if h == nil {
		return nil
	}
	return h.Servername
}

func (h *HostCriblTCP) GetWeight() *float64 {
	if h == nil {
		return nil
	}
	return h.Weight
}

// OutputModeCriblTCP - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeCriblTCP string

const (
	// OutputModeCriblTCPError Error
	OutputModeCriblTCPError OutputModeCriblTCP = "error"
	// OutputModeCriblTCPAlways Backpressure
	OutputModeCriblTCPAlways OutputModeCriblTCP = "always"
	// OutputModeCriblTCPBackpressure Always On
	OutputModeCriblTCPBackpressure OutputModeCriblTCP = "backpressure"
)

func (e OutputModeCriblTCP) ToPointer() *OutputModeCriblTCP {
	return &e
}

// PqCompressCompressionCriblTCP - Codec to use to compress the persisted data
type PqCompressCompressionCriblTCP string

const (
	// PqCompressCompressionCriblTCPNone None
	PqCompressCompressionCriblTCPNone PqCompressCompressionCriblTCP = "none"
	// PqCompressCompressionCriblTCPGzip Gzip
	PqCompressCompressionCriblTCPGzip PqCompressCompressionCriblTCP = "gzip"
)

func (e PqCompressCompressionCriblTCP) ToPointer() *PqCompressCompressionCriblTCP {
	return &e
}

// QueueFullBehaviorCriblTCP - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCriblTCP string

const (
	// QueueFullBehaviorCriblTCPBlock Block
	QueueFullBehaviorCriblTCPBlock QueueFullBehaviorCriblTCP = "block"
	// QueueFullBehaviorCriblTCPDrop Drop new data
	QueueFullBehaviorCriblTCPDrop QueueFullBehaviorCriblTCP = "drop"
)

func (e QueueFullBehaviorCriblTCP) ToPointer() *QueueFullBehaviorCriblTCP {
	return &e
}

type OutputPqControlsCriblTCP struct {
}

func (o OutputPqControlsCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputCriblTCP struct {
	// Unique ID for this output
	ID   *string            `json:"id,omitempty"`
	Type OutputTypeCriblTCP `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Use load-balanced destinations
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// Codec to use to compress the data before sending
	Compression *OutputCompressionCriblTCP `default:"gzip" json:"compression"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string                        `default:"0" json:"throttleRatePerSec"`
	TLS                *TLSSettingsClientSideCriblTCP `json:"tls,omitempty"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// The number of minutes before the internally generated authentication token expires, valid values between 1 and 60
	TokenTTLMinutes *float64 `default:"60" json:"tokenTTLMinutes"`
	// Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl TCP Source in Cribl.Cloud.
	AuthTokens []OutputAuthTokenCriblTCP `json:"authTokens,omitempty"`
	// Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.
	ExcludeFields []string `json:"excludeFields,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCriblTCP `default:"block" json:"onBackpressure"`
	Description    *string                       `json:"description,omitempty"`
	// The hostname of the receiver
	Host *string `json:"host,omitempty"`
	// The port to connect to on the provided host
	Port *float64 `default:"10300" json:"port"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of hosts to load-balance data to
	Hosts []HostCriblTCP `json:"hosts,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeCriblTCP `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionCriblTCP `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorCriblTCP `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsCriblTCP  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any             `additionalProperties:"true" json:"-"`
}

func (o OutputCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCriblTCP) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputCriblTCP) GetType() OutputTypeCriblTCP {
	if o == nil {
		return OutputTypeCriblTCP("")
	}
	return o.Type
}

func (o *OutputCriblTCP) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCriblTCP) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCriblTCP) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCriblTCP) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCriblTCP) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputCriblTCP) GetCompression() *OutputCompressionCriblTCP {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputCriblTCP) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputCriblTCP) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputCriblTCP) GetTLS() *TLSSettingsClientSideCriblTCP {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputCriblTCP) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputCriblTCP) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputCriblTCP) GetTokenTTLMinutes() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTTLMinutes
}

func (o *OutputCriblTCP) GetAuthTokens() []OutputAuthTokenCriblTCP {
	if o == nil {
		return nil
	}
	return o.AuthTokens
}

func (o *OutputCriblTCP) GetExcludeFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeFields
}

func (o *OutputCriblTCP) GetOnBackpressure() *BackpressureBehaviorCriblTCP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCriblTCP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCriblTCP) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputCriblTCP) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputCriblTCP) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputCriblTCP) GetHosts() []HostCriblTCP {
	if o == nil {
		return nil
	}
	return o.Hosts
}

func (o *OutputCriblTCP) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputCriblTCP) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputCriblTCP) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputCriblTCP) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputCriblTCP) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputCriblTCP) GetPqMode() *OutputModeCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCriblTCP) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputCriblTCP) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputCriblTCP) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCriblTCP) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCriblTCP) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCriblTCP) GetPqCompress() *PqCompressCompressionCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCriblTCP) GetPqOnBackpressure() *QueueFullBehaviorCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCriblTCP) GetPqControls() *OutputPqControlsCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputCriblTCP) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeDataset string

const (
	TypeDatasetDataset TypeDataset = "dataset"
)

func (e TypeDataset) ToPointer() *TypeDataset {
	return &e
}
func (e *TypeDataset) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dataset":
		*e = TypeDataset(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDataset: %v", v)
	}
}

// DefaultSeveritySeverity - Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.
type DefaultSeveritySeverity string

const (
	// DefaultSeveritySeverityFinest 0 - finest
	DefaultSeveritySeverityFinest DefaultSeveritySeverity = "finest"
	// DefaultSeveritySeverityFiner 1 - finer
	DefaultSeveritySeverityFiner DefaultSeveritySeverity = "finer"
	// DefaultSeveritySeverityFine 2 - fine
	DefaultSeveritySeverityFine DefaultSeveritySeverity = "fine"
	// DefaultSeveritySeverityInfo 3 - info
	DefaultSeveritySeverityInfo DefaultSeveritySeverity = "info"
	// DefaultSeveritySeverityWarning 4 - warning
	DefaultSeveritySeverityWarning DefaultSeveritySeverity = "warning"
	// DefaultSeveritySeverityError 5 - error
	DefaultSeveritySeverityError DefaultSeveritySeverity = "error"
	// DefaultSeveritySeverityFatal 6 - fatal
	DefaultSeveritySeverityFatal DefaultSeveritySeverity = "fatal"
)

func (e DefaultSeveritySeverity) ToPointer() *DefaultSeveritySeverity {
	return &e
}

type ResponseRetrySettingDataset struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingDataset) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingDataset) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingDataset) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingDataset) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsDataset struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsDataset) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsDataset) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsDataset) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsDataset) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// DataSetSite - DataSet site to which events should be sent
type DataSetSite string

const (
	// DataSetSiteUs US
	DataSetSiteUs DataSetSite = "us"
	// DataSetSiteEu Europe
	DataSetSiteEu DataSetSite = "eu"
	// DataSetSiteCustom Custom
	DataSetSiteCustom DataSetSite = "custom"
)

func (e DataSetSite) ToPointer() *DataSetSite {
	return &e
}

type ExtraHTTPHeaderDataset struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderDataset) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderDataset) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeDataset - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDataset string

const (
	// FailedRequestLoggingModeDatasetPayload Payload
	FailedRequestLoggingModeDatasetPayload FailedRequestLoggingModeDataset = "payload"
	// FailedRequestLoggingModeDatasetPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeDatasetPayloadAndHeaders FailedRequestLoggingModeDataset = "payloadAndHeaders"
	// FailedRequestLoggingModeDatasetNone None
	FailedRequestLoggingModeDatasetNone FailedRequestLoggingModeDataset = "none"
)

func (e FailedRequestLoggingModeDataset) ToPointer() *FailedRequestLoggingModeDataset {
	return &e
}

// BackpressureBehaviorDataset - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDataset string

const (
	// BackpressureBehaviorDatasetBlock Block
	BackpressureBehaviorDatasetBlock BackpressureBehaviorDataset = "block"
	// BackpressureBehaviorDatasetDrop Drop
	BackpressureBehaviorDatasetDrop BackpressureBehaviorDataset = "drop"
	// BackpressureBehaviorDatasetQueue Persistent Queue
	BackpressureBehaviorDatasetQueue BackpressureBehaviorDataset = "queue"
)

func (e BackpressureBehaviorDataset) ToPointer() *BackpressureBehaviorDataset {
	return &e
}

// AuthenticationMethodDataset - Enter API key directly, or select a stored secret
type AuthenticationMethodDataset string

const (
	AuthenticationMethodDatasetManual AuthenticationMethodDataset = "manual"
	AuthenticationMethodDatasetSecret AuthenticationMethodDataset = "secret"
)

func (e AuthenticationMethodDataset) ToPointer() *AuthenticationMethodDataset {
	return &e
}

// ModeDataset - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeDataset string

const (
	// ModeDatasetError Error
	ModeDatasetError ModeDataset = "error"
	// ModeDatasetAlways Backpressure
	ModeDatasetAlways ModeDataset = "always"
	// ModeDatasetBackpressure Always On
	ModeDatasetBackpressure ModeDataset = "backpressure"
)

func (e ModeDataset) ToPointer() *ModeDataset {
	return &e
}

// CompressionDataset - Codec to use to compress the persisted data
type CompressionDataset string

const (
	// CompressionDatasetNone None
	CompressionDatasetNone CompressionDataset = "none"
	// CompressionDatasetGzip Gzip
	CompressionDatasetGzip CompressionDataset = "gzip"
)

func (e CompressionDataset) ToPointer() *CompressionDataset {
	return &e
}

// QueueFullBehaviorDataset - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDataset string

const (
	// QueueFullBehaviorDatasetBlock Block
	QueueFullBehaviorDatasetBlock QueueFullBehaviorDataset = "block"
	// QueueFullBehaviorDatasetDrop Drop new data
	QueueFullBehaviorDatasetDrop QueueFullBehaviorDataset = "drop"
)

func (e QueueFullBehaviorDataset) ToPointer() *QueueFullBehaviorDataset {
	return &e
}

type PqControlsDataset struct {
}

func (p PqControlsDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputDataset struct {
	// Unique ID for this output
	ID   *string     `json:"id,omitempty"`
	Type TypeDataset `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the event field that contains the message or attributes to send. If not specified, all of the event's non-internal fields will be sent as attributes.
	MessageField *string `json:"messageField,omitempty"`
	// Fields to exclude from the event if the Message field is either unspecified or refers to an object. Ignored if the Message field is a string. If empty, we send all non-internal fields.
	ExcludeFields []string `json:"excludeFields,omitempty"`
	// Name of the event field that contains the `serverHost` identifier. If not specified, defaults to `cribl_<outputId>`.
	ServerHostField *string `json:"serverHostField,omitempty"`
	// Name of the event field that contains the timestamp. If not specified, defaults to `ts`, `_time`, or `Date.now()`, in that order.
	TimestampField *string `json:"timestampField,omitempty"`
	// Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.
	DefaultSeverity *DefaultSeveritySeverity `default:"info" json:"defaultSeverity"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingDataset `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDataset  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// DataSet site to which events should be sent
	Site *DataSetSite `default:"us" json:"site"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderDataset `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDataset `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDataset `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType *AuthenticationMethodDataset `default:"manual" json:"authType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	CustomURL          *string  `json:"customUrl,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeDataset `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionDataset `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorDataset `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsDataset        `json:"pqControls,omitempty"`
	// A 'Log Write Access' API key for the DataSet account
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret           *string        `json:"textSecret,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDataset) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDataset) GetType() TypeDataset {
	if o == nil {
		return TypeDataset("")
	}
	return o.Type
}

func (o *OutputDataset) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDataset) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDataset) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDataset) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDataset) GetMessageField() *string {
	if o == nil {
		return nil
	}
	return o.MessageField
}

func (o *OutputDataset) GetExcludeFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeFields
}

func (o *OutputDataset) GetServerHostField() *string {
	if o == nil {
		return nil
	}
	return o.ServerHostField
}

func (o *OutputDataset) GetTimestampField() *string {
	if o == nil {
		return nil
	}
	return o.TimestampField
}

func (o *OutputDataset) GetDefaultSeverity() *DefaultSeveritySeverity {
	if o == nil {
		return nil
	}
	return o.DefaultSeverity
}

func (o *OutputDataset) GetResponseRetrySettings() []ResponseRetrySettingDataset {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDataset) GetTimeoutRetrySettings() *TimeoutRetrySettingsDataset {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDataset) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDataset) GetSite() *DataSetSite {
	if o == nil {
		return nil
	}
	return o.Site
}

func (o *OutputDataset) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDataset) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDataset) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputDataset) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDataset) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDataset) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDataset) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDataset) GetExtraHTTPHeaders() []ExtraHTTPHeaderDataset {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDataset) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDataset) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDataset {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDataset) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDataset) GetOnBackpressure() *BackpressureBehaviorDataset {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDataset) GetAuthType() *AuthenticationMethodDataset {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputDataset) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputDataset) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDataset) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputDataset) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputDataset) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputDataset) GetPqMode() *ModeDataset {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDataset) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputDataset) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputDataset) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDataset) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDataset) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDataset) GetPqCompress() *CompressionDataset {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDataset) GetPqOnBackpressure() *QueueFullBehaviorDataset {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDataset) GetPqControls() *PqControlsDataset {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDataset) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputDataset) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputDataset) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeServiceNow string

const (
	TypeServiceNowServiceNow TypeServiceNow = "service_now"
)

func (e TypeServiceNow) ToPointer() *TypeServiceNow {
	return &e
}
func (e *TypeServiceNow) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "service_now":
		*e = TypeServiceNow(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeServiceNow: %v", v)
	}
}

// OTLPVersionServiceNow - The version of OTLP Protobuf definitions to use when structuring data to send
type OTLPVersionServiceNow string

const (
	// OTLPVersionServiceNowOneDot3Dot1 1.3.1
	OTLPVersionServiceNowOneDot3Dot1 OTLPVersionServiceNow = "1.3.1"
)

func (e OTLPVersionServiceNow) ToPointer() *OTLPVersionServiceNow {
	return &e
}

// ProtocolServiceNow - Select a transport option for OpenTelemetry
type ProtocolServiceNow string

const (
	// ProtocolServiceNowGrpc gRPC
	ProtocolServiceNowGrpc ProtocolServiceNow = "grpc"
	// ProtocolServiceNowHTTP HTTP
	ProtocolServiceNowHTTP ProtocolServiceNow = "http"
)

func (e ProtocolServiceNow) ToPointer() *ProtocolServiceNow {
	return &e
}

// CompressCompressionServiceNow - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type CompressCompressionServiceNow string

const (
	// CompressCompressionServiceNowNone None
	CompressCompressionServiceNowNone CompressCompressionServiceNow = "none"
	// CompressCompressionServiceNowDeflate Deflate
	CompressCompressionServiceNowDeflate CompressCompressionServiceNow = "deflate"
	// CompressCompressionServiceNowGzip Gzip
	CompressCompressionServiceNowGzip CompressCompressionServiceNow = "gzip"
)

func (e CompressCompressionServiceNow) ToPointer() *CompressCompressionServiceNow {
	return &e
}

// HTTPCompressCompressionServiceNow - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompressionServiceNow string

const (
	// HTTPCompressCompressionServiceNowNone None
	HTTPCompressCompressionServiceNowNone HTTPCompressCompressionServiceNow = "none"
	// HTTPCompressCompressionServiceNowGzip Gzip
	HTTPCompressCompressionServiceNowGzip HTTPCompressCompressionServiceNow = "gzip"
)

func (e HTTPCompressCompressionServiceNow) ToPointer() *HTTPCompressCompressionServiceNow {
	return &e
}

type MetadatumServiceNow struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (m MetadatumServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(m, "", false)
}

func (m *MetadatumServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &m, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (m *MetadatumServiceNow) GetKey() *string {
	if m == nil {
		return nil
	}
	return m.Key
}

func (m *MetadatumServiceNow) GetValue() string {
	if m == nil {
		return ""
	}
	return m.Value
}

// FailedRequestLoggingModeServiceNow - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeServiceNow string

const (
	// FailedRequestLoggingModeServiceNowPayload Payload
	FailedRequestLoggingModeServiceNowPayload FailedRequestLoggingModeServiceNow = "payload"
	// FailedRequestLoggingModeServiceNowPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeServiceNowPayloadAndHeaders FailedRequestLoggingModeServiceNow = "payloadAndHeaders"
	// FailedRequestLoggingModeServiceNowNone None
	FailedRequestLoggingModeServiceNowNone FailedRequestLoggingModeServiceNow = "none"
)

func (e FailedRequestLoggingModeServiceNow) ToPointer() *FailedRequestLoggingModeServiceNow {
	return &e
}

// BackpressureBehaviorServiceNow - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorServiceNow string

const (
	// BackpressureBehaviorServiceNowBlock Block
	BackpressureBehaviorServiceNowBlock BackpressureBehaviorServiceNow = "block"
	// BackpressureBehaviorServiceNowDrop Drop
	BackpressureBehaviorServiceNowDrop BackpressureBehaviorServiceNow = "drop"
	// BackpressureBehaviorServiceNowQueue Persistent Queue
	BackpressureBehaviorServiceNowQueue BackpressureBehaviorServiceNow = "queue"
)

func (e BackpressureBehaviorServiceNow) ToPointer() *BackpressureBehaviorServiceNow {
	return &e
}

type ExtraHTTPHeaderServiceNow struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderServiceNow) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderServiceNow) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

type ResponseRetrySettingServiceNow struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingServiceNow) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingServiceNow) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingServiceNow) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingServiceNow) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsServiceNow struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsServiceNow) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsServiceNow) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsServiceNow) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsServiceNow) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type MinimumTLSVersionServiceNow string

const (
	MinimumTLSVersionServiceNowTlSv1  MinimumTLSVersionServiceNow = "TLSv1"
	MinimumTLSVersionServiceNowTlSv11 MinimumTLSVersionServiceNow = "TLSv1.1"
	MinimumTLSVersionServiceNowTlSv12 MinimumTLSVersionServiceNow = "TLSv1.2"
	MinimumTLSVersionServiceNowTlSv13 MinimumTLSVersionServiceNow = "TLSv1.3"
)

func (e MinimumTLSVersionServiceNow) ToPointer() *MinimumTLSVersionServiceNow {
	return &e
}

type MaximumTLSVersionServiceNow string

const (
	MaximumTLSVersionServiceNowTlSv1  MaximumTLSVersionServiceNow = "TLSv1"
	MaximumTLSVersionServiceNowTlSv11 MaximumTLSVersionServiceNow = "TLSv1.1"
	MaximumTLSVersionServiceNowTlSv12 MaximumTLSVersionServiceNow = "TLSv1.2"
	MaximumTLSVersionServiceNowTlSv13 MaximumTLSVersionServiceNow = "TLSv1.3"
)

func (e MaximumTLSVersionServiceNow) ToPointer() *MaximumTLSVersionServiceNow {
	return &e
}

type TLSSettingsClientSideServiceNow struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                      `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionServiceNow `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionServiceNow `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideServiceNow) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideServiceNow) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideServiceNow) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideServiceNow) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideServiceNow) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideServiceNow) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideServiceNow) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideServiceNow) GetMinVersion() *MinimumTLSVersionServiceNow {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideServiceNow) GetMaxVersion() *MaximumTLSVersionServiceNow {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// ModeServiceNow - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeServiceNow string

const (
	// ModeServiceNowError Error
	ModeServiceNowError ModeServiceNow = "error"
	// ModeServiceNowAlways Backpressure
	ModeServiceNowAlways ModeServiceNow = "always"
	// ModeServiceNowBackpressure Always On
	ModeServiceNowBackpressure ModeServiceNow = "backpressure"
)

func (e ModeServiceNow) ToPointer() *ModeServiceNow {
	return &e
}

// PqCompressCompressionServiceNow - Codec to use to compress the persisted data
type PqCompressCompressionServiceNow string

const (
	// PqCompressCompressionServiceNowNone None
	PqCompressCompressionServiceNowNone PqCompressCompressionServiceNow = "none"
	// PqCompressCompressionServiceNowGzip Gzip
	PqCompressCompressionServiceNowGzip PqCompressCompressionServiceNow = "gzip"
)

func (e PqCompressCompressionServiceNow) ToPointer() *PqCompressCompressionServiceNow {
	return &e
}

// QueueFullBehaviorServiceNow - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorServiceNow string

const (
	// QueueFullBehaviorServiceNowBlock Block
	QueueFullBehaviorServiceNowBlock QueueFullBehaviorServiceNow = "block"
	// QueueFullBehaviorServiceNowDrop Drop new data
	QueueFullBehaviorServiceNowDrop QueueFullBehaviorServiceNow = "drop"
)

func (e QueueFullBehaviorServiceNow) ToPointer() *QueueFullBehaviorServiceNow {
	return &e
}

type PqControlsServiceNow struct {
}

func (p PqControlsServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputServiceNow struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type TypeServiceNow `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint where ServiceNow events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)
	Endpoint *string `default:"ingest.lightstep.com:443" json:"endpoint"`
	// Select or create a stored text secret
	TokenSecret   string  `json:"tokenSecret"`
	AuthTokenName *string `default:"lightstep-access-token" json:"authTokenName"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *OTLPVersionServiceNow `default:"1.3.1" json:"otlpVersion"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"2048" json:"maxPayloadSizeKB"`
	// Select a transport option for OpenTelemetry
	Protocol *ProtocolServiceNow `default:"grpc" json:"protocol"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *CompressCompressionServiceNow `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompressionServiceNow `default:"gzip" json:"httpCompress"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []MetadatumServiceNow `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeServiceNow `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorServiceNow `default:"block" json:"onBackpressure"`
	Description    *string                         `json:"description,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderServiceNow `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingServiceNow `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsServiceNow  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool                            `default:"true" json:"responseHonorRetryAfterHeader"`
	TLS                           *TLSSettingsClientSideServiceNow `json:"tls,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeServiceNow `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionServiceNow `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorServiceNow `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsServiceNow        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any               `additionalProperties:"true" json:"-"`
}

func (o OutputServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "tokenSecret"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputServiceNow) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputServiceNow) GetType() TypeServiceNow {
	if o == nil {
		return TypeServiceNow("")
	}
	return o.Type
}

func (o *OutputServiceNow) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputServiceNow) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputServiceNow) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputServiceNow) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputServiceNow) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputServiceNow) GetTokenSecret() string {
	if o == nil {
		return ""
	}
	return o.TokenSecret
}

func (o *OutputServiceNow) GetAuthTokenName() *string {
	if o == nil {
		return nil
	}
	return o.AuthTokenName
}

func (o *OutputServiceNow) GetOtlpVersion() *OTLPVersionServiceNow {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputServiceNow) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputServiceNow) GetProtocol() *ProtocolServiceNow {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputServiceNow) GetCompress() *CompressCompressionServiceNow {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputServiceNow) GetHTTPCompress() *HTTPCompressCompressionServiceNow {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputServiceNow) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputServiceNow) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputServiceNow) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputServiceNow) GetMetadata() []MetadatumServiceNow {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputServiceNow) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputServiceNow) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputServiceNow) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputServiceNow) GetFailedRequestLoggingMode() *FailedRequestLoggingModeServiceNow {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputServiceNow) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputServiceNow) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputServiceNow) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputServiceNow) GetOnBackpressure() *BackpressureBehaviorServiceNow {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputServiceNow) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputServiceNow) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputServiceNow) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputServiceNow) GetExtraHTTPHeaders() []ExtraHTTPHeaderServiceNow {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputServiceNow) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputServiceNow) GetResponseRetrySettings() []ResponseRetrySettingServiceNow {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputServiceNow) GetTimeoutRetrySettings() *TimeoutRetrySettingsServiceNow {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputServiceNow) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputServiceNow) GetTLS() *TLSSettingsClientSideServiceNow {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputServiceNow) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputServiceNow) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputServiceNow) GetPqMode() *ModeServiceNow {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputServiceNow) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputServiceNow) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputServiceNow) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputServiceNow) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputServiceNow) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputServiceNow) GetPqCompress() *PqCompressCompressionServiceNow {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputServiceNow) GetPqOnBackpressure() *QueueFullBehaviorServiceNow {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputServiceNow) GetPqControls() *PqControlsServiceNow {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputServiceNow) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeOpenTelemetry string

const (
	OutputTypeOpenTelemetryOpenTelemetry OutputTypeOpenTelemetry = "open_telemetry"
)

func (e OutputTypeOpenTelemetry) ToPointer() *OutputTypeOpenTelemetry {
	return &e
}
func (e *OutputTypeOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "open_telemetry":
		*e = OutputTypeOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeOpenTelemetry: %v", v)
	}
}

// OutputProtocolOpenTelemetry - Select a transport option for OpenTelemetry
type OutputProtocolOpenTelemetry string

const (
	// OutputProtocolOpenTelemetryGrpc gRPC
	OutputProtocolOpenTelemetryGrpc OutputProtocolOpenTelemetry = "grpc"
	// OutputProtocolOpenTelemetryHTTP HTTP
	OutputProtocolOpenTelemetryHTTP OutputProtocolOpenTelemetry = "http"
)

func (e OutputProtocolOpenTelemetry) ToPointer() *OutputProtocolOpenTelemetry {
	return &e
}

// OutputOTLPVersionOpenTelemetry - The version of OTLP Protobuf definitions to use when structuring data to send
type OutputOTLPVersionOpenTelemetry string

const (
	// OutputOTLPVersionOpenTelemetryZeroDot10Dot0 0.10.0
	OutputOTLPVersionOpenTelemetryZeroDot10Dot0 OutputOTLPVersionOpenTelemetry = "0.10.0"
	// OutputOTLPVersionOpenTelemetryOneDot3Dot1 1.3.1
	OutputOTLPVersionOpenTelemetryOneDot3Dot1 OutputOTLPVersionOpenTelemetry = "1.3.1"
)

func (e OutputOTLPVersionOpenTelemetry) ToPointer() *OutputOTLPVersionOpenTelemetry {
	return &e
}

// OutputCompressCompressionOpenTelemetry - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type OutputCompressCompressionOpenTelemetry string

const (
	// OutputCompressCompressionOpenTelemetryNone None
	OutputCompressCompressionOpenTelemetryNone OutputCompressCompressionOpenTelemetry = "none"
	// OutputCompressCompressionOpenTelemetryDeflate Deflate
	OutputCompressCompressionOpenTelemetryDeflate OutputCompressCompressionOpenTelemetry = "deflate"
	// OutputCompressCompressionOpenTelemetryGzip Gzip
	OutputCompressCompressionOpenTelemetryGzip OutputCompressCompressionOpenTelemetry = "gzip"
)

func (e OutputCompressCompressionOpenTelemetry) ToPointer() *OutputCompressCompressionOpenTelemetry {
	return &e
}

// HTTPCompressCompressionOpenTelemetry - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompressionOpenTelemetry string

const (
	// HTTPCompressCompressionOpenTelemetryNone None
	HTTPCompressCompressionOpenTelemetryNone HTTPCompressCompressionOpenTelemetry = "none"
	// HTTPCompressCompressionOpenTelemetryGzip Gzip
	HTTPCompressCompressionOpenTelemetryGzip HTTPCompressCompressionOpenTelemetry = "gzip"
)

func (e HTTPCompressCompressionOpenTelemetry) ToPointer() *HTTPCompressCompressionOpenTelemetry {
	return &e
}

// OutputAuthenticationTypeOpenTelemetry - OpenTelemetry authentication type
type OutputAuthenticationTypeOpenTelemetry string

const (
	OutputAuthenticationTypeOpenTelemetryNone              OutputAuthenticationTypeOpenTelemetry = "none"
	OutputAuthenticationTypeOpenTelemetryBasic             OutputAuthenticationTypeOpenTelemetry = "basic"
	OutputAuthenticationTypeOpenTelemetryCredentialsSecret OutputAuthenticationTypeOpenTelemetry = "credentialsSecret"
	OutputAuthenticationTypeOpenTelemetryToken             OutputAuthenticationTypeOpenTelemetry = "token"
	OutputAuthenticationTypeOpenTelemetryTextSecret        OutputAuthenticationTypeOpenTelemetry = "textSecret"
	OutputAuthenticationTypeOpenTelemetryOauth             OutputAuthenticationTypeOpenTelemetry = "oauth"
)

func (e OutputAuthenticationTypeOpenTelemetry) ToPointer() *OutputAuthenticationTypeOpenTelemetry {
	return &e
}

type OutputMetadatumOpenTelemetry struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (o OutputMetadatumOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMetadatumOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputMetadatumOpenTelemetry) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *OutputMetadatumOpenTelemetry) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeOpenTelemetry - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeOpenTelemetry string

const (
	// FailedRequestLoggingModeOpenTelemetryPayload Payload
	FailedRequestLoggingModeOpenTelemetryPayload FailedRequestLoggingModeOpenTelemetry = "payload"
	// FailedRequestLoggingModeOpenTelemetryPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeOpenTelemetryPayloadAndHeaders FailedRequestLoggingModeOpenTelemetry = "payloadAndHeaders"
	// FailedRequestLoggingModeOpenTelemetryNone None
	FailedRequestLoggingModeOpenTelemetryNone FailedRequestLoggingModeOpenTelemetry = "none"
)

func (e FailedRequestLoggingModeOpenTelemetry) ToPointer() *FailedRequestLoggingModeOpenTelemetry {
	return &e
}

// BackpressureBehaviorOpenTelemetry - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorOpenTelemetry string

const (
	// BackpressureBehaviorOpenTelemetryBlock Block
	BackpressureBehaviorOpenTelemetryBlock BackpressureBehaviorOpenTelemetry = "block"
	// BackpressureBehaviorOpenTelemetryDrop Drop
	BackpressureBehaviorOpenTelemetryDrop BackpressureBehaviorOpenTelemetry = "drop"
	// BackpressureBehaviorOpenTelemetryQueue Persistent Queue
	BackpressureBehaviorOpenTelemetryQueue BackpressureBehaviorOpenTelemetry = "queue"
)

func (e BackpressureBehaviorOpenTelemetry) ToPointer() *BackpressureBehaviorOpenTelemetry {
	return &e
}

type OutputOauthParamOpenTelemetry struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o OutputOauthParamOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputOauthParamOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputOauthParamOpenTelemetry) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OutputOauthParamOpenTelemetry) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputOauthHeaderOpenTelemetry struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o OutputOauthHeaderOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputOauthHeaderOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputOauthHeaderOpenTelemetry) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OutputOauthHeaderOpenTelemetry) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ExtraHTTPHeaderOpenTelemetry struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderOpenTelemetry) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderOpenTelemetry) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

type ResponseRetrySettingOpenTelemetry struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingOpenTelemetry) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingOpenTelemetry) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingOpenTelemetry) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingOpenTelemetry) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsOpenTelemetry struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsOpenTelemetry) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsOpenTelemetry) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsOpenTelemetry) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsOpenTelemetry) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type OutputMinimumTLSVersionOpenTelemetry string

const (
	OutputMinimumTLSVersionOpenTelemetryTlSv1  OutputMinimumTLSVersionOpenTelemetry = "TLSv1"
	OutputMinimumTLSVersionOpenTelemetryTlSv11 OutputMinimumTLSVersionOpenTelemetry = "TLSv1.1"
	OutputMinimumTLSVersionOpenTelemetryTlSv12 OutputMinimumTLSVersionOpenTelemetry = "TLSv1.2"
	OutputMinimumTLSVersionOpenTelemetryTlSv13 OutputMinimumTLSVersionOpenTelemetry = "TLSv1.3"
)

func (e OutputMinimumTLSVersionOpenTelemetry) ToPointer() *OutputMinimumTLSVersionOpenTelemetry {
	return &e
}

type OutputMaximumTLSVersionOpenTelemetry string

const (
	OutputMaximumTLSVersionOpenTelemetryTlSv1  OutputMaximumTLSVersionOpenTelemetry = "TLSv1"
	OutputMaximumTLSVersionOpenTelemetryTlSv11 OutputMaximumTLSVersionOpenTelemetry = "TLSv1.1"
	OutputMaximumTLSVersionOpenTelemetryTlSv12 OutputMaximumTLSVersionOpenTelemetry = "TLSv1.2"
	OutputMaximumTLSVersionOpenTelemetryTlSv13 OutputMaximumTLSVersionOpenTelemetry = "TLSv1.3"
)

func (e OutputMaximumTLSVersionOpenTelemetry) ToPointer() *OutputMaximumTLSVersionOpenTelemetry {
	return &e
}

type TLSSettingsClientSideOpenTelemetry struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                               `json:"passphrase,omitempty"`
	MinVersion *OutputMinimumTLSVersionOpenTelemetry `json:"minVersion,omitempty"`
	MaxVersion *OutputMaximumTLSVersionOpenTelemetry `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideOpenTelemetry) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideOpenTelemetry) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideOpenTelemetry) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideOpenTelemetry) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideOpenTelemetry) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideOpenTelemetry) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideOpenTelemetry) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideOpenTelemetry) GetMinVersion() *OutputMinimumTLSVersionOpenTelemetry {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideOpenTelemetry) GetMaxVersion() *OutputMaximumTLSVersionOpenTelemetry {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// OutputModeOpenTelemetry - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeOpenTelemetry string

const (
	// OutputModeOpenTelemetryError Error
	OutputModeOpenTelemetryError OutputModeOpenTelemetry = "error"
	// OutputModeOpenTelemetryAlways Backpressure
	OutputModeOpenTelemetryAlways OutputModeOpenTelemetry = "always"
	// OutputModeOpenTelemetryBackpressure Always On
	OutputModeOpenTelemetryBackpressure OutputModeOpenTelemetry = "backpressure"
)

func (e OutputModeOpenTelemetry) ToPointer() *OutputModeOpenTelemetry {
	return &e
}

// PqCompressCompressionOpenTelemetry - Codec to use to compress the persisted data
type PqCompressCompressionOpenTelemetry string

const (
	// PqCompressCompressionOpenTelemetryNone None
	PqCompressCompressionOpenTelemetryNone PqCompressCompressionOpenTelemetry = "none"
	// PqCompressCompressionOpenTelemetryGzip Gzip
	PqCompressCompressionOpenTelemetryGzip PqCompressCompressionOpenTelemetry = "gzip"
)

func (e PqCompressCompressionOpenTelemetry) ToPointer() *PqCompressCompressionOpenTelemetry {
	return &e
}

// QueueFullBehaviorOpenTelemetry - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorOpenTelemetry string

const (
	// QueueFullBehaviorOpenTelemetryBlock Block
	QueueFullBehaviorOpenTelemetryBlock QueueFullBehaviorOpenTelemetry = "block"
	// QueueFullBehaviorOpenTelemetryDrop Drop new data
	QueueFullBehaviorOpenTelemetryDrop QueueFullBehaviorOpenTelemetry = "drop"
)

func (e QueueFullBehaviorOpenTelemetry) ToPointer() *QueueFullBehaviorOpenTelemetry {
	return &e
}

type OutputPqControlsOpenTelemetry struct {
}

func (o OutputPqControlsOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputOpenTelemetry struct {
	// Unique ID for this output
	ID   *string                 `json:"id,omitempty"`
	Type OutputTypeOpenTelemetry `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select a transport option for OpenTelemetry
	Protocol *OutputProtocolOpenTelemetry `default:"grpc" json:"protocol"`
	// The endpoint where OTel events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Unspecified ports will default to 4317, unless the endpoint is an HTTPS-based URL or TLS is enabled, in which case 443 will be used.
	Endpoint string `json:"endpoint"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *OutputOTLPVersionOpenTelemetry `default:"0.10.0" json:"otlpVersion"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *OutputCompressCompressionOpenTelemetry `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompressionOpenTelemetry `default:"gzip" json:"httpCompress"`
	// OpenTelemetry authentication type
	AuthType *OutputAuthenticationTypeOpenTelemetry `default:"none" json:"authType"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []OutputMetadatumOpenTelemetry `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeOpenTelemetry `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorOpenTelemetry `default:"block" json:"onBackpressure"`
	Description    *string                            `json:"description,omitempty"`
	Username       *string                            `json:"username,omitempty"`
	Password       *string                            `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OutputOauthParamOpenTelemetry `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OutputOauthHeaderOpenTelemetry `json:"oauthHeaders,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderOpenTelemetry `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingOpenTelemetry `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsOpenTelemetry  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool                               `default:"true" json:"responseHonorRetryAfterHeader"`
	TLS                           *TLSSettingsClientSideOpenTelemetry `json:"tls,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeOpenTelemetry `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionOpenTelemetry `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorOpenTelemetry `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsOpenTelemetry  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                  `additionalProperties:"true" json:"-"`
}

func (o OutputOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "endpoint"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputOpenTelemetry) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputOpenTelemetry) GetType() OutputTypeOpenTelemetry {
	if o == nil {
		return OutputTypeOpenTelemetry("")
	}
	return o.Type
}

func (o *OutputOpenTelemetry) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputOpenTelemetry) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputOpenTelemetry) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputOpenTelemetry) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputOpenTelemetry) GetProtocol() *OutputProtocolOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputOpenTelemetry) GetEndpoint() string {
	if o == nil {
		return ""
	}
	return o.Endpoint
}

func (o *OutputOpenTelemetry) GetOtlpVersion() *OutputOTLPVersionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputOpenTelemetry) GetCompress() *OutputCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputOpenTelemetry) GetHTTPCompress() *HTTPCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputOpenTelemetry) GetAuthType() *OutputAuthenticationTypeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputOpenTelemetry) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputOpenTelemetry) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputOpenTelemetry) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputOpenTelemetry) GetMetadata() []OutputMetadatumOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputOpenTelemetry) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputOpenTelemetry) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputOpenTelemetry) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputOpenTelemetry) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputOpenTelemetry) GetFailedRequestLoggingMode() *FailedRequestLoggingModeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputOpenTelemetry) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputOpenTelemetry) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputOpenTelemetry) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputOpenTelemetry) GetOnBackpressure() *BackpressureBehaviorOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputOpenTelemetry) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputOpenTelemetry) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputOpenTelemetry) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputOpenTelemetry) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputOpenTelemetry) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputOpenTelemetry) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputOpenTelemetry) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputOpenTelemetry) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputOpenTelemetry) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputOpenTelemetry) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputOpenTelemetry) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputOpenTelemetry) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputOpenTelemetry) GetOauthParams() []OutputOauthParamOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputOpenTelemetry) GetOauthHeaders() []OutputOauthHeaderOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputOpenTelemetry) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputOpenTelemetry) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputOpenTelemetry) GetExtraHTTPHeaders() []ExtraHTTPHeaderOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputOpenTelemetry) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputOpenTelemetry) GetResponseRetrySettings() []ResponseRetrySettingOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputOpenTelemetry) GetTimeoutRetrySettings() *TimeoutRetrySettingsOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputOpenTelemetry) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputOpenTelemetry) GetTLS() *TLSSettingsClientSideOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputOpenTelemetry) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputOpenTelemetry) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputOpenTelemetry) GetPqMode() *OutputModeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputOpenTelemetry) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputOpenTelemetry) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputOpenTelemetry) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputOpenTelemetry) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputOpenTelemetry) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputOpenTelemetry) GetPqCompress() *PqCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputOpenTelemetry) GetPqOnBackpressure() *QueueFullBehaviorOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputOpenTelemetry) GetPqControls() *OutputPqControlsOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputOpenTelemetry) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeRing string

const (
	TypeRingRing TypeRing = "ring"
)

func (e TypeRing) ToPointer() *TypeRing {
	return &e
}
func (e *TypeRing) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "ring":
		*e = TypeRing(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeRing: %v", v)
	}
}

// DataFormatRing - Format of the output data.
type DataFormatRing string

const (
	DataFormatRingJSON DataFormatRing = "json"
	DataFormatRingRaw  DataFormatRing = "raw"
)

func (e DataFormatRing) ToPointer() *DataFormatRing {
	return &e
}

type OutputDataCompressionFormat string

const (
	OutputDataCompressionFormatNone OutputDataCompressionFormat = "none"
	OutputDataCompressionFormatGzip OutputDataCompressionFormat = "gzip"
)

func (e OutputDataCompressionFormat) ToPointer() *OutputDataCompressionFormat {
	return &e
}

// BackpressureBehaviorRing - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorRing string

const (
	// BackpressureBehaviorRingBlock Block
	BackpressureBehaviorRingBlock BackpressureBehaviorRing = "block"
	// BackpressureBehaviorRingDrop Drop
	BackpressureBehaviorRingDrop BackpressureBehaviorRing = "drop"
)

func (e BackpressureBehaviorRing) ToPointer() *BackpressureBehaviorRing {
	return &e
}

type OutputRing struct {
	// Unique ID for this output
	ID   *string  `json:"id,omitempty"`
	Type TypeRing `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Format of the output data.
	Format *DataFormatRing `default:"json" json:"format"`
	// JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition.
	PartitionExpr *string `json:"partitionExpr,omitempty"`
	// Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.
	MaxDataSize *string `default:"1GB" json:"maxDataSize"`
	// Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.
	MaxDataTime *string                      `default:"24h" json:"maxDataTime"`
	Compress    *OutputDataCompressionFormat `default:"gzip" json:"compress"`
	// Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>
	DestPath *string `json:"destPath,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure       *BackpressureBehaviorRing `default:"block" json:"onBackpressure"`
	Description          *string                   `json:"description,omitempty"`
	AdditionalProperties map[string]any            `additionalProperties:"true" json:"-"`
}

func (o OutputRing) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputRing) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputRing) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputRing) GetType() TypeRing {
	if o == nil {
		return TypeRing("")
	}
	return o.Type
}

func (o *OutputRing) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputRing) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputRing) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputRing) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputRing) GetFormat() *DataFormatRing {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputRing) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputRing) GetMaxDataSize() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataSize
}

func (o *OutputRing) GetMaxDataTime() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataTime
}

func (o *OutputRing) GetCompress() *OutputDataCompressionFormat {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputRing) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputRing) GetOnBackpressure() *BackpressureBehaviorRing {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputRing) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputRing) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypePrometheus string

const (
	OutputTypePrometheusPrometheus OutputTypePrometheus = "prometheus"
)

func (e OutputTypePrometheus) ToPointer() *OutputTypePrometheus {
	return &e
}
func (e *OutputTypePrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "prometheus":
		*e = OutputTypePrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypePrometheus: %v", v)
	}
}

type ExtraHTTPHeaderPrometheus struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderPrometheus) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderPrometheus) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModePrometheus - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModePrometheus string

const (
	// FailedRequestLoggingModePrometheusPayload Payload
	FailedRequestLoggingModePrometheusPayload FailedRequestLoggingModePrometheus = "payload"
	// FailedRequestLoggingModePrometheusPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModePrometheusPayloadAndHeaders FailedRequestLoggingModePrometheus = "payloadAndHeaders"
	// FailedRequestLoggingModePrometheusNone None
	FailedRequestLoggingModePrometheusNone FailedRequestLoggingModePrometheus = "none"
)

func (e FailedRequestLoggingModePrometheus) ToPointer() *FailedRequestLoggingModePrometheus {
	return &e
}

type ResponseRetrySettingPrometheus struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingPrometheus) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingPrometheus) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingPrometheus) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingPrometheus) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsPrometheus struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsPrometheus) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsPrometheus) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsPrometheus) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsPrometheus) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorPrometheus - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorPrometheus string

const (
	// BackpressureBehaviorPrometheusBlock Block
	BackpressureBehaviorPrometheusBlock BackpressureBehaviorPrometheus = "block"
	// BackpressureBehaviorPrometheusDrop Drop
	BackpressureBehaviorPrometheusDrop BackpressureBehaviorPrometheus = "drop"
	// BackpressureBehaviorPrometheusQueue Persistent Queue
	BackpressureBehaviorPrometheusQueue BackpressureBehaviorPrometheus = "queue"
)

func (e BackpressureBehaviorPrometheus) ToPointer() *BackpressureBehaviorPrometheus {
	return &e
}

// AuthenticationTypePrometheus - Remote Write authentication type
type AuthenticationTypePrometheus string

const (
	AuthenticationTypePrometheusNone              AuthenticationTypePrometheus = "none"
	AuthenticationTypePrometheusBasic             AuthenticationTypePrometheus = "basic"
	AuthenticationTypePrometheusCredentialsSecret AuthenticationTypePrometheus = "credentialsSecret"
	AuthenticationTypePrometheusToken             AuthenticationTypePrometheus = "token"
	AuthenticationTypePrometheusTextSecret        AuthenticationTypePrometheus = "textSecret"
	AuthenticationTypePrometheusOauth             AuthenticationTypePrometheus = "oauth"
)

func (e AuthenticationTypePrometheus) ToPointer() *AuthenticationTypePrometheus {
	return &e
}

// OutputModePrometheus - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModePrometheus string

const (
	// OutputModePrometheusError Error
	OutputModePrometheusError OutputModePrometheus = "error"
	// OutputModePrometheusAlways Backpressure
	OutputModePrometheusAlways OutputModePrometheus = "always"
	// OutputModePrometheusBackpressure Always On
	OutputModePrometheusBackpressure OutputModePrometheus = "backpressure"
)

func (e OutputModePrometheus) ToPointer() *OutputModePrometheus {
	return &e
}

// PqCompressCompressionPrometheus - Codec to use to compress the persisted data
type PqCompressCompressionPrometheus string

const (
	// PqCompressCompressionPrometheusNone None
	PqCompressCompressionPrometheusNone PqCompressCompressionPrometheus = "none"
	// PqCompressCompressionPrometheusGzip Gzip
	PqCompressCompressionPrometheusGzip PqCompressCompressionPrometheus = "gzip"
)

func (e PqCompressCompressionPrometheus) ToPointer() *PqCompressCompressionPrometheus {
	return &e
}

// QueueFullBehaviorPrometheus - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorPrometheus string

const (
	// QueueFullBehaviorPrometheusBlock Block
	QueueFullBehaviorPrometheusBlock QueueFullBehaviorPrometheus = "block"
	// QueueFullBehaviorPrometheusDrop Drop new data
	QueueFullBehaviorPrometheusDrop QueueFullBehaviorPrometheus = "drop"
)

func (e QueueFullBehaviorPrometheus) ToPointer() *QueueFullBehaviorPrometheus {
	return &e
}

type OutputPqControlsPrometheus struct {
}

func (o OutputPqControlsPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OauthParamPrometheus struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o OauthParamPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthParamPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthParamPrometheus) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamPrometheus) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderPrometheus struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o OauthHeaderPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthHeaderPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthHeaderPrometheus) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderPrometheus) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputPrometheus struct {
	// Unique ID for this output
	ID   *string              `json:"id,omitempty"`
	Type OutputTypePrometheus `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions to generated metrics.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send metrics to
	URL string `json:"url"`
	// JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.
	MetricRenameExpr *string `default:"name.replace(/[^a-zA-Z0-9_]/g, '_')" json:"metricRenameExpr"`
	// Generate and send metadata (`type` and `metricFamilyName`) requests
	SendMetadata *bool `default:"true" json:"sendMetadata"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderPrometheus `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModePrometheus `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingPrometheus `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsPrometheus  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorPrometheus `default:"block" json:"onBackpressure"`
	// Remote Write authentication type
	AuthType    *AuthenticationTypePrometheus `default:"none" json:"authType"`
	Description *string                       `json:"description,omitempty"`
	// How frequently metrics metadata is sent out. Value cannot be smaller than the base Flush period set above.
	MetricsFlushPeriodSec *float64 `default:"60" json:"metricsFlushPeriodSec"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModePrometheus `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionPrometheus `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorPrometheus `default:"block" json:"pqOnBackpressure"`
	PqControls       *OutputPqControlsPrometheus  `json:"pqControls,omitempty"`
	Username         *string                      `json:"username,omitempty"`
	Password         *string                      `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamPrometheus `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders         []OauthHeaderPrometheus `json:"oauthHeaders,omitempty"`
	AdditionalProperties map[string]any          `additionalProperties:"true" json:"-"`
}

func (o OutputPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "url"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputPrometheus) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputPrometheus) GetType() OutputTypePrometheus {
	if o == nil {
		return OutputTypePrometheus("")
	}
	return o.Type
}

func (o *OutputPrometheus) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputPrometheus) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputPrometheus) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputPrometheus) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputPrometheus) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputPrometheus) GetMetricRenameExpr() *string {
	if o == nil {
		return nil
	}
	return o.MetricRenameExpr
}

func (o *OutputPrometheus) GetSendMetadata() *bool {
	if o == nil {
		return nil
	}
	return o.SendMetadata
}

func (o *OutputPrometheus) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputPrometheus) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputPrometheus) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputPrometheus) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputPrometheus) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputPrometheus) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputPrometheus) GetExtraHTTPHeaders() []ExtraHTTPHeaderPrometheus {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputPrometheus) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputPrometheus) GetFailedRequestLoggingMode() *FailedRequestLoggingModePrometheus {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputPrometheus) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputPrometheus) GetResponseRetrySettings() []ResponseRetrySettingPrometheus {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputPrometheus) GetTimeoutRetrySettings() *TimeoutRetrySettingsPrometheus {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputPrometheus) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputPrometheus) GetOnBackpressure() *BackpressureBehaviorPrometheus {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputPrometheus) GetAuthType() *AuthenticationTypePrometheus {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputPrometheus) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputPrometheus) GetMetricsFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MetricsFlushPeriodSec
}

func (o *OutputPrometheus) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputPrometheus) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputPrometheus) GetPqMode() *OutputModePrometheus {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputPrometheus) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputPrometheus) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputPrometheus) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputPrometheus) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputPrometheus) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputPrometheus) GetPqCompress() *PqCompressCompressionPrometheus {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputPrometheus) GetPqOnBackpressure() *QueueFullBehaviorPrometheus {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputPrometheus) GetPqControls() *OutputPqControlsPrometheus {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputPrometheus) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputPrometheus) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputPrometheus) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputPrometheus) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputPrometheus) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputPrometheus) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputPrometheus) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputPrometheus) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputPrometheus) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputPrometheus) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputPrometheus) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputPrometheus) GetOauthParams() []OauthParamPrometheus {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputPrometheus) GetOauthHeaders() []OauthHeaderPrometheus {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputPrometheus) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeLoki string

const (
	OutputTypeLokiLoki OutputTypeLoki = "loki"
)

func (e OutputTypeLoki) ToPointer() *OutputTypeLoki {
	return &e
}
func (e *OutputTypeLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "loki":
		*e = OutputTypeLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeLoki: %v", v)
	}
}

// MessageFormatLoki - Format to use when sending logs to Loki (Protobuf or JSON)
type MessageFormatLoki string

const (
	// MessageFormatLokiProtobuf Protobuf
	MessageFormatLokiProtobuf MessageFormatLoki = "protobuf"
	// MessageFormatLokiJSON JSON
	MessageFormatLokiJSON MessageFormatLoki = "json"
)

func (e MessageFormatLoki) ToPointer() *MessageFormatLoki {
	return &e
}

type LabelLoki struct {
	Name  *string `default:"" json:"name"`
	Value string  `json:"value"`
}

func (l LabelLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(l, "", false)
}

func (l *LabelLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &l, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (l *LabelLoki) GetName() *string {
	if l == nil {
		return nil
	}
	return l.Name
}

func (l *LabelLoki) GetValue() string {
	if l == nil {
		return ""
	}
	return l.Value
}

type OutputAuthenticationTypeLoki string

const (
	// OutputAuthenticationTypeLokiNone None
	OutputAuthenticationTypeLokiNone OutputAuthenticationTypeLoki = "none"
	// OutputAuthenticationTypeLokiToken Auth token
	OutputAuthenticationTypeLokiToken OutputAuthenticationTypeLoki = "token"
	// OutputAuthenticationTypeLokiTextSecret Auth token (text secret)
	OutputAuthenticationTypeLokiTextSecret OutputAuthenticationTypeLoki = "textSecret"
	// OutputAuthenticationTypeLokiBasic Basic
	OutputAuthenticationTypeLokiBasic OutputAuthenticationTypeLoki = "basic"
	// OutputAuthenticationTypeLokiCredentialsSecret Basic (credentials secret)
	OutputAuthenticationTypeLokiCredentialsSecret OutputAuthenticationTypeLoki = "credentialsSecret"
)

func (e OutputAuthenticationTypeLoki) ToPointer() *OutputAuthenticationTypeLoki {
	return &e
}

type ExtraHTTPHeaderLoki struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderLoki) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderLoki) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeLoki - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeLoki string

const (
	// FailedRequestLoggingModeLokiPayload Payload
	FailedRequestLoggingModeLokiPayload FailedRequestLoggingModeLoki = "payload"
	// FailedRequestLoggingModeLokiPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeLokiPayloadAndHeaders FailedRequestLoggingModeLoki = "payloadAndHeaders"
	// FailedRequestLoggingModeLokiNone None
	FailedRequestLoggingModeLokiNone FailedRequestLoggingModeLoki = "none"
)

func (e FailedRequestLoggingModeLoki) ToPointer() *FailedRequestLoggingModeLoki {
	return &e
}

type ResponseRetrySettingLoki struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingLoki) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingLoki) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingLoki) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingLoki) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsLoki struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsLoki) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsLoki) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsLoki) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsLoki) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorLoki - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorLoki string

const (
	// BackpressureBehaviorLokiBlock Block
	BackpressureBehaviorLokiBlock BackpressureBehaviorLoki = "block"
	// BackpressureBehaviorLokiDrop Drop
	BackpressureBehaviorLokiDrop BackpressureBehaviorLoki = "drop"
	// BackpressureBehaviorLokiQueue Persistent Queue
	BackpressureBehaviorLokiQueue BackpressureBehaviorLoki = "queue"
)

func (e BackpressureBehaviorLoki) ToPointer() *BackpressureBehaviorLoki {
	return &e
}

// OutputModeLoki - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeLoki string

const (
	// OutputModeLokiError Error
	OutputModeLokiError OutputModeLoki = "error"
	// OutputModeLokiAlways Backpressure
	OutputModeLokiAlways OutputModeLoki = "always"
	// OutputModeLokiBackpressure Always On
	OutputModeLokiBackpressure OutputModeLoki = "backpressure"
)

func (e OutputModeLoki) ToPointer() *OutputModeLoki {
	return &e
}

// PqCompressCompressionLoki - Codec to use to compress the persisted data
type PqCompressCompressionLoki string

const (
	// PqCompressCompressionLokiNone None
	PqCompressCompressionLokiNone PqCompressCompressionLoki = "none"
	// PqCompressCompressionLokiGzip Gzip
	PqCompressCompressionLokiGzip PqCompressCompressionLoki = "gzip"
)

func (e PqCompressCompressionLoki) ToPointer() *PqCompressCompressionLoki {
	return &e
}

// QueueFullBehaviorLoki - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorLoki string

const (
	// QueueFullBehaviorLokiBlock Block
	QueueFullBehaviorLokiBlock QueueFullBehaviorLoki = "block"
	// QueueFullBehaviorLokiDrop Drop new data
	QueueFullBehaviorLokiDrop QueueFullBehaviorLoki = "drop"
)

func (e QueueFullBehaviorLoki) ToPointer() *QueueFullBehaviorLoki {
	return &e
}

type OutputPqControlsLoki struct {
}

func (o OutputPqControlsLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputLoki struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type OutputTypeLoki `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as labels to generated logs.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send logs to
	URL string `json:"url"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Format to use when sending logs to Loki (Protobuf or JSON)
	MessageFormat *MessageFormatLoki `default:"protobuf" json:"messageFormat"`
	// List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'
	Labels   []LabelLoki                   `json:"labels,omitempty"`
	AuthType *OutputAuthenticationTypeLoki `default:"none" json:"authType"`
	// Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki to complain about entries being delivered out of order.
	Concurrency *float64 `default:"1" json:"concurrency"`
	// Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Defaults to 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	FlushPeriodSec *float64 `default:"15" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderLoki `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeLoki `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingLoki `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsLoki  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Add per-event HTTP headers from the __headers field to outgoing requests. Events with different headers are batched and sent separately.
	EnableDynamicHeaders *bool `default:"false" json:"enableDynamicHeaders"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorLoki `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeLoki `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionLoki `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorLoki `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsLoki  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any         `additionalProperties:"true" json:"-"`
}

func (o OutputLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "url"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputLoki) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputLoki) GetType() OutputTypeLoki {
	if o == nil {
		return OutputTypeLoki("")
	}
	return o.Type
}

func (o *OutputLoki) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputLoki) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputLoki) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputLoki) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputLoki) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputLoki) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputLoki) GetMessageFormat() *MessageFormatLoki {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputLoki) GetLabels() []LabelLoki {
	if o == nil {
		return nil
	}
	return o.Labels
}

func (o *OutputLoki) GetAuthType() *OutputAuthenticationTypeLoki {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputLoki) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputLoki) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputLoki) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputLoki) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputLoki) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputLoki) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputLoki) GetExtraHTTPHeaders() []ExtraHTTPHeaderLoki {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputLoki) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputLoki) GetFailedRequestLoggingMode() *FailedRequestLoggingModeLoki {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputLoki) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputLoki) GetResponseRetrySettings() []ResponseRetrySettingLoki {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputLoki) GetTimeoutRetrySettings() *TimeoutRetrySettingsLoki {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputLoki) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputLoki) GetEnableDynamicHeaders() *bool {
	if o == nil {
		return nil
	}
	return o.EnableDynamicHeaders
}

func (o *OutputLoki) GetOnBackpressure() *BackpressureBehaviorLoki {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputLoki) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputLoki) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputLoki) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputLoki) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputLoki) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputLoki) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputLoki) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputLoki) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputLoki) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputLoki) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputLoki) GetPqMode() *OutputModeLoki {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputLoki) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputLoki) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputLoki) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputLoki) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputLoki) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputLoki) GetPqCompress() *PqCompressCompressionLoki {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputLoki) GetPqOnBackpressure() *QueueFullBehaviorLoki {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputLoki) GetPqControls() *OutputPqControlsLoki {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputLoki) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputGrafanaCloudType2 string

const (
	OutputGrafanaCloudType2GrafanaCloud OutputGrafanaCloudType2 = "grafana_cloud"
)

func (e OutputGrafanaCloudType2) ToPointer() *OutputGrafanaCloudType2 {
	return &e
}
func (e *OutputGrafanaCloudType2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grafana_cloud":
		*e = OutputGrafanaCloudType2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudType2: %v", v)
	}
}

// OutputGrafanaCloudMessageFormat2 - Format to use when sending logs to Loki (Protobuf or JSON)
type OutputGrafanaCloudMessageFormat2 string

const (
	// OutputGrafanaCloudMessageFormat2Protobuf Protobuf
	OutputGrafanaCloudMessageFormat2Protobuf OutputGrafanaCloudMessageFormat2 = "protobuf"
	// OutputGrafanaCloudMessageFormat2JSON JSON
	OutputGrafanaCloudMessageFormat2JSON OutputGrafanaCloudMessageFormat2 = "json"
)

func (e OutputGrafanaCloudMessageFormat2) ToPointer() *OutputGrafanaCloudMessageFormat2 {
	return &e
}

type OutputGrafanaCloudLabel2 struct {
	Name  *string `default:"" json:"name"`
	Value string  `json:"value"`
}

func (o OutputGrafanaCloudLabel2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudLabel2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudLabel2) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudLabel2) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputGrafanaCloudPrometheusAuthAuthenticationType2 string

const (
	// OutputGrafanaCloudPrometheusAuthAuthenticationType2None None
	OutputGrafanaCloudPrometheusAuthAuthenticationType2None OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "none"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType2Token Auth token
	OutputGrafanaCloudPrometheusAuthAuthenticationType2Token OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "token"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType2TextSecret Auth token (text secret)
	OutputGrafanaCloudPrometheusAuthAuthenticationType2TextSecret OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "textSecret"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType2Basic Basic
	OutputGrafanaCloudPrometheusAuthAuthenticationType2Basic OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "basic"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType2CredentialsSecret Basic (credentials secret)
	OutputGrafanaCloudPrometheusAuthAuthenticationType2CredentialsSecret OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "credentialsSecret"
)

func (e OutputGrafanaCloudPrometheusAuthAuthenticationType2) ToPointer() *OutputGrafanaCloudPrometheusAuthAuthenticationType2 {
	return &e
}

type OutputPrometheusAuth2 struct {
	AuthType *OutputGrafanaCloudPrometheusAuthAuthenticationType2 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputPrometheusAuth2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPrometheusAuth2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputPrometheusAuth2) GetAuthType() *OutputGrafanaCloudPrometheusAuthAuthenticationType2 {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputPrometheusAuth2) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputPrometheusAuth2) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputPrometheusAuth2) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputPrometheusAuth2) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputPrometheusAuth2) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type OutputGrafanaCloudLokiAuthAuthenticationType2 string

const (
	// OutputGrafanaCloudLokiAuthAuthenticationType2None None
	OutputGrafanaCloudLokiAuthAuthenticationType2None OutputGrafanaCloudLokiAuthAuthenticationType2 = "none"
	// OutputGrafanaCloudLokiAuthAuthenticationType2Token Auth token
	OutputGrafanaCloudLokiAuthAuthenticationType2Token OutputGrafanaCloudLokiAuthAuthenticationType2 = "token"
	// OutputGrafanaCloudLokiAuthAuthenticationType2TextSecret Auth token (text secret)
	OutputGrafanaCloudLokiAuthAuthenticationType2TextSecret OutputGrafanaCloudLokiAuthAuthenticationType2 = "textSecret"
	// OutputGrafanaCloudLokiAuthAuthenticationType2Basic Basic
	OutputGrafanaCloudLokiAuthAuthenticationType2Basic OutputGrafanaCloudLokiAuthAuthenticationType2 = "basic"
	// OutputGrafanaCloudLokiAuthAuthenticationType2CredentialsSecret Basic (credentials secret)
	OutputGrafanaCloudLokiAuthAuthenticationType2CredentialsSecret OutputGrafanaCloudLokiAuthAuthenticationType2 = "credentialsSecret"
)

func (e OutputGrafanaCloudLokiAuthAuthenticationType2) ToPointer() *OutputGrafanaCloudLokiAuthAuthenticationType2 {
	return &e
}

type OutputLokiAuth2 struct {
	AuthType *OutputGrafanaCloudLokiAuthAuthenticationType2 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputLokiAuth2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputLokiAuth2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputLokiAuth2) GetAuthType() *OutputGrafanaCloudLokiAuthAuthenticationType2 {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputLokiAuth2) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputLokiAuth2) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputLokiAuth2) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputLokiAuth2) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputLokiAuth2) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type OutputGrafanaCloudExtraHTTPHeader2 struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o OutputGrafanaCloudExtraHTTPHeader2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudExtraHTTPHeader2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudExtraHTTPHeader2) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudExtraHTTPHeader2) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// OutputGrafanaCloudFailedRequestLoggingMode2 - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type OutputGrafanaCloudFailedRequestLoggingMode2 string

const (
	// OutputGrafanaCloudFailedRequestLoggingMode2Payload Payload
	OutputGrafanaCloudFailedRequestLoggingMode2Payload OutputGrafanaCloudFailedRequestLoggingMode2 = "payload"
	// OutputGrafanaCloudFailedRequestLoggingMode2PayloadAndHeaders Payload + Headers
	OutputGrafanaCloudFailedRequestLoggingMode2PayloadAndHeaders OutputGrafanaCloudFailedRequestLoggingMode2 = "payloadAndHeaders"
	// OutputGrafanaCloudFailedRequestLoggingMode2None None
	OutputGrafanaCloudFailedRequestLoggingMode2None OutputGrafanaCloudFailedRequestLoggingMode2 = "none"
)

func (e OutputGrafanaCloudFailedRequestLoggingMode2) ToPointer() *OutputGrafanaCloudFailedRequestLoggingMode2 {
	return &e
}

type OutputGrafanaCloudResponseRetrySetting2 struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudResponseRetrySetting2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudResponseRetrySetting2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type OutputGrafanaCloudTimeoutRetrySettings2 struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudTimeoutRetrySettings2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// OutputGrafanaCloudBackpressureBehavior2 - How to handle events when all receivers are exerting backpressure
type OutputGrafanaCloudBackpressureBehavior2 string

const (
	// OutputGrafanaCloudBackpressureBehavior2Block Block
	OutputGrafanaCloudBackpressureBehavior2Block OutputGrafanaCloudBackpressureBehavior2 = "block"
	// OutputGrafanaCloudBackpressureBehavior2Drop Drop
	OutputGrafanaCloudBackpressureBehavior2Drop OutputGrafanaCloudBackpressureBehavior2 = "drop"
	// OutputGrafanaCloudBackpressureBehavior2Queue Persistent Queue
	OutputGrafanaCloudBackpressureBehavior2Queue OutputGrafanaCloudBackpressureBehavior2 = "queue"
)

func (e OutputGrafanaCloudBackpressureBehavior2) ToPointer() *OutputGrafanaCloudBackpressureBehavior2 {
	return &e
}

// OutputGrafanaCloudMode2 - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputGrafanaCloudMode2 string

const (
	// OutputGrafanaCloudMode2Error Error
	OutputGrafanaCloudMode2Error OutputGrafanaCloudMode2 = "error"
	// OutputGrafanaCloudMode2Always Backpressure
	OutputGrafanaCloudMode2Always OutputGrafanaCloudMode2 = "always"
	// OutputGrafanaCloudMode2Backpressure Always On
	OutputGrafanaCloudMode2Backpressure OutputGrafanaCloudMode2 = "backpressure"
)

func (e OutputGrafanaCloudMode2) ToPointer() *OutputGrafanaCloudMode2 {
	return &e
}

// OutputGrafanaCloudCompression2 - Codec to use to compress the persisted data
type OutputGrafanaCloudCompression2 string

const (
	// OutputGrafanaCloudCompression2None None
	OutputGrafanaCloudCompression2None OutputGrafanaCloudCompression2 = "none"
	// OutputGrafanaCloudCompression2Gzip Gzip
	OutputGrafanaCloudCompression2Gzip OutputGrafanaCloudCompression2 = "gzip"
)

func (e OutputGrafanaCloudCompression2) ToPointer() *OutputGrafanaCloudCompression2 {
	return &e
}

// OutputGrafanaCloudQueueFullBehavior2 - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type OutputGrafanaCloudQueueFullBehavior2 string

const (
	// OutputGrafanaCloudQueueFullBehavior2Block Block
	OutputGrafanaCloudQueueFullBehavior2Block OutputGrafanaCloudQueueFullBehavior2 = "block"
	// OutputGrafanaCloudQueueFullBehavior2Drop Drop new data
	OutputGrafanaCloudQueueFullBehavior2Drop OutputGrafanaCloudQueueFullBehavior2 = "drop"
)

func (e OutputGrafanaCloudQueueFullBehavior2) ToPointer() *OutputGrafanaCloudQueueFullBehavior2 {
	return &e
}

type OutputGrafanaCloudPqControls2 struct {
}

func (o OutputGrafanaCloudPqControls2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudPqControls2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGrafanaCloudGrafanaCloud2 struct {
	// Unique ID for this output
	ID   *string                 `json:"id,omitempty"`
	Type OutputGrafanaCloudType2 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net
	LokiURL *string `json:"lokiUrl,omitempty"`
	// The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push
	PrometheusURL string `json:"prometheusUrl"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Format to use when sending logs to Loki (Protobuf or JSON)
	MessageFormat *OutputGrafanaCloudMessageFormat2 `default:"protobuf" json:"messageFormat"`
	// List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'
	Labels []OutputGrafanaCloudLabel2 `json:"labels,omitempty"`
	// JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.
	MetricRenameExpr *string                `default:"name.replace(/[^a-zA-Z0-9_]/g, '_')" json:"metricRenameExpr"`
	PrometheusAuth   *OutputPrometheusAuth2 `json:"prometheusAuth,omitempty"`
	LokiAuth         *OutputLokiAuth2       `json:"lokiAuth,omitempty"`
	// Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.
	Concurrency *float64 `default:"1" json:"concurrency"`
	// Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	FlushPeriodSec *float64 `default:"15" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []OutputGrafanaCloudExtraHTTPHeader2 `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *OutputGrafanaCloudFailedRequestLoggingMode2 `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []OutputGrafanaCloudResponseRetrySetting2 `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *OutputGrafanaCloudTimeoutRetrySettings2  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *OutputGrafanaCloudBackpressureBehavior2 `default:"block" json:"onBackpressure"`
	Description    *string                                  `json:"description,omitempty"`
	// Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default.
	Compress *bool `default:"true" json:"compress"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputGrafanaCloudMode2 `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *OutputGrafanaCloudCompression2 `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *OutputGrafanaCloudQueueFullBehavior2 `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputGrafanaCloudPqControls2        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                        `additionalProperties:"true" json:"-"`
}

func (o OutputGrafanaCloudGrafanaCloud2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudGrafanaCloud2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "prometheusUrl"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetType() OutputGrafanaCloudType2 {
	if o == nil {
		return OutputGrafanaCloudType2("")
	}
	return o.Type
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetLokiURL() *string {
	if o == nil {
		return nil
	}
	return o.LokiURL
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPrometheusURL() string {
	if o == nil {
		return ""
	}
	return o.PrometheusURL
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMessageFormat() *OutputGrafanaCloudMessageFormat2 {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetLabels() []OutputGrafanaCloudLabel2 {
	if o == nil {
		return nil
	}
	return o.Labels
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMetricRenameExpr() *string {
	if o == nil {
		return nil
	}
	return o.MetricRenameExpr
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPrometheusAuth() *OutputPrometheusAuth2 {
	if o == nil {
		return nil
	}
	return o.PrometheusAuth
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetLokiAuth() *OutputLokiAuth2 {
	if o == nil {
		return nil
	}
	return o.LokiAuth
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetExtraHTTPHeaders() []OutputGrafanaCloudExtraHTTPHeader2 {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetFailedRequestLoggingMode() *OutputGrafanaCloudFailedRequestLoggingMode2 {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetResponseRetrySettings() []OutputGrafanaCloudResponseRetrySetting2 {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetTimeoutRetrySettings() *OutputGrafanaCloudTimeoutRetrySettings2 {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetOnBackpressure() *OutputGrafanaCloudBackpressureBehavior2 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMode() *OutputGrafanaCloudMode2 {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqCompress() *OutputGrafanaCloudCompression2 {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqOnBackpressure() *OutputGrafanaCloudQueueFullBehavior2 {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqControls() *OutputGrafanaCloudPqControls2 {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputGrafanaCloudType1 string

const (
	OutputGrafanaCloudType1GrafanaCloud OutputGrafanaCloudType1 = "grafana_cloud"
)

func (e OutputGrafanaCloudType1) ToPointer() *OutputGrafanaCloudType1 {
	return &e
}
func (e *OutputGrafanaCloudType1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grafana_cloud":
		*e = OutputGrafanaCloudType1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudType1: %v", v)
	}
}

// OutputGrafanaCloudMessageFormat1 - Format to use when sending logs to Loki (Protobuf or JSON)
type OutputGrafanaCloudMessageFormat1 string

const (
	// OutputGrafanaCloudMessageFormat1Protobuf Protobuf
	OutputGrafanaCloudMessageFormat1Protobuf OutputGrafanaCloudMessageFormat1 = "protobuf"
	// OutputGrafanaCloudMessageFormat1JSON JSON
	OutputGrafanaCloudMessageFormat1JSON OutputGrafanaCloudMessageFormat1 = "json"
)

func (e OutputGrafanaCloudMessageFormat1) ToPointer() *OutputGrafanaCloudMessageFormat1 {
	return &e
}

type OutputGrafanaCloudLabel1 struct {
	Name  *string `default:"" json:"name"`
	Value string  `json:"value"`
}

func (o OutputGrafanaCloudLabel1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudLabel1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudLabel1) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudLabel1) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputGrafanaCloudPrometheusAuthAuthenticationType1 string

const (
	// OutputGrafanaCloudPrometheusAuthAuthenticationType1None None
	OutputGrafanaCloudPrometheusAuthAuthenticationType1None OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "none"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType1Token Auth token
	OutputGrafanaCloudPrometheusAuthAuthenticationType1Token OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "token"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType1TextSecret Auth token (text secret)
	OutputGrafanaCloudPrometheusAuthAuthenticationType1TextSecret OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "textSecret"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType1Basic Basic
	OutputGrafanaCloudPrometheusAuthAuthenticationType1Basic OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "basic"
	// OutputGrafanaCloudPrometheusAuthAuthenticationType1CredentialsSecret Basic (credentials secret)
	OutputGrafanaCloudPrometheusAuthAuthenticationType1CredentialsSecret OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "credentialsSecret"
)

func (e OutputGrafanaCloudPrometheusAuthAuthenticationType1) ToPointer() *OutputGrafanaCloudPrometheusAuthAuthenticationType1 {
	return &e
}

type OutputPrometheusAuth1 struct {
	AuthType *OutputGrafanaCloudPrometheusAuthAuthenticationType1 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputPrometheusAuth1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPrometheusAuth1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputPrometheusAuth1) GetAuthType() *OutputGrafanaCloudPrometheusAuthAuthenticationType1 {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputPrometheusAuth1) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputPrometheusAuth1) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputPrometheusAuth1) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputPrometheusAuth1) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputPrometheusAuth1) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type OutputGrafanaCloudLokiAuthAuthenticationType1 string

const (
	// OutputGrafanaCloudLokiAuthAuthenticationType1None None
	OutputGrafanaCloudLokiAuthAuthenticationType1None OutputGrafanaCloudLokiAuthAuthenticationType1 = "none"
	// OutputGrafanaCloudLokiAuthAuthenticationType1Token Auth token
	OutputGrafanaCloudLokiAuthAuthenticationType1Token OutputGrafanaCloudLokiAuthAuthenticationType1 = "token"
	// OutputGrafanaCloudLokiAuthAuthenticationType1TextSecret Auth token (text secret)
	OutputGrafanaCloudLokiAuthAuthenticationType1TextSecret OutputGrafanaCloudLokiAuthAuthenticationType1 = "textSecret"
	// OutputGrafanaCloudLokiAuthAuthenticationType1Basic Basic
	OutputGrafanaCloudLokiAuthAuthenticationType1Basic OutputGrafanaCloudLokiAuthAuthenticationType1 = "basic"
	// OutputGrafanaCloudLokiAuthAuthenticationType1CredentialsSecret Basic (credentials secret)
	OutputGrafanaCloudLokiAuthAuthenticationType1CredentialsSecret OutputGrafanaCloudLokiAuthAuthenticationType1 = "credentialsSecret"
)

func (e OutputGrafanaCloudLokiAuthAuthenticationType1) ToPointer() *OutputGrafanaCloudLokiAuthAuthenticationType1 {
	return &e
}

type OutputLokiAuth1 struct {
	AuthType *OutputGrafanaCloudLokiAuthAuthenticationType1 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputLokiAuth1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputLokiAuth1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputLokiAuth1) GetAuthType() *OutputGrafanaCloudLokiAuthAuthenticationType1 {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputLokiAuth1) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputLokiAuth1) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputLokiAuth1) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputLokiAuth1) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputLokiAuth1) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type OutputGrafanaCloudExtraHTTPHeader1 struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o OutputGrafanaCloudExtraHTTPHeader1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudExtraHTTPHeader1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudExtraHTTPHeader1) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudExtraHTTPHeader1) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// OutputGrafanaCloudFailedRequestLoggingMode1 - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type OutputGrafanaCloudFailedRequestLoggingMode1 string

const (
	// OutputGrafanaCloudFailedRequestLoggingMode1Payload Payload
	OutputGrafanaCloudFailedRequestLoggingMode1Payload OutputGrafanaCloudFailedRequestLoggingMode1 = "payload"
	// OutputGrafanaCloudFailedRequestLoggingMode1PayloadAndHeaders Payload + Headers
	OutputGrafanaCloudFailedRequestLoggingMode1PayloadAndHeaders OutputGrafanaCloudFailedRequestLoggingMode1 = "payloadAndHeaders"
	// OutputGrafanaCloudFailedRequestLoggingMode1None None
	OutputGrafanaCloudFailedRequestLoggingMode1None OutputGrafanaCloudFailedRequestLoggingMode1 = "none"
)

func (e OutputGrafanaCloudFailedRequestLoggingMode1) ToPointer() *OutputGrafanaCloudFailedRequestLoggingMode1 {
	return &e
}

type OutputGrafanaCloudResponseRetrySetting1 struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudResponseRetrySetting1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudResponseRetrySetting1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type OutputGrafanaCloudTimeoutRetrySettings1 struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudTimeoutRetrySettings1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// OutputGrafanaCloudBackpressureBehavior1 - How to handle events when all receivers are exerting backpressure
type OutputGrafanaCloudBackpressureBehavior1 string

const (
	// OutputGrafanaCloudBackpressureBehavior1Block Block
	OutputGrafanaCloudBackpressureBehavior1Block OutputGrafanaCloudBackpressureBehavior1 = "block"
	// OutputGrafanaCloudBackpressureBehavior1Drop Drop
	OutputGrafanaCloudBackpressureBehavior1Drop OutputGrafanaCloudBackpressureBehavior1 = "drop"
	// OutputGrafanaCloudBackpressureBehavior1Queue Persistent Queue
	OutputGrafanaCloudBackpressureBehavior1Queue OutputGrafanaCloudBackpressureBehavior1 = "queue"
)

func (e OutputGrafanaCloudBackpressureBehavior1) ToPointer() *OutputGrafanaCloudBackpressureBehavior1 {
	return &e
}

// OutputGrafanaCloudMode1 - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputGrafanaCloudMode1 string

const (
	// OutputGrafanaCloudMode1Error Error
	OutputGrafanaCloudMode1Error OutputGrafanaCloudMode1 = "error"
	// OutputGrafanaCloudMode1Always Backpressure
	OutputGrafanaCloudMode1Always OutputGrafanaCloudMode1 = "always"
	// OutputGrafanaCloudMode1Backpressure Always On
	OutputGrafanaCloudMode1Backpressure OutputGrafanaCloudMode1 = "backpressure"
)

func (e OutputGrafanaCloudMode1) ToPointer() *OutputGrafanaCloudMode1 {
	return &e
}

// OutputGrafanaCloudCompression1 - Codec to use to compress the persisted data
type OutputGrafanaCloudCompression1 string

const (
	// OutputGrafanaCloudCompression1None None
	OutputGrafanaCloudCompression1None OutputGrafanaCloudCompression1 = "none"
	// OutputGrafanaCloudCompression1Gzip Gzip
	OutputGrafanaCloudCompression1Gzip OutputGrafanaCloudCompression1 = "gzip"
)

func (e OutputGrafanaCloudCompression1) ToPointer() *OutputGrafanaCloudCompression1 {
	return &e
}

// OutputGrafanaCloudQueueFullBehavior1 - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type OutputGrafanaCloudQueueFullBehavior1 string

const (
	// OutputGrafanaCloudQueueFullBehavior1Block Block
	OutputGrafanaCloudQueueFullBehavior1Block OutputGrafanaCloudQueueFullBehavior1 = "block"
	// OutputGrafanaCloudQueueFullBehavior1Drop Drop new data
	OutputGrafanaCloudQueueFullBehavior1Drop OutputGrafanaCloudQueueFullBehavior1 = "drop"
)

func (e OutputGrafanaCloudQueueFullBehavior1) ToPointer() *OutputGrafanaCloudQueueFullBehavior1 {
	return &e
}

type OutputGrafanaCloudPqControls1 struct {
}

func (o OutputGrafanaCloudPqControls1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudPqControls1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGrafanaCloudGrafanaCloud1 struct {
	// Unique ID for this output
	ID   *string                 `json:"id,omitempty"`
	Type OutputGrafanaCloudType1 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net
	LokiURL string `json:"lokiUrl"`
	// The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push
	PrometheusURL *string `json:"prometheusUrl,omitempty"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Format to use when sending logs to Loki (Protobuf or JSON)
	MessageFormat *OutputGrafanaCloudMessageFormat1 `default:"protobuf" json:"messageFormat"`
	// List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'
	Labels []OutputGrafanaCloudLabel1 `json:"labels,omitempty"`
	// JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.
	MetricRenameExpr *string                `default:"name.replace(/[^a-zA-Z0-9_]/g, '_')" json:"metricRenameExpr"`
	PrometheusAuth   *OutputPrometheusAuth1 `json:"prometheusAuth,omitempty"`
	LokiAuth         *OutputLokiAuth1       `json:"lokiAuth,omitempty"`
	// Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.
	Concurrency *float64 `default:"1" json:"concurrency"`
	// Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	FlushPeriodSec *float64 `default:"15" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []OutputGrafanaCloudExtraHTTPHeader1 `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *OutputGrafanaCloudFailedRequestLoggingMode1 `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []OutputGrafanaCloudResponseRetrySetting1 `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *OutputGrafanaCloudTimeoutRetrySettings1  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *OutputGrafanaCloudBackpressureBehavior1 `default:"block" json:"onBackpressure"`
	Description    *string                                  `json:"description,omitempty"`
	// Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default.
	Compress *bool `default:"true" json:"compress"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputGrafanaCloudMode1 `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *OutputGrafanaCloudCompression1 `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *OutputGrafanaCloudQueueFullBehavior1 `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputGrafanaCloudPqControls1        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                        `additionalProperties:"true" json:"-"`
}

func (o OutputGrafanaCloudGrafanaCloud1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudGrafanaCloud1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "lokiUrl"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetType() OutputGrafanaCloudType1 {
	if o == nil {
		return OutputGrafanaCloudType1("")
	}
	return o.Type
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetLokiURL() string {
	if o == nil {
		return ""
	}
	return o.LokiURL
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPrometheusURL() *string {
	if o == nil {
		return nil
	}
	return o.PrometheusURL
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMessageFormat() *OutputGrafanaCloudMessageFormat1 {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetLabels() []OutputGrafanaCloudLabel1 {
	if o == nil {
		return nil
	}
	return o.Labels
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMetricRenameExpr() *string {
	if o == nil {
		return nil
	}
	return o.MetricRenameExpr
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPrometheusAuth() *OutputPrometheusAuth1 {
	if o == nil {
		return nil
	}
	return o.PrometheusAuth
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetLokiAuth() *OutputLokiAuth1 {
	if o == nil {
		return nil
	}
	return o.LokiAuth
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetExtraHTTPHeaders() []OutputGrafanaCloudExtraHTTPHeader1 {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetFailedRequestLoggingMode() *OutputGrafanaCloudFailedRequestLoggingMode1 {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetResponseRetrySettings() []OutputGrafanaCloudResponseRetrySetting1 {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetTimeoutRetrySettings() *OutputGrafanaCloudTimeoutRetrySettings1 {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetOnBackpressure() *OutputGrafanaCloudBackpressureBehavior1 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMode() *OutputGrafanaCloudMode1 {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqCompress() *OutputGrafanaCloudCompression1 {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqOnBackpressure() *OutputGrafanaCloudQueueFullBehavior1 {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqControls() *OutputGrafanaCloudPqControls1 {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputGrafanaCloudType string

const (
	OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud1 OutputGrafanaCloudType = "OutputGrafanaCloud_GrafanaCloud_1"
	OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud2 OutputGrafanaCloudType = "OutputGrafanaCloud_GrafanaCloud_2"
)

type OutputGrafanaCloud struct {
	OutputGrafanaCloudGrafanaCloud1 *OutputGrafanaCloudGrafanaCloud1 `queryParam:"inline,name=OutputGrafanaCloud"`
	OutputGrafanaCloudGrafanaCloud2 *OutputGrafanaCloudGrafanaCloud2 `queryParam:"inline,name=OutputGrafanaCloud"`

	Type OutputGrafanaCloudType
}

func CreateOutputGrafanaCloudOutputGrafanaCloudGrafanaCloud1(outputGrafanaCloudGrafanaCloud1 OutputGrafanaCloudGrafanaCloud1) OutputGrafanaCloud {
	typ := OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud1

	return OutputGrafanaCloud{
		OutputGrafanaCloudGrafanaCloud1: &outputGrafanaCloudGrafanaCloud1,
		Type:                            typ,
	}
}

func CreateOutputGrafanaCloudOutputGrafanaCloudGrafanaCloud2(outputGrafanaCloudGrafanaCloud2 OutputGrafanaCloudGrafanaCloud2) OutputGrafanaCloud {
	typ := OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud2

	return OutputGrafanaCloud{
		OutputGrafanaCloudGrafanaCloud2: &outputGrafanaCloudGrafanaCloud2,
		Type:                            typ,
	}
}

func (u *OutputGrafanaCloud) UnmarshalJSON(data []byte) error {

	var outputGrafanaCloudGrafanaCloud1 OutputGrafanaCloudGrafanaCloud1 = OutputGrafanaCloudGrafanaCloud1{}
	if err := utils.UnmarshalJSON(data, &outputGrafanaCloudGrafanaCloud1, "", true, nil); err == nil {
		u.OutputGrafanaCloudGrafanaCloud1 = &outputGrafanaCloudGrafanaCloud1
		u.Type = OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud1
		return nil
	}

	var outputGrafanaCloudGrafanaCloud2 OutputGrafanaCloudGrafanaCloud2 = OutputGrafanaCloudGrafanaCloud2{}
	if err := utils.UnmarshalJSON(data, &outputGrafanaCloudGrafanaCloud2, "", true, nil); err == nil {
		u.OutputGrafanaCloudGrafanaCloud2 = &outputGrafanaCloudGrafanaCloud2
		u.Type = OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud2
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for OutputGrafanaCloud", string(data))
}

func (u OutputGrafanaCloud) MarshalJSON() ([]byte, error) {
	if u.OutputGrafanaCloudGrafanaCloud1 != nil {
		return utils.MarshalJSON(u.OutputGrafanaCloudGrafanaCloud1, "", true)
	}

	if u.OutputGrafanaCloudGrafanaCloud2 != nil {
		return utils.MarshalJSON(u.OutputGrafanaCloudGrafanaCloud2, "", true)
	}

	return nil, errors.New("could not marshal union type OutputGrafanaCloud: all fields are null")
}

type TypeDatadog string

const (
	TypeDatadogDatadog TypeDatadog = "datadog"
)

func (e TypeDatadog) ToPointer() *TypeDatadog {
	return &e
}
func (e *TypeDatadog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "datadog":
		*e = TypeDatadog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDatadog: %v", v)
	}
}

// SendLogsAs - The content type to use when sending logs
type SendLogsAs string

const (
	// SendLogsAsText text/plain
	SendLogsAsText SendLogsAs = "text"
	// SendLogsAsJSON application/json
	SendLogsAsJSON SendLogsAs = "json"
)

func (e SendLogsAs) ToPointer() *SendLogsAs {
	return &e
}

// SeverityDatadog - Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.
type SeverityDatadog string

const (
	// SeverityDatadogEmergency emergency
	SeverityDatadogEmergency SeverityDatadog = "emergency"
	// SeverityDatadogAlert alert
	SeverityDatadogAlert SeverityDatadog = "alert"
	// SeverityDatadogCritical critical
	SeverityDatadogCritical SeverityDatadog = "critical"
	// SeverityDatadogError error
	SeverityDatadogError SeverityDatadog = "error"
	// SeverityDatadogWarning warning
	SeverityDatadogWarning SeverityDatadog = "warning"
	// SeverityDatadogNotice notice
	SeverityDatadogNotice SeverityDatadog = "notice"
	// SeverityDatadogInfo info
	SeverityDatadogInfo SeverityDatadog = "info"
	// SeverityDatadogDebug debug
	SeverityDatadogDebug SeverityDatadog = "debug"
)

func (e SeverityDatadog) ToPointer() *SeverityDatadog {
	return &e
}

// DatadogSite - Datadog site to which events should be sent
type DatadogSite string

const (
	// DatadogSiteUs US
	DatadogSiteUs DatadogSite = "us"
	// DatadogSiteUs3 US3
	DatadogSiteUs3 DatadogSite = "us3"
	// DatadogSiteUs5 US5
	DatadogSiteUs5 DatadogSite = "us5"
	// DatadogSiteEu Europe
	DatadogSiteEu DatadogSite = "eu"
	// DatadogSiteFed1 US1-FED
	DatadogSiteFed1 DatadogSite = "fed1"
	// DatadogSiteAp1 AP1
	DatadogSiteAp1 DatadogSite = "ap1"
	// DatadogSiteCustom Custom
	DatadogSiteCustom DatadogSite = "custom"
)

func (e DatadogSite) ToPointer() *DatadogSite {
	return &e
}

type ExtraHTTPHeaderDatadog struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderDatadog) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderDatadog) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeDatadog - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDatadog string

const (
	// FailedRequestLoggingModeDatadogPayload Payload
	FailedRequestLoggingModeDatadogPayload FailedRequestLoggingModeDatadog = "payload"
	// FailedRequestLoggingModeDatadogPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeDatadogPayloadAndHeaders FailedRequestLoggingModeDatadog = "payloadAndHeaders"
	// FailedRequestLoggingModeDatadogNone None
	FailedRequestLoggingModeDatadogNone FailedRequestLoggingModeDatadog = "none"
)

func (e FailedRequestLoggingModeDatadog) ToPointer() *FailedRequestLoggingModeDatadog {
	return &e
}

type ResponseRetrySettingDatadog struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingDatadog) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingDatadog) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingDatadog) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingDatadog) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsDatadog struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsDatadog) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsDatadog) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsDatadog) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsDatadog) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorDatadog - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorDatadog string

const (
	// BackpressureBehaviorDatadogBlock Block
	BackpressureBehaviorDatadogBlock BackpressureBehaviorDatadog = "block"
	// BackpressureBehaviorDatadogDrop Drop
	BackpressureBehaviorDatadogDrop BackpressureBehaviorDatadog = "drop"
	// BackpressureBehaviorDatadogQueue Persistent Queue
	BackpressureBehaviorDatadogQueue BackpressureBehaviorDatadog = "queue"
)

func (e BackpressureBehaviorDatadog) ToPointer() *BackpressureBehaviorDatadog {
	return &e
}

// AuthenticationMethodDatadog - Enter API key directly, or select a stored secret
type AuthenticationMethodDatadog string

const (
	AuthenticationMethodDatadogManual AuthenticationMethodDatadog = "manual"
	AuthenticationMethodDatadogSecret AuthenticationMethodDatadog = "secret"
)

func (e AuthenticationMethodDatadog) ToPointer() *AuthenticationMethodDatadog {
	return &e
}

// ModeDatadog - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeDatadog string

const (
	// ModeDatadogError Error
	ModeDatadogError ModeDatadog = "error"
	// ModeDatadogAlways Backpressure
	ModeDatadogAlways ModeDatadog = "always"
	// ModeDatadogBackpressure Always On
	ModeDatadogBackpressure ModeDatadog = "backpressure"
)

func (e ModeDatadog) ToPointer() *ModeDatadog {
	return &e
}

// CompressionDatadog - Codec to use to compress the persisted data
type CompressionDatadog string

const (
	// CompressionDatadogNone None
	CompressionDatadogNone CompressionDatadog = "none"
	// CompressionDatadogGzip Gzip
	CompressionDatadogGzip CompressionDatadog = "gzip"
)

func (e CompressionDatadog) ToPointer() *CompressionDatadog {
	return &e
}

// QueueFullBehaviorDatadog - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDatadog string

const (
	// QueueFullBehaviorDatadogBlock Block
	QueueFullBehaviorDatadogBlock QueueFullBehaviorDatadog = "block"
	// QueueFullBehaviorDatadogDrop Drop new data
	QueueFullBehaviorDatadogDrop QueueFullBehaviorDatadog = "drop"
)

func (e QueueFullBehaviorDatadog) ToPointer() *QueueFullBehaviorDatadog {
	return &e
}

type PqControlsDatadog struct {
}

func (p PqControlsDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputDatadog struct {
	// Unique ID for this output
	ID   *string     `json:"id,omitempty"`
	Type TypeDatadog `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The content type to use when sending logs
	ContentType *SendLogsAs `default:"json" json:"contentType"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Name of the source to send with logs. When you send logs as JSON objects, the event's 'source' field (if set) will override this value.
	Source *string `json:"source,omitempty"`
	// Name of the host to send with logs. When you send logs as JSON objects, the event's 'host' field (if set) will override this value.
	Host *string `json:"host,omitempty"`
	// Name of the service to send with logs. When you send logs as JSON objects, the event's '__service' field (if set) will override this value.
	Service *string `json:"service,omitempty"`
	// List of tags to send with logs, such as 'env:prod' and 'env_staging:east'
	Tags []string `json:"tags,omitempty"`
	// Batch events by API key and the ddtags field on the event. When disabled, batches events only by API key. If incoming events have high cardinality in the ddtags field, disabling this setting may improve Destination performance.
	BatchByTags *bool `default:"true" json:"batchByTags"`
	// Allow API key to be set from the event's '__agent_api_key' field
	AllowAPIKeyFromEvents *bool `default:"false" json:"allowApiKeyFromEvents"`
	// Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.
	Severity *SeverityDatadog `json:"severity,omitempty"`
	// Datadog site to which events should be sent
	Site *DatadogSite `default:"us" json:"site"`
	// If not enabled, Datadog will transform 'counter' metrics to 'gauge'. [Learn more about Datadog metrics types.](https://docs.datadoghq.com/metrics/types/?tab=count)
	SendCountersAsCount *bool `default:"false" json:"sendCountersAsCount"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderDatadog `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDatadog `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingDatadog `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDatadog  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDatadog `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType *AuthenticationMethodDatadog `default:"manual" json:"authType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	CustomURL          *string  `json:"customUrl,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeDatadog `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionDatadog `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorDatadog `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsDatadog        `json:"pqControls,omitempty"`
	// Organization's API key in Datadog
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret           *string        `json:"textSecret,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDatadog) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDatadog) GetType() TypeDatadog {
	if o == nil {
		return TypeDatadog("")
	}
	return o.Type
}

func (o *OutputDatadog) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDatadog) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDatadog) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDatadog) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDatadog) GetContentType() *SendLogsAs {
	if o == nil {
		return nil
	}
	return o.ContentType
}

func (o *OutputDatadog) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputDatadog) GetSource() *string {
	if o == nil {
		return nil
	}
	return o.Source
}

func (o *OutputDatadog) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputDatadog) GetService() *string {
	if o == nil {
		return nil
	}
	return o.Service
}

func (o *OutputDatadog) GetTags() []string {
	if o == nil {
		return nil
	}
	return o.Tags
}

func (o *OutputDatadog) GetBatchByTags() *bool {
	if o == nil {
		return nil
	}
	return o.BatchByTags
}

func (o *OutputDatadog) GetAllowAPIKeyFromEvents() *bool {
	if o == nil {
		return nil
	}
	return o.AllowAPIKeyFromEvents
}

func (o *OutputDatadog) GetSeverity() *SeverityDatadog {
	if o == nil {
		return nil
	}
	return o.Severity
}

func (o *OutputDatadog) GetSite() *DatadogSite {
	if o == nil {
		return nil
	}
	return o.Site
}

func (o *OutputDatadog) GetSendCountersAsCount() *bool {
	if o == nil {
		return nil
	}
	return o.SendCountersAsCount
}

func (o *OutputDatadog) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDatadog) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDatadog) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputDatadog) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDatadog) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDatadog) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDatadog) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDatadog) GetExtraHTTPHeaders() []ExtraHTTPHeaderDatadog {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDatadog) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDatadog) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDatadog {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDatadog) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDatadog) GetResponseRetrySettings() []ResponseRetrySettingDatadog {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDatadog) GetTimeoutRetrySettings() *TimeoutRetrySettingsDatadog {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDatadog) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDatadog) GetOnBackpressure() *BackpressureBehaviorDatadog {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDatadog) GetAuthType() *AuthenticationMethodDatadog {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputDatadog) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputDatadog) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDatadog) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputDatadog) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputDatadog) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputDatadog) GetPqMode() *ModeDatadog {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDatadog) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputDatadog) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputDatadog) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDatadog) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDatadog) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDatadog) GetPqCompress() *CompressionDatadog {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDatadog) GetPqOnBackpressure() *QueueFullBehaviorDatadog {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDatadog) GetPqControls() *PqControlsDatadog {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDatadog) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputDatadog) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputDatadog) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeSumoLogic string

const (
	TypeSumoLogicSumoLogic TypeSumoLogic = "sumo_logic"
)

func (e TypeSumoLogic) ToPointer() *TypeSumoLogic {
	return &e
}
func (e *TypeSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sumo_logic":
		*e = TypeSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSumoLogic: %v", v)
	}
}

// DataFormatSumoLogic - Preserve the raw event format instead of JSONifying it
type DataFormatSumoLogic string

const (
	// DataFormatSumoLogicJSON JSON
	DataFormatSumoLogicJSON DataFormatSumoLogic = "json"
	// DataFormatSumoLogicRaw Raw
	DataFormatSumoLogicRaw DataFormatSumoLogic = "raw"
)

func (e DataFormatSumoLogic) ToPointer() *DataFormatSumoLogic {
	return &e
}

type ExtraHTTPHeaderSumoLogic struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderSumoLogic) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderSumoLogic) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeSumoLogic - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSumoLogic string

const (
	// FailedRequestLoggingModeSumoLogicPayload Payload
	FailedRequestLoggingModeSumoLogicPayload FailedRequestLoggingModeSumoLogic = "payload"
	// FailedRequestLoggingModeSumoLogicPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeSumoLogicPayloadAndHeaders FailedRequestLoggingModeSumoLogic = "payloadAndHeaders"
	// FailedRequestLoggingModeSumoLogicNone None
	FailedRequestLoggingModeSumoLogicNone FailedRequestLoggingModeSumoLogic = "none"
)

func (e FailedRequestLoggingModeSumoLogic) ToPointer() *FailedRequestLoggingModeSumoLogic {
	return &e
}

type ResponseRetrySettingSumoLogic struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingSumoLogic) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingSumoLogic) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingSumoLogic) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingSumoLogic) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsSumoLogic struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsSumoLogic) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsSumoLogic) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsSumoLogic) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsSumoLogic) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorSumoLogic - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSumoLogic string

const (
	// BackpressureBehaviorSumoLogicBlock Block
	BackpressureBehaviorSumoLogicBlock BackpressureBehaviorSumoLogic = "block"
	// BackpressureBehaviorSumoLogicDrop Drop
	BackpressureBehaviorSumoLogicDrop BackpressureBehaviorSumoLogic = "drop"
	// BackpressureBehaviorSumoLogicQueue Persistent Queue
	BackpressureBehaviorSumoLogicQueue BackpressureBehaviorSumoLogic = "queue"
)

func (e BackpressureBehaviorSumoLogic) ToPointer() *BackpressureBehaviorSumoLogic {
	return &e
}

// ModeSumoLogic - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSumoLogic string

const (
	// ModeSumoLogicError Error
	ModeSumoLogicError ModeSumoLogic = "error"
	// ModeSumoLogicAlways Backpressure
	ModeSumoLogicAlways ModeSumoLogic = "always"
	// ModeSumoLogicBackpressure Always On
	ModeSumoLogicBackpressure ModeSumoLogic = "backpressure"
)

func (e ModeSumoLogic) ToPointer() *ModeSumoLogic {
	return &e
}

// CompressionSumoLogic - Codec to use to compress the persisted data
type CompressionSumoLogic string

const (
	// CompressionSumoLogicNone None
	CompressionSumoLogicNone CompressionSumoLogic = "none"
	// CompressionSumoLogicGzip Gzip
	CompressionSumoLogicGzip CompressionSumoLogic = "gzip"
)

func (e CompressionSumoLogic) ToPointer() *CompressionSumoLogic {
	return &e
}

// QueueFullBehaviorSumoLogic - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSumoLogic string

const (
	// QueueFullBehaviorSumoLogicBlock Block
	QueueFullBehaviorSumoLogicBlock QueueFullBehaviorSumoLogic = "block"
	// QueueFullBehaviorSumoLogicDrop Drop new data
	QueueFullBehaviorSumoLogicDrop QueueFullBehaviorSumoLogic = "drop"
)

func (e QueueFullBehaviorSumoLogic) ToPointer() *QueueFullBehaviorSumoLogic {
	return &e
}

type PqControlsSumoLogic struct {
}

func (p PqControlsSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSumoLogic struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeSumoLogic `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Sumo Logic HTTP collector URL to which events should be sent
	URL string `json:"url"`
	// Override the source name configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceName field.
	CustomSource *string `json:"customSource,omitempty"`
	// Override the source category configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceCategory field.
	CustomCategory *string `json:"customCategory,omitempty"`
	// Preserve the raw event format instead of JSONifying it
	Format *DataFormatSumoLogic `default:"json" json:"format"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderSumoLogic `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSumoLogic `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSumoLogic `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSumoLogic  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSumoLogic `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSumoLogic `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSumoLogic `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorSumoLogic `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsSumoLogic        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any              `additionalProperties:"true" json:"-"`
}

func (o OutputSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "url"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSumoLogic) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSumoLogic) GetType() TypeSumoLogic {
	if o == nil {
		return TypeSumoLogic("")
	}
	return o.Type
}

func (o *OutputSumoLogic) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSumoLogic) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSumoLogic) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSumoLogic) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSumoLogic) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputSumoLogic) GetCustomSource() *string {
	if o == nil {
		return nil
	}
	return o.CustomSource
}

func (o *OutputSumoLogic) GetCustomCategory() *string {
	if o == nil {
		return nil
	}
	return o.CustomCategory
}

func (o *OutputSumoLogic) GetFormat() *DataFormatSumoLogic {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputSumoLogic) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSumoLogic) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSumoLogic) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSumoLogic) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSumoLogic) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSumoLogic) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSumoLogic) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSumoLogic) GetExtraHTTPHeaders() []ExtraHTTPHeaderSumoLogic {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSumoLogic) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSumoLogic) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSumoLogic {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSumoLogic) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSumoLogic) GetResponseRetrySettings() []ResponseRetrySettingSumoLogic {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSumoLogic) GetTimeoutRetrySettings() *TimeoutRetrySettingsSumoLogic {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSumoLogic) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSumoLogic) GetOnBackpressure() *BackpressureBehaviorSumoLogic {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSumoLogic) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputSumoLogic) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSumoLogic) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSumoLogic) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSumoLogic) GetPqMode() *ModeSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSumoLogic) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSumoLogic) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSumoLogic) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSumoLogic) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSumoLogic) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSumoLogic) GetPqCompress() *CompressionSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSumoLogic) GetPqOnBackpressure() *QueueFullBehaviorSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSumoLogic) GetPqControls() *PqControlsSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSumoLogic) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeSnmp string

const (
	OutputTypeSnmpSnmp OutputTypeSnmp = "snmp"
)

func (e OutputTypeSnmp) ToPointer() *OutputTypeSnmp {
	return &e
}
func (e *OutputTypeSnmp) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "snmp":
		*e = OutputTypeSnmp(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSnmp: %v", v)
	}
}

type HostSnmp struct {
	// Destination host
	Host string `json:"host"`
	// Destination port, default is 162
	Port *float64 `default:"162" json:"port"`
}

func (h HostSnmp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostSnmp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host"}); err != nil {
		return err
	}
	return nil
}

func (h *HostSnmp) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostSnmp) GetPort() *float64 {
	if h == nil {
		return nil
	}
	return h.Port
}

type OutputSnmp struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type OutputTypeSnmp `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// One or more SNMP destinations to forward traps to
	Hosts []HostSnmp `json:"hosts"`
	// How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every trap sent will incur a DNS lookup.
	DNSResolvePeriodSec  *float64       `default:"0" json:"dnsResolvePeriodSec"`
	Description          *string        `json:"description,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputSnmp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSnmp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "hosts"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSnmp) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSnmp) GetType() OutputTypeSnmp {
	if o == nil {
		return OutputTypeSnmp("")
	}
	return o.Type
}

func (o *OutputSnmp) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSnmp) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSnmp) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSnmp) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSnmp) GetHosts() []HostSnmp {
	if o == nil {
		return []HostSnmp{}
	}
	return o.Hosts
}

func (o *OutputSnmp) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSnmp) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSnmp) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeSqs string

const (
	OutputTypeSqsSqs OutputTypeSqs = "sqs"
)

func (e OutputTypeSqs) ToPointer() *OutputTypeSqs {
	return &e
}
func (e *OutputTypeSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sqs":
		*e = OutputTypeSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSqs: %v", v)
	}
}

// OutputQueueType - The queue type used (or created). Defaults to Standard.
type OutputQueueType string

const (
	// OutputQueueTypeStandard Standard
	OutputQueueTypeStandard OutputQueueType = "standard"
	// OutputQueueTypeFifo FIFO
	OutputQueueTypeFifo OutputQueueType = "fifo"
)

func (e OutputQueueType) ToPointer() *OutputQueueType {
	return &e
}

// OutputAuthenticationMethodSqs - AWS authentication method. Choose Auto to use IAM roles.
type OutputAuthenticationMethodSqs string

const (
	// OutputAuthenticationMethodSqsAuto Auto
	OutputAuthenticationMethodSqsAuto OutputAuthenticationMethodSqs = "auto"
	// OutputAuthenticationMethodSqsManual Manual
	OutputAuthenticationMethodSqsManual OutputAuthenticationMethodSqs = "manual"
	// OutputAuthenticationMethodSqsSecret Secret Key pair
	OutputAuthenticationMethodSqsSecret OutputAuthenticationMethodSqs = "secret"
)

func (e OutputAuthenticationMethodSqs) ToPointer() *OutputAuthenticationMethodSqs {
	return &e
}

// OutputSignatureVersionSqs - Signature version to use for signing SQS requests
type OutputSignatureVersionSqs string

const (
	OutputSignatureVersionSqsV2 OutputSignatureVersionSqs = "v2"
	OutputSignatureVersionSqsV4 OutputSignatureVersionSqs = "v4"
)

func (e OutputSignatureVersionSqs) ToPointer() *OutputSignatureVersionSqs {
	return &e
}

// BackpressureBehaviorSqs - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSqs string

const (
	// BackpressureBehaviorSqsBlock Block
	BackpressureBehaviorSqsBlock BackpressureBehaviorSqs = "block"
	// BackpressureBehaviorSqsDrop Drop
	BackpressureBehaviorSqsDrop BackpressureBehaviorSqs = "drop"
	// BackpressureBehaviorSqsQueue Persistent Queue
	BackpressureBehaviorSqsQueue BackpressureBehaviorSqs = "queue"
)

func (e BackpressureBehaviorSqs) ToPointer() *BackpressureBehaviorSqs {
	return &e
}

// OutputModeSqs - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeSqs string

const (
	// OutputModeSqsError Error
	OutputModeSqsError OutputModeSqs = "error"
	// OutputModeSqsAlways Backpressure
	OutputModeSqsAlways OutputModeSqs = "always"
	// OutputModeSqsBackpressure Always On
	OutputModeSqsBackpressure OutputModeSqs = "backpressure"
)

func (e OutputModeSqs) ToPointer() *OutputModeSqs {
	return &e
}

// PqCompressCompressionSqs - Codec to use to compress the persisted data
type PqCompressCompressionSqs string

const (
	// PqCompressCompressionSqsNone None
	PqCompressCompressionSqsNone PqCompressCompressionSqs = "none"
	// PqCompressCompressionSqsGzip Gzip
	PqCompressCompressionSqsGzip PqCompressCompressionSqs = "gzip"
)

func (e PqCompressCompressionSqs) ToPointer() *PqCompressCompressionSqs {
	return &e
}

// QueueFullBehaviorSqs - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSqs string

const (
	// QueueFullBehaviorSqsBlock Block
	QueueFullBehaviorSqsBlock QueueFullBehaviorSqs = "block"
	// QueueFullBehaviorSqsDrop Drop new data
	QueueFullBehaviorSqsDrop QueueFullBehaviorSqs = "drop"
)

func (e QueueFullBehaviorSqs) ToPointer() *QueueFullBehaviorSqs {
	return &e
}

type OutputPqControlsSqs struct {
}

func (o OutputPqControlsSqs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsSqs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSqs struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type OutputTypeSqs `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The name, URL, or ARN of the SQS queue to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
	QueueName string `json:"queueName"`
	// The queue type used (or created). Defaults to Standard.
	QueueType OutputQueueType `json:"queueType"`
	// SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
	AwsAccountID *string `json:"awsAccountId,omitempty"`
	// This parameter applies only to FIFO queues. The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Use event field __messageGroupId to override this value.
	MessageGroupID *string `default:"cribl" json:"messageGroupId"`
	// Create queue if it does not exist.
	CreateQueue *bool `default:"true" json:"createQueue"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputAuthenticationMethodSqs `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                        `json:"awsSecretKey,omitempty"`
	// AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.
	Region *string `json:"region,omitempty"`
	// SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing SQS requests
	SignatureVersion *OutputSignatureVersionSqs `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access SQS
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of queued batches before blocking.
	MaxQueueSize *float64 `default:"100" json:"maxQueueSize"`
	// Maximum size (KB) of batches to send. Per the SQS spec, the max allowed value is 256 KB.
	MaxRecordSizeKB *float64 `default:"256" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// The maximum number of in-progress API requests before backpressure is applied.
	MaxInProgress *float64 `default:"10" json:"maxInProgress"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSqs `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeSqs `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionSqs `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorSqs `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsSqs  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any        `additionalProperties:"true" json:"-"`
}

func (o OutputSqs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSqs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "queueName", "queueType"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSqs) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSqs) GetType() OutputTypeSqs {
	if o == nil {
		return OutputTypeSqs("")
	}
	return o.Type
}

func (o *OutputSqs) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSqs) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSqs) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSqs) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSqs) GetQueueName() string {
	if o == nil {
		return ""
	}
	return o.QueueName
}

func (o *OutputSqs) GetQueueType() OutputQueueType {
	if o == nil {
		return OutputQueueType("")
	}
	return o.QueueType
}

func (o *OutputSqs) GetAwsAccountID() *string {
	if o == nil {
		return nil
	}
	return o.AwsAccountID
}

func (o *OutputSqs) GetMessageGroupID() *string {
	if o == nil {
		return nil
	}
	return o.MessageGroupID
}

func (o *OutputSqs) GetCreateQueue() *bool {
	if o == nil {
		return nil
	}
	return o.CreateQueue
}

func (o *OutputSqs) GetAwsAuthenticationMethod() *OutputAuthenticationMethodSqs {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSqs) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSqs) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputSqs) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSqs) GetSignatureVersion() *OutputSignatureVersionSqs {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSqs) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSqs) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSqs) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSqs) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputSqs) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSqs) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSqs) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputSqs) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputSqs) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSqs) GetMaxInProgress() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxInProgress
}

func (o *OutputSqs) GetOnBackpressure() *BackpressureBehaviorSqs {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSqs) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSqs) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSqs) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSqs) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSqs) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSqs) GetPqMode() *OutputModeSqs {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSqs) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSqs) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSqs) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSqs) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSqs) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSqs) GetPqCompress() *PqCompressCompressionSqs {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSqs) GetPqOnBackpressure() *QueueFullBehaviorSqs {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSqs) GetPqControls() *OutputPqControlsSqs {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSqs) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeSns string

const (
	TypeSnsSns TypeSns = "sns"
)

func (e TypeSns) ToPointer() *TypeSns {
	return &e
}
func (e *TypeSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sns":
		*e = TypeSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSns: %v", v)
	}
}

// AuthenticationMethodSns - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodSns string

const (
	// AuthenticationMethodSnsAuto Auto
	AuthenticationMethodSnsAuto AuthenticationMethodSns = "auto"
	// AuthenticationMethodSnsManual Manual
	AuthenticationMethodSnsManual AuthenticationMethodSns = "manual"
	// AuthenticationMethodSnsSecret Secret Key pair
	AuthenticationMethodSnsSecret AuthenticationMethodSns = "secret"
)

func (e AuthenticationMethodSns) ToPointer() *AuthenticationMethodSns {
	return &e
}

// SignatureVersionSns - Signature version to use for signing SNS requests
type SignatureVersionSns string

const (
	SignatureVersionSnsV2 SignatureVersionSns = "v2"
	SignatureVersionSnsV4 SignatureVersionSns = "v4"
)

func (e SignatureVersionSns) ToPointer() *SignatureVersionSns {
	return &e
}

// BackpressureBehaviorSns - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSns string

const (
	// BackpressureBehaviorSnsBlock Block
	BackpressureBehaviorSnsBlock BackpressureBehaviorSns = "block"
	// BackpressureBehaviorSnsDrop Drop
	BackpressureBehaviorSnsDrop BackpressureBehaviorSns = "drop"
	// BackpressureBehaviorSnsQueue Persistent Queue
	BackpressureBehaviorSnsQueue BackpressureBehaviorSns = "queue"
)

func (e BackpressureBehaviorSns) ToPointer() *BackpressureBehaviorSns {
	return &e
}

// ModeSns - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSns string

const (
	// ModeSnsError Error
	ModeSnsError ModeSns = "error"
	// ModeSnsAlways Backpressure
	ModeSnsAlways ModeSns = "always"
	// ModeSnsBackpressure Always On
	ModeSnsBackpressure ModeSns = "backpressure"
)

func (e ModeSns) ToPointer() *ModeSns {
	return &e
}

// CompressionSns - Codec to use to compress the persisted data
type CompressionSns string

const (
	// CompressionSnsNone None
	CompressionSnsNone CompressionSns = "none"
	// CompressionSnsGzip Gzip
	CompressionSnsGzip CompressionSns = "gzip"
)

func (e CompressionSns) ToPointer() *CompressionSns {
	return &e
}

// QueueFullBehaviorSns - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSns string

const (
	// QueueFullBehaviorSnsBlock Block
	QueueFullBehaviorSnsBlock QueueFullBehaviorSns = "block"
	// QueueFullBehaviorSnsDrop Drop new data
	QueueFullBehaviorSnsDrop QueueFullBehaviorSns = "drop"
)

func (e QueueFullBehaviorSns) ToPointer() *QueueFullBehaviorSns {
	return &e
}

type PqControlsSns struct {
}

func (p PqControlsSns) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSns) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSns struct {
	// Unique ID for this output
	ID   *string `json:"id,omitempty"`
	Type TypeSns `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The ARN of the SNS topic to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. E.g., 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`
	TopicArn string `json:"topicArn"`
	// Messages in the same group are processed in a FIFO manner. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
	MessageGroupID string `json:"messageGroupId"`
	// Maximum number of retries before the output returns an error. Note that not all errors are retryable. The retries use an exponential backoff policy.
	MaxRetries *float64 `json:"maxRetries,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodSns `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                  `json:"awsSecretKey,omitempty"`
	// Region where the SNS is located
	Region *string `json:"region,omitempty"`
	// SNS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SNS-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing SNS requests
	SignatureVersion *SignatureVersionSns `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access SNS
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSns `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSns `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSns `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorSns `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsSns        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any        `additionalProperties:"true" json:"-"`
}

func (o OutputSns) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSns) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "topicArn", "messageGroupId"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSns) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSns) GetType() TypeSns {
	if o == nil {
		return TypeSns("")
	}
	return o.Type
}

func (o *OutputSns) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSns) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSns) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSns) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSns) GetTopicArn() string {
	if o == nil {
		return ""
	}
	return o.TopicArn
}

func (o *OutputSns) GetMessageGroupID() string {
	if o == nil {
		return ""
	}
	return o.MessageGroupID
}

func (o *OutputSns) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputSns) GetAwsAuthenticationMethod() *AuthenticationMethodSns {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSns) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSns) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputSns) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSns) GetSignatureVersion() *SignatureVersionSns {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSns) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSns) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSns) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSns) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputSns) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSns) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSns) GetOnBackpressure() *BackpressureBehaviorSns {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSns) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSns) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSns) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSns) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSns) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSns) GetPqMode() *ModeSns {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSns) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSns) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSns) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSns) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSns) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSns) GetPqCompress() *CompressionSns {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSns) GetPqOnBackpressure() *QueueFullBehaviorSns {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSns) GetPqControls() *PqControlsSns {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSns) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeRouter string

const (
	TypeRouterRouter TypeRouter = "router"
)

func (e TypeRouter) ToPointer() *TypeRouter {
	return &e
}
func (e *TypeRouter) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "router":
		*e = TypeRouter(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeRouter: %v", v)
	}
}

type OutputRule struct {
	// JavaScript expression to select events to send to output
	Filter string `json:"filter"`
	// Output to send matching events to
	Output string `json:"output"`
	// Description of this rule's purpose
	Description *string `json:"description,omitempty"`
	// Flag to control whether to stop the event from being checked against other rules
	Final *bool `default:"true" json:"final"`
}

func (o OutputRule) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputRule) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"filter", "output"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputRule) GetFilter() string {
	if o == nil {
		return ""
	}
	return o.Filter
}

func (o *OutputRule) GetOutput() string {
	if o == nil {
		return ""
	}
	return o.Output
}

func (o *OutputRule) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputRule) GetFinal() *bool {
	if o == nil {
		return nil
	}
	return o.Final
}

type OutputRouter struct {
	// Unique ID for this output
	ID   *string    `json:"id,omitempty"`
	Type TypeRouter `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Event routing rules
	Rules                []OutputRule   `json:"rules"`
	Description          *string        `json:"description,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputRouter) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputRouter) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "rules"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputRouter) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputRouter) GetType() TypeRouter {
	if o == nil {
		return TypeRouter("")
	}
	return o.Type
}

func (o *OutputRouter) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputRouter) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputRouter) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputRouter) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputRouter) GetRules() []OutputRule {
	if o == nil {
		return []OutputRule{}
	}
	return o.Rules
}

func (o *OutputRouter) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputRouter) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeGraphite string

const (
	TypeGraphiteGraphite TypeGraphite = "graphite"
)

func (e TypeGraphite) ToPointer() *TypeGraphite {
	return &e
}
func (e *TypeGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "graphite":
		*e = TypeGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGraphite: %v", v)
	}
}

// DestinationProtocolGraphite - Protocol to use when communicating with the destination.
type DestinationProtocolGraphite string

const (
	// DestinationProtocolGraphiteUDP UDP
	DestinationProtocolGraphiteUDP DestinationProtocolGraphite = "udp"
	// DestinationProtocolGraphiteTCP TCP
	DestinationProtocolGraphiteTCP DestinationProtocolGraphite = "tcp"
)

func (e DestinationProtocolGraphite) ToPointer() *DestinationProtocolGraphite {
	return &e
}

// BackpressureBehaviorGraphite - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGraphite string

const (
	// BackpressureBehaviorGraphiteBlock Block
	BackpressureBehaviorGraphiteBlock BackpressureBehaviorGraphite = "block"
	// BackpressureBehaviorGraphiteDrop Drop
	BackpressureBehaviorGraphiteDrop BackpressureBehaviorGraphite = "drop"
	// BackpressureBehaviorGraphiteQueue Persistent Queue
	BackpressureBehaviorGraphiteQueue BackpressureBehaviorGraphite = "queue"
)

func (e BackpressureBehaviorGraphite) ToPointer() *BackpressureBehaviorGraphite {
	return &e
}

// ModeGraphite - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeGraphite string

const (
	// ModeGraphiteError Error
	ModeGraphiteError ModeGraphite = "error"
	// ModeGraphiteAlways Backpressure
	ModeGraphiteAlways ModeGraphite = "always"
	// ModeGraphiteBackpressure Always On
	ModeGraphiteBackpressure ModeGraphite = "backpressure"
)

func (e ModeGraphite) ToPointer() *ModeGraphite {
	return &e
}

// CompressionGraphite - Codec to use to compress the persisted data
type CompressionGraphite string

const (
	// CompressionGraphiteNone None
	CompressionGraphiteNone CompressionGraphite = "none"
	// CompressionGraphiteGzip Gzip
	CompressionGraphiteGzip CompressionGraphite = "gzip"
)

func (e CompressionGraphite) ToPointer() *CompressionGraphite {
	return &e
}

// QueueFullBehaviorGraphite - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGraphite string

const (
	// QueueFullBehaviorGraphiteBlock Block
	QueueFullBehaviorGraphiteBlock QueueFullBehaviorGraphite = "block"
	// QueueFullBehaviorGraphiteDrop Drop new data
	QueueFullBehaviorGraphiteDrop QueueFullBehaviorGraphite = "drop"
)

func (e QueueFullBehaviorGraphite) ToPointer() *QueueFullBehaviorGraphite {
	return &e
}

type PqControlsGraphite struct {
}

func (p PqControlsGraphite) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsGraphite) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGraphite struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type TypeGraphite `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolGraphite `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGraphite `default:"block" json:"onBackpressure"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeGraphite `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionGraphite `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorGraphite `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsGraphite        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any             `additionalProperties:"true" json:"-"`
}

func (o OutputGraphite) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGraphite) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "host"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGraphite) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGraphite) GetType() TypeGraphite {
	if o == nil {
		return TypeGraphite("")
	}
	return o.Type
}

func (o *OutputGraphite) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGraphite) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGraphite) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGraphite) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGraphite) GetProtocol() *DestinationProtocolGraphite {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputGraphite) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputGraphite) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputGraphite) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputGraphite) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGraphite) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputGraphite) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGraphite) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputGraphite) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputGraphite) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputGraphite) GetOnBackpressure() *BackpressureBehaviorGraphite {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGraphite) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGraphite) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGraphite) GetPqMode() *ModeGraphite {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGraphite) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGraphite) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGraphite) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGraphite) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGraphite) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGraphite) GetPqCompress() *CompressionGraphite {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGraphite) GetPqOnBackpressure() *QueueFullBehaviorGraphite {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGraphite) GetPqControls() *PqControlsGraphite {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGraphite) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeStatsdExt string

const (
	TypeStatsdExtStatsdExt TypeStatsdExt = "statsd_ext"
)

func (e TypeStatsdExt) ToPointer() *TypeStatsdExt {
	return &e
}
func (e *TypeStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "statsd_ext":
		*e = TypeStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeStatsdExt: %v", v)
	}
}

// DestinationProtocolStatsdExt - Protocol to use when communicating with the destination.
type DestinationProtocolStatsdExt string

const (
	// DestinationProtocolStatsdExtUDP UDP
	DestinationProtocolStatsdExtUDP DestinationProtocolStatsdExt = "udp"
	// DestinationProtocolStatsdExtTCP TCP
	DestinationProtocolStatsdExtTCP DestinationProtocolStatsdExt = "tcp"
)

func (e DestinationProtocolStatsdExt) ToPointer() *DestinationProtocolStatsdExt {
	return &e
}

// BackpressureBehaviorStatsdExt - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorStatsdExt string

const (
	// BackpressureBehaviorStatsdExtBlock Block
	BackpressureBehaviorStatsdExtBlock BackpressureBehaviorStatsdExt = "block"
	// BackpressureBehaviorStatsdExtDrop Drop
	BackpressureBehaviorStatsdExtDrop BackpressureBehaviorStatsdExt = "drop"
	// BackpressureBehaviorStatsdExtQueue Persistent Queue
	BackpressureBehaviorStatsdExtQueue BackpressureBehaviorStatsdExt = "queue"
)

func (e BackpressureBehaviorStatsdExt) ToPointer() *BackpressureBehaviorStatsdExt {
	return &e
}

// ModeStatsdExt - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeStatsdExt string

const (
	// ModeStatsdExtError Error
	ModeStatsdExtError ModeStatsdExt = "error"
	// ModeStatsdExtAlways Backpressure
	ModeStatsdExtAlways ModeStatsdExt = "always"
	// ModeStatsdExtBackpressure Always On
	ModeStatsdExtBackpressure ModeStatsdExt = "backpressure"
)

func (e ModeStatsdExt) ToPointer() *ModeStatsdExt {
	return &e
}

// CompressionStatsdExt - Codec to use to compress the persisted data
type CompressionStatsdExt string

const (
	// CompressionStatsdExtNone None
	CompressionStatsdExtNone CompressionStatsdExt = "none"
	// CompressionStatsdExtGzip Gzip
	CompressionStatsdExtGzip CompressionStatsdExt = "gzip"
)

func (e CompressionStatsdExt) ToPointer() *CompressionStatsdExt {
	return &e
}

// QueueFullBehaviorStatsdExt - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorStatsdExt string

const (
	// QueueFullBehaviorStatsdExtBlock Block
	QueueFullBehaviorStatsdExtBlock QueueFullBehaviorStatsdExt = "block"
	// QueueFullBehaviorStatsdExtDrop Drop new data
	QueueFullBehaviorStatsdExtDrop QueueFullBehaviorStatsdExt = "drop"
)

func (e QueueFullBehaviorStatsdExt) ToPointer() *QueueFullBehaviorStatsdExt {
	return &e
}

type PqControlsStatsdExt struct {
}

func (p PqControlsStatsdExt) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsStatsdExt) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputStatsdExt struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeStatsdExt `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolStatsdExt `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorStatsdExt `default:"block" json:"onBackpressure"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeStatsdExt `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionStatsdExt `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorStatsdExt `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsStatsdExt        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any              `additionalProperties:"true" json:"-"`
}

func (o OutputStatsdExt) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputStatsdExt) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "host"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputStatsdExt) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputStatsdExt) GetType() TypeStatsdExt {
	if o == nil {
		return TypeStatsdExt("")
	}
	return o.Type
}

func (o *OutputStatsdExt) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputStatsdExt) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputStatsdExt) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputStatsdExt) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputStatsdExt) GetProtocol() *DestinationProtocolStatsdExt {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputStatsdExt) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputStatsdExt) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputStatsdExt) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputStatsdExt) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputStatsdExt) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputStatsdExt) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputStatsdExt) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputStatsdExt) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputStatsdExt) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputStatsdExt) GetOnBackpressure() *BackpressureBehaviorStatsdExt {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputStatsdExt) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputStatsdExt) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputStatsdExt) GetPqMode() *ModeStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputStatsdExt) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputStatsdExt) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputStatsdExt) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputStatsdExt) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputStatsdExt) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputStatsdExt) GetPqCompress() *CompressionStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputStatsdExt) GetPqOnBackpressure() *QueueFullBehaviorStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputStatsdExt) GetPqControls() *PqControlsStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputStatsdExt) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeStatsd string

const (
	TypeStatsdStatsd TypeStatsd = "statsd"
)

func (e TypeStatsd) ToPointer() *TypeStatsd {
	return &e
}
func (e *TypeStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "statsd":
		*e = TypeStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeStatsd: %v", v)
	}
}

// DestinationProtocolStatsd - Protocol to use when communicating with the destination.
type DestinationProtocolStatsd string

const (
	// DestinationProtocolStatsdUDP UDP
	DestinationProtocolStatsdUDP DestinationProtocolStatsd = "udp"
	// DestinationProtocolStatsdTCP TCP
	DestinationProtocolStatsdTCP DestinationProtocolStatsd = "tcp"
)

func (e DestinationProtocolStatsd) ToPointer() *DestinationProtocolStatsd {
	return &e
}

// BackpressureBehaviorStatsd - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorStatsd string

const (
	// BackpressureBehaviorStatsdBlock Block
	BackpressureBehaviorStatsdBlock BackpressureBehaviorStatsd = "block"
	// BackpressureBehaviorStatsdDrop Drop
	BackpressureBehaviorStatsdDrop BackpressureBehaviorStatsd = "drop"
	// BackpressureBehaviorStatsdQueue Persistent Queue
	BackpressureBehaviorStatsdQueue BackpressureBehaviorStatsd = "queue"
)

func (e BackpressureBehaviorStatsd) ToPointer() *BackpressureBehaviorStatsd {
	return &e
}

// ModeStatsd - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeStatsd string

const (
	// ModeStatsdError Error
	ModeStatsdError ModeStatsd = "error"
	// ModeStatsdAlways Backpressure
	ModeStatsdAlways ModeStatsd = "always"
	// ModeStatsdBackpressure Always On
	ModeStatsdBackpressure ModeStatsd = "backpressure"
)

func (e ModeStatsd) ToPointer() *ModeStatsd {
	return &e
}

// CompressionStatsd - Codec to use to compress the persisted data
type CompressionStatsd string

const (
	// CompressionStatsdNone None
	CompressionStatsdNone CompressionStatsd = "none"
	// CompressionStatsdGzip Gzip
	CompressionStatsdGzip CompressionStatsd = "gzip"
)

func (e CompressionStatsd) ToPointer() *CompressionStatsd {
	return &e
}

// QueueFullBehaviorStatsd - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorStatsd string

const (
	// QueueFullBehaviorStatsdBlock Block
	QueueFullBehaviorStatsdBlock QueueFullBehaviorStatsd = "block"
	// QueueFullBehaviorStatsdDrop Drop new data
	QueueFullBehaviorStatsdDrop QueueFullBehaviorStatsd = "drop"
)

func (e QueueFullBehaviorStatsd) ToPointer() *QueueFullBehaviorStatsd {
	return &e
}

type PqControlsStatsd struct {
}

func (p PqControlsStatsd) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsStatsd) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputStatsd struct {
	// Unique ID for this output
	ID   *string    `json:"id,omitempty"`
	Type TypeStatsd `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolStatsd `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorStatsd `default:"block" json:"onBackpressure"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeStatsd `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionStatsd `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorStatsd `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsStatsd        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any           `additionalProperties:"true" json:"-"`
}

func (o OutputStatsd) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputStatsd) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "host"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputStatsd) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputStatsd) GetType() TypeStatsd {
	if o == nil {
		return TypeStatsd("")
	}
	return o.Type
}

func (o *OutputStatsd) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputStatsd) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputStatsd) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputStatsd) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputStatsd) GetProtocol() *DestinationProtocolStatsd {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputStatsd) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputStatsd) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputStatsd) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputStatsd) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputStatsd) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputStatsd) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputStatsd) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputStatsd) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputStatsd) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputStatsd) GetOnBackpressure() *BackpressureBehaviorStatsd {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputStatsd) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputStatsd) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputStatsd) GetPqMode() *ModeStatsd {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputStatsd) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputStatsd) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputStatsd) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputStatsd) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputStatsd) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputStatsd) GetPqCompress() *CompressionStatsd {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputStatsd) GetPqOnBackpressure() *QueueFullBehaviorStatsd {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputStatsd) GetPqControls() *PqControlsStatsd {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputStatsd) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeMinio string

const (
	TypeMinioMinio TypeMinio = "minio"
)

func (e TypeMinio) ToPointer() *TypeMinio {
	return &e
}
func (e *TypeMinio) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "minio":
		*e = TypeMinio(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeMinio: %v", v)
	}
}

// AuthenticationMethodMinio - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodMinio string

const (
	// AuthenticationMethodMinioAuto Auto
	AuthenticationMethodMinioAuto AuthenticationMethodMinio = "auto"
	// AuthenticationMethodMinioManual Manual
	AuthenticationMethodMinioManual AuthenticationMethodMinio = "manual"
	// AuthenticationMethodMinioSecret Secret Key pair
	AuthenticationMethodMinioSecret AuthenticationMethodMinio = "secret"
)

func (e AuthenticationMethodMinio) ToPointer() *AuthenticationMethodMinio {
	return &e
}

// SignatureVersionMinio - Signature version to use for signing MinIO requests
type SignatureVersionMinio string

const (
	SignatureVersionMinioV2 SignatureVersionMinio = "v2"
	SignatureVersionMinioV4 SignatureVersionMinio = "v4"
)

func (e SignatureVersionMinio) ToPointer() *SignatureVersionMinio {
	return &e
}

// ObjectACLMinio - Object ACL to assign to uploaded objects
type ObjectACLMinio string

const (
	// ObjectACLMinioPrivate Private
	ObjectACLMinioPrivate ObjectACLMinio = "private"
	// ObjectACLMinioPublicRead Public Read Only
	ObjectACLMinioPublicRead ObjectACLMinio = "public-read"
	// ObjectACLMinioPublicReadWrite Public Read/Write
	ObjectACLMinioPublicReadWrite ObjectACLMinio = "public-read-write"
	// ObjectACLMinioAuthenticatedRead Authenticated Read Only
	ObjectACLMinioAuthenticatedRead ObjectACLMinio = "authenticated-read"
	// ObjectACLMinioAwsExecRead AWS EC2 AMI Read Only
	ObjectACLMinioAwsExecRead ObjectACLMinio = "aws-exec-read"
	// ObjectACLMinioBucketOwnerRead Bucket Owner Read Only
	ObjectACLMinioBucketOwnerRead ObjectACLMinio = "bucket-owner-read"
	// ObjectACLMinioBucketOwnerFullControl Bucket Owner Full Control
	ObjectACLMinioBucketOwnerFullControl ObjectACLMinio = "bucket-owner-full-control"
)

func (e ObjectACLMinio) ToPointer() *ObjectACLMinio {
	return &e
}

// StorageClassMinio - Storage class to select for uploaded objects
type StorageClassMinio string

const (
	// StorageClassMinioStandard Standard
	StorageClassMinioStandard StorageClassMinio = "STANDARD"
	// StorageClassMinioReducedRedundancy Reduced Redundancy Storage
	StorageClassMinioReducedRedundancy StorageClassMinio = "REDUCED_REDUNDANCY"
)

func (e StorageClassMinio) ToPointer() *StorageClassMinio {
	return &e
}

// ServerSideEncryptionMinio - Server-side encryption for uploaded objects
type ServerSideEncryptionMinio string

const (
	// ServerSideEncryptionMinioAes256 Amazon S3 Managed Key
	ServerSideEncryptionMinioAes256 ServerSideEncryptionMinio = "AES256"
)

func (e ServerSideEncryptionMinio) ToPointer() *ServerSideEncryptionMinio {
	return &e
}

// DataFormatMinio - Format of the output data
type DataFormatMinio string

const (
	// DataFormatMinioJSON JSON
	DataFormatMinioJSON DataFormatMinio = "json"
	// DataFormatMinioRaw Raw
	DataFormatMinioRaw DataFormatMinio = "raw"
	// DataFormatMinioParquet Parquet
	DataFormatMinioParquet DataFormatMinio = "parquet"
)

func (e DataFormatMinio) ToPointer() *DataFormatMinio {
	return &e
}

// BackpressureBehaviorMinio - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorMinio string

const (
	// BackpressureBehaviorMinioBlock Block
	BackpressureBehaviorMinioBlock BackpressureBehaviorMinio = "block"
	// BackpressureBehaviorMinioDrop Drop
	BackpressureBehaviorMinioDrop BackpressureBehaviorMinio = "drop"
)

func (e BackpressureBehaviorMinio) ToPointer() *BackpressureBehaviorMinio {
	return &e
}

// DiskSpaceProtectionMinio - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionMinio string

const (
	// DiskSpaceProtectionMinioBlock Block
	DiskSpaceProtectionMinioBlock DiskSpaceProtectionMinio = "block"
	// DiskSpaceProtectionMinioDrop Drop
	DiskSpaceProtectionMinioDrop DiskSpaceProtectionMinio = "drop"
)

func (e DiskSpaceProtectionMinio) ToPointer() *DiskSpaceProtectionMinio {
	return &e
}

// CompressionMinio - Data compression format to apply to HTTP content before it is delivered
type CompressionMinio string

const (
	CompressionMinioNone CompressionMinio = "none"
	CompressionMinioGzip CompressionMinio = "gzip"
)

func (e CompressionMinio) ToPointer() *CompressionMinio {
	return &e
}

// CompressionLevelMinio - Compression level to apply before moving files to final destination
type CompressionLevelMinio string

const (
	// CompressionLevelMinioBestSpeed Best Speed
	CompressionLevelMinioBestSpeed CompressionLevelMinio = "best_speed"
	// CompressionLevelMinioNormal Normal
	CompressionLevelMinioNormal CompressionLevelMinio = "normal"
	// CompressionLevelMinioBestCompression Best Compression
	CompressionLevelMinioBestCompression CompressionLevelMinio = "best_compression"
)

func (e CompressionLevelMinio) ToPointer() *CompressionLevelMinio {
	return &e
}

// ParquetVersionMinio - Determines which data types are supported and how they are represented
type ParquetVersionMinio string

const (
	// ParquetVersionMinioParquet10 1.0
	ParquetVersionMinioParquet10 ParquetVersionMinio = "PARQUET_1_0"
	// ParquetVersionMinioParquet24 2.4
	ParquetVersionMinioParquet24 ParquetVersionMinio = "PARQUET_2_4"
	// ParquetVersionMinioParquet26 2.6
	ParquetVersionMinioParquet26 ParquetVersionMinio = "PARQUET_2_6"
)

func (e ParquetVersionMinio) ToPointer() *ParquetVersionMinio {
	return &e
}

// DataPageVersionMinio - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionMinio string

const (
	// DataPageVersionMinioDataPageV1 V1
	DataPageVersionMinioDataPageV1 DataPageVersionMinio = "DATA_PAGE_V1"
	// DataPageVersionMinioDataPageV2 V2
	DataPageVersionMinioDataPageV2 DataPageVersionMinio = "DATA_PAGE_V2"
)

func (e DataPageVersionMinio) ToPointer() *DataPageVersionMinio {
	return &e
}

type KeyValueMetadatumMinio struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumMinio) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumMinio) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumMinio) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumMinio) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputMinio struct {
	// Unique ID for this output
	ID   *string   `json:"id,omitempty"`
	Type TypeMinio `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// MinIO service url (e.g. http://minioHost:9000)
	Endpoint string `json:"endpoint"`
	// Name of the destination MinIO bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodMinio `default:"auto" json:"awsAuthenticationMethod"`
	// Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// Region where the MinIO service/cluster is located
	Region *string `json:"region,omitempty"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.
	DestPath *string `json:"destPath,omitempty"`
	// Signature version to use for signing MinIO requests
	SignatureVersion *SignatureVersionMinio `default:"v4" json:"signatureVersion"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLMinio `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass *StorageClassMinio `json:"storageClass,omitempty"`
	// Server-side encryption for uploaded objects
	ServerSideEncryption *ServerSideEncryptionMinio `json:"serverSideEncryption,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatMinio `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorMinio `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionMinio `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	Description            *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionMinio `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelMinio `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionMinio `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionMinio `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumMinio `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum          *float64       `default:"20" json:"maxRetryNum"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputMinio) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMinio) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "endpoint", "bucket"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputMinio) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputMinio) GetType() TypeMinio {
	if o == nil {
		return TypeMinio("")
	}
	return o.Type
}

func (o *OutputMinio) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputMinio) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputMinio) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputMinio) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputMinio) GetEndpoint() string {
	if o == nil {
		return ""
	}
	return o.Endpoint
}

func (o *OutputMinio) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputMinio) GetAwsAuthenticationMethod() *AuthenticationMethodMinio {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputMinio) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputMinio) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputMinio) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputMinio) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputMinio) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputMinio) GetSignatureVersion() *SignatureVersionMinio {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputMinio) GetObjectACL() *ObjectACLMinio {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputMinio) GetStorageClass() *StorageClassMinio {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputMinio) GetServerSideEncryption() *ServerSideEncryptionMinio {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputMinio) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputMinio) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputMinio) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputMinio) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputMinio) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputMinio) GetFormat() *DataFormatMinio {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputMinio) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputMinio) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputMinio) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputMinio) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputMinio) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputMinio) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputMinio) GetOnBackpressure() *BackpressureBehaviorMinio {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputMinio) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputMinio) GetOnDiskFullBackpressure() *DiskSpaceProtectionMinio {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputMinio) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputMinio) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputMinio) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputMinio) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputMinio) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputMinio) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputMinio) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputMinio) GetCompress() *CompressionMinio {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputMinio) GetCompressionLevel() *CompressionLevelMinio {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputMinio) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputMinio) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputMinio) GetParquetVersion() *ParquetVersionMinio {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputMinio) GetParquetDataPageVersion() *DataPageVersionMinio {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputMinio) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputMinio) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputMinio) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputMinio) GetKeyValueMetadata() []KeyValueMetadatumMinio {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputMinio) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputMinio) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputMinio) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputMinio) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputMinio) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputMinio) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputMinio) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputMinio) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeCloudwatch string

const (
	TypeCloudwatchCloudwatch TypeCloudwatch = "cloudwatch"
)

func (e TypeCloudwatch) ToPointer() *TypeCloudwatch {
	return &e
}
func (e *TypeCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cloudwatch":
		*e = TypeCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCloudwatch: %v", v)
	}
}

// AuthenticationMethodCloudwatch - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodCloudwatch string

const (
	// AuthenticationMethodCloudwatchAuto Auto
	AuthenticationMethodCloudwatchAuto AuthenticationMethodCloudwatch = "auto"
	// AuthenticationMethodCloudwatchManual Manual
	AuthenticationMethodCloudwatchManual AuthenticationMethodCloudwatch = "manual"
	// AuthenticationMethodCloudwatchSecret Secret Key pair
	AuthenticationMethodCloudwatchSecret AuthenticationMethodCloudwatch = "secret"
)

func (e AuthenticationMethodCloudwatch) ToPointer() *AuthenticationMethodCloudwatch {
	return &e
}

// BackpressureBehaviorCloudwatch - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorCloudwatch string

const (
	// BackpressureBehaviorCloudwatchBlock Block
	BackpressureBehaviorCloudwatchBlock BackpressureBehaviorCloudwatch = "block"
	// BackpressureBehaviorCloudwatchDrop Drop
	BackpressureBehaviorCloudwatchDrop BackpressureBehaviorCloudwatch = "drop"
	// BackpressureBehaviorCloudwatchQueue Persistent Queue
	BackpressureBehaviorCloudwatchQueue BackpressureBehaviorCloudwatch = "queue"
)

func (e BackpressureBehaviorCloudwatch) ToPointer() *BackpressureBehaviorCloudwatch {
	return &e
}

// ModeCloudwatch - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeCloudwatch string

const (
	// ModeCloudwatchError Error
	ModeCloudwatchError ModeCloudwatch = "error"
	// ModeCloudwatchAlways Backpressure
	ModeCloudwatchAlways ModeCloudwatch = "always"
	// ModeCloudwatchBackpressure Always On
	ModeCloudwatchBackpressure ModeCloudwatch = "backpressure"
)

func (e ModeCloudwatch) ToPointer() *ModeCloudwatch {
	return &e
}

// CompressionCloudwatch - Codec to use to compress the persisted data
type CompressionCloudwatch string

const (
	// CompressionCloudwatchNone None
	CompressionCloudwatchNone CompressionCloudwatch = "none"
	// CompressionCloudwatchGzip Gzip
	CompressionCloudwatchGzip CompressionCloudwatch = "gzip"
)

func (e CompressionCloudwatch) ToPointer() *CompressionCloudwatch {
	return &e
}

// QueueFullBehaviorCloudwatch - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCloudwatch string

const (
	// QueueFullBehaviorCloudwatchBlock Block
	QueueFullBehaviorCloudwatchBlock QueueFullBehaviorCloudwatch = "block"
	// QueueFullBehaviorCloudwatchDrop Drop new data
	QueueFullBehaviorCloudwatchDrop QueueFullBehaviorCloudwatch = "drop"
)

func (e QueueFullBehaviorCloudwatch) ToPointer() *QueueFullBehaviorCloudwatch {
	return &e
}

type PqControlsCloudwatch struct {
}

func (p PqControlsCloudwatch) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsCloudwatch) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputCloudwatch struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type TypeCloudwatch `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// CloudWatch log group to associate events with
	LogGroupName string `json:"logGroupName"`
	// Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId
	LogStreamName string `json:"logStreamName"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodCloudwatch `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                         `json:"awsSecretKey,omitempty"`
	// Region where the CloudWatchLogs is located
	Region string `json:"region"`
	// CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access CloudWatchLogs
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of queued batches before blocking
	MaxQueueSize *float64 `default:"5" json:"maxQueueSize"`
	// Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size
	MaxRecordSizeKB *float64 `default:"1024" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCloudwatch `default:"block" json:"onBackpressure"`
	Description    *string                         `json:"description,omitempty"`
	AwsAPIKey      *string                         `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeCloudwatch `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionCloudwatch `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorCloudwatch `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsCloudwatch        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any               `additionalProperties:"true" json:"-"`
}

func (o OutputCloudwatch) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCloudwatch) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "logGroupName", "logStreamName", "region"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCloudwatch) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputCloudwatch) GetType() TypeCloudwatch {
	if o == nil {
		return TypeCloudwatch("")
	}
	return o.Type
}

func (o *OutputCloudwatch) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCloudwatch) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCloudwatch) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCloudwatch) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCloudwatch) GetLogGroupName() string {
	if o == nil {
		return ""
	}
	return o.LogGroupName
}

func (o *OutputCloudwatch) GetLogStreamName() string {
	if o == nil {
		return ""
	}
	return o.LogStreamName
}

func (o *OutputCloudwatch) GetAwsAuthenticationMethod() *AuthenticationMethodCloudwatch {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputCloudwatch) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputCloudwatch) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputCloudwatch) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputCloudwatch) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputCloudwatch) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCloudwatch) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputCloudwatch) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputCloudwatch) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputCloudwatch) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputCloudwatch) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputCloudwatch) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputCloudwatch) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCloudwatch) GetOnBackpressure() *BackpressureBehaviorCloudwatch {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCloudwatch) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCloudwatch) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputCloudwatch) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputCloudwatch) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputCloudwatch) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputCloudwatch) GetPqMode() *ModeCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCloudwatch) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputCloudwatch) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputCloudwatch) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCloudwatch) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCloudwatch) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCloudwatch) GetPqCompress() *CompressionCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCloudwatch) GetPqOnBackpressure() *QueueFullBehaviorCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCloudwatch) GetPqControls() *PqControlsCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputCloudwatch) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeInfluxdb string

const (
	TypeInfluxdbInfluxdb TypeInfluxdb = "influxdb"
)

func (e TypeInfluxdb) ToPointer() *TypeInfluxdb {
	return &e
}
func (e *TypeInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "influxdb":
		*e = TypeInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeInfluxdb: %v", v)
	}
}

// TimestampPrecision - Sets the precision for the supplied Unix time values. Defaults to milliseconds.
type TimestampPrecision string

const (
	// TimestampPrecisionNs Nanoseconds
	TimestampPrecisionNs TimestampPrecision = "ns"
	// TimestampPrecisionU Microseconds
	TimestampPrecisionU TimestampPrecision = "u"
	// TimestampPrecisionMs Milliseconds
	TimestampPrecisionMs TimestampPrecision = "ms"
	// TimestampPrecisionS Seconds
	TimestampPrecisionS TimestampPrecision = "s"
	// TimestampPrecisionM Minutes
	TimestampPrecisionM TimestampPrecision = "m"
	// TimestampPrecisionH Hours
	TimestampPrecisionH TimestampPrecision = "h"
)

func (e TimestampPrecision) ToPointer() *TimestampPrecision {
	return &e
}

type ExtraHTTPHeaderInfluxdb struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderInfluxdb) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderInfluxdb) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeInfluxdb - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeInfluxdb string

const (
	// FailedRequestLoggingModeInfluxdbPayload Payload
	FailedRequestLoggingModeInfluxdbPayload FailedRequestLoggingModeInfluxdb = "payload"
	// FailedRequestLoggingModeInfluxdbPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeInfluxdbPayloadAndHeaders FailedRequestLoggingModeInfluxdb = "payloadAndHeaders"
	// FailedRequestLoggingModeInfluxdbNone None
	FailedRequestLoggingModeInfluxdbNone FailedRequestLoggingModeInfluxdb = "none"
)

func (e FailedRequestLoggingModeInfluxdb) ToPointer() *FailedRequestLoggingModeInfluxdb {
	return &e
}

type ResponseRetrySettingInfluxdb struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingInfluxdb) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingInfluxdb) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingInfluxdb) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingInfluxdb) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsInfluxdb struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsInfluxdb) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsInfluxdb) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsInfluxdb) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsInfluxdb) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorInfluxdb - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorInfluxdb string

const (
	// BackpressureBehaviorInfluxdbBlock Block
	BackpressureBehaviorInfluxdbBlock BackpressureBehaviorInfluxdb = "block"
	// BackpressureBehaviorInfluxdbDrop Drop
	BackpressureBehaviorInfluxdbDrop BackpressureBehaviorInfluxdb = "drop"
	// BackpressureBehaviorInfluxdbQueue Persistent Queue
	BackpressureBehaviorInfluxdbQueue BackpressureBehaviorInfluxdb = "queue"
)

func (e BackpressureBehaviorInfluxdb) ToPointer() *BackpressureBehaviorInfluxdb {
	return &e
}

// AuthenticationTypeInfluxdb - InfluxDB authentication type
type AuthenticationTypeInfluxdb string

const (
	AuthenticationTypeInfluxdbNone              AuthenticationTypeInfluxdb = "none"
	AuthenticationTypeInfluxdbBasic             AuthenticationTypeInfluxdb = "basic"
	AuthenticationTypeInfluxdbCredentialsSecret AuthenticationTypeInfluxdb = "credentialsSecret"
	AuthenticationTypeInfluxdbToken             AuthenticationTypeInfluxdb = "token"
	AuthenticationTypeInfluxdbTextSecret        AuthenticationTypeInfluxdb = "textSecret"
	AuthenticationTypeInfluxdbOauth             AuthenticationTypeInfluxdb = "oauth"
)

func (e AuthenticationTypeInfluxdb) ToPointer() *AuthenticationTypeInfluxdb {
	return &e
}

// ModeInfluxdb - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeInfluxdb string

const (
	// ModeInfluxdbError Error
	ModeInfluxdbError ModeInfluxdb = "error"
	// ModeInfluxdbAlways Backpressure
	ModeInfluxdbAlways ModeInfluxdb = "always"
	// ModeInfluxdbBackpressure Always On
	ModeInfluxdbBackpressure ModeInfluxdb = "backpressure"
)

func (e ModeInfluxdb) ToPointer() *ModeInfluxdb {
	return &e
}

// CompressionInfluxdb - Codec to use to compress the persisted data
type CompressionInfluxdb string

const (
	// CompressionInfluxdbNone None
	CompressionInfluxdbNone CompressionInfluxdb = "none"
	// CompressionInfluxdbGzip Gzip
	CompressionInfluxdbGzip CompressionInfluxdb = "gzip"
)

func (e CompressionInfluxdb) ToPointer() *CompressionInfluxdb {
	return &e
}

// QueueFullBehaviorInfluxdb - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorInfluxdb string

const (
	// QueueFullBehaviorInfluxdbBlock Block
	QueueFullBehaviorInfluxdbBlock QueueFullBehaviorInfluxdb = "block"
	// QueueFullBehaviorInfluxdbDrop Drop new data
	QueueFullBehaviorInfluxdbDrop QueueFullBehaviorInfluxdb = "drop"
)

func (e QueueFullBehaviorInfluxdb) ToPointer() *QueueFullBehaviorInfluxdb {
	return &e
}

type PqControlsInfluxdb struct {
}

func (p PqControlsInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OauthParamInfluxdb struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o OauthParamInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthParamInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthParamInfluxdb) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamInfluxdb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderInfluxdb struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o OauthHeaderInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthHeaderInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthHeaderInfluxdb) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderInfluxdb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputInfluxdb struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type TypeInfluxdb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL of an InfluxDB cluster to send events to, e.g., http://localhost:8086/write
	URL string `json:"url"`
	// The v2 API can be enabled with InfluxDB versions 1.8 and later.
	UseV2API *bool `default:"false" json:"useV2API"`
	// Sets the precision for the supplied Unix time values. Defaults to milliseconds.
	TimestampPrecision *TimestampPrecision `default:"ms" json:"timestampPrecision"`
	// Enabling this will pull the value field from the metric name. E,g, 'db.query.user' will use 'db.query' as the measurement and 'user' as the value field.
	DynamicValueFieldName *bool `default:"true" json:"dynamicValueFieldName"`
	// Name of the field in which to store the metric when sending to InfluxDB. If dynamic generation is enabled and fails, this will be used as a fallback.
	ValueFieldName *string `default:"value" json:"valueFieldName"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderInfluxdb `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeInfluxdb `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingInfluxdb `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsInfluxdb  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorInfluxdb `default:"block" json:"onBackpressure"`
	// InfluxDB authentication type
	AuthType    *AuthenticationTypeInfluxdb `default:"none" json:"authType"`
	Description *string                     `json:"description,omitempty"`
	// Database to write to.
	Database *string `json:"database,omitempty"`
	// Bucket to write to.
	Bucket *string `json:"bucket,omitempty"`
	// Organization ID for this bucket.
	Org *string `json:"org,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeInfluxdb `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionInfluxdb `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorInfluxdb `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsInfluxdb        `json:"pqControls,omitempty"`
	Username         *string                    `json:"username,omitempty"`
	Password         *string                    `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamInfluxdb `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders         []OauthHeaderInfluxdb `json:"oauthHeaders,omitempty"`
	AdditionalProperties map[string]any        `additionalProperties:"true" json:"-"`
}

func (o OutputInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "url"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputInfluxdb) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputInfluxdb) GetType() TypeInfluxdb {
	if o == nil {
		return TypeInfluxdb("")
	}
	return o.Type
}

func (o *OutputInfluxdb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputInfluxdb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputInfluxdb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputInfluxdb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputInfluxdb) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputInfluxdb) GetUseV2API() *bool {
	if o == nil {
		return nil
	}
	return o.UseV2API
}

func (o *OutputInfluxdb) GetTimestampPrecision() *TimestampPrecision {
	if o == nil {
		return nil
	}
	return o.TimestampPrecision
}

func (o *OutputInfluxdb) GetDynamicValueFieldName() *bool {
	if o == nil {
		return nil
	}
	return o.DynamicValueFieldName
}

func (o *OutputInfluxdb) GetValueFieldName() *string {
	if o == nil {
		return nil
	}
	return o.ValueFieldName
}

func (o *OutputInfluxdb) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputInfluxdb) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputInfluxdb) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputInfluxdb) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputInfluxdb) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputInfluxdb) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputInfluxdb) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputInfluxdb) GetExtraHTTPHeaders() []ExtraHTTPHeaderInfluxdb {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputInfluxdb) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputInfluxdb) GetFailedRequestLoggingMode() *FailedRequestLoggingModeInfluxdb {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputInfluxdb) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputInfluxdb) GetResponseRetrySettings() []ResponseRetrySettingInfluxdb {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputInfluxdb) GetTimeoutRetrySettings() *TimeoutRetrySettingsInfluxdb {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputInfluxdb) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputInfluxdb) GetOnBackpressure() *BackpressureBehaviorInfluxdb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputInfluxdb) GetAuthType() *AuthenticationTypeInfluxdb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputInfluxdb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputInfluxdb) GetDatabase() *string {
	if o == nil {
		return nil
	}
	return o.Database
}

func (o *OutputInfluxdb) GetBucket() *string {
	if o == nil {
		return nil
	}
	return o.Bucket
}

func (o *OutputInfluxdb) GetOrg() *string {
	if o == nil {
		return nil
	}
	return o.Org
}

func (o *OutputInfluxdb) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputInfluxdb) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputInfluxdb) GetPqMode() *ModeInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputInfluxdb) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputInfluxdb) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputInfluxdb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputInfluxdb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputInfluxdb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputInfluxdb) GetPqCompress() *CompressionInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputInfluxdb) GetPqOnBackpressure() *QueueFullBehaviorInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputInfluxdb) GetPqControls() *PqControlsInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputInfluxdb) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputInfluxdb) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputInfluxdb) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputInfluxdb) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputInfluxdb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputInfluxdb) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputInfluxdb) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputInfluxdb) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputInfluxdb) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputInfluxdb) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputInfluxdb) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputInfluxdb) GetOauthParams() []OauthParamInfluxdb {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputInfluxdb) GetOauthHeaders() []OauthHeaderInfluxdb {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputInfluxdb) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeNewrelicEvents string

const (
	TypeNewrelicEventsNewrelicEvents TypeNewrelicEvents = "newrelic_events"
)

func (e TypeNewrelicEvents) ToPointer() *TypeNewrelicEvents {
	return &e
}
func (e *TypeNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "newrelic_events":
		*e = TypeNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeNewrelicEvents: %v", v)
	}
}

// RegionNewrelicEvents - Which New Relic region endpoint to use.
type RegionNewrelicEvents string

const (
	// RegionNewrelicEventsUs US
	RegionNewrelicEventsUs RegionNewrelicEvents = "US"
	// RegionNewrelicEventsEu Europe
	RegionNewrelicEventsEu RegionNewrelicEvents = "EU"
	// RegionNewrelicEventsCustom Custom
	RegionNewrelicEventsCustom RegionNewrelicEvents = "Custom"
)

func (e RegionNewrelicEvents) ToPointer() *RegionNewrelicEvents {
	return &e
}

type ExtraHTTPHeaderNewrelicEvents struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderNewrelicEvents) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderNewrelicEvents) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeNewrelicEvents - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeNewrelicEvents string

const (
	// FailedRequestLoggingModeNewrelicEventsPayload Payload
	FailedRequestLoggingModeNewrelicEventsPayload FailedRequestLoggingModeNewrelicEvents = "payload"
	// FailedRequestLoggingModeNewrelicEventsPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeNewrelicEventsPayloadAndHeaders FailedRequestLoggingModeNewrelicEvents = "payloadAndHeaders"
	// FailedRequestLoggingModeNewrelicEventsNone None
	FailedRequestLoggingModeNewrelicEventsNone FailedRequestLoggingModeNewrelicEvents = "none"
)

func (e FailedRequestLoggingModeNewrelicEvents) ToPointer() *FailedRequestLoggingModeNewrelicEvents {
	return &e
}

type ResponseRetrySettingNewrelicEvents struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingNewrelicEvents) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingNewrelicEvents) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingNewrelicEvents) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingNewrelicEvents) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsNewrelicEvents struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsNewrelicEvents) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsNewrelicEvents) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsNewrelicEvents) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsNewrelicEvents) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorNewrelicEvents - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorNewrelicEvents string

const (
	// BackpressureBehaviorNewrelicEventsBlock Block
	BackpressureBehaviorNewrelicEventsBlock BackpressureBehaviorNewrelicEvents = "block"
	// BackpressureBehaviorNewrelicEventsDrop Drop
	BackpressureBehaviorNewrelicEventsDrop BackpressureBehaviorNewrelicEvents = "drop"
	// BackpressureBehaviorNewrelicEventsQueue Persistent Queue
	BackpressureBehaviorNewrelicEventsQueue BackpressureBehaviorNewrelicEvents = "queue"
)

func (e BackpressureBehaviorNewrelicEvents) ToPointer() *BackpressureBehaviorNewrelicEvents {
	return &e
}

// AuthenticationMethodNewrelicEvents - Enter API key directly, or select a stored secret
type AuthenticationMethodNewrelicEvents string

const (
	AuthenticationMethodNewrelicEventsManual AuthenticationMethodNewrelicEvents = "manual"
	AuthenticationMethodNewrelicEventsSecret AuthenticationMethodNewrelicEvents = "secret"
)

func (e AuthenticationMethodNewrelicEvents) ToPointer() *AuthenticationMethodNewrelicEvents {
	return &e
}

// ModeNewrelicEvents - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeNewrelicEvents string

const (
	// ModeNewrelicEventsError Error
	ModeNewrelicEventsError ModeNewrelicEvents = "error"
	// ModeNewrelicEventsAlways Backpressure
	ModeNewrelicEventsAlways ModeNewrelicEvents = "always"
	// ModeNewrelicEventsBackpressure Always On
	ModeNewrelicEventsBackpressure ModeNewrelicEvents = "backpressure"
)

func (e ModeNewrelicEvents) ToPointer() *ModeNewrelicEvents {
	return &e
}

// CompressionNewrelicEvents - Codec to use to compress the persisted data
type CompressionNewrelicEvents string

const (
	// CompressionNewrelicEventsNone None
	CompressionNewrelicEventsNone CompressionNewrelicEvents = "none"
	// CompressionNewrelicEventsGzip Gzip
	CompressionNewrelicEventsGzip CompressionNewrelicEvents = "gzip"
)

func (e CompressionNewrelicEvents) ToPointer() *CompressionNewrelicEvents {
	return &e
}

// QueueFullBehaviorNewrelicEvents - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorNewrelicEvents string

const (
	// QueueFullBehaviorNewrelicEventsBlock Block
	QueueFullBehaviorNewrelicEventsBlock QueueFullBehaviorNewrelicEvents = "block"
	// QueueFullBehaviorNewrelicEventsDrop Drop new data
	QueueFullBehaviorNewrelicEventsDrop QueueFullBehaviorNewrelicEvents = "drop"
)

func (e QueueFullBehaviorNewrelicEvents) ToPointer() *QueueFullBehaviorNewrelicEvents {
	return &e
}

type PqControlsNewrelicEvents struct {
}

func (p PqControlsNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputNewrelicEvents struct {
	// Unique ID for this output
	ID   *string            `json:"id,omitempty"`
	Type TypeNewrelicEvents `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Which New Relic region endpoint to use.
	Region *RegionNewrelicEvents `default:"US" json:"region"`
	// New Relic account ID
	AccountID string `json:"accountId"`
	// Default eventType to use when not present in an event. For more information, see [here](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/data-requirements-limits-custom-event-data/#reserved-words).
	EventType string `json:"eventType"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderNewrelicEvents `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeNewrelicEvents `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingNewrelicEvents `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsNewrelicEvents  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorNewrelicEvents `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType    *AuthenticationMethodNewrelicEvents `default:"manual" json:"authType"`
	Description *string                             `json:"description,omitempty"`
	CustomURL   *string                             `json:"customUrl,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeNewrelicEvents `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionNewrelicEvents `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorNewrelicEvents `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsNewrelicEvents        `json:"pqControls,omitempty"`
	// New Relic API key. Can be overridden using __newRelic_apiKey field.
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret           *string        `json:"textSecret,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "accountId", "eventType"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputNewrelicEvents) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputNewrelicEvents) GetType() TypeNewrelicEvents {
	if o == nil {
		return TypeNewrelicEvents("")
	}
	return o.Type
}

func (o *OutputNewrelicEvents) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputNewrelicEvents) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputNewrelicEvents) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputNewrelicEvents) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputNewrelicEvents) GetRegion() *RegionNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputNewrelicEvents) GetAccountID() string {
	if o == nil {
		return ""
	}
	return o.AccountID
}

func (o *OutputNewrelicEvents) GetEventType() string {
	if o == nil {
		return ""
	}
	return o.EventType
}

func (o *OutputNewrelicEvents) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputNewrelicEvents) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputNewrelicEvents) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputNewrelicEvents) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputNewrelicEvents) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputNewrelicEvents) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputNewrelicEvents) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputNewrelicEvents) GetExtraHTTPHeaders() []ExtraHTTPHeaderNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputNewrelicEvents) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputNewrelicEvents) GetFailedRequestLoggingMode() *FailedRequestLoggingModeNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputNewrelicEvents) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputNewrelicEvents) GetResponseRetrySettings() []ResponseRetrySettingNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputNewrelicEvents) GetTimeoutRetrySettings() *TimeoutRetrySettingsNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputNewrelicEvents) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputNewrelicEvents) GetOnBackpressure() *BackpressureBehaviorNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputNewrelicEvents) GetAuthType() *AuthenticationMethodNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputNewrelicEvents) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputNewrelicEvents) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputNewrelicEvents) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputNewrelicEvents) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputNewrelicEvents) GetPqMode() *ModeNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputNewrelicEvents) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputNewrelicEvents) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputNewrelicEvents) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputNewrelicEvents) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputNewrelicEvents) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputNewrelicEvents) GetPqCompress() *CompressionNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputNewrelicEvents) GetPqOnBackpressure() *QueueFullBehaviorNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputNewrelicEvents) GetPqControls() *PqControlsNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputNewrelicEvents) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputNewrelicEvents) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputNewrelicEvents) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeNewrelic string

const (
	TypeNewrelicNewrelic TypeNewrelic = "newrelic"
)

func (e TypeNewrelic) ToPointer() *TypeNewrelic {
	return &e
}
func (e *TypeNewrelic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "newrelic":
		*e = TypeNewrelic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeNewrelic: %v", v)
	}
}

// RegionNewrelic - Which New Relic region endpoint to use.
type RegionNewrelic string

const (
	// RegionNewrelicUs US
	RegionNewrelicUs RegionNewrelic = "US"
	// RegionNewrelicEu Europe
	RegionNewrelicEu RegionNewrelic = "EU"
	// RegionNewrelicCustom Custom
	RegionNewrelicCustom RegionNewrelic = "Custom"
)

func (e RegionNewrelic) ToPointer() *RegionNewrelic {
	return &e
}

type FieldName string

const (
	FieldNameService   FieldName = "service"
	FieldNameHostname  FieldName = "hostname"
	FieldNameTimestamp FieldName = "timestamp"
	FieldNameAuditID   FieldName = "auditId"
)

func (e FieldName) ToPointer() *FieldName {
	return &e
}

type MetadatumNewrelic struct {
	Name FieldName `json:"name"`
	// JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)
	Value string `json:"value"`
}

func (m MetadatumNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(m, "", false)
}

func (m *MetadatumNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &m, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (m *MetadatumNewrelic) GetName() FieldName {
	if m == nil {
		return FieldName("")
	}
	return m.Name
}

func (m *MetadatumNewrelic) GetValue() string {
	if m == nil {
		return ""
	}
	return m.Value
}

type ExtraHTTPHeaderNewrelic struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderNewrelic) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderNewrelic) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeNewrelic - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeNewrelic string

const (
	// FailedRequestLoggingModeNewrelicPayload Payload
	FailedRequestLoggingModeNewrelicPayload FailedRequestLoggingModeNewrelic = "payload"
	// FailedRequestLoggingModeNewrelicPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeNewrelicPayloadAndHeaders FailedRequestLoggingModeNewrelic = "payloadAndHeaders"
	// FailedRequestLoggingModeNewrelicNone None
	FailedRequestLoggingModeNewrelicNone FailedRequestLoggingModeNewrelic = "none"
)

func (e FailedRequestLoggingModeNewrelic) ToPointer() *FailedRequestLoggingModeNewrelic {
	return &e
}

type ResponseRetrySettingNewrelic struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingNewrelic) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingNewrelic) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingNewrelic) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingNewrelic) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsNewrelic struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsNewrelic) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsNewrelic) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsNewrelic) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsNewrelic) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorNewrelic - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorNewrelic string

const (
	// BackpressureBehaviorNewrelicBlock Block
	BackpressureBehaviorNewrelicBlock BackpressureBehaviorNewrelic = "block"
	// BackpressureBehaviorNewrelicDrop Drop
	BackpressureBehaviorNewrelicDrop BackpressureBehaviorNewrelic = "drop"
	// BackpressureBehaviorNewrelicQueue Persistent Queue
	BackpressureBehaviorNewrelicQueue BackpressureBehaviorNewrelic = "queue"
)

func (e BackpressureBehaviorNewrelic) ToPointer() *BackpressureBehaviorNewrelic {
	return &e
}

// AuthenticationMethodNewrelic - Enter API key directly, or select a stored secret
type AuthenticationMethodNewrelic string

const (
	AuthenticationMethodNewrelicManual AuthenticationMethodNewrelic = "manual"
	AuthenticationMethodNewrelicSecret AuthenticationMethodNewrelic = "secret"
)

func (e AuthenticationMethodNewrelic) ToPointer() *AuthenticationMethodNewrelic {
	return &e
}

// ModeNewrelic - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeNewrelic string

const (
	// ModeNewrelicError Error
	ModeNewrelicError ModeNewrelic = "error"
	// ModeNewrelicAlways Backpressure
	ModeNewrelicAlways ModeNewrelic = "always"
	// ModeNewrelicBackpressure Always On
	ModeNewrelicBackpressure ModeNewrelic = "backpressure"
)

func (e ModeNewrelic) ToPointer() *ModeNewrelic {
	return &e
}

// CompressionNewrelic - Codec to use to compress the persisted data
type CompressionNewrelic string

const (
	// CompressionNewrelicNone None
	CompressionNewrelicNone CompressionNewrelic = "none"
	// CompressionNewrelicGzip Gzip
	CompressionNewrelicGzip CompressionNewrelic = "gzip"
)

func (e CompressionNewrelic) ToPointer() *CompressionNewrelic {
	return &e
}

// QueueFullBehaviorNewrelic - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorNewrelic string

const (
	// QueueFullBehaviorNewrelicBlock Block
	QueueFullBehaviorNewrelicBlock QueueFullBehaviorNewrelic = "block"
	// QueueFullBehaviorNewrelicDrop Drop new data
	QueueFullBehaviorNewrelicDrop QueueFullBehaviorNewrelic = "drop"
)

func (e QueueFullBehaviorNewrelic) ToPointer() *QueueFullBehaviorNewrelic {
	return &e
}

type PqControlsNewrelic struct {
}

func (p PqControlsNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputNewrelic struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type TypeNewrelic `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Which New Relic region endpoint to use.
	Region *RegionNewrelic `default:"US" json:"region"`
	// Name of the logtype to send with events, e.g.: observability, access_log. The event's 'sourcetype' field (if set) will override this value.
	LogType *string `default:"" json:"logType"`
	// Name of field to send as log message value. If not present, event will be serialized and sent as JSON.
	MessageField *string `default:"" json:"messageField"`
	// Fields to add to events from this input
	Metadata []MetadatumNewrelic `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderNewrelic `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeNewrelic `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingNewrelic `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsNewrelic  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorNewrelic `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType *AuthenticationMethodNewrelic `default:"manual" json:"authType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	CustomURL          *string  `json:"customUrl,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeNewrelic `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionNewrelic `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorNewrelic `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsNewrelic        `json:"pqControls,omitempty"`
	// New Relic API key. Can be overridden using __newRelic_apiKey field.
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret           *string        `json:"textSecret,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputNewrelic) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputNewrelic) GetType() TypeNewrelic {
	if o == nil {
		return TypeNewrelic("")
	}
	return o.Type
}

func (o *OutputNewrelic) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputNewrelic) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputNewrelic) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputNewrelic) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputNewrelic) GetRegion() *RegionNewrelic {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputNewrelic) GetLogType() *string {
	if o == nil {
		return nil
	}
	return o.LogType
}

func (o *OutputNewrelic) GetMessageField() *string {
	if o == nil {
		return nil
	}
	return o.MessageField
}

func (o *OutputNewrelic) GetMetadata() []MetadatumNewrelic {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputNewrelic) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputNewrelic) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputNewrelic) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputNewrelic) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputNewrelic) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputNewrelic) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputNewrelic) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputNewrelic) GetExtraHTTPHeaders() []ExtraHTTPHeaderNewrelic {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputNewrelic) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputNewrelic) GetFailedRequestLoggingMode() *FailedRequestLoggingModeNewrelic {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputNewrelic) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputNewrelic) GetResponseRetrySettings() []ResponseRetrySettingNewrelic {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputNewrelic) GetTimeoutRetrySettings() *TimeoutRetrySettingsNewrelic {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputNewrelic) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputNewrelic) GetOnBackpressure() *BackpressureBehaviorNewrelic {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputNewrelic) GetAuthType() *AuthenticationMethodNewrelic {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputNewrelic) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputNewrelic) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputNewrelic) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputNewrelic) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputNewrelic) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputNewrelic) GetPqMode() *ModeNewrelic {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputNewrelic) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputNewrelic) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputNewrelic) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputNewrelic) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputNewrelic) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputNewrelic) GetPqCompress() *CompressionNewrelic {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputNewrelic) GetPqOnBackpressure() *QueueFullBehaviorNewrelic {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputNewrelic) GetPqControls() *PqControlsNewrelic {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputNewrelic) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputNewrelic) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputNewrelic) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeElasticCloud string

const (
	TypeElasticCloudElasticCloud TypeElasticCloud = "elastic_cloud"
)

func (e TypeElasticCloud) ToPointer() *TypeElasticCloud {
	return &e
}
func (e *TypeElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "elastic_cloud":
		*e = TypeElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeElasticCloud: %v", v)
	}
}

type ExtraHTTPHeaderElasticCloud struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderElasticCloud) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderElasticCloud) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeElasticCloud - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeElasticCloud string

const (
	// FailedRequestLoggingModeElasticCloudPayload Payload
	FailedRequestLoggingModeElasticCloudPayload FailedRequestLoggingModeElasticCloud = "payload"
	// FailedRequestLoggingModeElasticCloudPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeElasticCloudPayloadAndHeaders FailedRequestLoggingModeElasticCloud = "payloadAndHeaders"
	// FailedRequestLoggingModeElasticCloudNone None
	FailedRequestLoggingModeElasticCloudNone FailedRequestLoggingModeElasticCloud = "none"
)

func (e FailedRequestLoggingModeElasticCloud) ToPointer() *FailedRequestLoggingModeElasticCloud {
	return &e
}

type ExtraParamElasticCloud struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (e ExtraParamElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraParamElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraParamElasticCloud) GetName() string {
	if e == nil {
		return ""
	}
	return e.Name
}

func (e *ExtraParamElasticCloud) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// AuthenticationMethodElasticCloud - Enter credentials directly, or select a stored secret
type AuthenticationMethodElasticCloud string

const (
	AuthenticationMethodElasticCloudManual       AuthenticationMethodElasticCloud = "manual"
	AuthenticationMethodElasticCloudSecret       AuthenticationMethodElasticCloud = "secret"
	AuthenticationMethodElasticCloudManualAPIKey AuthenticationMethodElasticCloud = "manualAPIKey"
	AuthenticationMethodElasticCloudTextSecret   AuthenticationMethodElasticCloud = "textSecret"
)

func (e AuthenticationMethodElasticCloud) ToPointer() *AuthenticationMethodElasticCloud {
	return &e
}

type AuthElasticCloud struct {
	Disabled *bool   `default:"false" json:"disabled"`
	Username *string `json:"username,omitempty"`
	Password *string `json:"password,omitempty"`
	// Enter credentials directly, or select a stored secret
	AuthType *AuthenticationMethodElasticCloud `default:"manual" json:"authType"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Enter API key directly
	ManualAPIKey *string `json:"manualAPIKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (a AuthElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (a *AuthElasticCloud) GetDisabled() *bool {
	if a == nil {
		return nil
	}
	return a.Disabled
}

func (a *AuthElasticCloud) GetUsername() *string {
	if a == nil {
		return nil
	}
	return a.Username
}

func (a *AuthElasticCloud) GetPassword() *string {
	if a == nil {
		return nil
	}
	return a.Password
}

func (a *AuthElasticCloud) GetAuthType() *AuthenticationMethodElasticCloud {
	if a == nil {
		return nil
	}
	return a.AuthType
}

func (a *AuthElasticCloud) GetCredentialsSecret() *string {
	if a == nil {
		return nil
	}
	return a.CredentialsSecret
}

func (a *AuthElasticCloud) GetManualAPIKey() *string {
	if a == nil {
		return nil
	}
	return a.ManualAPIKey
}

func (a *AuthElasticCloud) GetTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.TextSecret
}

type ResponseRetrySettingElasticCloud struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingElasticCloud) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingElasticCloud) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingElasticCloud) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingElasticCloud) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsElasticCloud struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsElasticCloud) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsElasticCloud) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsElasticCloud) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsElasticCloud) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorElasticCloud - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorElasticCloud string

const (
	// BackpressureBehaviorElasticCloudBlock Block
	BackpressureBehaviorElasticCloudBlock BackpressureBehaviorElasticCloud = "block"
	// BackpressureBehaviorElasticCloudDrop Drop
	BackpressureBehaviorElasticCloudDrop BackpressureBehaviorElasticCloud = "drop"
	// BackpressureBehaviorElasticCloudQueue Persistent Queue
	BackpressureBehaviorElasticCloudQueue BackpressureBehaviorElasticCloud = "queue"
)

func (e BackpressureBehaviorElasticCloud) ToPointer() *BackpressureBehaviorElasticCloud {
	return &e
}

// ModeElasticCloud - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeElasticCloud string

const (
	// ModeElasticCloudError Error
	ModeElasticCloudError ModeElasticCloud = "error"
	// ModeElasticCloudAlways Backpressure
	ModeElasticCloudAlways ModeElasticCloud = "always"
	// ModeElasticCloudBackpressure Always On
	ModeElasticCloudBackpressure ModeElasticCloud = "backpressure"
)

func (e ModeElasticCloud) ToPointer() *ModeElasticCloud {
	return &e
}

// CompressionElasticCloud - Codec to use to compress the persisted data
type CompressionElasticCloud string

const (
	// CompressionElasticCloudNone None
	CompressionElasticCloudNone CompressionElasticCloud = "none"
	// CompressionElasticCloudGzip Gzip
	CompressionElasticCloudGzip CompressionElasticCloud = "gzip"
)

func (e CompressionElasticCloud) ToPointer() *CompressionElasticCloud {
	return &e
}

// QueueFullBehaviorElasticCloud - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorElasticCloud string

const (
	// QueueFullBehaviorElasticCloudBlock Block
	QueueFullBehaviorElasticCloudBlock QueueFullBehaviorElasticCloud = "block"
	// QueueFullBehaviorElasticCloudDrop Drop new data
	QueueFullBehaviorElasticCloudDrop QueueFullBehaviorElasticCloud = "drop"
)

func (e QueueFullBehaviorElasticCloud) ToPointer() *QueueFullBehaviorElasticCloud {
	return &e
}

type PqControlsElasticCloud struct {
}

func (p PqControlsElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputElasticCloud struct {
	// Unique ID for this output
	ID   *string          `json:"id,omitempty"`
	Type TypeElasticCloud `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter Cloud ID of the Elastic Cloud environment to send events to
	URL string `json:"url"`
	// Data stream or index to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.
	Index string `json:"index"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderElasticCloud `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeElasticCloud `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Extra parameters to use in HTTP requests
	ExtraParams []ExtraParamElasticCloud `json:"extraParams,omitempty"`
	Auth        *AuthElasticCloud        `json:"auth,omitempty"`
	// Optional Elastic Cloud Destination pipeline
	ElasticPipeline *string `json:"elasticPipeline,omitempty"`
	// Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)
	IncludeDocID *bool `default:"true" json:"includeDocId"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingElasticCloud `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsElasticCloud  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorElasticCloud `default:"block" json:"onBackpressure"`
	Description    *string                           `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeElasticCloud `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionElasticCloud `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorElasticCloud `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsElasticCloud        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                 `additionalProperties:"true" json:"-"`
}

func (o OutputElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "url", "index"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputElasticCloud) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputElasticCloud) GetType() TypeElasticCloud {
	if o == nil {
		return TypeElasticCloud("")
	}
	return o.Type
}

func (o *OutputElasticCloud) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputElasticCloud) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputElasticCloud) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputElasticCloud) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputElasticCloud) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputElasticCloud) GetIndex() string {
	if o == nil {
		return ""
	}
	return o.Index
}

func (o *OutputElasticCloud) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputElasticCloud) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputElasticCloud) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputElasticCloud) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputElasticCloud) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputElasticCloud) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputElasticCloud) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputElasticCloud) GetExtraHTTPHeaders() []ExtraHTTPHeaderElasticCloud {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputElasticCloud) GetFailedRequestLoggingMode() *FailedRequestLoggingModeElasticCloud {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputElasticCloud) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputElasticCloud) GetExtraParams() []ExtraParamElasticCloud {
	if o == nil {
		return nil
	}
	return o.ExtraParams
}

func (o *OutputElasticCloud) GetAuth() *AuthElasticCloud {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputElasticCloud) GetElasticPipeline() *string {
	if o == nil {
		return nil
	}
	return o.ElasticPipeline
}

func (o *OutputElasticCloud) GetIncludeDocID() *bool {
	if o == nil {
		return nil
	}
	return o.IncludeDocID
}

func (o *OutputElasticCloud) GetResponseRetrySettings() []ResponseRetrySettingElasticCloud {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputElasticCloud) GetTimeoutRetrySettings() *TimeoutRetrySettingsElasticCloud {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputElasticCloud) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputElasticCloud) GetOnBackpressure() *BackpressureBehaviorElasticCloud {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputElasticCloud) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputElasticCloud) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputElasticCloud) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputElasticCloud) GetPqMode() *ModeElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputElasticCloud) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputElasticCloud) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputElasticCloud) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputElasticCloud) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputElasticCloud) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputElasticCloud) GetPqCompress() *CompressionElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputElasticCloud) GetPqOnBackpressure() *QueueFullBehaviorElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputElasticCloud) GetPqControls() *PqControlsElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputElasticCloud) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeElastic string

const (
	OutputTypeElasticElastic OutputTypeElastic = "elastic"
)

func (e OutputTypeElastic) ToPointer() *OutputTypeElastic {
	return &e
}
func (e *OutputTypeElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "elastic":
		*e = OutputTypeElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeElastic: %v", v)
	}
}

type OutputExtraHTTPHeaderElastic struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (o OutputExtraHTTPHeaderElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputExtraHTTPHeaderElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputExtraHTTPHeaderElastic) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputExtraHTTPHeaderElastic) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeElastic - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeElastic string

const (
	// FailedRequestLoggingModeElasticPayload Payload
	FailedRequestLoggingModeElasticPayload FailedRequestLoggingModeElastic = "payload"
	// FailedRequestLoggingModeElasticPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeElasticPayloadAndHeaders FailedRequestLoggingModeElastic = "payloadAndHeaders"
	// FailedRequestLoggingModeElasticNone None
	FailedRequestLoggingModeElasticNone FailedRequestLoggingModeElastic = "none"
)

func (e FailedRequestLoggingModeElastic) ToPointer() *FailedRequestLoggingModeElastic {
	return &e
}

type ResponseRetrySettingElastic struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingElastic) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingElastic) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingElastic) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingElastic) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsElastic struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsElastic) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsElastic) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsElastic) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsElastic) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type ExtraParamElastic struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (e ExtraParamElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraParamElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraParamElastic) GetName() string {
	if e == nil {
		return ""
	}
	return e.Name
}

func (e *ExtraParamElastic) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// AuthAuthenticationMethodElastic - Enter credentials directly, or select a stored secret
type AuthAuthenticationMethodElastic string

const (
	AuthAuthenticationMethodElasticManual       AuthAuthenticationMethodElastic = "manual"
	AuthAuthenticationMethodElasticSecret       AuthAuthenticationMethodElastic = "secret"
	AuthAuthenticationMethodElasticManualAPIKey AuthAuthenticationMethodElastic = "manualAPIKey"
	AuthAuthenticationMethodElasticTextSecret   AuthAuthenticationMethodElastic = "textSecret"
)

func (e AuthAuthenticationMethodElastic) ToPointer() *AuthAuthenticationMethodElastic {
	return &e
}

type AuthElastic struct {
	Disabled *bool   `default:"true" json:"disabled"`
	Username *string `json:"username,omitempty"`
	Password *string `json:"password,omitempty"`
	// Enter credentials directly, or select a stored secret
	AuthType *AuthAuthenticationMethodElastic `default:"manual" json:"authType"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Enter API key directly
	ManualAPIKey *string `json:"manualAPIKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (a AuthElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (a *AuthElastic) GetDisabled() *bool {
	if a == nil {
		return nil
	}
	return a.Disabled
}

func (a *AuthElastic) GetUsername() *string {
	if a == nil {
		return nil
	}
	return a.Username
}

func (a *AuthElastic) GetPassword() *string {
	if a == nil {
		return nil
	}
	return a.Password
}

func (a *AuthElastic) GetAuthType() *AuthAuthenticationMethodElastic {
	if a == nil {
		return nil
	}
	return a.AuthType
}

func (a *AuthElastic) GetCredentialsSecret() *string {
	if a == nil {
		return nil
	}
	return a.CredentialsSecret
}

func (a *AuthElastic) GetManualAPIKey() *string {
	if a == nil {
		return nil
	}
	return a.ManualAPIKey
}

func (a *AuthElastic) GetTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.TextSecret
}

// ElasticVersion - Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.
type ElasticVersion string

const (
	// ElasticVersionAuto Auto
	ElasticVersionAuto ElasticVersion = "auto"
	// ElasticVersionSix 6.x
	ElasticVersionSix ElasticVersion = "6"
	// ElasticVersionSeven 7.x
	ElasticVersionSeven ElasticVersion = "7"
)

func (e ElasticVersion) ToPointer() *ElasticVersion {
	return &e
}

// WriteAction - Action to use when writing events. Must be set to `Create` when writing to a data stream.
type WriteAction string

const (
	// WriteActionIndex Index
	WriteActionIndex WriteAction = "index"
	// WriteActionCreate Create
	WriteActionCreate WriteAction = "create"
)

func (e WriteAction) ToPointer() *WriteAction {
	return &e
}

// BackpressureBehaviorElastic - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorElastic string

const (
	// BackpressureBehaviorElasticBlock Block
	BackpressureBehaviorElasticBlock BackpressureBehaviorElastic = "block"
	// BackpressureBehaviorElasticDrop Drop
	BackpressureBehaviorElasticDrop BackpressureBehaviorElastic = "drop"
	// BackpressureBehaviorElasticQueue Persistent Queue
	BackpressureBehaviorElasticQueue BackpressureBehaviorElastic = "queue"
)

func (e BackpressureBehaviorElastic) ToPointer() *BackpressureBehaviorElastic {
	return &e
}

type URLElastic struct {
	// The URL to an Elastic node to send events to. Example: http://elastic:9200/_bulk
	URL string `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, []string{"url"}); err != nil {
		return err
	}
	return nil
}

func (u *URLElastic) GetURL() string {
	if u == nil {
		return ""
	}
	return u.URL
}

func (u *URLElastic) GetWeight() *float64 {
	if u == nil {
		return nil
	}
	return u.Weight
}

// OutputModeElastic - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeElastic string

const (
	// OutputModeElasticError Error
	OutputModeElasticError OutputModeElastic = "error"
	// OutputModeElasticAlways Backpressure
	OutputModeElasticAlways OutputModeElastic = "always"
	// OutputModeElasticBackpressure Always On
	OutputModeElasticBackpressure OutputModeElastic = "backpressure"
)

func (e OutputModeElastic) ToPointer() *OutputModeElastic {
	return &e
}

// PqCompressCompressionElastic - Codec to use to compress the persisted data
type PqCompressCompressionElastic string

const (
	// PqCompressCompressionElasticNone None
	PqCompressCompressionElasticNone PqCompressCompressionElastic = "none"
	// PqCompressCompressionElasticGzip Gzip
	PqCompressCompressionElasticGzip PqCompressCompressionElastic = "gzip"
)

func (e PqCompressCompressionElastic) ToPointer() *PqCompressCompressionElastic {
	return &e
}

// QueueFullBehaviorElastic - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorElastic string

const (
	// QueueFullBehaviorElasticBlock Block
	QueueFullBehaviorElasticBlock QueueFullBehaviorElastic = "block"
	// QueueFullBehaviorElasticDrop Drop new data
	QueueFullBehaviorElasticDrop QueueFullBehaviorElastic = "drop"
)

func (e QueueFullBehaviorElastic) ToPointer() *QueueFullBehaviorElastic {
	return &e
}

type OutputPqControlsElastic struct {
}

func (o OutputPqControlsElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputElastic struct {
	// Unique ID for this output
	ID   *string           `json:"id,omitempty"`
	Type OutputTypeElastic `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// Index or data stream to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.
	Index string `json:"index"`
	// Document type to use for events. Can be overwritten by an event's __type field.
	DocType *string `json:"docType,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []OutputExtraHTTPHeaderElastic `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeElastic `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingElastic `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsElastic  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool               `default:"true" json:"responseHonorRetryAfterHeader"`
	ExtraParams                   []ExtraParamElastic `json:"extraParams,omitempty"`
	Auth                          *AuthElastic        `json:"auth,omitempty"`
	// Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.
	ElasticVersion *ElasticVersion `default:"auto" json:"elasticVersion"`
	// Optional Elasticsearch destination pipeline
	ElasticPipeline *string `json:"elasticPipeline,omitempty"`
	// Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)
	IncludeDocID *bool `default:"false" json:"includeDocId"`
	// Action to use when writing events. Must be set to `Create` when writing to a data stream.
	WriteAction *WriteAction `default:"create" json:"writeAction"`
	// Retry failed events when a bulk request to Elastic is successful, but the response body returns an error for one or more events in the batch
	RetryPartialErrors *bool `default:"false" json:"retryPartialErrors"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorElastic `default:"block" json:"onBackpressure"`
	Description    *string                      `json:"description,omitempty"`
	// The Cloud ID or URL to an Elastic cluster to send events to. Example: http://elastic:9200/_bulk
	URL *string `json:"url,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool        `default:"false" json:"excludeSelf"`
	Urls        []URLElastic `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeElastic `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionElastic `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorElastic `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsElastic  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any            `additionalProperties:"true" json:"-"`
}

func (o OutputElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "index"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputElastic) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputElastic) GetType() OutputTypeElastic {
	if o == nil {
		return OutputTypeElastic("")
	}
	return o.Type
}

func (o *OutputElastic) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputElastic) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputElastic) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputElastic) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputElastic) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputElastic) GetIndex() string {
	if o == nil {
		return ""
	}
	return o.Index
}

func (o *OutputElastic) GetDocType() *string {
	if o == nil {
		return nil
	}
	return o.DocType
}

func (o *OutputElastic) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputElastic) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputElastic) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputElastic) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputElastic) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputElastic) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputElastic) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputElastic) GetExtraHTTPHeaders() []OutputExtraHTTPHeaderElastic {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputElastic) GetFailedRequestLoggingMode() *FailedRequestLoggingModeElastic {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputElastic) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputElastic) GetResponseRetrySettings() []ResponseRetrySettingElastic {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputElastic) GetTimeoutRetrySettings() *TimeoutRetrySettingsElastic {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputElastic) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputElastic) GetExtraParams() []ExtraParamElastic {
	if o == nil {
		return nil
	}
	return o.ExtraParams
}

func (o *OutputElastic) GetAuth() *AuthElastic {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputElastic) GetElasticVersion() *ElasticVersion {
	if o == nil {
		return nil
	}
	return o.ElasticVersion
}

func (o *OutputElastic) GetElasticPipeline() *string {
	if o == nil {
		return nil
	}
	return o.ElasticPipeline
}

func (o *OutputElastic) GetIncludeDocID() *bool {
	if o == nil {
		return nil
	}
	return o.IncludeDocID
}

func (o *OutputElastic) GetWriteAction() *WriteAction {
	if o == nil {
		return nil
	}
	return o.WriteAction
}

func (o *OutputElastic) GetRetryPartialErrors() *bool {
	if o == nil {
		return nil
	}
	return o.RetryPartialErrors
}

func (o *OutputElastic) GetOnBackpressure() *BackpressureBehaviorElastic {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputElastic) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputElastic) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputElastic) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputElastic) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputElastic) GetUrls() []URLElastic {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputElastic) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputElastic) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputElastic) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputElastic) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputElastic) GetPqMode() *OutputModeElastic {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputElastic) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputElastic) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputElastic) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputElastic) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputElastic) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputElastic) GetPqCompress() *PqCompressCompressionElastic {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputElastic) GetPqOnBackpressure() *QueueFullBehaviorElastic {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputElastic) GetPqControls() *OutputPqControlsElastic {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputElastic) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeMsk string

const (
	OutputTypeMskMsk OutputTypeMsk = "msk"
)

func (e OutputTypeMsk) ToPointer() *OutputTypeMsk {
	return &e
}
func (e *OutputTypeMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "msk":
		*e = OutputTypeMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeMsk: %v", v)
	}
}

// AcknowledgmentsMsk - Control the number of required acknowledgments.
type AcknowledgmentsMsk int64

const (
	// AcknowledgmentsMskOne Leader
	AcknowledgmentsMskOne AcknowledgmentsMsk = 1
	// AcknowledgmentsMskZero None
	AcknowledgmentsMskZero AcknowledgmentsMsk = 0
	// AcknowledgmentsMskMinus1 All
	AcknowledgmentsMskMinus1 AcknowledgmentsMsk = -1
)

func (e AcknowledgmentsMsk) ToPointer() *AcknowledgmentsMsk {
	return &e
}

// RecordDataFormatMsk - Format to use to serialize events before writing to Kafka.
type RecordDataFormatMsk string

const (
	// RecordDataFormatMskJSON JSON
	RecordDataFormatMskJSON RecordDataFormatMsk = "json"
	// RecordDataFormatMskRaw Field _raw
	RecordDataFormatMskRaw RecordDataFormatMsk = "raw"
	// RecordDataFormatMskProtobuf Protobuf
	RecordDataFormatMskProtobuf RecordDataFormatMsk = "protobuf"
)

func (e RecordDataFormatMsk) ToPointer() *RecordDataFormatMsk {
	return &e
}

// OutputCompressionMsk - Codec to use to compress the data before sending to Kafka
type OutputCompressionMsk string

const (
	// OutputCompressionMskNone None
	OutputCompressionMskNone OutputCompressionMsk = "none"
	// OutputCompressionMskGzip Gzip
	OutputCompressionMskGzip OutputCompressionMsk = "gzip"
	// OutputCompressionMskSnappy Snappy
	OutputCompressionMskSnappy OutputCompressionMsk = "snappy"
	// OutputCompressionMskLz4 LZ4
	OutputCompressionMskLz4 OutputCompressionMsk = "lz4"
	// OutputCompressionMskZstd ZSTD
	OutputCompressionMskZstd OutputCompressionMsk = "zstd"
)

func (e OutputCompressionMsk) ToPointer() *OutputCompressionMsk {
	return &e
}

// OutputAuthMsk - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type OutputAuthMsk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputAuthMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputAuthMsk) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type OutputKafkaSchemaRegistryMinimumTLSVersionMsk string

const (
	OutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv1  OutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1"
	OutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv11 OutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.1"
	OutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv12 OutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.2"
	OutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv13 OutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMinimumTLSVersionMsk) ToPointer() *OutputKafkaSchemaRegistryMinimumTLSVersionMsk {
	return &e
}

type OutputKafkaSchemaRegistryMaximumTLSVersionMsk string

const (
	OutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv1  OutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1"
	OutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv11 OutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.1"
	OutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv12 OutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.2"
	OutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv13 OutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMaximumTLSVersionMsk) ToPointer() *OutputKafkaSchemaRegistryMaximumTLSVersionMsk {
	return &e
}

type OutputKafkaSchemaRegistryTLSSettingsClientSideMsk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                        `json:"passphrase,omitempty"`
	MinVersion *OutputKafkaSchemaRegistryMinimumTLSVersionMsk `json:"minVersion,omitempty"`
	MaxVersion *OutputKafkaSchemaRegistryMaximumTLSVersionMsk `json:"maxVersion,omitempty"`
}

func (o OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetMinVersion() *OutputKafkaSchemaRegistryMinimumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetMaxVersion() *OutputKafkaSchemaRegistryMaximumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

type OutputKafkaSchemaRegistryAuthenticationMsk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *OutputAuthMsk                                     `json:"auth,omitempty"`
	TLS  *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (o OutputKafkaSchemaRegistryAuthenticationMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetSchemaRegistryURL() *string {
	if o == nil {
		return nil
	}
	return o.SchemaRegistryURL
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetAuth() *OutputAuthMsk {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetTLS() *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetDefaultKeySchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultKeySchemaID
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetDefaultValueSchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultValueSchemaID
}

// OutputAuthenticationMethodMsk - AWS authentication method. Choose Auto to use IAM roles.
type OutputAuthenticationMethodMsk string

const (
	// OutputAuthenticationMethodMskAuto Auto
	OutputAuthenticationMethodMskAuto OutputAuthenticationMethodMsk = "auto"
	// OutputAuthenticationMethodMskManual Manual
	OutputAuthenticationMethodMskManual OutputAuthenticationMethodMsk = "manual"
	// OutputAuthenticationMethodMskSecret Secret Key pair
	OutputAuthenticationMethodMskSecret OutputAuthenticationMethodMsk = "secret"
)

func (e OutputAuthenticationMethodMsk) ToPointer() *OutputAuthenticationMethodMsk {
	return &e
}

// OutputSignatureVersionMsk - Signature version to use for signing MSK cluster requests
type OutputSignatureVersionMsk string

const (
	OutputSignatureVersionMskV2 OutputSignatureVersionMsk = "v2"
	OutputSignatureVersionMskV4 OutputSignatureVersionMsk = "v4"
)

func (e OutputSignatureVersionMsk) ToPointer() *OutputSignatureVersionMsk {
	return &e
}

type OutputMinimumTLSVersionMsk string

const (
	OutputMinimumTLSVersionMskTlSv1  OutputMinimumTLSVersionMsk = "TLSv1"
	OutputMinimumTLSVersionMskTlSv11 OutputMinimumTLSVersionMsk = "TLSv1.1"
	OutputMinimumTLSVersionMskTlSv12 OutputMinimumTLSVersionMsk = "TLSv1.2"
	OutputMinimumTLSVersionMskTlSv13 OutputMinimumTLSVersionMsk = "TLSv1.3"
)

func (e OutputMinimumTLSVersionMsk) ToPointer() *OutputMinimumTLSVersionMsk {
	return &e
}

type OutputMaximumTLSVersionMsk string

const (
	OutputMaximumTLSVersionMskTlSv1  OutputMaximumTLSVersionMsk = "TLSv1"
	OutputMaximumTLSVersionMskTlSv11 OutputMaximumTLSVersionMsk = "TLSv1.1"
	OutputMaximumTLSVersionMskTlSv12 OutputMaximumTLSVersionMsk = "TLSv1.2"
	OutputMaximumTLSVersionMskTlSv13 OutputMaximumTLSVersionMsk = "TLSv1.3"
)

func (e OutputMaximumTLSVersionMsk) ToPointer() *OutputMaximumTLSVersionMsk {
	return &e
}

type OutputTLSSettingsClientSideMsk struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                     `json:"passphrase,omitempty"`
	MinVersion *OutputMinimumTLSVersionMsk `json:"minVersion,omitempty"`
	MaxVersion *OutputMaximumTLSVersionMsk `json:"maxVersion,omitempty"`
}

func (o OutputTLSSettingsClientSideMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputTLSSettingsClientSideMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputTLSSettingsClientSideMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputTLSSettingsClientSideMsk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputTLSSettingsClientSideMsk) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputTLSSettingsClientSideMsk) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputTLSSettingsClientSideMsk) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputTLSSettingsClientSideMsk) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputTLSSettingsClientSideMsk) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputTLSSettingsClientSideMsk) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputTLSSettingsClientSideMsk) GetMinVersion() *OutputMinimumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputTLSSettingsClientSideMsk) GetMaxVersion() *OutputMaximumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// BackpressureBehaviorMsk - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorMsk string

const (
	// BackpressureBehaviorMskBlock Block
	BackpressureBehaviorMskBlock BackpressureBehaviorMsk = "block"
	// BackpressureBehaviorMskDrop Drop
	BackpressureBehaviorMskDrop BackpressureBehaviorMsk = "drop"
	// BackpressureBehaviorMskQueue Persistent Queue
	BackpressureBehaviorMskQueue BackpressureBehaviorMsk = "queue"
)

func (e BackpressureBehaviorMsk) ToPointer() *BackpressureBehaviorMsk {
	return &e
}

// OutputModeMsk - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeMsk string

const (
	// OutputModeMskError Error
	OutputModeMskError OutputModeMsk = "error"
	// OutputModeMskAlways Backpressure
	OutputModeMskAlways OutputModeMsk = "always"
	// OutputModeMskBackpressure Always On
	OutputModeMskBackpressure OutputModeMsk = "backpressure"
)

func (e OutputModeMsk) ToPointer() *OutputModeMsk {
	return &e
}

// PqCompressCompressionMsk - Codec to use to compress the persisted data
type PqCompressCompressionMsk string

const (
	// PqCompressCompressionMskNone None
	PqCompressCompressionMskNone PqCompressCompressionMsk = "none"
	// PqCompressCompressionMskGzip Gzip
	PqCompressCompressionMskGzip PqCompressCompressionMsk = "gzip"
)

func (e PqCompressCompressionMsk) ToPointer() *PqCompressCompressionMsk {
	return &e
}

// QueueFullBehaviorMsk - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorMsk string

const (
	// QueueFullBehaviorMskBlock Block
	QueueFullBehaviorMskBlock QueueFullBehaviorMsk = "block"
	// QueueFullBehaviorMskDrop Drop new data
	QueueFullBehaviorMskDrop QueueFullBehaviorMsk = "drop"
)

func (e QueueFullBehaviorMsk) ToPointer() *QueueFullBehaviorMsk {
	return &e
}

type OutputPqControlsMsk struct {
}

func (o OutputPqControlsMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputMsk struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type OutputTypeMsk `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.
	Brokers []string `json:"brokers"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsMsk `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatMsk `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *OutputCompressionMsk `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                    `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *OutputKafkaSchemaRegistryAuthenticationMsk `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputAuthenticationMethodMsk `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                        `json:"awsSecretKey,omitempty"`
	// Region where the MSK cluster is located
	Region string `json:"region"`
	// MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing MSK cluster requests
	SignatureVersion *OutputSignatureVersionMsk `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access MSK
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64                        `default:"3600" json:"durationSeconds"`
	TLS             *OutputTLSSettingsClientSideMsk `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorMsk `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// Select the type of object you want the Protobuf definitions to use for event encoding
	ProtobufEncodingID *string `json:"protobufEncodingId,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeMsk `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionMsk `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorMsk `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsMsk  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any        `additionalProperties:"true" json:"-"`
}

func (o OutputMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "brokers", "topic", "region"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputMsk) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputMsk) GetType() OutputTypeMsk {
	if o == nil {
		return OutputTypeMsk("")
	}
	return o.Type
}

func (o *OutputMsk) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputMsk) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputMsk) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputMsk) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputMsk) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputMsk) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputMsk) GetAck() *AcknowledgmentsMsk {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputMsk) GetFormat() *RecordDataFormatMsk {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputMsk) GetCompression() *OutputCompressionMsk {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputMsk) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputMsk) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputMsk) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputMsk) GetKafkaSchemaRegistry() *OutputKafkaSchemaRegistryAuthenticationMsk {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputMsk) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputMsk) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputMsk) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputMsk) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputMsk) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputMsk) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputMsk) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputMsk) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputMsk) GetAwsAuthenticationMethod() *OutputAuthenticationMethodMsk {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputMsk) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputMsk) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputMsk) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputMsk) GetSignatureVersion() *OutputSignatureVersionMsk {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputMsk) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputMsk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputMsk) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputMsk) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputMsk) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputMsk) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputMsk) GetTLS() *OutputTLSSettingsClientSideMsk {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputMsk) GetOnBackpressure() *BackpressureBehaviorMsk {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputMsk) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputMsk) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputMsk) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputMsk) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputMsk) GetProtobufEncodingID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufEncodingID
}

func (o *OutputMsk) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputMsk) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputMsk) GetPqMode() *OutputModeMsk {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputMsk) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputMsk) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputMsk) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputMsk) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputMsk) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputMsk) GetPqCompress() *PqCompressCompressionMsk {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputMsk) GetPqOnBackpressure() *QueueFullBehaviorMsk {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputMsk) GetPqControls() *OutputPqControlsMsk {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputMsk) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeConfluentCloud string

const (
	OutputTypeConfluentCloudConfluentCloud OutputTypeConfluentCloud = "confluent_cloud"
)

func (e OutputTypeConfluentCloud) ToPointer() *OutputTypeConfluentCloud {
	return &e
}
func (e *OutputTypeConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "confluent_cloud":
		*e = OutputTypeConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeConfluentCloud: %v", v)
	}
}

type OutputMinimumTLSVersionConfluentCloud string

const (
	OutputMinimumTLSVersionConfluentCloudTlSv1  OutputMinimumTLSVersionConfluentCloud = "TLSv1"
	OutputMinimumTLSVersionConfluentCloudTlSv11 OutputMinimumTLSVersionConfluentCloud = "TLSv1.1"
	OutputMinimumTLSVersionConfluentCloudTlSv12 OutputMinimumTLSVersionConfluentCloud = "TLSv1.2"
	OutputMinimumTLSVersionConfluentCloudTlSv13 OutputMinimumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e OutputMinimumTLSVersionConfluentCloud) ToPointer() *OutputMinimumTLSVersionConfluentCloud {
	return &e
}

type OutputMaximumTLSVersionConfluentCloud string

const (
	OutputMaximumTLSVersionConfluentCloudTlSv1  OutputMaximumTLSVersionConfluentCloud = "TLSv1"
	OutputMaximumTLSVersionConfluentCloudTlSv11 OutputMaximumTLSVersionConfluentCloud = "TLSv1.1"
	OutputMaximumTLSVersionConfluentCloudTlSv12 OutputMaximumTLSVersionConfluentCloud = "TLSv1.2"
	OutputMaximumTLSVersionConfluentCloudTlSv13 OutputMaximumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e OutputMaximumTLSVersionConfluentCloud) ToPointer() *OutputMaximumTLSVersionConfluentCloud {
	return &e
}

type OutputTLSSettingsClientSideConfluentCloud struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                `json:"passphrase,omitempty"`
	MinVersion *OutputMinimumTLSVersionConfluentCloud `json:"minVersion,omitempty"`
	MaxVersion *OutputMaximumTLSVersionConfluentCloud `json:"maxVersion,omitempty"`
}

func (o OutputTLSSettingsClientSideConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputTLSSettingsClientSideConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetMinVersion() *OutputMinimumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetMaxVersion() *OutputMaximumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// AcknowledgmentsConfluentCloud - Control the number of required acknowledgments.
type AcknowledgmentsConfluentCloud int64

const (
	// AcknowledgmentsConfluentCloudOne Leader
	AcknowledgmentsConfluentCloudOne AcknowledgmentsConfluentCloud = 1
	// AcknowledgmentsConfluentCloudZero None
	AcknowledgmentsConfluentCloudZero AcknowledgmentsConfluentCloud = 0
	// AcknowledgmentsConfluentCloudMinus1 All
	AcknowledgmentsConfluentCloudMinus1 AcknowledgmentsConfluentCloud = -1
)

func (e AcknowledgmentsConfluentCloud) ToPointer() *AcknowledgmentsConfluentCloud {
	return &e
}

// RecordDataFormatConfluentCloud - Format to use to serialize events before writing to Kafka.
type RecordDataFormatConfluentCloud string

const (
	// RecordDataFormatConfluentCloudJSON JSON
	RecordDataFormatConfluentCloudJSON RecordDataFormatConfluentCloud = "json"
	// RecordDataFormatConfluentCloudRaw Field _raw
	RecordDataFormatConfluentCloudRaw RecordDataFormatConfluentCloud = "raw"
	// RecordDataFormatConfluentCloudProtobuf Protobuf
	RecordDataFormatConfluentCloudProtobuf RecordDataFormatConfluentCloud = "protobuf"
)

func (e RecordDataFormatConfluentCloud) ToPointer() *RecordDataFormatConfluentCloud {
	return &e
}

// OutputCompressionConfluentCloud - Codec to use to compress the data before sending to Kafka
type OutputCompressionConfluentCloud string

const (
	// OutputCompressionConfluentCloudNone None
	OutputCompressionConfluentCloudNone OutputCompressionConfluentCloud = "none"
	// OutputCompressionConfluentCloudGzip Gzip
	OutputCompressionConfluentCloudGzip OutputCompressionConfluentCloud = "gzip"
	// OutputCompressionConfluentCloudSnappy Snappy
	OutputCompressionConfluentCloudSnappy OutputCompressionConfluentCloud = "snappy"
	// OutputCompressionConfluentCloudLz4 LZ4
	OutputCompressionConfluentCloudLz4 OutputCompressionConfluentCloud = "lz4"
	// OutputCompressionConfluentCloudZstd ZSTD
	OutputCompressionConfluentCloudZstd OutputCompressionConfluentCloud = "zstd"
)

func (e OutputCompressionConfluentCloud) ToPointer() *OutputCompressionConfluentCloud {
	return &e
}

// OutputAuthConfluentCloud - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type OutputAuthConfluentCloud struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputAuthConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputAuthConfluentCloud) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud string

const (
	OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv1  OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1"
	OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv11 OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.1"
	OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv12 OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.2"
	OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv13 OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud) ToPointer() *OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud {
	return &e
}

type OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud string

const (
	OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv1  OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1"
	OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv11 OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.1"
	OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv12 OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.2"
	OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv13 OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud) ToPointer() *OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud {
	return &e
}

type OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                                   `json:"passphrase,omitempty"`
	MinVersion *OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud `json:"minVersion,omitempty"`
	MaxVersion *OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud `json:"maxVersion,omitempty"`
}

func (o OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetMinVersion() *OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetMaxVersion() *OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

type OutputKafkaSchemaRegistryAuthenticationConfluentCloud struct {
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *OutputAuthConfluentCloud                                     `json:"auth,omitempty"`
	TLS  *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (o OutputKafkaSchemaRegistryAuthenticationConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetSchemaRegistryURL() *string {
	if o == nil {
		return nil
	}
	return o.SchemaRegistryURL
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetAuth() *OutputAuthConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetTLS() *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDefaultKeySchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultKeySchemaID
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDefaultValueSchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultValueSchemaID
}

// OutputAuthenticationMethodConfluentCloud - Enter credentials directly, or select a stored secret
type OutputAuthenticationMethodConfluentCloud string

const (
	OutputAuthenticationMethodConfluentCloudManual OutputAuthenticationMethodConfluentCloud = "manual"
	OutputAuthenticationMethodConfluentCloudSecret OutputAuthenticationMethodConfluentCloud = "secret"
)

func (e OutputAuthenticationMethodConfluentCloud) ToPointer() *OutputAuthenticationMethodConfluentCloud {
	return &e
}

type OutputSASLMechanismConfluentCloud string

const (
	// OutputSASLMechanismConfluentCloudPlain PLAIN
	OutputSASLMechanismConfluentCloudPlain OutputSASLMechanismConfluentCloud = "plain"
	// OutputSASLMechanismConfluentCloudScramSha256 SCRAM-SHA-256
	OutputSASLMechanismConfluentCloudScramSha256 OutputSASLMechanismConfluentCloud = "scram-sha-256"
	// OutputSASLMechanismConfluentCloudScramSha512 SCRAM-SHA-512
	OutputSASLMechanismConfluentCloudScramSha512 OutputSASLMechanismConfluentCloud = "scram-sha-512"
	// OutputSASLMechanismConfluentCloudKerberos GSSAPI/Kerberos
	OutputSASLMechanismConfluentCloudKerberos OutputSASLMechanismConfluentCloud = "kerberos"
)

func (e OutputSASLMechanismConfluentCloud) ToPointer() *OutputSASLMechanismConfluentCloud {
	return &e
}

type OutputOauthParamConfluentCloud struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (o OutputOauthParamConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputOauthParamConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputOauthParamConfluentCloud) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OutputOauthParamConfluentCloud) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputSaslExtensionConfluentCloud struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (o OutputSaslExtensionConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSaslExtensionConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSaslExtensionConfluentCloud) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OutputSaslExtensionConfluentCloud) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// OutputAuthenticationConfluentCloud - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type OutputAuthenticationConfluentCloud struct {
	Disabled *bool   `default:"true" json:"disabled"`
	Username *string `json:"username,omitempty"`
	Password *string `json:"password,omitempty"`
	// Enter credentials directly, or select a stored secret
	AuthType *OutputAuthenticationMethodConfluentCloud `default:"manual" json:"authType"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string                            `json:"credentialsSecret,omitempty"`
	Mechanism         *OutputSASLMechanismConfluentCloud `default:"plain" json:"mechanism"`
	// Location of keytab file for authentication principal
	KeytabLocation *string `json:"keytabLocation,omitempty"`
	// Authentication principal, such as `kafka_user@example.com`
	Principal *string `json:"principal,omitempty"`
	// Kerberos service class for Kafka brokers, such as `kafka`
	BrokerServiceClass *string `json:"brokerServiceClass,omitempty"`
	// Enable OAuth authentication
	OauthEnabled *bool `default:"false" json:"oauthEnabled"`
	// URL of the token endpoint to use for OAuth authentication
	TokenURL *string `json:"tokenUrl,omitempty"`
	// Client ID to use for OAuth authentication
	ClientID        *string `json:"clientId,omitempty"`
	OauthSecretType *string `default:"secret" json:"oauthSecretType"`
	// Select or create a stored text secret
	ClientTextSecret *string `json:"clientTextSecret,omitempty"`
	// Additional fields to send to the token endpoint, such as scope or audience
	OauthParams []OutputOauthParamConfluentCloud `json:"oauthParams,omitempty"`
	// Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId
	SaslExtensions []OutputSaslExtensionConfluentCloud `json:"saslExtensions,omitempty"`
}

func (o OutputAuthenticationConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthenticationConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthenticationConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputAuthenticationConfluentCloud) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputAuthenticationConfluentCloud) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputAuthenticationConfluentCloud) GetAuthType() *OutputAuthenticationMethodConfluentCloud {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputAuthenticationConfluentCloud) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputAuthenticationConfluentCloud) GetMechanism() *OutputSASLMechanismConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Mechanism
}

func (o *OutputAuthenticationConfluentCloud) GetKeytabLocation() *string {
	if o == nil {
		return nil
	}
	return o.KeytabLocation
}

func (o *OutputAuthenticationConfluentCloud) GetPrincipal() *string {
	if o == nil {
		return nil
	}
	return o.Principal
}

func (o *OutputAuthenticationConfluentCloud) GetBrokerServiceClass() *string {
	if o == nil {
		return nil
	}
	return o.BrokerServiceClass
}

func (o *OutputAuthenticationConfluentCloud) GetOauthEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.OauthEnabled
}

func (o *OutputAuthenticationConfluentCloud) GetTokenURL() *string {
	if o == nil {
		return nil
	}
	return o.TokenURL
}

func (o *OutputAuthenticationConfluentCloud) GetClientID() *string {
	if o == nil {
		return nil
	}
	return o.ClientID
}

func (o *OutputAuthenticationConfluentCloud) GetOauthSecretType() *string {
	if o == nil {
		return nil
	}
	return o.OauthSecretType
}

func (o *OutputAuthenticationConfluentCloud) GetClientTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.ClientTextSecret
}

func (o *OutputAuthenticationConfluentCloud) GetOauthParams() []OutputOauthParamConfluentCloud {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputAuthenticationConfluentCloud) GetSaslExtensions() []OutputSaslExtensionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.SaslExtensions
}

// BackpressureBehaviorConfluentCloud - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorConfluentCloud string

const (
	// BackpressureBehaviorConfluentCloudBlock Block
	BackpressureBehaviorConfluentCloudBlock BackpressureBehaviorConfluentCloud = "block"
	// BackpressureBehaviorConfluentCloudDrop Drop
	BackpressureBehaviorConfluentCloudDrop BackpressureBehaviorConfluentCloud = "drop"
	// BackpressureBehaviorConfluentCloudQueue Persistent Queue
	BackpressureBehaviorConfluentCloudQueue BackpressureBehaviorConfluentCloud = "queue"
)

func (e BackpressureBehaviorConfluentCloud) ToPointer() *BackpressureBehaviorConfluentCloud {
	return &e
}

// OutputModeConfluentCloud - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeConfluentCloud string

const (
	// OutputModeConfluentCloudError Error
	OutputModeConfluentCloudError OutputModeConfluentCloud = "error"
	// OutputModeConfluentCloudAlways Backpressure
	OutputModeConfluentCloudAlways OutputModeConfluentCloud = "always"
	// OutputModeConfluentCloudBackpressure Always On
	OutputModeConfluentCloudBackpressure OutputModeConfluentCloud = "backpressure"
)

func (e OutputModeConfluentCloud) ToPointer() *OutputModeConfluentCloud {
	return &e
}

// PqCompressCompressionConfluentCloud - Codec to use to compress the persisted data
type PqCompressCompressionConfluentCloud string

const (
	// PqCompressCompressionConfluentCloudNone None
	PqCompressCompressionConfluentCloudNone PqCompressCompressionConfluentCloud = "none"
	// PqCompressCompressionConfluentCloudGzip Gzip
	PqCompressCompressionConfluentCloudGzip PqCompressCompressionConfluentCloud = "gzip"
)

func (e PqCompressCompressionConfluentCloud) ToPointer() *PqCompressCompressionConfluentCloud {
	return &e
}

// QueueFullBehaviorConfluentCloud - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorConfluentCloud string

const (
	// QueueFullBehaviorConfluentCloudBlock Block
	QueueFullBehaviorConfluentCloudBlock QueueFullBehaviorConfluentCloud = "block"
	// QueueFullBehaviorConfluentCloudDrop Drop new data
	QueueFullBehaviorConfluentCloudDrop QueueFullBehaviorConfluentCloud = "drop"
)

func (e QueueFullBehaviorConfluentCloud) ToPointer() *QueueFullBehaviorConfluentCloud {
	return &e
}

type OutputPqControlsConfluentCloud struct {
}

func (o OutputPqControlsConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputConfluentCloud struct {
	// Unique ID for this output
	ID   *string                  `json:"id,omitempty"`
	Type OutputTypeConfluentCloud `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092.
	Brokers []string                                   `json:"brokers"`
	TLS     *OutputTLSSettingsClientSideConfluentCloud `json:"tls,omitempty"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsConfluentCloud `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatConfluentCloud `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *OutputCompressionConfluentCloud `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                               `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *OutputKafkaSchemaRegistryAuthenticationConfluentCloud `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *OutputAuthenticationConfluentCloud `json:"sasl,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorConfluentCloud `default:"block" json:"onBackpressure"`
	Description    *string                             `json:"description,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// Select the type of object you want the Protobuf definitions to use for event encoding
	ProtobufEncodingID *string `json:"protobufEncodingId,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeConfluentCloud `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionConfluentCloud `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorConfluentCloud `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsConfluentCloud  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                   `additionalProperties:"true" json:"-"`
}

func (o OutputConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "brokers", "topic"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputConfluentCloud) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputConfluentCloud) GetType() OutputTypeConfluentCloud {
	if o == nil {
		return OutputTypeConfluentCloud("")
	}
	return o.Type
}

func (o *OutputConfluentCloud) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputConfluentCloud) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputConfluentCloud) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputConfluentCloud) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputConfluentCloud) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputConfluentCloud) GetTLS() *OutputTLSSettingsClientSideConfluentCloud {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputConfluentCloud) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputConfluentCloud) GetAck() *AcknowledgmentsConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputConfluentCloud) GetFormat() *RecordDataFormatConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputConfluentCloud) GetCompression() *OutputCompressionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputConfluentCloud) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputConfluentCloud) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputConfluentCloud) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputConfluentCloud) GetKafkaSchemaRegistry() *OutputKafkaSchemaRegistryAuthenticationConfluentCloud {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputConfluentCloud) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputConfluentCloud) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputConfluentCloud) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputConfluentCloud) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputConfluentCloud) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputConfluentCloud) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputConfluentCloud) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputConfluentCloud) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputConfluentCloud) GetSasl() *OutputAuthenticationConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputConfluentCloud) GetOnBackpressure() *BackpressureBehaviorConfluentCloud {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputConfluentCloud) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputConfluentCloud) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputConfluentCloud) GetProtobufEncodingID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufEncodingID
}

func (o *OutputConfluentCloud) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputConfluentCloud) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputConfluentCloud) GetPqMode() *OutputModeConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputConfluentCloud) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputConfluentCloud) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputConfluentCloud) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputConfluentCloud) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputConfluentCloud) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputConfluentCloud) GetPqCompress() *PqCompressCompressionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputConfluentCloud) GetPqOnBackpressure() *QueueFullBehaviorConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputConfluentCloud) GetPqControls() *OutputPqControlsConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputConfluentCloud) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeKafka string

const (
	OutputTypeKafkaKafka OutputTypeKafka = "kafka"
)

func (e OutputTypeKafka) ToPointer() *OutputTypeKafka {
	return &e
}
func (e *OutputTypeKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "kafka":
		*e = OutputTypeKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeKafka: %v", v)
	}
}

// AcknowledgmentsKafka - Control the number of required acknowledgments.
type AcknowledgmentsKafka int64

const (
	// AcknowledgmentsKafkaOne Leader
	AcknowledgmentsKafkaOne AcknowledgmentsKafka = 1
	// AcknowledgmentsKafkaZero None
	AcknowledgmentsKafkaZero AcknowledgmentsKafka = 0
	// AcknowledgmentsKafkaMinus1 All
	AcknowledgmentsKafkaMinus1 AcknowledgmentsKafka = -1
)

func (e AcknowledgmentsKafka) ToPointer() *AcknowledgmentsKafka {
	return &e
}

// RecordDataFormatKafka - Format to use to serialize events before writing to Kafka.
type RecordDataFormatKafka string

const (
	// RecordDataFormatKafkaJSON JSON
	RecordDataFormatKafkaJSON RecordDataFormatKafka = "json"
	// RecordDataFormatKafkaRaw Field _raw
	RecordDataFormatKafkaRaw RecordDataFormatKafka = "raw"
	// RecordDataFormatKafkaProtobuf Protobuf
	RecordDataFormatKafkaProtobuf RecordDataFormatKafka = "protobuf"
)

func (e RecordDataFormatKafka) ToPointer() *RecordDataFormatKafka {
	return &e
}

// OutputCompressionKafka - Codec to use to compress the data before sending to Kafka
type OutputCompressionKafka string

const (
	// OutputCompressionKafkaNone None
	OutputCompressionKafkaNone OutputCompressionKafka = "none"
	// OutputCompressionKafkaGzip Gzip
	OutputCompressionKafkaGzip OutputCompressionKafka = "gzip"
	// OutputCompressionKafkaSnappy Snappy
	OutputCompressionKafkaSnappy OutputCompressionKafka = "snappy"
	// OutputCompressionKafkaLz4 LZ4
	OutputCompressionKafkaLz4 OutputCompressionKafka = "lz4"
	// OutputCompressionKafkaZstd ZSTD
	OutputCompressionKafkaZstd OutputCompressionKafka = "zstd"
)

func (e OutputCompressionKafka) ToPointer() *OutputCompressionKafka {
	return &e
}

// OutputAuthKafka - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type OutputAuthKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputAuthKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputAuthKafka) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type OutputKafkaSchemaRegistryMinimumTLSVersionKafka string

const (
	OutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv1  OutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1"
	OutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv11 OutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.1"
	OutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv12 OutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.2"
	OutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv13 OutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMinimumTLSVersionKafka) ToPointer() *OutputKafkaSchemaRegistryMinimumTLSVersionKafka {
	return &e
}

type OutputKafkaSchemaRegistryMaximumTLSVersionKafka string

const (
	OutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv1  OutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1"
	OutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv11 OutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.1"
	OutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv12 OutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.2"
	OutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv13 OutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMaximumTLSVersionKafka) ToPointer() *OutputKafkaSchemaRegistryMaximumTLSVersionKafka {
	return &e
}

type OutputKafkaSchemaRegistryTLSSettingsClientSideKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                                          `json:"passphrase,omitempty"`
	MinVersion *OutputKafkaSchemaRegistryMinimumTLSVersionKafka `json:"minVersion,omitempty"`
	MaxVersion *OutputKafkaSchemaRegistryMaximumTLSVersionKafka `json:"maxVersion,omitempty"`
}

func (o OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetMinVersion() *OutputKafkaSchemaRegistryMinimumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetMaxVersion() *OutputKafkaSchemaRegistryMaximumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

type OutputKafkaSchemaRegistryAuthenticationKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *OutputAuthKafka                                     `json:"auth,omitempty"`
	TLS  *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (o OutputKafkaSchemaRegistryAuthenticationKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetSchemaRegistryURL() *string {
	if o == nil {
		return nil
	}
	return o.SchemaRegistryURL
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetAuth() *OutputAuthKafka {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetTLS() *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetDefaultKeySchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultKeySchemaID
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetDefaultValueSchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultValueSchemaID
}

// OutputAuthenticationMethodKafka - Enter credentials directly, or select a stored secret
type OutputAuthenticationMethodKafka string

const (
	OutputAuthenticationMethodKafkaManual OutputAuthenticationMethodKafka = "manual"
	OutputAuthenticationMethodKafkaSecret OutputAuthenticationMethodKafka = "secret"
)

func (e OutputAuthenticationMethodKafka) ToPointer() *OutputAuthenticationMethodKafka {
	return &e
}

type OutputSASLMechanismKafka string

const (
	// OutputSASLMechanismKafkaPlain PLAIN
	OutputSASLMechanismKafkaPlain OutputSASLMechanismKafka = "plain"
	// OutputSASLMechanismKafkaScramSha256 SCRAM-SHA-256
	OutputSASLMechanismKafkaScramSha256 OutputSASLMechanismKafka = "scram-sha-256"
	// OutputSASLMechanismKafkaScramSha512 SCRAM-SHA-512
	OutputSASLMechanismKafkaScramSha512 OutputSASLMechanismKafka = "scram-sha-512"
	// OutputSASLMechanismKafkaKerberos GSSAPI/Kerberos
	OutputSASLMechanismKafkaKerberos OutputSASLMechanismKafka = "kerberos"
)

func (e OutputSASLMechanismKafka) ToPointer() *OutputSASLMechanismKafka {
	return &e
}

type OutputOauthParamKafka struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (o OutputOauthParamKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputOauthParamKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputOauthParamKafka) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OutputOauthParamKafka) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputSaslExtensionKafka struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

func (o OutputSaslExtensionKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSaslExtensionKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSaslExtensionKafka) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OutputSaslExtensionKafka) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// OutputAuthenticationKafka - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type OutputAuthenticationKafka struct {
	Disabled *bool   `default:"true" json:"disabled"`
	Username *string `json:"username,omitempty"`
	Password *string `json:"password,omitempty"`
	// Enter credentials directly, or select a stored secret
	AuthType *OutputAuthenticationMethodKafka `default:"manual" json:"authType"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string                   `json:"credentialsSecret,omitempty"`
	Mechanism         *OutputSASLMechanismKafka `default:"plain" json:"mechanism"`
	// Location of keytab file for authentication principal
	KeytabLocation *string `json:"keytabLocation,omitempty"`
	// Authentication principal, such as `kafka_user@example.com`
	Principal *string `json:"principal,omitempty"`
	// Kerberos service class for Kafka brokers, such as `kafka`
	BrokerServiceClass *string `json:"brokerServiceClass,omitempty"`
	// Enable OAuth authentication
	OauthEnabled *bool `default:"false" json:"oauthEnabled"`
	// URL of the token endpoint to use for OAuth authentication
	TokenURL *string `json:"tokenUrl,omitempty"`
	// Client ID to use for OAuth authentication
	ClientID        *string `json:"clientId,omitempty"`
	OauthSecretType *string `default:"secret" json:"oauthSecretType"`
	// Select or create a stored text secret
	ClientTextSecret *string `json:"clientTextSecret,omitempty"`
	// Additional fields to send to the token endpoint, such as scope or audience
	OauthParams []OutputOauthParamKafka `json:"oauthParams,omitempty"`
	// Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId
	SaslExtensions []OutputSaslExtensionKafka `json:"saslExtensions,omitempty"`
}

func (o OutputAuthenticationKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthenticationKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthenticationKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputAuthenticationKafka) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputAuthenticationKafka) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputAuthenticationKafka) GetAuthType() *OutputAuthenticationMethodKafka {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputAuthenticationKafka) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputAuthenticationKafka) GetMechanism() *OutputSASLMechanismKafka {
	if o == nil {
		return nil
	}
	return o.Mechanism
}

func (o *OutputAuthenticationKafka) GetKeytabLocation() *string {
	if o == nil {
		return nil
	}
	return o.KeytabLocation
}

func (o *OutputAuthenticationKafka) GetPrincipal() *string {
	if o == nil {
		return nil
	}
	return o.Principal
}

func (o *OutputAuthenticationKafka) GetBrokerServiceClass() *string {
	if o == nil {
		return nil
	}
	return o.BrokerServiceClass
}

func (o *OutputAuthenticationKafka) GetOauthEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.OauthEnabled
}

func (o *OutputAuthenticationKafka) GetTokenURL() *string {
	if o == nil {
		return nil
	}
	return o.TokenURL
}

func (o *OutputAuthenticationKafka) GetClientID() *string {
	if o == nil {
		return nil
	}
	return o.ClientID
}

func (o *OutputAuthenticationKafka) GetOauthSecretType() *string {
	if o == nil {
		return nil
	}
	return o.OauthSecretType
}

func (o *OutputAuthenticationKafka) GetClientTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.ClientTextSecret
}

func (o *OutputAuthenticationKafka) GetOauthParams() []OutputOauthParamKafka {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputAuthenticationKafka) GetSaslExtensions() []OutputSaslExtensionKafka {
	if o == nil {
		return nil
	}
	return o.SaslExtensions
}

type OutputMinimumTLSVersionKafka string

const (
	OutputMinimumTLSVersionKafkaTlSv1  OutputMinimumTLSVersionKafka = "TLSv1"
	OutputMinimumTLSVersionKafkaTlSv11 OutputMinimumTLSVersionKafka = "TLSv1.1"
	OutputMinimumTLSVersionKafkaTlSv12 OutputMinimumTLSVersionKafka = "TLSv1.2"
	OutputMinimumTLSVersionKafkaTlSv13 OutputMinimumTLSVersionKafka = "TLSv1.3"
)

func (e OutputMinimumTLSVersionKafka) ToPointer() *OutputMinimumTLSVersionKafka {
	return &e
}

type OutputMaximumTLSVersionKafka string

const (
	OutputMaximumTLSVersionKafkaTlSv1  OutputMaximumTLSVersionKafka = "TLSv1"
	OutputMaximumTLSVersionKafkaTlSv11 OutputMaximumTLSVersionKafka = "TLSv1.1"
	OutputMaximumTLSVersionKafkaTlSv12 OutputMaximumTLSVersionKafka = "TLSv1.2"
	OutputMaximumTLSVersionKafkaTlSv13 OutputMaximumTLSVersionKafka = "TLSv1.3"
)

func (e OutputMaximumTLSVersionKafka) ToPointer() *OutputMaximumTLSVersionKafka {
	return &e
}

type OutputTLSSettingsClientSideKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                       `json:"passphrase,omitempty"`
	MinVersion *OutputMinimumTLSVersionKafka `json:"minVersion,omitempty"`
	MaxVersion *OutputMaximumTLSVersionKafka `json:"maxVersion,omitempty"`
}

func (o OutputTLSSettingsClientSideKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputTLSSettingsClientSideKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (o *OutputTLSSettingsClientSideKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputTLSSettingsClientSideKafka) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputTLSSettingsClientSideKafka) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputTLSSettingsClientSideKafka) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputTLSSettingsClientSideKafka) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputTLSSettingsClientSideKafka) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputTLSSettingsClientSideKafka) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputTLSSettingsClientSideKafka) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputTLSSettingsClientSideKafka) GetMinVersion() *OutputMinimumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputTLSSettingsClientSideKafka) GetMaxVersion() *OutputMaximumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// BackpressureBehaviorKafka - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorKafka string

const (
	// BackpressureBehaviorKafkaBlock Block
	BackpressureBehaviorKafkaBlock BackpressureBehaviorKafka = "block"
	// BackpressureBehaviorKafkaDrop Drop
	BackpressureBehaviorKafkaDrop BackpressureBehaviorKafka = "drop"
	// BackpressureBehaviorKafkaQueue Persistent Queue
	BackpressureBehaviorKafkaQueue BackpressureBehaviorKafka = "queue"
)

func (e BackpressureBehaviorKafka) ToPointer() *BackpressureBehaviorKafka {
	return &e
}

// OutputModeKafka - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeKafka string

const (
	// OutputModeKafkaError Error
	OutputModeKafkaError OutputModeKafka = "error"
	// OutputModeKafkaAlways Backpressure
	OutputModeKafkaAlways OutputModeKafka = "always"
	// OutputModeKafkaBackpressure Always On
	OutputModeKafkaBackpressure OutputModeKafka = "backpressure"
)

func (e OutputModeKafka) ToPointer() *OutputModeKafka {
	return &e
}

// PqCompressCompressionKafka - Codec to use to compress the persisted data
type PqCompressCompressionKafka string

const (
	// PqCompressCompressionKafkaNone None
	PqCompressCompressionKafkaNone PqCompressCompressionKafka = "none"
	// PqCompressCompressionKafkaGzip Gzip
	PqCompressCompressionKafkaGzip PqCompressCompressionKafka = "gzip"
)

func (e PqCompressCompressionKafka) ToPointer() *PqCompressCompressionKafka {
	return &e
}

// QueueFullBehaviorKafka - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorKafka string

const (
	// QueueFullBehaviorKafkaBlock Block
	QueueFullBehaviorKafkaBlock QueueFullBehaviorKafka = "block"
	// QueueFullBehaviorKafkaDrop Drop new data
	QueueFullBehaviorKafkaDrop QueueFullBehaviorKafka = "drop"
)

func (e QueueFullBehaviorKafka) ToPointer() *QueueFullBehaviorKafka {
	return &e
}

type OutputPqControlsKafka struct {
}

func (o OutputPqControlsKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputKafka struct {
	// Unique ID for this output
	ID   *string         `json:"id,omitempty"`
	Type OutputTypeKafka `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.
	Brokers []string `json:"brokers"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsKafka `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatKafka `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *OutputCompressionKafka `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                      `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *OutputKafkaSchemaRegistryAuthenticationKafka `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *OutputAuthenticationKafka        `json:"sasl,omitempty"`
	TLS  *OutputTLSSettingsClientSideKafka `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorKafka `default:"block" json:"onBackpressure"`
	Description    *string                    `json:"description,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// Select the type of object you want the Protobuf definitions to use for event encoding
	ProtobufEncodingID *string `json:"protobufEncodingId,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeKafka `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionKafka `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorKafka `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsKafka  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any          `additionalProperties:"true" json:"-"`
}

func (o OutputKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "brokers", "topic"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafka) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputKafka) GetType() OutputTypeKafka {
	if o == nil {
		return OutputTypeKafka("")
	}
	return o.Type
}

func (o *OutputKafka) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputKafka) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputKafka) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputKafka) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputKafka) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputKafka) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputKafka) GetAck() *AcknowledgmentsKafka {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputKafka) GetFormat() *RecordDataFormatKafka {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputKafka) GetCompression() *OutputCompressionKafka {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputKafka) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputKafka) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputKafka) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputKafka) GetKafkaSchemaRegistry() *OutputKafkaSchemaRegistryAuthenticationKafka {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputKafka) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputKafka) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputKafka) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputKafka) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputKafka) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputKafka) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputKafka) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputKafka) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputKafka) GetSasl() *OutputAuthenticationKafka {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputKafka) GetTLS() *OutputTLSSettingsClientSideKafka {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputKafka) GetOnBackpressure() *BackpressureBehaviorKafka {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputKafka) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputKafka) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputKafka) GetProtobufEncodingID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufEncodingID
}

func (o *OutputKafka) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputKafka) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputKafka) GetPqMode() *OutputModeKafka {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputKafka) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputKafka) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputKafka) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputKafka) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputKafka) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputKafka) GetPqCompress() *PqCompressCompressionKafka {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputKafka) GetPqOnBackpressure() *QueueFullBehaviorKafka {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputKafka) GetPqControls() *OutputPqControlsKafka {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputKafka) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeExabeam string

const (
	TypeExabeamExabeam TypeExabeam = "exabeam"
)

func (e TypeExabeam) ToPointer() *TypeExabeam {
	return &e
}
func (e *TypeExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "exabeam":
		*e = TypeExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeExabeam: %v", v)
	}
}

// SignatureVersionExabeam - Signature version to use for signing Google Cloud Storage requests
type SignatureVersionExabeam string

const (
	SignatureVersionExabeamV2 SignatureVersionExabeam = "v2"
	SignatureVersionExabeamV4 SignatureVersionExabeam = "v4"
)

func (e SignatureVersionExabeam) ToPointer() *SignatureVersionExabeam {
	return &e
}

// ObjectACLExabeam - Object ACL to assign to uploaded objects
type ObjectACLExabeam string

const (
	// ObjectACLExabeamPrivate private
	ObjectACLExabeamPrivate ObjectACLExabeam = "private"
	// ObjectACLExabeamBucketOwnerRead bucket-owner-read
	ObjectACLExabeamBucketOwnerRead ObjectACLExabeam = "bucket-owner-read"
	// ObjectACLExabeamBucketOwnerFullControl bucket-owner-full-control
	ObjectACLExabeamBucketOwnerFullControl ObjectACLExabeam = "bucket-owner-full-control"
	// ObjectACLExabeamProjectPrivate project-private
	ObjectACLExabeamProjectPrivate ObjectACLExabeam = "project-private"
	// ObjectACLExabeamAuthenticatedRead authenticated-read
	ObjectACLExabeamAuthenticatedRead ObjectACLExabeam = "authenticated-read"
	// ObjectACLExabeamPublicRead public-read
	ObjectACLExabeamPublicRead ObjectACLExabeam = "public-read"
)

func (e ObjectACLExabeam) ToPointer() *ObjectACLExabeam {
	return &e
}

// StorageClassExabeam - Storage class to select for uploaded objects
type StorageClassExabeam string

const (
	// StorageClassExabeamStandard Standard Storage
	StorageClassExabeamStandard StorageClassExabeam = "STANDARD"
	// StorageClassExabeamNearline Nearline Storage
	StorageClassExabeamNearline StorageClassExabeam = "NEARLINE"
	// StorageClassExabeamColdline Coldline Storage
	StorageClassExabeamColdline StorageClassExabeam = "COLDLINE"
	// StorageClassExabeamArchive Archive Storage
	StorageClassExabeamArchive StorageClassExabeam = "ARCHIVE"
)

func (e StorageClassExabeam) ToPointer() *StorageClassExabeam {
	return &e
}

// BackpressureBehaviorExabeam - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorExabeam string

const (
	// BackpressureBehaviorExabeamBlock Block
	BackpressureBehaviorExabeamBlock BackpressureBehaviorExabeam = "block"
	// BackpressureBehaviorExabeamDrop Drop
	BackpressureBehaviorExabeamDrop BackpressureBehaviorExabeam = "drop"
)

func (e BackpressureBehaviorExabeam) ToPointer() *BackpressureBehaviorExabeam {
	return &e
}

// DiskSpaceProtectionExabeam - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionExabeam string

const (
	// DiskSpaceProtectionExabeamBlock Block
	DiskSpaceProtectionExabeamBlock DiskSpaceProtectionExabeam = "block"
	// DiskSpaceProtectionExabeamDrop Drop
	DiskSpaceProtectionExabeamDrop DiskSpaceProtectionExabeam = "drop"
)

func (e DiskSpaceProtectionExabeam) ToPointer() *DiskSpaceProtectionExabeam {
	return &e
}

type OutputExabeam struct {
	// Unique ID for this output
	ID   *string     `json:"id,omitempty"`
	Type TypeExabeam `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// Region where the bucket is located
	Region string `json:"region"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Google Cloud Storage service endpoint
	Endpoint *string `default:"https://storage.googleapis.com" json:"endpoint"`
	// Signature version to use for signing Google Cloud Storage requests
	SignatureVersion *SignatureVersionExabeam `default:"v4" json:"signatureVersion"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLExabeam `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass *StorageClassExabeam `json:"storageClass,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorExabeam `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionExabeam `default:"block" json:"onDiskFullBackpressure"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"10" json:"maxFileSizeMB"`
	// Enter an encoded string containing Exabeam configurations
	EncodedConfiguration *string `json:"encodedConfiguration,omitempty"`
	// ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888
	//
	CollectorInstanceID string `json:"collectorInstanceId"`
	// Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants.
	SiteName *string `json:"siteName,omitempty"`
	// Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name.
	SiteID         *string `json:"siteId,omitempty"`
	TimezoneOffset *string `json:"timezoneOffset,omitempty"`
	// HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	Description  *string `json:"description,omitempty"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum          *float64       `default:"20" json:"maxRetryNum"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputExabeam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputExabeam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "bucket", "region", "collectorInstanceId"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputExabeam) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputExabeam) GetType() TypeExabeam {
	if o == nil {
		return TypeExabeam("")
	}
	return o.Type
}

func (o *OutputExabeam) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputExabeam) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputExabeam) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputExabeam) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputExabeam) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputExabeam) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputExabeam) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputExabeam) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputExabeam) GetSignatureVersion() *SignatureVersionExabeam {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputExabeam) GetObjectACL() *ObjectACLExabeam {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputExabeam) GetStorageClass() *StorageClassExabeam {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputExabeam) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputExabeam) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputExabeam) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputExabeam) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputExabeam) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputExabeam) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputExabeam) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputExabeam) GetOnBackpressure() *BackpressureBehaviorExabeam {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputExabeam) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputExabeam) GetOnDiskFullBackpressure() *DiskSpaceProtectionExabeam {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputExabeam) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputExabeam) GetEncodedConfiguration() *string {
	if o == nil {
		return nil
	}
	return o.EncodedConfiguration
}

func (o *OutputExabeam) GetCollectorInstanceID() string {
	if o == nil {
		return ""
	}
	return o.CollectorInstanceID
}

func (o *OutputExabeam) GetSiteName() *string {
	if o == nil {
		return nil
	}
	return o.SiteName
}

func (o *OutputExabeam) GetSiteID() *string {
	if o == nil {
		return nil
	}
	return o.SiteID
}

func (o *OutputExabeam) GetTimezoneOffset() *string {
	if o == nil {
		return nil
	}
	return o.TimezoneOffset
}

func (o *OutputExabeam) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputExabeam) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputExabeam) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputExabeam) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputExabeam) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputExabeam) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputExabeam) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputExabeam) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeGooglePubsub string

const (
	OutputTypeGooglePubsubGooglePubsub OutputTypeGooglePubsub = "google_pubsub"
)

func (e OutputTypeGooglePubsub) ToPointer() *OutputTypeGooglePubsub {
	return &e
}
func (e *OutputTypeGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_pubsub":
		*e = OutputTypeGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeGooglePubsub: %v", v)
	}
}

// OutputGoogleAuthenticationMethodGooglePubsub - Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
type OutputGoogleAuthenticationMethodGooglePubsub string

const (
	// OutputGoogleAuthenticationMethodGooglePubsubAuto Auto
	OutputGoogleAuthenticationMethodGooglePubsubAuto OutputGoogleAuthenticationMethodGooglePubsub = "auto"
	// OutputGoogleAuthenticationMethodGooglePubsubManual Manual
	OutputGoogleAuthenticationMethodGooglePubsubManual OutputGoogleAuthenticationMethodGooglePubsub = "manual"
	// OutputGoogleAuthenticationMethodGooglePubsubSecret Secret
	OutputGoogleAuthenticationMethodGooglePubsubSecret OutputGoogleAuthenticationMethodGooglePubsub = "secret"
)

func (e OutputGoogleAuthenticationMethodGooglePubsub) ToPointer() *OutputGoogleAuthenticationMethodGooglePubsub {
	return &e
}

// BackpressureBehaviorGooglePubsub - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGooglePubsub string

const (
	// BackpressureBehaviorGooglePubsubBlock Block
	BackpressureBehaviorGooglePubsubBlock BackpressureBehaviorGooglePubsub = "block"
	// BackpressureBehaviorGooglePubsubDrop Drop
	BackpressureBehaviorGooglePubsubDrop BackpressureBehaviorGooglePubsub = "drop"
	// BackpressureBehaviorGooglePubsubQueue Persistent Queue
	BackpressureBehaviorGooglePubsubQueue BackpressureBehaviorGooglePubsub = "queue"
)

func (e BackpressureBehaviorGooglePubsub) ToPointer() *BackpressureBehaviorGooglePubsub {
	return &e
}

// OutputModeGooglePubsub - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeGooglePubsub string

const (
	// OutputModeGooglePubsubError Error
	OutputModeGooglePubsubError OutputModeGooglePubsub = "error"
	// OutputModeGooglePubsubAlways Backpressure
	OutputModeGooglePubsubAlways OutputModeGooglePubsub = "always"
	// OutputModeGooglePubsubBackpressure Always On
	OutputModeGooglePubsubBackpressure OutputModeGooglePubsub = "backpressure"
)

func (e OutputModeGooglePubsub) ToPointer() *OutputModeGooglePubsub {
	return &e
}

// PqCompressCompressionGooglePubsub - Codec to use to compress the persisted data
type PqCompressCompressionGooglePubsub string

const (
	// PqCompressCompressionGooglePubsubNone None
	PqCompressCompressionGooglePubsubNone PqCompressCompressionGooglePubsub = "none"
	// PqCompressCompressionGooglePubsubGzip Gzip
	PqCompressCompressionGooglePubsubGzip PqCompressCompressionGooglePubsub = "gzip"
)

func (e PqCompressCompressionGooglePubsub) ToPointer() *PqCompressCompressionGooglePubsub {
	return &e
}

// QueueFullBehaviorGooglePubsub - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGooglePubsub string

const (
	// QueueFullBehaviorGooglePubsubBlock Block
	QueueFullBehaviorGooglePubsubBlock QueueFullBehaviorGooglePubsub = "block"
	// QueueFullBehaviorGooglePubsubDrop Drop new data
	QueueFullBehaviorGooglePubsubDrop QueueFullBehaviorGooglePubsub = "drop"
)

func (e QueueFullBehaviorGooglePubsub) ToPointer() *QueueFullBehaviorGooglePubsub {
	return &e
}

type OutputPqControlsGooglePubsub struct {
}

func (o OutputPqControlsGooglePubsub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsGooglePubsub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGooglePubsub struct {
	// Unique ID for this output
	ID   *string                `json:"id,omitempty"`
	Type OutputTypeGooglePubsub `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// ID of the topic to send events to.
	TopicName string `json:"topicName"`
	// If enabled, create topic if it does not exist.
	CreateTopic *bool `default:"false" json:"createTopic"`
	// If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled.
	OrderedDelivery *bool `default:"false" json:"orderedDelivery"`
	// Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.
	Region *string `json:"region,omitempty"`
	// Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
	GoogleAuthMethod *OutputGoogleAuthenticationMethodGooglePubsub `default:"manual" json:"googleAuthMethod"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	Secret *string `json:"secret,omitempty"`
	// The maximum number of items the Google API should batch before it sends them to the topic.
	BatchSize *float64 `default:"1000" json:"batchSize"`
	// The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached).
	BatchTimeout *float64 `default:"100" json:"batchTimeout"`
	// Maximum number of queued batches before blocking.
	MaxQueueSize *float64 `default:"100" json:"maxQueueSize"`
	// Maximum size (KB) of batches to send.
	MaxRecordSizeKB *float64 `default:"256" json:"maxRecordSizeKB"`
	// Maximum time to wait before sending a batch (when batch size limit is not reached)
	FlushPeriod *float64 `default:"1" json:"flushPeriod"`
	// The maximum number of in-progress API requests before backpressure is applied.
	MaxInProgress *float64 `default:"10" json:"maxInProgress"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGooglePubsub `default:"block" json:"onBackpressure"`
	Description    *string                           `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeGooglePubsub `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionGooglePubsub `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorGooglePubsub `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsGooglePubsub  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                 `additionalProperties:"true" json:"-"`
}

func (o OutputGooglePubsub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGooglePubsub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "topicName"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGooglePubsub) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGooglePubsub) GetType() OutputTypeGooglePubsub {
	if o == nil {
		return OutputTypeGooglePubsub("")
	}
	return o.Type
}

func (o *OutputGooglePubsub) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGooglePubsub) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGooglePubsub) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGooglePubsub) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGooglePubsub) GetTopicName() string {
	if o == nil {
		return ""
	}
	return o.TopicName
}

func (o *OutputGooglePubsub) GetCreateTopic() *bool {
	if o == nil {
		return nil
	}
	return o.CreateTopic
}

func (o *OutputGooglePubsub) GetOrderedDelivery() *bool {
	if o == nil {
		return nil
	}
	return o.OrderedDelivery
}

func (o *OutputGooglePubsub) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputGooglePubsub) GetGoogleAuthMethod() *OutputGoogleAuthenticationMethodGooglePubsub {
	if o == nil {
		return nil
	}
	return o.GoogleAuthMethod
}

func (o *OutputGooglePubsub) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGooglePubsub) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputGooglePubsub) GetBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.BatchSize
}

func (o *OutputGooglePubsub) GetBatchTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.BatchTimeout
}

func (o *OutputGooglePubsub) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputGooglePubsub) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputGooglePubsub) GetFlushPeriod() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriod
}

func (o *OutputGooglePubsub) GetMaxInProgress() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxInProgress
}

func (o *OutputGooglePubsub) GetOnBackpressure() *BackpressureBehaviorGooglePubsub {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGooglePubsub) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGooglePubsub) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGooglePubsub) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGooglePubsub) GetPqMode() *OutputModeGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGooglePubsub) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGooglePubsub) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGooglePubsub) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGooglePubsub) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGooglePubsub) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGooglePubsub) GetPqCompress() *PqCompressCompressionGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGooglePubsub) GetPqOnBackpressure() *QueueFullBehaviorGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGooglePubsub) GetPqControls() *OutputPqControlsGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGooglePubsub) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeGoogleCloudLogging string

const (
	TypeGoogleCloudLoggingGoogleCloudLogging TypeGoogleCloudLogging = "google_cloud_logging"
)

func (e TypeGoogleCloudLogging) ToPointer() *TypeGoogleCloudLogging {
	return &e
}
func (e *TypeGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_cloud_logging":
		*e = TypeGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleCloudLogging: %v", v)
	}
}

type LogLocationType string

const (
	// LogLocationTypeProject Project
	LogLocationTypeProject LogLocationType = "project"
	// LogLocationTypeOrganization Organization
	LogLocationTypeOrganization LogLocationType = "organization"
	// LogLocationTypeBillingAccount Billing Account
	LogLocationTypeBillingAccount LogLocationType = "billingAccount"
	// LogLocationTypeFolder Folder
	LogLocationTypeFolder LogLocationType = "folder"
)

func (e LogLocationType) ToPointer() *LogLocationType {
	return &e
}

// PayloadFormat - Format to use when sending payload. Defaults to Text.
type PayloadFormat string

const (
	// PayloadFormatText Text
	PayloadFormatText PayloadFormat = "text"
	// PayloadFormatJSON JSON
	PayloadFormatJSON PayloadFormat = "json"
)

func (e PayloadFormat) ToPointer() *PayloadFormat {
	return &e
}

type LogLabel struct {
	// Label name
	Label string `json:"label"`
	// JavaScript expression to compute the label's value.
	ValueExpression string `json:"valueExpression"`
}

func (l LogLabel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(l, "", false)
}

func (l *LogLabel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &l, "", false, []string{"label", "valueExpression"}); err != nil {
		return err
	}
	return nil
}

func (l *LogLabel) GetLabel() string {
	if l == nil {
		return ""
	}
	return l.Label
}

func (l *LogLabel) GetValueExpression() string {
	if l == nil {
		return ""
	}
	return l.ValueExpression
}

type ResourceTypeLabel struct {
	// Label name
	Label string `json:"label"`
	// JavaScript expression to compute the label's value.
	ValueExpression string `json:"valueExpression"`
}

func (r ResourceTypeLabel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResourceTypeLabel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"label", "valueExpression"}); err != nil {
		return err
	}
	return nil
}

func (r *ResourceTypeLabel) GetLabel() string {
	if r == nil {
		return ""
	}
	return r.Label
}

func (r *ResourceTypeLabel) GetValueExpression() string {
	if r == nil {
		return ""
	}
	return r.ValueExpression
}

// GoogleAuthenticationMethodGoogleCloudLogging - Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
type GoogleAuthenticationMethodGoogleCloudLogging string

const (
	// GoogleAuthenticationMethodGoogleCloudLoggingAuto Auto
	GoogleAuthenticationMethodGoogleCloudLoggingAuto GoogleAuthenticationMethodGoogleCloudLogging = "auto"
	// GoogleAuthenticationMethodGoogleCloudLoggingManual Manual
	GoogleAuthenticationMethodGoogleCloudLoggingManual GoogleAuthenticationMethodGoogleCloudLogging = "manual"
	// GoogleAuthenticationMethodGoogleCloudLoggingSecret Secret
	GoogleAuthenticationMethodGoogleCloudLoggingSecret GoogleAuthenticationMethodGoogleCloudLogging = "secret"
)

func (e GoogleAuthenticationMethodGoogleCloudLogging) ToPointer() *GoogleAuthenticationMethodGoogleCloudLogging {
	return &e
}

// BackpressureBehaviorGoogleCloudLogging - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGoogleCloudLogging string

const (
	// BackpressureBehaviorGoogleCloudLoggingBlock Block
	BackpressureBehaviorGoogleCloudLoggingBlock BackpressureBehaviorGoogleCloudLogging = "block"
	// BackpressureBehaviorGoogleCloudLoggingDrop Drop
	BackpressureBehaviorGoogleCloudLoggingDrop BackpressureBehaviorGoogleCloudLogging = "drop"
	// BackpressureBehaviorGoogleCloudLoggingQueue Persistent Queue
	BackpressureBehaviorGoogleCloudLoggingQueue BackpressureBehaviorGoogleCloudLogging = "queue"
)

func (e BackpressureBehaviorGoogleCloudLogging) ToPointer() *BackpressureBehaviorGoogleCloudLogging {
	return &e
}

// ModeGoogleCloudLogging - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeGoogleCloudLogging string

const (
	// ModeGoogleCloudLoggingError Error
	ModeGoogleCloudLoggingError ModeGoogleCloudLogging = "error"
	// ModeGoogleCloudLoggingAlways Backpressure
	ModeGoogleCloudLoggingAlways ModeGoogleCloudLogging = "always"
	// ModeGoogleCloudLoggingBackpressure Always On
	ModeGoogleCloudLoggingBackpressure ModeGoogleCloudLogging = "backpressure"
)

func (e ModeGoogleCloudLogging) ToPointer() *ModeGoogleCloudLogging {
	return &e
}

// CompressionGoogleCloudLogging - Codec to use to compress the persisted data
type CompressionGoogleCloudLogging string

const (
	// CompressionGoogleCloudLoggingNone None
	CompressionGoogleCloudLoggingNone CompressionGoogleCloudLogging = "none"
	// CompressionGoogleCloudLoggingGzip Gzip
	CompressionGoogleCloudLoggingGzip CompressionGoogleCloudLogging = "gzip"
)

func (e CompressionGoogleCloudLogging) ToPointer() *CompressionGoogleCloudLogging {
	return &e
}

// QueueFullBehaviorGoogleCloudLogging - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGoogleCloudLogging string

const (
	// QueueFullBehaviorGoogleCloudLoggingBlock Block
	QueueFullBehaviorGoogleCloudLoggingBlock QueueFullBehaviorGoogleCloudLogging = "block"
	// QueueFullBehaviorGoogleCloudLoggingDrop Drop new data
	QueueFullBehaviorGoogleCloudLoggingDrop QueueFullBehaviorGoogleCloudLogging = "drop"
)

func (e QueueFullBehaviorGoogleCloudLogging) ToPointer() *QueueFullBehaviorGoogleCloudLogging {
	return &e
}

type PqControlsGoogleCloudLogging struct {
}

func (p PqControlsGoogleCloudLogging) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGoogleCloudLogging struct {
	// Unique ID for this output
	ID   *string                `json:"id,omitempty"`
	Type TypeGoogleCloudLogging `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags      []string        `json:"streamtags,omitempty"`
	LogLocationType LogLocationType `json:"logLocationType"`
	// JavaScript expression to compute the value of the log name. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.
	LogNameExpression string `json:"logNameExpression"`
	SanitizeLogNames  *bool  `default:"false" json:"sanitizeLogNames"`
	// Format to use when sending payload. Defaults to Text.
	PayloadFormat *PayloadFormat `default:"text" json:"payloadFormat"`
	// Labels to apply to the log entry
	LogLabels []LogLabel `json:"logLabels,omitempty"`
	// JavaScript expression to compute the value of the managed resource type field. Must evaluate to one of the valid values [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types). Defaults to "global".
	ResourceTypeExpression *string `json:"resourceTypeExpression,omitempty"`
	// Labels to apply to the managed resource. These must correspond to the valid labels for the specified resource type (see [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types)). Otherwise, they will be dropped by Google Cloud Logging.
	ResourceTypeLabels []ResourceTypeLabel `json:"resourceTypeLabels,omitempty"`
	// JavaScript expression to compute the value of the severity field. Must evaluate to one of the severity values supported by Google Cloud Logging [here](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) (case insensitive). Defaults to "DEFAULT".
	SeverityExpression *string `json:"severityExpression,omitempty"`
	// JavaScript expression to compute the value of the insert ID field.
	InsertIDExpression *string `json:"insertIdExpression,omitempty"`
	// Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
	GoogleAuthMethod *GoogleAuthenticationMethodGoogleCloudLogging `default:"manual" json:"googleAuthMethod"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	Secret *string `json:"secret,omitempty"`
	// Maximum size, in KB, of the request body.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Max number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Maximum number of ongoing requests before blocking.
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it.
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum number of requests to limit to per second.
	ThrottleRateReqPerSec *int64 `json:"throttleRateReqPerSec,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request method as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestMethodExpression *string `json:"requestMethodExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request URL as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestURLExpression *string `json:"requestUrlExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestSizeExpression *string `json:"requestSizeExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request method as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	StatusExpression *string `json:"statusExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP response size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ResponseSizeExpression *string `json:"responseSizeExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request user agent as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	UserAgentExpression *string `json:"userAgentExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request remote IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RemoteIPExpression *string `json:"remoteIpExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request server IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ServerIPExpression *string `json:"serverIpExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request referer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RefererExpression *string `json:"refererExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request latency, formatted as <seconds>.<nanoseconds>s (for example, 1.23s). See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	LatencyExpression *string `json:"latencyExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache lookup as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheLookupExpression *string `json:"cacheLookupExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache hit as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheHitExpression *string `json:"cacheHitExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache validated with origin server as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheValidatedExpression *string `json:"cacheValidatedExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache fill bytes as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheFillBytesExpression *string `json:"cacheFillBytesExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request protocol as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ProtocolExpression *string `json:"protocolExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation ID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	IDExpression *string `json:"idExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation producer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	ProducerExpression *string `json:"producerExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation first flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	FirstExpression *string `json:"firstExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation last flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	LastExpression *string `json:"lastExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location file as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	FileExpression *string `json:"fileExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location line as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	LineExpression *string `json:"lineExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location function as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	FunctionExpression *string `json:"functionExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split UID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	UIDExpression *string `json:"uidExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split index as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	IndexExpression *string `json:"indexExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split total splits as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	TotalSplitsExpression *string `json:"totalSplitsExpression,omitempty"`
	// A JavaScript expression that evaluates to the REST resource name of the trace being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	TraceExpression *string `json:"traceExpression,omitempty"`
	// A JavaScript expression that evaluates to the ID of the cloud trace span associated with the current operation in which the log is being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	SpanIDExpression *string `json:"spanIdExpression,omitempty"`
	// A JavaScript expression that evaluates to the the sampling decision of the span associated with the log entry. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	TraceSampledExpression *string `json:"traceSampledExpression,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGoogleCloudLogging `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// JavaScript expression to compute the value of the folder ID with which log entries should be associated. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.
	LogLocationExpression string `json:"logLocationExpression"`
	// JavaScript expression to compute the value of the payload. Must evaluate to a JavaScript object value. If an invalid value is encountered it will result in the default value instead. Defaults to the entire event.
	PayloadExpression *string `json:"payloadExpression,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeGoogleCloudLogging `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionGoogleCloudLogging `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorGoogleCloudLogging `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsGoogleCloudLogging        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                       `additionalProperties:"true" json:"-"`
}

func (o OutputGoogleCloudLogging) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "logLocationType", "logNameExpression", "logLocationExpression"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleCloudLogging) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGoogleCloudLogging) GetType() TypeGoogleCloudLogging {
	if o == nil {
		return TypeGoogleCloudLogging("")
	}
	return o.Type
}

func (o *OutputGoogleCloudLogging) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleCloudLogging) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleCloudLogging) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleCloudLogging) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleCloudLogging) GetLogLocationType() LogLocationType {
	if o == nil {
		return LogLocationType("")
	}
	return o.LogLocationType
}

func (o *OutputGoogleCloudLogging) GetLogNameExpression() string {
	if o == nil {
		return ""
	}
	return o.LogNameExpression
}

func (o *OutputGoogleCloudLogging) GetSanitizeLogNames() *bool {
	if o == nil {
		return nil
	}
	return o.SanitizeLogNames
}

func (o *OutputGoogleCloudLogging) GetPayloadFormat() *PayloadFormat {
	if o == nil {
		return nil
	}
	return o.PayloadFormat
}

func (o *OutputGoogleCloudLogging) GetLogLabels() []LogLabel {
	if o == nil {
		return nil
	}
	return o.LogLabels
}

func (o *OutputGoogleCloudLogging) GetResourceTypeExpression() *string {
	if o == nil {
		return nil
	}
	return o.ResourceTypeExpression
}

func (o *OutputGoogleCloudLogging) GetResourceTypeLabels() []ResourceTypeLabel {
	if o == nil {
		return nil
	}
	return o.ResourceTypeLabels
}

func (o *OutputGoogleCloudLogging) GetSeverityExpression() *string {
	if o == nil {
		return nil
	}
	return o.SeverityExpression
}

func (o *OutputGoogleCloudLogging) GetInsertIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.InsertIDExpression
}

func (o *OutputGoogleCloudLogging) GetGoogleAuthMethod() *GoogleAuthenticationMethodGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.GoogleAuthMethod
}

func (o *OutputGoogleCloudLogging) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGoogleCloudLogging) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputGoogleCloudLogging) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGoogleCloudLogging) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGoogleCloudLogging) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGoogleCloudLogging) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGoogleCloudLogging) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputGoogleCloudLogging) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGoogleCloudLogging) GetThrottleRateReqPerSec() *int64 {
	if o == nil {
		return nil
	}
	return o.ThrottleRateReqPerSec
}

func (o *OutputGoogleCloudLogging) GetRequestMethodExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestMethodExpression
}

func (o *OutputGoogleCloudLogging) GetRequestURLExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestURLExpression
}

func (o *OutputGoogleCloudLogging) GetRequestSizeExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestSizeExpression
}

func (o *OutputGoogleCloudLogging) GetStatusExpression() *string {
	if o == nil {
		return nil
	}
	return o.StatusExpression
}

func (o *OutputGoogleCloudLogging) GetResponseSizeExpression() *string {
	if o == nil {
		return nil
	}
	return o.ResponseSizeExpression
}

func (o *OutputGoogleCloudLogging) GetUserAgentExpression() *string {
	if o == nil {
		return nil
	}
	return o.UserAgentExpression
}

func (o *OutputGoogleCloudLogging) GetRemoteIPExpression() *string {
	if o == nil {
		return nil
	}
	return o.RemoteIPExpression
}

func (o *OutputGoogleCloudLogging) GetServerIPExpression() *string {
	if o == nil {
		return nil
	}
	return o.ServerIPExpression
}

func (o *OutputGoogleCloudLogging) GetRefererExpression() *string {
	if o == nil {
		return nil
	}
	return o.RefererExpression
}

func (o *OutputGoogleCloudLogging) GetLatencyExpression() *string {
	if o == nil {
		return nil
	}
	return o.LatencyExpression
}

func (o *OutputGoogleCloudLogging) GetCacheLookupExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheLookupExpression
}

func (o *OutputGoogleCloudLogging) GetCacheHitExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheHitExpression
}

func (o *OutputGoogleCloudLogging) GetCacheValidatedExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheValidatedExpression
}

func (o *OutputGoogleCloudLogging) GetCacheFillBytesExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheFillBytesExpression
}

func (o *OutputGoogleCloudLogging) GetProtocolExpression() *string {
	if o == nil {
		return nil
	}
	return o.ProtocolExpression
}

func (o *OutputGoogleCloudLogging) GetIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.IDExpression
}

func (o *OutputGoogleCloudLogging) GetProducerExpression() *string {
	if o == nil {
		return nil
	}
	return o.ProducerExpression
}

func (o *OutputGoogleCloudLogging) GetFirstExpression() *string {
	if o == nil {
		return nil
	}
	return o.FirstExpression
}

func (o *OutputGoogleCloudLogging) GetLastExpression() *string {
	if o == nil {
		return nil
	}
	return o.LastExpression
}

func (o *OutputGoogleCloudLogging) GetFileExpression() *string {
	if o == nil {
		return nil
	}
	return o.FileExpression
}

func (o *OutputGoogleCloudLogging) GetLineExpression() *string {
	if o == nil {
		return nil
	}
	return o.LineExpression
}

func (o *OutputGoogleCloudLogging) GetFunctionExpression() *string {
	if o == nil {
		return nil
	}
	return o.FunctionExpression
}

func (o *OutputGoogleCloudLogging) GetUIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.UIDExpression
}

func (o *OutputGoogleCloudLogging) GetIndexExpression() *string {
	if o == nil {
		return nil
	}
	return o.IndexExpression
}

func (o *OutputGoogleCloudLogging) GetTotalSplitsExpression() *string {
	if o == nil {
		return nil
	}
	return o.TotalSplitsExpression
}

func (o *OutputGoogleCloudLogging) GetTraceExpression() *string {
	if o == nil {
		return nil
	}
	return o.TraceExpression
}

func (o *OutputGoogleCloudLogging) GetSpanIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.SpanIDExpression
}

func (o *OutputGoogleCloudLogging) GetTraceSampledExpression() *string {
	if o == nil {
		return nil
	}
	return o.TraceSampledExpression
}

func (o *OutputGoogleCloudLogging) GetOnBackpressure() *BackpressureBehaviorGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleCloudLogging) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputGoogleCloudLogging) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleCloudLogging) GetLogLocationExpression() string {
	if o == nil {
		return ""
	}
	return o.LogLocationExpression
}

func (o *OutputGoogleCloudLogging) GetPayloadExpression() *string {
	if o == nil {
		return nil
	}
	return o.PayloadExpression
}

func (o *OutputGoogleCloudLogging) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGoogleCloudLogging) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGoogleCloudLogging) GetPqMode() *ModeGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGoogleCloudLogging) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGoogleCloudLogging) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGoogleCloudLogging) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGoogleCloudLogging) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGoogleCloudLogging) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGoogleCloudLogging) GetPqCompress() *CompressionGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGoogleCloudLogging) GetPqOnBackpressure() *QueueFullBehaviorGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGoogleCloudLogging) GetPqControls() *PqControlsGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGoogleCloudLogging) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeGoogleCloudStorage string

const (
	TypeGoogleCloudStorageGoogleCloudStorage TypeGoogleCloudStorage = "google_cloud_storage"
)

func (e TypeGoogleCloudStorage) ToPointer() *TypeGoogleCloudStorage {
	return &e
}
func (e *TypeGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_cloud_storage":
		*e = TypeGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleCloudStorage: %v", v)
	}
}

// SignatureVersionGoogleCloudStorage - Signature version to use for signing Google Cloud Storage requests
type SignatureVersionGoogleCloudStorage string

const (
	SignatureVersionGoogleCloudStorageV2 SignatureVersionGoogleCloudStorage = "v2"
	SignatureVersionGoogleCloudStorageV4 SignatureVersionGoogleCloudStorage = "v4"
)

func (e SignatureVersionGoogleCloudStorage) ToPointer() *SignatureVersionGoogleCloudStorage {
	return &e
}

type AuthenticationMethodGoogleCloudStorage string

const (
	// AuthenticationMethodGoogleCloudStorageAuto auto
	AuthenticationMethodGoogleCloudStorageAuto AuthenticationMethodGoogleCloudStorage = "auto"
	// AuthenticationMethodGoogleCloudStorageManual manual
	AuthenticationMethodGoogleCloudStorageManual AuthenticationMethodGoogleCloudStorage = "manual"
	// AuthenticationMethodGoogleCloudStorageSecret Secret Key pair
	AuthenticationMethodGoogleCloudStorageSecret AuthenticationMethodGoogleCloudStorage = "secret"
)

func (e AuthenticationMethodGoogleCloudStorage) ToPointer() *AuthenticationMethodGoogleCloudStorage {
	return &e
}

// ObjectACLGoogleCloudStorage - Object ACL to assign to uploaded objects
type ObjectACLGoogleCloudStorage string

const (
	// ObjectACLGoogleCloudStoragePrivate private
	ObjectACLGoogleCloudStoragePrivate ObjectACLGoogleCloudStorage = "private"
	// ObjectACLGoogleCloudStorageBucketOwnerRead bucket-owner-read
	ObjectACLGoogleCloudStorageBucketOwnerRead ObjectACLGoogleCloudStorage = "bucket-owner-read"
	// ObjectACLGoogleCloudStorageBucketOwnerFullControl bucket-owner-full-control
	ObjectACLGoogleCloudStorageBucketOwnerFullControl ObjectACLGoogleCloudStorage = "bucket-owner-full-control"
	// ObjectACLGoogleCloudStorageProjectPrivate project-private
	ObjectACLGoogleCloudStorageProjectPrivate ObjectACLGoogleCloudStorage = "project-private"
	// ObjectACLGoogleCloudStorageAuthenticatedRead authenticated-read
	ObjectACLGoogleCloudStorageAuthenticatedRead ObjectACLGoogleCloudStorage = "authenticated-read"
	// ObjectACLGoogleCloudStoragePublicRead public-read
	ObjectACLGoogleCloudStoragePublicRead ObjectACLGoogleCloudStorage = "public-read"
)

func (e ObjectACLGoogleCloudStorage) ToPointer() *ObjectACLGoogleCloudStorage {
	return &e
}

// StorageClassGoogleCloudStorage - Storage class to select for uploaded objects
type StorageClassGoogleCloudStorage string

const (
	// StorageClassGoogleCloudStorageStandard Standard Storage
	StorageClassGoogleCloudStorageStandard StorageClassGoogleCloudStorage = "STANDARD"
	// StorageClassGoogleCloudStorageNearline Nearline Storage
	StorageClassGoogleCloudStorageNearline StorageClassGoogleCloudStorage = "NEARLINE"
	// StorageClassGoogleCloudStorageColdline Coldline Storage
	StorageClassGoogleCloudStorageColdline StorageClassGoogleCloudStorage = "COLDLINE"
	// StorageClassGoogleCloudStorageArchive Archive Storage
	StorageClassGoogleCloudStorageArchive StorageClassGoogleCloudStorage = "ARCHIVE"
)

func (e StorageClassGoogleCloudStorage) ToPointer() *StorageClassGoogleCloudStorage {
	return &e
}

// DataFormatGoogleCloudStorage - Format of the output data
type DataFormatGoogleCloudStorage string

const (
	// DataFormatGoogleCloudStorageJSON JSON
	DataFormatGoogleCloudStorageJSON DataFormatGoogleCloudStorage = "json"
	// DataFormatGoogleCloudStorageRaw Raw
	DataFormatGoogleCloudStorageRaw DataFormatGoogleCloudStorage = "raw"
	// DataFormatGoogleCloudStorageParquet Parquet
	DataFormatGoogleCloudStorageParquet DataFormatGoogleCloudStorage = "parquet"
)

func (e DataFormatGoogleCloudStorage) ToPointer() *DataFormatGoogleCloudStorage {
	return &e
}

// BackpressureBehaviorGoogleCloudStorage - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGoogleCloudStorage string

const (
	// BackpressureBehaviorGoogleCloudStorageBlock Block
	BackpressureBehaviorGoogleCloudStorageBlock BackpressureBehaviorGoogleCloudStorage = "block"
	// BackpressureBehaviorGoogleCloudStorageDrop Drop
	BackpressureBehaviorGoogleCloudStorageDrop BackpressureBehaviorGoogleCloudStorage = "drop"
)

func (e BackpressureBehaviorGoogleCloudStorage) ToPointer() *BackpressureBehaviorGoogleCloudStorage {
	return &e
}

// DiskSpaceProtectionGoogleCloudStorage - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionGoogleCloudStorage string

const (
	// DiskSpaceProtectionGoogleCloudStorageBlock Block
	DiskSpaceProtectionGoogleCloudStorageBlock DiskSpaceProtectionGoogleCloudStorage = "block"
	// DiskSpaceProtectionGoogleCloudStorageDrop Drop
	DiskSpaceProtectionGoogleCloudStorageDrop DiskSpaceProtectionGoogleCloudStorage = "drop"
)

func (e DiskSpaceProtectionGoogleCloudStorage) ToPointer() *DiskSpaceProtectionGoogleCloudStorage {
	return &e
}

// CompressionGoogleCloudStorage - Data compression format to apply to HTTP content before it is delivered
type CompressionGoogleCloudStorage string

const (
	CompressionGoogleCloudStorageNone CompressionGoogleCloudStorage = "none"
	CompressionGoogleCloudStorageGzip CompressionGoogleCloudStorage = "gzip"
)

func (e CompressionGoogleCloudStorage) ToPointer() *CompressionGoogleCloudStorage {
	return &e
}

// CompressionLevelGoogleCloudStorage - Compression level to apply before moving files to final destination
type CompressionLevelGoogleCloudStorage string

const (
	// CompressionLevelGoogleCloudStorageBestSpeed Best Speed
	CompressionLevelGoogleCloudStorageBestSpeed CompressionLevelGoogleCloudStorage = "best_speed"
	// CompressionLevelGoogleCloudStorageNormal Normal
	CompressionLevelGoogleCloudStorageNormal CompressionLevelGoogleCloudStorage = "normal"
	// CompressionLevelGoogleCloudStorageBestCompression Best Compression
	CompressionLevelGoogleCloudStorageBestCompression CompressionLevelGoogleCloudStorage = "best_compression"
)

func (e CompressionLevelGoogleCloudStorage) ToPointer() *CompressionLevelGoogleCloudStorage {
	return &e
}

// ParquetVersionGoogleCloudStorage - Determines which data types are supported and how they are represented
type ParquetVersionGoogleCloudStorage string

const (
	// ParquetVersionGoogleCloudStorageParquet10 1.0
	ParquetVersionGoogleCloudStorageParquet10 ParquetVersionGoogleCloudStorage = "PARQUET_1_0"
	// ParquetVersionGoogleCloudStorageParquet24 2.4
	ParquetVersionGoogleCloudStorageParquet24 ParquetVersionGoogleCloudStorage = "PARQUET_2_4"
	// ParquetVersionGoogleCloudStorageParquet26 2.6
	ParquetVersionGoogleCloudStorageParquet26 ParquetVersionGoogleCloudStorage = "PARQUET_2_6"
)

func (e ParquetVersionGoogleCloudStorage) ToPointer() *ParquetVersionGoogleCloudStorage {
	return &e
}

// DataPageVersionGoogleCloudStorage - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionGoogleCloudStorage string

const (
	// DataPageVersionGoogleCloudStorageDataPageV1 V1
	DataPageVersionGoogleCloudStorageDataPageV1 DataPageVersionGoogleCloudStorage = "DATA_PAGE_V1"
	// DataPageVersionGoogleCloudStorageDataPageV2 V2
	DataPageVersionGoogleCloudStorageDataPageV2 DataPageVersionGoogleCloudStorage = "DATA_PAGE_V2"
)

func (e DataPageVersionGoogleCloudStorage) ToPointer() *DataPageVersionGoogleCloudStorage {
	return &e
}

type KeyValueMetadatumGoogleCloudStorage struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumGoogleCloudStorage) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumGoogleCloudStorage) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumGoogleCloudStorage) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputGoogleCloudStorage struct {
	// Unique ID for this output
	ID   *string                `json:"id,omitempty"`
	Type TypeGoogleCloudStorage `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// Region where the bucket is located
	Region string `json:"region"`
	// Google Cloud Storage service endpoint
	Endpoint *string `default:"https://storage.googleapis.com" json:"endpoint"`
	// Signature version to use for signing Google Cloud Storage requests
	SignatureVersion        *SignatureVersionGoogleCloudStorage     `default:"v4" json:"signatureVersion"`
	AwsAuthenticationMethod *AuthenticationMethodGoogleCloudStorage `default:"manual" json:"awsAuthenticationMethod"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`
	DestPath *string `default:"" json:"destPath"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLGoogleCloudStorage `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass *StorageClassGoogleCloudStorage `json:"storageClass,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatGoogleCloudStorage `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGoogleCloudStorage `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionGoogleCloudStorage `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool   `default:"false" json:"forceCloseOnShutdown"`
	Description          *string `json:"description,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionGoogleCloudStorage `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelGoogleCloudStorage `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionGoogleCloudStorage `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionGoogleCloudStorage `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumGoogleCloudStorage `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
	// HMAC access key. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// HMAC secret. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret            *string        `json:"awsSecret,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputGoogleCloudStorage) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "bucket", "region"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleCloudStorage) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGoogleCloudStorage) GetType() TypeGoogleCloudStorage {
	if o == nil {
		return TypeGoogleCloudStorage("")
	}
	return o.Type
}

func (o *OutputGoogleCloudStorage) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleCloudStorage) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleCloudStorage) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleCloudStorage) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleCloudStorage) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputGoogleCloudStorage) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputGoogleCloudStorage) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputGoogleCloudStorage) GetSignatureVersion() *SignatureVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputGoogleCloudStorage) GetAwsAuthenticationMethod() *AuthenticationMethodGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputGoogleCloudStorage) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputGoogleCloudStorage) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputGoogleCloudStorage) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputGoogleCloudStorage) GetObjectACL() *ObjectACLGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputGoogleCloudStorage) GetStorageClass() *StorageClassGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputGoogleCloudStorage) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputGoogleCloudStorage) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGoogleCloudStorage) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputGoogleCloudStorage) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputGoogleCloudStorage) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputGoogleCloudStorage) GetFormat() *DataFormatGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputGoogleCloudStorage) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputGoogleCloudStorage) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputGoogleCloudStorage) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputGoogleCloudStorage) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputGoogleCloudStorage) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputGoogleCloudStorage) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputGoogleCloudStorage) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputGoogleCloudStorage) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputGoogleCloudStorage) GetOnBackpressure() *BackpressureBehaviorGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleCloudStorage) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputGoogleCloudStorage) GetOnDiskFullBackpressure() *DiskSpaceProtectionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputGoogleCloudStorage) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputGoogleCloudStorage) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleCloudStorage) GetCompress() *CompressionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGoogleCloudStorage) GetCompressionLevel() *CompressionLevelGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputGoogleCloudStorage) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputGoogleCloudStorage) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputGoogleCloudStorage) GetParquetVersion() *ParquetVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputGoogleCloudStorage) GetParquetDataPageVersion() *DataPageVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputGoogleCloudStorage) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputGoogleCloudStorage) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputGoogleCloudStorage) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputGoogleCloudStorage) GetKeyValueMetadata() []KeyValueMetadatumGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputGoogleCloudStorage) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputGoogleCloudStorage) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputGoogleCloudStorage) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputGoogleCloudStorage) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputGoogleCloudStorage) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputGoogleCloudStorage) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputGoogleCloudStorage) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputGoogleCloudStorage) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputGoogleCloudStorage) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputGoogleCloudStorage) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputGoogleCloudStorage) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeGoogleChronicle string

const (
	TypeGoogleChronicleGoogleChronicle TypeGoogleChronicle = "google_chronicle"
)

func (e TypeGoogleChronicle) ToPointer() *TypeGoogleChronicle {
	return &e
}
func (e *TypeGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_chronicle":
		*e = TypeGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleChronicle: %v", v)
	}
}

type OutputAPIVersion string

const (
	// OutputAPIVersionV1 V1
	OutputAPIVersionV1 OutputAPIVersion = "v1"
	// OutputAPIVersionV2 V2
	OutputAPIVersionV2 OutputAPIVersion = "v2"
)

func (e OutputAPIVersion) ToPointer() *OutputAPIVersion {
	return &e
}

type AuthenticationMethodGoogleChronicle string

const (
	// AuthenticationMethodGoogleChronicleManual API key
	AuthenticationMethodGoogleChronicleManual AuthenticationMethodGoogleChronicle = "manual"
	// AuthenticationMethodGoogleChronicleSecret API key secret
	AuthenticationMethodGoogleChronicleSecret AuthenticationMethodGoogleChronicle = "secret"
	// AuthenticationMethodGoogleChronicleServiceAccount Service account credentials
	AuthenticationMethodGoogleChronicleServiceAccount AuthenticationMethodGoogleChronicle = "serviceAccount"
	// AuthenticationMethodGoogleChronicleServiceAccountSecret Service account credentials secret
	AuthenticationMethodGoogleChronicleServiceAccountSecret AuthenticationMethodGoogleChronicle = "serviceAccountSecret"
)

func (e AuthenticationMethodGoogleChronicle) ToPointer() *AuthenticationMethodGoogleChronicle {
	return &e
}

type ResponseRetrySettingGoogleChronicle struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingGoogleChronicle) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingGoogleChronicle) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingGoogleChronicle) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingGoogleChronicle) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsGoogleChronicle struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsGoogleChronicle) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsGoogleChronicle) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsGoogleChronicle) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsGoogleChronicle) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

type SendEventsAs string

const (
	// SendEventsAsUnstructured Unstructured
	SendEventsAsUnstructured SendEventsAs = "unstructured"
	// SendEventsAsUdm UDM
	SendEventsAsUdm SendEventsAs = "udm"
)

func (e SendEventsAs) ToPointer() *SendEventsAs {
	return &e
}

type ExtraHTTPHeaderGoogleChronicle struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderGoogleChronicle) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderGoogleChronicle) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeGoogleChronicle - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeGoogleChronicle string

const (
	// FailedRequestLoggingModeGoogleChroniclePayload Payload
	FailedRequestLoggingModeGoogleChroniclePayload FailedRequestLoggingModeGoogleChronicle = "payload"
	// FailedRequestLoggingModeGoogleChroniclePayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeGoogleChroniclePayloadAndHeaders FailedRequestLoggingModeGoogleChronicle = "payloadAndHeaders"
	// FailedRequestLoggingModeGoogleChronicleNone None
	FailedRequestLoggingModeGoogleChronicleNone FailedRequestLoggingModeGoogleChronicle = "none"
)

func (e FailedRequestLoggingModeGoogleChronicle) ToPointer() *FailedRequestLoggingModeGoogleChronicle {
	return &e
}

// BackpressureBehaviorGoogleChronicle - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorGoogleChronicle string

const (
	// BackpressureBehaviorGoogleChronicleBlock Block
	BackpressureBehaviorGoogleChronicleBlock BackpressureBehaviorGoogleChronicle = "block"
	// BackpressureBehaviorGoogleChronicleDrop Drop
	BackpressureBehaviorGoogleChronicleDrop BackpressureBehaviorGoogleChronicle = "drop"
	// BackpressureBehaviorGoogleChronicleQueue Persistent Queue
	BackpressureBehaviorGoogleChronicleQueue BackpressureBehaviorGoogleChronicle = "queue"
)

func (e BackpressureBehaviorGoogleChronicle) ToPointer() *BackpressureBehaviorGoogleChronicle {
	return &e
}

type ExtraLogType struct {
	LogType     string  `json:"logType"`
	Description *string `json:"description,omitempty"`
}

func (e ExtraLogType) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraLogType) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"logType"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraLogType) GetLogType() string {
	if e == nil {
		return ""
	}
	return e.LogType
}

func (e *ExtraLogType) GetDescription() *string {
	if e == nil {
		return nil
	}
	return e.Description
}

type CustomLabelGoogleChronicle struct {
	Key   string `json:"key"`
	Value string `json:"value"`
}

func (c CustomLabelGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CustomLabelGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, []string{"key", "value"}); err != nil {
		return err
	}
	return nil
}

func (c *CustomLabelGoogleChronicle) GetKey() string {
	if c == nil {
		return ""
	}
	return c.Key
}

func (c *CustomLabelGoogleChronicle) GetValue() string {
	if c == nil {
		return ""
	}
	return c.Value
}

// UDMType - Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent.
type UDMType string

const (
	UDMTypeEntities UDMType = "entities"
	UDMTypeLogs     UDMType = "logs"
)

func (e UDMType) ToPointer() *UDMType {
	return &e
}

// ModeGoogleChronicle - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeGoogleChronicle string

const (
	// ModeGoogleChronicleError Error
	ModeGoogleChronicleError ModeGoogleChronicle = "error"
	// ModeGoogleChronicleAlways Backpressure
	ModeGoogleChronicleAlways ModeGoogleChronicle = "always"
	// ModeGoogleChronicleBackpressure Always On
	ModeGoogleChronicleBackpressure ModeGoogleChronicle = "backpressure"
)

func (e ModeGoogleChronicle) ToPointer() *ModeGoogleChronicle {
	return &e
}

// CompressionGoogleChronicle - Codec to use to compress the persisted data
type CompressionGoogleChronicle string

const (
	// CompressionGoogleChronicleNone None
	CompressionGoogleChronicleNone CompressionGoogleChronicle = "none"
	// CompressionGoogleChronicleGzip Gzip
	CompressionGoogleChronicleGzip CompressionGoogleChronicle = "gzip"
)

func (e CompressionGoogleChronicle) ToPointer() *CompressionGoogleChronicle {
	return &e
}

// QueueFullBehaviorGoogleChronicle - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGoogleChronicle string

const (
	// QueueFullBehaviorGoogleChronicleBlock Block
	QueueFullBehaviorGoogleChronicleBlock QueueFullBehaviorGoogleChronicle = "block"
	// QueueFullBehaviorGoogleChronicleDrop Drop new data
	QueueFullBehaviorGoogleChronicleDrop QueueFullBehaviorGoogleChronicle = "drop"
)

func (e QueueFullBehaviorGoogleChronicle) ToPointer() *QueueFullBehaviorGoogleChronicle {
	return &e
}

type PqControlsGoogleChronicle struct {
}

func (p PqControlsGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputGoogleChronicle struct {
	// Unique ID for this output
	ID   *string             `json:"id,omitempty"`
	Type TypeGoogleChronicle `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags           []string                             `json:"streamtags,omitempty"`
	APIVersion           *OutputAPIVersion                    `default:"v1" json:"apiVersion"`
	AuthenticationMethod *AuthenticationMethodGoogleChronicle `default:"serviceAccount" json:"authenticationMethod"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingGoogleChronicle `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsGoogleChronicle  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool         `default:"false" json:"responseHonorRetryAfterHeader"`
	LogFormatType                 *SendEventsAs `default:"unstructured" json:"logFormatType"`
	// Regional endpoint to send events to
	Region *string `json:"region,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"90" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderGoogleChronicle `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeGoogleChronicle `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGoogleChronicle `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Custom log types. If the value "Custom" is selected in the setting "Default log type" above, the first custom log type in this table will be automatically selected as default log type.
	ExtraLogTypes []ExtraLogType `json:"extraLogTypes,omitempty"`
	// Default log type value to send to SecOps. Can be overwritten by event field __logType.
	LogType *string `json:"logType,omitempty"`
	// Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.
	LogTextField *string `json:"logTextField,omitempty"`
	// A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication.
	CustomerID *string `json:"customerId,omitempty"`
	// User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.
	Namespace *string `json:"namespace,omitempty"`
	// Custom labels to be added to every batch
	CustomLabels []CustomLabelGoogleChronicle `json:"customLabels,omitempty"`
	// Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent.
	UdmType *UDMType `default:"logs" json:"udmType"`
	// Organization's API key in Google SecOps
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	APIKeySecret *string `json:"apiKeySecret,omitempty"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	ServiceAccountCredentialsSecret *string `json:"serviceAccountCredentialsSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeGoogleChronicle `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionGoogleChronicle `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorGoogleChronicle `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsGoogleChronicle        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                    `additionalProperties:"true" json:"-"`
}

func (o OutputGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleChronicle) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGoogleChronicle) GetType() TypeGoogleChronicle {
	if o == nil {
		return TypeGoogleChronicle("")
	}
	return o.Type
}

func (o *OutputGoogleChronicle) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleChronicle) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleChronicle) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleChronicle) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleChronicle) GetAPIVersion() *OutputAPIVersion {
	if o == nil {
		return nil
	}
	return o.APIVersion
}

func (o *OutputGoogleChronicle) GetAuthenticationMethod() *AuthenticationMethodGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.AuthenticationMethod
}

func (o *OutputGoogleChronicle) GetResponseRetrySettings() []ResponseRetrySettingGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputGoogleChronicle) GetTimeoutRetrySettings() *TimeoutRetrySettingsGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputGoogleChronicle) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputGoogleChronicle) GetLogFormatType() *SendEventsAs {
	if o == nil {
		return nil
	}
	return o.LogFormatType
}

func (o *OutputGoogleChronicle) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputGoogleChronicle) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGoogleChronicle) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGoogleChronicle) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGoogleChronicle) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGoogleChronicle) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGoogleChronicle) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGoogleChronicle) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGoogleChronicle) GetExtraHTTPHeaders() []ExtraHTTPHeaderGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputGoogleChronicle) GetFailedRequestLoggingMode() *FailedRequestLoggingModeGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputGoogleChronicle) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputGoogleChronicle) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputGoogleChronicle) GetOnBackpressure() *BackpressureBehaviorGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleChronicle) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputGoogleChronicle) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleChronicle) GetExtraLogTypes() []ExtraLogType {
	if o == nil {
		return nil
	}
	return o.ExtraLogTypes
}

func (o *OutputGoogleChronicle) GetLogType() *string {
	if o == nil {
		return nil
	}
	return o.LogType
}

func (o *OutputGoogleChronicle) GetLogTextField() *string {
	if o == nil {
		return nil
	}
	return o.LogTextField
}

func (o *OutputGoogleChronicle) GetCustomerID() *string {
	if o == nil {
		return nil
	}
	return o.CustomerID
}

func (o *OutputGoogleChronicle) GetNamespace() *string {
	if o == nil {
		return nil
	}
	return o.Namespace
}

func (o *OutputGoogleChronicle) GetCustomLabels() []CustomLabelGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.CustomLabels
}

func (o *OutputGoogleChronicle) GetUdmType() *UDMType {
	if o == nil {
		return nil
	}
	return o.UdmType
}

func (o *OutputGoogleChronicle) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputGoogleChronicle) GetAPIKeySecret() *string {
	if o == nil {
		return nil
	}
	return o.APIKeySecret
}

func (o *OutputGoogleChronicle) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGoogleChronicle) GetServiceAccountCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentialsSecret
}

func (o *OutputGoogleChronicle) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputGoogleChronicle) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputGoogleChronicle) GetPqMode() *ModeGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGoogleChronicle) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputGoogleChronicle) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputGoogleChronicle) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGoogleChronicle) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGoogleChronicle) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGoogleChronicle) GetPqCompress() *CompressionGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGoogleChronicle) GetPqOnBackpressure() *QueueFullBehaviorGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGoogleChronicle) GetPqControls() *PqControlsGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGoogleChronicle) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeAzureEventhub string

const (
	TypeAzureEventhubAzureEventhub TypeAzureEventhub = "azure_eventhub"
)

func (e TypeAzureEventhub) ToPointer() *TypeAzureEventhub {
	return &e
}
func (e *TypeAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_eventhub":
		*e = TypeAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureEventhub: %v", v)
	}
}

// AcknowledgmentsAzureEventhub - Control the number of required acknowledgments
type AcknowledgmentsAzureEventhub int64

const (
	// AcknowledgmentsAzureEventhubOne Leader
	AcknowledgmentsAzureEventhubOne AcknowledgmentsAzureEventhub = 1
	// AcknowledgmentsAzureEventhubZero None
	AcknowledgmentsAzureEventhubZero AcknowledgmentsAzureEventhub = 0
	// AcknowledgmentsAzureEventhubMinus1 All
	AcknowledgmentsAzureEventhubMinus1 AcknowledgmentsAzureEventhub = -1
)

func (e AcknowledgmentsAzureEventhub) ToPointer() *AcknowledgmentsAzureEventhub {
	return &e
}

// RecordDataFormatAzureEventhub - Format to use to serialize events before writing to the Event Hubs Kafka brokers
type RecordDataFormatAzureEventhub string

const (
	// RecordDataFormatAzureEventhubJSON JSON
	RecordDataFormatAzureEventhubJSON RecordDataFormatAzureEventhub = "json"
	// RecordDataFormatAzureEventhubRaw Field _raw
	RecordDataFormatAzureEventhubRaw RecordDataFormatAzureEventhub = "raw"
)

func (e RecordDataFormatAzureEventhub) ToPointer() *RecordDataFormatAzureEventhub {
	return &e
}

// AuthTypeAuthenticationMethodAzureEventhub - Enter password directly, or select a stored secret
type AuthTypeAuthenticationMethodAzureEventhub string

const (
	AuthTypeAuthenticationMethodAzureEventhubManual AuthTypeAuthenticationMethodAzureEventhub = "manual"
	AuthTypeAuthenticationMethodAzureEventhubSecret AuthTypeAuthenticationMethodAzureEventhub = "secret"
)

func (e AuthTypeAuthenticationMethodAzureEventhub) ToPointer() *AuthTypeAuthenticationMethodAzureEventhub {
	return &e
}

type SASLMechanismAzureEventhub string

const (
	// SASLMechanismAzureEventhubPlain PLAIN
	SASLMechanismAzureEventhubPlain SASLMechanismAzureEventhub = "plain"
	// SASLMechanismAzureEventhubOauthbearer OAUTHBEARER
	SASLMechanismAzureEventhubOauthbearer SASLMechanismAzureEventhub = "oauthbearer"
)

func (e SASLMechanismAzureEventhub) ToPointer() *SASLMechanismAzureEventhub {
	return &e
}

type ClientSecretAuthTypeAuthenticationMethodAzureEventhub string

const (
	ClientSecretAuthTypeAuthenticationMethodAzureEventhubManual      ClientSecretAuthTypeAuthenticationMethodAzureEventhub = "manual"
	ClientSecretAuthTypeAuthenticationMethodAzureEventhubSecret      ClientSecretAuthTypeAuthenticationMethodAzureEventhub = "secret"
	ClientSecretAuthTypeAuthenticationMethodAzureEventhubCertificate ClientSecretAuthTypeAuthenticationMethodAzureEventhub = "certificate"
)

func (e ClientSecretAuthTypeAuthenticationMethodAzureEventhub) ToPointer() *ClientSecretAuthTypeAuthenticationMethodAzureEventhub {
	return &e
}

// MicrosoftEntraIDAuthenticationEndpointAzureEventhub - Endpoint used to acquire authentication tokens from Azure
type MicrosoftEntraIDAuthenticationEndpointAzureEventhub string

const (
	MicrosoftEntraIDAuthenticationEndpointAzureEventhubHTTPSLoginMicrosoftonlineCom       MicrosoftEntraIDAuthenticationEndpointAzureEventhub = "https://login.microsoftonline.com"
	MicrosoftEntraIDAuthenticationEndpointAzureEventhubHTTPSLoginMicrosoftonlineUs        MicrosoftEntraIDAuthenticationEndpointAzureEventhub = "https://login.microsoftonline.us"
	MicrosoftEntraIDAuthenticationEndpointAzureEventhubHTTPSLoginPartnerMicrosoftonlineCn MicrosoftEntraIDAuthenticationEndpointAzureEventhub = "https://login.partner.microsoftonline.cn"
)

func (e MicrosoftEntraIDAuthenticationEndpointAzureEventhub) ToPointer() *MicrosoftEntraIDAuthenticationEndpointAzureEventhub {
	return &e
}

// AuthenticationAzureEventhub - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type AuthenticationAzureEventhub struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Enter password directly, or select a stored secret
	AuthType *AuthTypeAuthenticationMethodAzureEventhub `default:"manual" json:"authType"`
	// Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace
	Password *string `json:"password,omitempty"`
	// Select or create a stored text secret
	TextSecret *string                     `json:"textSecret,omitempty"`
	Mechanism  *SASLMechanismAzureEventhub `default:"plain" json:"mechanism"`
	// The username for authentication. For Event Hubs, this should always be $ConnectionString.
	Username             *string                                                `default:"$ConnectionString" json:"username"`
	ClientSecretAuthType *ClientSecretAuthTypeAuthenticationMethodAzureEventhub `default:"manual" json:"clientSecretAuthType"`
	// client_secret to pass in the OAuth request parameter
	ClientSecret *string `json:"clientSecret,omitempty"`
	// Select or create a stored text secret
	ClientTextSecret *string `json:"clientTextSecret,omitempty"`
	// Select or create a stored certificate
	CertificateName *string `json:"certificateName,omitempty"`
	CertPath        *string `json:"certPath,omitempty"`
	PrivKeyPath     *string `json:"privKeyPath,omitempty"`
	Passphrase      *string `json:"passphrase,omitempty"`
	// Endpoint used to acquire authentication tokens from Azure
	OauthEndpoint *MicrosoftEntraIDAuthenticationEndpointAzureEventhub `default:"https://login.microsoftonline.com" json:"oauthEndpoint"`
	// client_id to pass in the OAuth request parameter
	ClientID *string `json:"clientId,omitempty"`
	// Directory ID (tenant identifier) in Azure Active Directory
	TenantID *string `json:"tenantId,omitempty"`
	// Scope to pass in the OAuth request parameter
	Scope *string `json:"scope,omitempty"`
}

func (a AuthenticationAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthenticationAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (a *AuthenticationAzureEventhub) GetDisabled() *bool {
	if a == nil {
		return nil
	}
	return a.Disabled
}

func (a *AuthenticationAzureEventhub) GetAuthType() *AuthTypeAuthenticationMethodAzureEventhub {
	if a == nil {
		return nil
	}
	return a.AuthType
}

func (a *AuthenticationAzureEventhub) GetPassword() *string {
	if a == nil {
		return nil
	}
	return a.Password
}

func (a *AuthenticationAzureEventhub) GetTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.TextSecret
}

func (a *AuthenticationAzureEventhub) GetMechanism() *SASLMechanismAzureEventhub {
	if a == nil {
		return nil
	}
	return a.Mechanism
}

func (a *AuthenticationAzureEventhub) GetUsername() *string {
	if a == nil {
		return nil
	}
	return a.Username
}

func (a *AuthenticationAzureEventhub) GetClientSecretAuthType() *ClientSecretAuthTypeAuthenticationMethodAzureEventhub {
	if a == nil {
		return nil
	}
	return a.ClientSecretAuthType
}

func (a *AuthenticationAzureEventhub) GetClientSecret() *string {
	if a == nil {
		return nil
	}
	return a.ClientSecret
}

func (a *AuthenticationAzureEventhub) GetClientTextSecret() *string {
	if a == nil {
		return nil
	}
	return a.ClientTextSecret
}

func (a *AuthenticationAzureEventhub) GetCertificateName() *string {
	if a == nil {
		return nil
	}
	return a.CertificateName
}

func (a *AuthenticationAzureEventhub) GetCertPath() *string {
	if a == nil {
		return nil
	}
	return a.CertPath
}

func (a *AuthenticationAzureEventhub) GetPrivKeyPath() *string {
	if a == nil {
		return nil
	}
	return a.PrivKeyPath
}

func (a *AuthenticationAzureEventhub) GetPassphrase() *string {
	if a == nil {
		return nil
	}
	return a.Passphrase
}

func (a *AuthenticationAzureEventhub) GetOauthEndpoint() *MicrosoftEntraIDAuthenticationEndpointAzureEventhub {
	if a == nil {
		return nil
	}
	return a.OauthEndpoint
}

func (a *AuthenticationAzureEventhub) GetClientID() *string {
	if a == nil {
		return nil
	}
	return a.ClientID
}

func (a *AuthenticationAzureEventhub) GetTenantID() *string {
	if a == nil {
		return nil
	}
	return a.TenantID
}

func (a *AuthenticationAzureEventhub) GetScope() *string {
	if a == nil {
		return nil
	}
	return a.Scope
}

type TLSSettingsClientSideAzureEventhub struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
}

func (t TLSSettingsClientSideAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideAzureEventhub) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideAzureEventhub) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

// BackpressureBehaviorAzureEventhub - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureEventhub string

const (
	// BackpressureBehaviorAzureEventhubBlock Block
	BackpressureBehaviorAzureEventhubBlock BackpressureBehaviorAzureEventhub = "block"
	// BackpressureBehaviorAzureEventhubDrop Drop
	BackpressureBehaviorAzureEventhubDrop BackpressureBehaviorAzureEventhub = "drop"
	// BackpressureBehaviorAzureEventhubQueue Persistent Queue
	BackpressureBehaviorAzureEventhubQueue BackpressureBehaviorAzureEventhub = "queue"
)

func (e BackpressureBehaviorAzureEventhub) ToPointer() *BackpressureBehaviorAzureEventhub {
	return &e
}

// ModeAzureEventhub - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureEventhub string

const (
	// ModeAzureEventhubError Error
	ModeAzureEventhubError ModeAzureEventhub = "error"
	// ModeAzureEventhubAlways Backpressure
	ModeAzureEventhubAlways ModeAzureEventhub = "always"
	// ModeAzureEventhubBackpressure Always On
	ModeAzureEventhubBackpressure ModeAzureEventhub = "backpressure"
)

func (e ModeAzureEventhub) ToPointer() *ModeAzureEventhub {
	return &e
}

// CompressionAzureEventhub - Codec to use to compress the persisted data
type CompressionAzureEventhub string

const (
	// CompressionAzureEventhubNone None
	CompressionAzureEventhubNone CompressionAzureEventhub = "none"
	// CompressionAzureEventhubGzip Gzip
	CompressionAzureEventhubGzip CompressionAzureEventhub = "gzip"
)

func (e CompressionAzureEventhub) ToPointer() *CompressionAzureEventhub {
	return &e
}

// QueueFullBehaviorAzureEventhub - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureEventhub string

const (
	// QueueFullBehaviorAzureEventhubBlock Block
	QueueFullBehaviorAzureEventhubBlock QueueFullBehaviorAzureEventhub = "block"
	// QueueFullBehaviorAzureEventhubDrop Drop new data
	QueueFullBehaviorAzureEventhubDrop QueueFullBehaviorAzureEventhub = "drop"
)

func (e QueueFullBehaviorAzureEventhub) ToPointer() *QueueFullBehaviorAzureEventhub {
	return &e
}

type PqControlsAzureEventhub struct {
}

func (p PqControlsAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputAzureEventhub struct {
	// Unique ID for this output
	ID   *string           `json:"id,omitempty"`
	Type TypeAzureEventhub `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// List of Event Hubs Kafka brokers to connect to, eg. yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.
	Brokers []string `json:"brokers"`
	// The name of the Event Hub (Kafka Topic) to publish events. Can be overwritten using field __topicOut.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments
	Ack *AcknowledgmentsAzureEventhub `default:"1" json:"ack"`
	// Format to use to serialize events before writing to the Event Hubs Kafka brokers
	Format *RecordDataFormatAzureEventhub `default:"json" json:"format"`
	// Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// Maximum number of events in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *AuthenticationAzureEventhub        `json:"sasl,omitempty"`
	TLS  *TLSSettingsClientSideAzureEventhub `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureEventhub `default:"block" json:"onBackpressure"`
	Description    *string                            `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeAzureEventhub `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionAzureEventhub `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorAzureEventhub `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsAzureEventhub        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any                  `additionalProperties:"true" json:"-"`
}

func (o OutputAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "brokers", "topic"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureEventhub) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputAzureEventhub) GetType() TypeAzureEventhub {
	if o == nil {
		return TypeAzureEventhub("")
	}
	return o.Type
}

func (o *OutputAzureEventhub) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureEventhub) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureEventhub) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureEventhub) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureEventhub) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputAzureEventhub) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputAzureEventhub) GetAck() *AcknowledgmentsAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputAzureEventhub) GetFormat() *RecordDataFormatAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureEventhub) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputAzureEventhub) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputAzureEventhub) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureEventhub) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputAzureEventhub) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputAzureEventhub) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputAzureEventhub) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputAzureEventhub) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputAzureEventhub) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputAzureEventhub) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputAzureEventhub) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputAzureEventhub) GetSasl() *AuthenticationAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputAzureEventhub) GetTLS() *TLSSettingsClientSideAzureEventhub {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputAzureEventhub) GetOnBackpressure() *BackpressureBehaviorAzureEventhub {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureEventhub) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureEventhub) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputAzureEventhub) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputAzureEventhub) GetPqMode() *ModeAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureEventhub) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputAzureEventhub) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputAzureEventhub) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureEventhub) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureEventhub) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureEventhub) GetPqCompress() *CompressionAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureEventhub) GetPqOnBackpressure() *QueueFullBehaviorAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureEventhub) GetPqControls() *PqControlsAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputAzureEventhub) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeHoneycomb string

const (
	TypeHoneycombHoneycomb TypeHoneycomb = "honeycomb"
)

func (e TypeHoneycomb) ToPointer() *TypeHoneycomb {
	return &e
}
func (e *TypeHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "honeycomb":
		*e = TypeHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeHoneycomb: %v", v)
	}
}

type ExtraHTTPHeaderHoneycomb struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderHoneycomb) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderHoneycomb) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeHoneycomb - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeHoneycomb string

const (
	// FailedRequestLoggingModeHoneycombPayload Payload
	FailedRequestLoggingModeHoneycombPayload FailedRequestLoggingModeHoneycomb = "payload"
	// FailedRequestLoggingModeHoneycombPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeHoneycombPayloadAndHeaders FailedRequestLoggingModeHoneycomb = "payloadAndHeaders"
	// FailedRequestLoggingModeHoneycombNone None
	FailedRequestLoggingModeHoneycombNone FailedRequestLoggingModeHoneycomb = "none"
)

func (e FailedRequestLoggingModeHoneycomb) ToPointer() *FailedRequestLoggingModeHoneycomb {
	return &e
}

type ResponseRetrySettingHoneycomb struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingHoneycomb) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingHoneycomb) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingHoneycomb) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingHoneycomb) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsHoneycomb struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsHoneycomb) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsHoneycomb) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsHoneycomb) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsHoneycomb) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorHoneycomb - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorHoneycomb string

const (
	// BackpressureBehaviorHoneycombBlock Block
	BackpressureBehaviorHoneycombBlock BackpressureBehaviorHoneycomb = "block"
	// BackpressureBehaviorHoneycombDrop Drop
	BackpressureBehaviorHoneycombDrop BackpressureBehaviorHoneycomb = "drop"
	// BackpressureBehaviorHoneycombQueue Persistent Queue
	BackpressureBehaviorHoneycombQueue BackpressureBehaviorHoneycomb = "queue"
)

func (e BackpressureBehaviorHoneycomb) ToPointer() *BackpressureBehaviorHoneycomb {
	return &e
}

// AuthenticationMethodHoneycomb - Enter API key directly, or select a stored secret
type AuthenticationMethodHoneycomb string

const (
	AuthenticationMethodHoneycombManual AuthenticationMethodHoneycomb = "manual"
	AuthenticationMethodHoneycombSecret AuthenticationMethodHoneycomb = "secret"
)

func (e AuthenticationMethodHoneycomb) ToPointer() *AuthenticationMethodHoneycomb {
	return &e
}

// ModeHoneycomb - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeHoneycomb string

const (
	// ModeHoneycombError Error
	ModeHoneycombError ModeHoneycomb = "error"
	// ModeHoneycombAlways Backpressure
	ModeHoneycombAlways ModeHoneycomb = "always"
	// ModeHoneycombBackpressure Always On
	ModeHoneycombBackpressure ModeHoneycomb = "backpressure"
)

func (e ModeHoneycomb) ToPointer() *ModeHoneycomb {
	return &e
}

// CompressionHoneycomb - Codec to use to compress the persisted data
type CompressionHoneycomb string

const (
	// CompressionHoneycombNone None
	CompressionHoneycombNone CompressionHoneycomb = "none"
	// CompressionHoneycombGzip Gzip
	CompressionHoneycombGzip CompressionHoneycomb = "gzip"
)

func (e CompressionHoneycomb) ToPointer() *CompressionHoneycomb {
	return &e
}

// QueueFullBehaviorHoneycomb - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorHoneycomb string

const (
	// QueueFullBehaviorHoneycombBlock Block
	QueueFullBehaviorHoneycombBlock QueueFullBehaviorHoneycomb = "block"
	// QueueFullBehaviorHoneycombDrop Drop new data
	QueueFullBehaviorHoneycombDrop QueueFullBehaviorHoneycomb = "drop"
)

func (e QueueFullBehaviorHoneycomb) ToPointer() *QueueFullBehaviorHoneycomb {
	return &e
}

type PqControlsHoneycomb struct {
}

func (p PqControlsHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputHoneycomb struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeHoneycomb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the dataset to send events to – e.g., observability
	Dataset string `json:"dataset"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderHoneycomb `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeHoneycomb `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingHoneycomb `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsHoneycomb  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorHoneycomb `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType    *AuthenticationMethodHoneycomb `default:"manual" json:"authType"`
	Description *string                        `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeHoneycomb `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionHoneycomb `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorHoneycomb `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsHoneycomb        `json:"pqControls,omitempty"`
	// Team API key where the dataset belongs
	Team *string `json:"team,omitempty"`
	// Select or create a stored text secret
	TextSecret           *string        `json:"textSecret,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "dataset"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputHoneycomb) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputHoneycomb) GetType() TypeHoneycomb {
	if o == nil {
		return TypeHoneycomb("")
	}
	return o.Type
}

func (o *OutputHoneycomb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputHoneycomb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputHoneycomb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputHoneycomb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputHoneycomb) GetDataset() string {
	if o == nil {
		return ""
	}
	return o.Dataset
}

func (o *OutputHoneycomb) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputHoneycomb) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputHoneycomb) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputHoneycomb) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputHoneycomb) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputHoneycomb) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputHoneycomb) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputHoneycomb) GetExtraHTTPHeaders() []ExtraHTTPHeaderHoneycomb {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputHoneycomb) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputHoneycomb) GetFailedRequestLoggingMode() *FailedRequestLoggingModeHoneycomb {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputHoneycomb) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputHoneycomb) GetResponseRetrySettings() []ResponseRetrySettingHoneycomb {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputHoneycomb) GetTimeoutRetrySettings() *TimeoutRetrySettingsHoneycomb {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputHoneycomb) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputHoneycomb) GetOnBackpressure() *BackpressureBehaviorHoneycomb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputHoneycomb) GetAuthType() *AuthenticationMethodHoneycomb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputHoneycomb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputHoneycomb) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputHoneycomb) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputHoneycomb) GetPqMode() *ModeHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputHoneycomb) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputHoneycomb) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputHoneycomb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputHoneycomb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputHoneycomb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputHoneycomb) GetPqCompress() *CompressionHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputHoneycomb) GetPqOnBackpressure() *QueueFullBehaviorHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputHoneycomb) GetPqControls() *PqControlsHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputHoneycomb) GetTeam() *string {
	if o == nil {
		return nil
	}
	return o.Team
}

func (o *OutputHoneycomb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputHoneycomb) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeKinesis string

const (
	OutputTypeKinesisKinesis OutputTypeKinesis = "kinesis"
)

func (e OutputTypeKinesis) ToPointer() *OutputTypeKinesis {
	return &e
}
func (e *OutputTypeKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "kinesis":
		*e = OutputTypeKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeKinesis: %v", v)
	}
}

// OutputAuthenticationMethodKinesis - AWS authentication method. Choose Auto to use IAM roles.
type OutputAuthenticationMethodKinesis string

const (
	// OutputAuthenticationMethodKinesisAuto Auto
	OutputAuthenticationMethodKinesisAuto OutputAuthenticationMethodKinesis = "auto"
	// OutputAuthenticationMethodKinesisManual Manual
	OutputAuthenticationMethodKinesisManual OutputAuthenticationMethodKinesis = "manual"
	// OutputAuthenticationMethodKinesisSecret Secret Key pair
	OutputAuthenticationMethodKinesisSecret OutputAuthenticationMethodKinesis = "secret"
)

func (e OutputAuthenticationMethodKinesis) ToPointer() *OutputAuthenticationMethodKinesis {
	return &e
}

// OutputSignatureVersionKinesis - Signature version to use for signing Kinesis stream requests
type OutputSignatureVersionKinesis string

const (
	OutputSignatureVersionKinesisV2 OutputSignatureVersionKinesis = "v2"
	OutputSignatureVersionKinesisV4 OutputSignatureVersionKinesis = "v4"
)

func (e OutputSignatureVersionKinesis) ToPointer() *OutputSignatureVersionKinesis {
	return &e
}

// OutputCompressionKinesis - Compression type to use for records
type OutputCompressionKinesis string

const (
	// OutputCompressionKinesisNone None
	OutputCompressionKinesisNone OutputCompressionKinesis = "none"
	// OutputCompressionKinesisGzip Gzip
	OutputCompressionKinesisGzip OutputCompressionKinesis = "gzip"
)

func (e OutputCompressionKinesis) ToPointer() *OutputCompressionKinesis {
	return &e
}

// BackpressureBehaviorKinesis - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorKinesis string

const (
	// BackpressureBehaviorKinesisBlock Block
	BackpressureBehaviorKinesisBlock BackpressureBehaviorKinesis = "block"
	// BackpressureBehaviorKinesisDrop Drop
	BackpressureBehaviorKinesisDrop BackpressureBehaviorKinesis = "drop"
	// BackpressureBehaviorKinesisQueue Persistent Queue
	BackpressureBehaviorKinesisQueue BackpressureBehaviorKinesis = "queue"
)

func (e BackpressureBehaviorKinesis) ToPointer() *BackpressureBehaviorKinesis {
	return &e
}

// OutputModeKinesis - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeKinesis string

const (
	// OutputModeKinesisError Error
	OutputModeKinesisError OutputModeKinesis = "error"
	// OutputModeKinesisAlways Backpressure
	OutputModeKinesisAlways OutputModeKinesis = "always"
	// OutputModeKinesisBackpressure Always On
	OutputModeKinesisBackpressure OutputModeKinesis = "backpressure"
)

func (e OutputModeKinesis) ToPointer() *OutputModeKinesis {
	return &e
}

// PqCompressCompressionKinesis - Codec to use to compress the persisted data
type PqCompressCompressionKinesis string

const (
	// PqCompressCompressionKinesisNone None
	PqCompressCompressionKinesisNone PqCompressCompressionKinesis = "none"
	// PqCompressCompressionKinesisGzip Gzip
	PqCompressCompressionKinesisGzip PqCompressCompressionKinesis = "gzip"
)

func (e PqCompressCompressionKinesis) ToPointer() *PqCompressCompressionKinesis {
	return &e
}

// QueueFullBehaviorKinesis - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorKinesis string

const (
	// QueueFullBehaviorKinesisBlock Block
	QueueFullBehaviorKinesisBlock QueueFullBehaviorKinesis = "block"
	// QueueFullBehaviorKinesisDrop Drop new data
	QueueFullBehaviorKinesisDrop QueueFullBehaviorKinesis = "drop"
)

func (e QueueFullBehaviorKinesis) ToPointer() *QueueFullBehaviorKinesis {
	return &e
}

type OutputPqControlsKinesis struct {
}

func (o OutputPqControlsKinesis) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsKinesis) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputKinesis struct {
	// Unique ID for this output
	ID   *string           `json:"id,omitempty"`
	Type OutputTypeKinesis `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Kinesis stream name to send events to.
	StreamName string `json:"streamName"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputAuthenticationMethodKinesis `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                            `json:"awsSecretKey,omitempty"`
	// Region where the Kinesis stream is located
	Region string `json:"region"`
	// Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing Kinesis stream requests
	SignatureVersion *OutputSignatureVersionKinesis `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access Kinesis stream
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of ongoing put requests before blocking.
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size
	MaxRecordSizeKB *float64 `default:"1024" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Compression type to use for records
	Compression *OutputCompressionKinesis `default:"gzip" json:"compression"`
	// Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details.
	UseListShards *bool `default:"false" json:"useListShards"`
	// Batch events into a single record as NDJSON
	AsNdjson *bool `default:"true" json:"asNdjson"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorKinesis `default:"block" json:"onBackpressure"`
	Description    *string                      `json:"description,omitempty"`
	AwsAPIKey      *string                      `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Maximum number of records to send in a single request
	MaxEventsPerFlush *float64 `default:"500" json:"maxEventsPerFlush"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeKinesis `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionKinesis `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorKinesis `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsKinesis  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any            `additionalProperties:"true" json:"-"`
}

func (o OutputKinesis) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKinesis) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "streamName", "region"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputKinesis) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputKinesis) GetType() OutputTypeKinesis {
	if o == nil {
		return OutputTypeKinesis("")
	}
	return o.Type
}

func (o *OutputKinesis) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputKinesis) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputKinesis) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputKinesis) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputKinesis) GetStreamName() string {
	if o == nil {
		return ""
	}
	return o.StreamName
}

func (o *OutputKinesis) GetAwsAuthenticationMethod() *OutputAuthenticationMethodKinesis {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputKinesis) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputKinesis) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputKinesis) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputKinesis) GetSignatureVersion() *OutputSignatureVersionKinesis {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputKinesis) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputKinesis) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputKinesis) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputKinesis) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputKinesis) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputKinesis) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputKinesis) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputKinesis) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputKinesis) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputKinesis) GetCompression() *OutputCompressionKinesis {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputKinesis) GetUseListShards() *bool {
	if o == nil {
		return nil
	}
	return o.UseListShards
}

func (o *OutputKinesis) GetAsNdjson() *bool {
	if o == nil {
		return nil
	}
	return o.AsNdjson
}

func (o *OutputKinesis) GetOnBackpressure() *BackpressureBehaviorKinesis {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputKinesis) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputKinesis) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputKinesis) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputKinesis) GetMaxEventsPerFlush() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxEventsPerFlush
}

func (o *OutputKinesis) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputKinesis) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputKinesis) GetPqMode() *OutputModeKinesis {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputKinesis) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputKinesis) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputKinesis) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputKinesis) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputKinesis) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputKinesis) GetPqCompress() *PqCompressCompressionKinesis {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputKinesis) GetPqOnBackpressure() *QueueFullBehaviorKinesis {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputKinesis) GetPqControls() *OutputPqControlsKinesis {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputKinesis) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeAzureLogs string

const (
	TypeAzureLogsAzureLogs TypeAzureLogs = "azure_logs"
)

func (e TypeAzureLogs) ToPointer() *TypeAzureLogs {
	return &e
}
func (e *TypeAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_logs":
		*e = TypeAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureLogs: %v", v)
	}
}

type ExtraHTTPHeaderAzureLogs struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderAzureLogs) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderAzureLogs) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeAzureLogs - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeAzureLogs string

const (
	// FailedRequestLoggingModeAzureLogsPayload Payload
	FailedRequestLoggingModeAzureLogsPayload FailedRequestLoggingModeAzureLogs = "payload"
	// FailedRequestLoggingModeAzureLogsPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeAzureLogsPayloadAndHeaders FailedRequestLoggingModeAzureLogs = "payloadAndHeaders"
	// FailedRequestLoggingModeAzureLogsNone None
	FailedRequestLoggingModeAzureLogsNone FailedRequestLoggingModeAzureLogs = "none"
)

func (e FailedRequestLoggingModeAzureLogs) ToPointer() *FailedRequestLoggingModeAzureLogs {
	return &e
}

type ResponseRetrySettingAzureLogs struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingAzureLogs) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingAzureLogs) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingAzureLogs) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingAzureLogs) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsAzureLogs struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsAzureLogs) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsAzureLogs) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsAzureLogs) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsAzureLogs) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorAzureLogs - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureLogs string

const (
	// BackpressureBehaviorAzureLogsBlock Block
	BackpressureBehaviorAzureLogsBlock BackpressureBehaviorAzureLogs = "block"
	// BackpressureBehaviorAzureLogsDrop Drop
	BackpressureBehaviorAzureLogsDrop BackpressureBehaviorAzureLogs = "drop"
	// BackpressureBehaviorAzureLogsQueue Persistent Queue
	BackpressureBehaviorAzureLogsQueue BackpressureBehaviorAzureLogs = "queue"
)

func (e BackpressureBehaviorAzureLogs) ToPointer() *BackpressureBehaviorAzureLogs {
	return &e
}

// AuthenticationMethodAzureLogs - Enter workspace ID and workspace key directly, or select a stored secret
type AuthenticationMethodAzureLogs string

const (
	AuthenticationMethodAzureLogsManual AuthenticationMethodAzureLogs = "manual"
	AuthenticationMethodAzureLogsSecret AuthenticationMethodAzureLogs = "secret"
)

func (e AuthenticationMethodAzureLogs) ToPointer() *AuthenticationMethodAzureLogs {
	return &e
}

// ModeAzureLogs - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureLogs string

const (
	// ModeAzureLogsError Error
	ModeAzureLogsError ModeAzureLogs = "error"
	// ModeAzureLogsAlways Backpressure
	ModeAzureLogsAlways ModeAzureLogs = "always"
	// ModeAzureLogsBackpressure Always On
	ModeAzureLogsBackpressure ModeAzureLogs = "backpressure"
)

func (e ModeAzureLogs) ToPointer() *ModeAzureLogs {
	return &e
}

// CompressionAzureLogs - Codec to use to compress the persisted data
type CompressionAzureLogs string

const (
	// CompressionAzureLogsNone None
	CompressionAzureLogsNone CompressionAzureLogs = "none"
	// CompressionAzureLogsGzip Gzip
	CompressionAzureLogsGzip CompressionAzureLogs = "gzip"
)

func (e CompressionAzureLogs) ToPointer() *CompressionAzureLogs {
	return &e
}

// QueueFullBehaviorAzureLogs - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureLogs string

const (
	// QueueFullBehaviorAzureLogsBlock Block
	QueueFullBehaviorAzureLogsBlock QueueFullBehaviorAzureLogs = "block"
	// QueueFullBehaviorAzureLogsDrop Drop new data
	QueueFullBehaviorAzureLogsDrop QueueFullBehaviorAzureLogs = "drop"
)

func (e QueueFullBehaviorAzureLogs) ToPointer() *QueueFullBehaviorAzureLogs {
	return &e
}

type PqControlsAzureLogs struct {
}

func (p PqControlsAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputAzureLogs struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeAzureLogs `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType.
	LogType *string `default:"Cribl" json:"logType"`
	// Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.
	ResourceID *string `json:"resourceId,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	Compress         *bool    `json:"compress,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderAzureLogs `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeAzureLogs `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// The DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>.
	APIURL *string `default:".ods.opinsights.azure.com" json:"apiUrl"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingAzureLogs `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsAzureLogs  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureLogs `default:"block" json:"onBackpressure"`
	// Enter workspace ID and workspace key directly, or select a stored secret
	AuthType    *AuthenticationMethodAzureLogs `default:"manual" json:"authType"`
	Description *string                        `json:"description,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeAzureLogs `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionAzureLogs `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorAzureLogs `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsAzureLogs        `json:"pqControls,omitempty"`
	// Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.
	WorkspaceID *string `json:"workspaceId,omitempty"`
	// Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.
	WorkspaceKey *string `json:"workspaceKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	KeypairSecret        *string        `json:"keypairSecret,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureLogs) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputAzureLogs) GetType() TypeAzureLogs {
	if o == nil {
		return TypeAzureLogs("")
	}
	return o.Type
}

func (o *OutputAzureLogs) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureLogs) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureLogs) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureLogs) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureLogs) GetLogType() *string {
	if o == nil {
		return nil
	}
	return o.LogType
}

func (o *OutputAzureLogs) GetResourceID() *string {
	if o == nil {
		return nil
	}
	return o.ResourceID
}

func (o *OutputAzureLogs) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputAzureLogs) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputAzureLogs) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputAzureLogs) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureLogs) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputAzureLogs) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputAzureLogs) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureLogs) GetExtraHTTPHeaders() []ExtraHTTPHeaderAzureLogs {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputAzureLogs) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputAzureLogs) GetFailedRequestLoggingMode() *FailedRequestLoggingModeAzureLogs {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputAzureLogs) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputAzureLogs) GetAPIURL() *string {
	if o == nil {
		return nil
	}
	return o.APIURL
}

func (o *OutputAzureLogs) GetResponseRetrySettings() []ResponseRetrySettingAzureLogs {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputAzureLogs) GetTimeoutRetrySettings() *TimeoutRetrySettingsAzureLogs {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputAzureLogs) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputAzureLogs) GetOnBackpressure() *BackpressureBehaviorAzureLogs {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureLogs) GetAuthType() *AuthenticationMethodAzureLogs {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputAzureLogs) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureLogs) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputAzureLogs) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputAzureLogs) GetPqMode() *ModeAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureLogs) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputAzureLogs) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputAzureLogs) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureLogs) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureLogs) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureLogs) GetPqCompress() *CompressionAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureLogs) GetPqOnBackpressure() *QueueFullBehaviorAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureLogs) GetPqControls() *PqControlsAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputAzureLogs) GetWorkspaceID() *string {
	if o == nil {
		return nil
	}
	return o.WorkspaceID
}

func (o *OutputAzureLogs) GetWorkspaceKey() *string {
	if o == nil {
		return nil
	}
	return o.WorkspaceKey
}

func (o *OutputAzureLogs) GetKeypairSecret() *string {
	if o == nil {
		return nil
	}
	return o.KeypairSecret
}

func (o *OutputAzureLogs) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeAzureDataExplorer string

const (
	TypeAzureDataExplorerAzureDataExplorer TypeAzureDataExplorer = "azure_data_explorer"
)

func (e TypeAzureDataExplorer) ToPointer() *TypeAzureDataExplorer {
	return &e
}
func (e *TypeAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_data_explorer":
		*e = TypeAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureDataExplorer: %v", v)
	}
}

type IngestionMode string

const (
	// IngestionModeBatching Batching
	IngestionModeBatching IngestionMode = "batching"
	// IngestionModeStreaming Streaming
	IngestionModeStreaming IngestionMode = "streaming"
)

func (e IngestionMode) ToPointer() *IngestionMode {
	return &e
}

// MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer - Endpoint used to acquire authentication tokens from Azure
type MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer string

const (
	MicrosoftEntraIDAuthenticationEndpointAzureDataExplorerHTTPSLoginMicrosoftonlineCom       MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer = "https://login.microsoftonline.com"
	MicrosoftEntraIDAuthenticationEndpointAzureDataExplorerHTTPSLoginMicrosoftonlineUs        MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer = "https://login.microsoftonline.us"
	MicrosoftEntraIDAuthenticationEndpointAzureDataExplorerHTTPSLoginPartnerMicrosoftonlineCn MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer = "https://login.partner.microsoftonline.cn"
)

func (e MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer) ToPointer() *MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer {
	return &e
}

// OauthTypeAuthenticationMethod - The type of OAuth 2.0 client credentials grant flow to use
type OauthTypeAuthenticationMethod string

const (
	// OauthTypeAuthenticationMethodClientSecret Client secret
	OauthTypeAuthenticationMethodClientSecret OauthTypeAuthenticationMethod = "clientSecret"
	// OauthTypeAuthenticationMethodClientTextSecret Client secret (text secret)
	OauthTypeAuthenticationMethodClientTextSecret OauthTypeAuthenticationMethod = "clientTextSecret"
	// OauthTypeAuthenticationMethodCertificate Certificate
	OauthTypeAuthenticationMethodCertificate OauthTypeAuthenticationMethod = "certificate"
)

func (e OauthTypeAuthenticationMethod) ToPointer() *OauthTypeAuthenticationMethod {
	return &e
}

type CertificateAzureDataExplorer struct {
	// The certificate you registered as credentials for your app in the Azure portal
	CertificateName *string `json:"certificateName,omitempty"`
}

func (c CertificateAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(c, "", false)
}

func (c *CertificateAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &c, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (c *CertificateAzureDataExplorer) GetCertificateName() *string {
	if c == nil {
		return nil
	}
	return c.CertificateName
}

// DataFormatAzureDataExplorer - Format of the output data
type DataFormatAzureDataExplorer string

const (
	// DataFormatAzureDataExplorerJSON JSON
	DataFormatAzureDataExplorerJSON DataFormatAzureDataExplorer = "json"
	// DataFormatAzureDataExplorerRaw Raw
	DataFormatAzureDataExplorerRaw DataFormatAzureDataExplorer = "raw"
	// DataFormatAzureDataExplorerParquet Parquet
	DataFormatAzureDataExplorerParquet DataFormatAzureDataExplorer = "parquet"
)

func (e DataFormatAzureDataExplorer) ToPointer() *DataFormatAzureDataExplorer {
	return &e
}

// CompressCompressionAzureDataExplorer - Data compression format to apply to HTTP content before it is delivered
type CompressCompressionAzureDataExplorer string

const (
	CompressCompressionAzureDataExplorerNone CompressCompressionAzureDataExplorer = "none"
	CompressCompressionAzureDataExplorerGzip CompressCompressionAzureDataExplorer = "gzip"
)

func (e CompressCompressionAzureDataExplorer) ToPointer() *CompressCompressionAzureDataExplorer {
	return &e
}

// CompressionLevelAzureDataExplorer - Compression level to apply before moving files to final destination
type CompressionLevelAzureDataExplorer string

const (
	// CompressionLevelAzureDataExplorerBestSpeed Best Speed
	CompressionLevelAzureDataExplorerBestSpeed CompressionLevelAzureDataExplorer = "best_speed"
	// CompressionLevelAzureDataExplorerNormal Normal
	CompressionLevelAzureDataExplorerNormal CompressionLevelAzureDataExplorer = "normal"
	// CompressionLevelAzureDataExplorerBestCompression Best Compression
	CompressionLevelAzureDataExplorerBestCompression CompressionLevelAzureDataExplorer = "best_compression"
)

func (e CompressionLevelAzureDataExplorer) ToPointer() *CompressionLevelAzureDataExplorer {
	return &e
}

// ParquetVersionAzureDataExplorer - Determines which data types are supported and how they are represented
type ParquetVersionAzureDataExplorer string

const (
	// ParquetVersionAzureDataExplorerParquet10 1.0
	ParquetVersionAzureDataExplorerParquet10 ParquetVersionAzureDataExplorer = "PARQUET_1_0"
	// ParquetVersionAzureDataExplorerParquet24 2.4
	ParquetVersionAzureDataExplorerParquet24 ParquetVersionAzureDataExplorer = "PARQUET_2_4"
	// ParquetVersionAzureDataExplorerParquet26 2.6
	ParquetVersionAzureDataExplorerParquet26 ParquetVersionAzureDataExplorer = "PARQUET_2_6"
)

func (e ParquetVersionAzureDataExplorer) ToPointer() *ParquetVersionAzureDataExplorer {
	return &e
}

// DataPageVersionAzureDataExplorer - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionAzureDataExplorer string

const (
	// DataPageVersionAzureDataExplorerDataPageV1 V1
	DataPageVersionAzureDataExplorerDataPageV1 DataPageVersionAzureDataExplorer = "DATA_PAGE_V1"
	// DataPageVersionAzureDataExplorerDataPageV2 V2
	DataPageVersionAzureDataExplorerDataPageV2 DataPageVersionAzureDataExplorer = "DATA_PAGE_V2"
)

func (e DataPageVersionAzureDataExplorer) ToPointer() *DataPageVersionAzureDataExplorer {
	return &e
}

type KeyValueMetadatumAzureDataExplorer struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumAzureDataExplorer) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumAzureDataExplorer) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

// BackpressureBehaviorAzureDataExplorer - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureDataExplorer string

const (
	// BackpressureBehaviorAzureDataExplorerBlock Block
	BackpressureBehaviorAzureDataExplorerBlock BackpressureBehaviorAzureDataExplorer = "block"
	// BackpressureBehaviorAzureDataExplorerDrop Drop
	BackpressureBehaviorAzureDataExplorerDrop BackpressureBehaviorAzureDataExplorer = "drop"
	// BackpressureBehaviorAzureDataExplorerQueue Persistent Queue
	BackpressureBehaviorAzureDataExplorerQueue BackpressureBehaviorAzureDataExplorer = "queue"
)

func (e BackpressureBehaviorAzureDataExplorer) ToPointer() *BackpressureBehaviorAzureDataExplorer {
	return &e
}

// DiskSpaceProtectionAzureDataExplorer - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionAzureDataExplorer string

const (
	// DiskSpaceProtectionAzureDataExplorerBlock Block
	DiskSpaceProtectionAzureDataExplorerBlock DiskSpaceProtectionAzureDataExplorer = "block"
	// DiskSpaceProtectionAzureDataExplorerDrop Drop
	DiskSpaceProtectionAzureDataExplorerDrop DiskSpaceProtectionAzureDataExplorer = "drop"
)

func (e DiskSpaceProtectionAzureDataExplorer) ToPointer() *DiskSpaceProtectionAzureDataExplorer {
	return &e
}

type PrefixOptional string

const (
	// PrefixOptionalDropBy drop-by
	PrefixOptionalDropBy PrefixOptional = "dropBy"
	// PrefixOptionalIngestBy ingest-by
	PrefixOptionalIngestBy PrefixOptional = "ingestBy"
)

func (e PrefixOptional) ToPointer() *PrefixOptional {
	return &e
}

type ExtentTag struct {
	Prefix *PrefixOptional `json:"prefix,omitempty"`
	Value  string          `json:"value"`
}

func (e ExtentTag) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtentTag) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtentTag) GetPrefix() *PrefixOptional {
	if e == nil {
		return nil
	}
	return e.Prefix
}

func (e *ExtentTag) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

type IngestIfNotExist struct {
	Value string `json:"value"`
}

func (i IngestIfNotExist) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(i, "", false)
}

func (i *IngestIfNotExist) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &i, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (i *IngestIfNotExist) GetValue() string {
	if i == nil {
		return ""
	}
	return i.Value
}

// ReportLevel - Level of ingestion status reporting. Defaults to FailuresOnly.
type ReportLevel string

const (
	// ReportLevelFailuresOnly FailuresOnly
	ReportLevelFailuresOnly ReportLevel = "failuresOnly"
	// ReportLevelDoNotReport DoNotReport
	ReportLevelDoNotReport ReportLevel = "doNotReport"
	// ReportLevelFailuresAndSuccesses FailuresAndSuccesses
	ReportLevelFailuresAndSuccesses ReportLevel = "failuresAndSuccesses"
)

func (e ReportLevel) ToPointer() *ReportLevel {
	return &e
}

// ReportMethod - Target of the ingestion status reporting. Defaults to Queue.
type ReportMethod string

const (
	// ReportMethodQueue Queue
	ReportMethodQueue ReportMethod = "queue"
	// ReportMethodTable Table
	ReportMethodTable ReportMethod = "table"
	// ReportMethodQueueAndTable QueueAndTable
	ReportMethodQueueAndTable ReportMethod = "queueAndTable"
)

func (e ReportMethod) ToPointer() *ReportMethod {
	return &e
}

type AdditionalProperty struct {
	Key   string `json:"key"`
	Value string `json:"value"`
}

func (a AdditionalProperty) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AdditionalProperty) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, []string{"key", "value"}); err != nil {
		return err
	}
	return nil
}

func (a *AdditionalProperty) GetKey() string {
	if a == nil {
		return ""
	}
	return a.Key
}

func (a *AdditionalProperty) GetValue() string {
	if a == nil {
		return ""
	}
	return a.Value
}

type ResponseRetrySettingAzureDataExplorer struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingAzureDataExplorer) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingAzureDataExplorer) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingAzureDataExplorer) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingAzureDataExplorer) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsAzureDataExplorer struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsAzureDataExplorer) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsAzureDataExplorer) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsAzureDataExplorer) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsAzureDataExplorer) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// ModeAzureDataExplorer - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureDataExplorer string

const (
	// ModeAzureDataExplorerError Error
	ModeAzureDataExplorerError ModeAzureDataExplorer = "error"
	// ModeAzureDataExplorerAlways Backpressure
	ModeAzureDataExplorerAlways ModeAzureDataExplorer = "always"
	// ModeAzureDataExplorerBackpressure Always On
	ModeAzureDataExplorerBackpressure ModeAzureDataExplorer = "backpressure"
)

func (e ModeAzureDataExplorer) ToPointer() *ModeAzureDataExplorer {
	return &e
}

// PqCompressCompressionAzureDataExplorer - Codec to use to compress the persisted data
type PqCompressCompressionAzureDataExplorer string

const (
	// PqCompressCompressionAzureDataExplorerNone None
	PqCompressCompressionAzureDataExplorerNone PqCompressCompressionAzureDataExplorer = "none"
	// PqCompressCompressionAzureDataExplorerGzip Gzip
	PqCompressCompressionAzureDataExplorerGzip PqCompressCompressionAzureDataExplorer = "gzip"
)

func (e PqCompressCompressionAzureDataExplorer) ToPointer() *PqCompressCompressionAzureDataExplorer {
	return &e
}

// QueueFullBehaviorAzureDataExplorer - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureDataExplorer string

const (
	// QueueFullBehaviorAzureDataExplorerBlock Block
	QueueFullBehaviorAzureDataExplorerBlock QueueFullBehaviorAzureDataExplorer = "block"
	// QueueFullBehaviorAzureDataExplorerDrop Drop new data
	QueueFullBehaviorAzureDataExplorerDrop QueueFullBehaviorAzureDataExplorer = "drop"
)

func (e QueueFullBehaviorAzureDataExplorer) ToPointer() *QueueFullBehaviorAzureDataExplorer {
	return &e
}

type PqControlsAzureDataExplorer struct {
}

func (p PqControlsAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputAzureDataExplorer struct {
	// Unique ID for this output
	ID   *string               `json:"id,omitempty"`
	Type TypeAzureDataExplorer `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.
	ClusterURL string `json:"clusterUrl"`
	// Name of the database containing the table where data will be ingested
	Database string `json:"database"`
	// Name of the table to ingest data into
	Table string `json:"table"`
	// When saving or starting the Destination, validate the database name and credentials; also validate table name, except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.
	ValidateDatabaseSettings *bool          `default:"true" json:"validateDatabaseSettings"`
	IngestMode               *IngestionMode `default:"batching" json:"ingestMode"`
	// Endpoint used to acquire authentication tokens from Azure
	OauthEndpoint *MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer `default:"https://login.microsoftonline.com" json:"oauthEndpoint"`
	// Directory ID (tenant identifier) in Azure Active Directory
	TenantID string `json:"tenantId"`
	// client_id to pass in the OAuth request parameter
	ClientID string `json:"clientId"`
	// Scope to pass in the OAuth request parameter
	Scope string `json:"scope"`
	// The type of OAuth 2.0 client credentials grant flow to use
	OauthType   *OauthTypeAuthenticationMethod `default:"clientSecret" json:"oauthType"`
	Description *string                        `json:"description,omitempty"`
	// The client secret that you generated for your app in the Azure portal
	ClientSecret *string `json:"clientSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret  *string                       `json:"textSecret,omitempty"`
	Certificate *CertificateAzureDataExplorer `json:"certificate,omitempty"`
	// Format of the output data
	Format *DataFormatAzureDataExplorer `default:"json" json:"format"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressCompressionAzureDataExplorer `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelAzureDataExplorer `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionAzureDataExplorer `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionAzureDataExplorer `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumAzureDataExplorer `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
	// Send a JSON mapping object instead of specifying an existing named data mapping
	IsMappingObj *bool `default:"false" json:"isMappingObj"`
	// Enter a JSON object that defines your desired data mapping
	MappingObj *string `json:"mappingObj,omitempty"`
	// Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.
	MappingRef *string `json:"mappingRef,omitempty"`
	// The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.
	IngestURL *string `json:"ingestUrl,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureDataExplorer `default:"block" json:"onBackpressure"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// Maximum number of parts to upload in parallel per file
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionAzureDataExplorer `default:"block" json:"onDiskFullBackpressure"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Bypass the data management service's aggregation mechanism
	FlushImmediately *bool `default:"false" json:"flushImmediately"`
	// Prevent blob deletion after ingestion is complete
	RetainBlobOnSuccess *bool `default:"false" json:"retainBlobOnSuccess"`
	// Strings or tags associated with the extent (ingested data shard)
	ExtentTags []ExtentTag `json:"extentTags,omitempty"`
	// Prevents duplicate ingestion by verifying whether an extent with the specified ingest-by tag already exists
	IngestIfNotExists []IngestIfNotExist `json:"ingestIfNotExists,omitempty"`
	// Level of ingestion status reporting. Defaults to FailuresOnly.
	ReportLevel *ReportLevel `default:"failuresOnly" json:"reportLevel"`
	// Target of the ingestion status reporting. Defaults to Queue.
	ReportMethod *ReportMethod `default:"queue" json:"reportMethod"`
	// Optionally, enter additional configuration properties to send to the ingestion service
	AdditionalProperties []AdditionalProperty `json:"additionalProperties,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingAzureDataExplorer `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsAzureDataExplorer  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeAzureDataExplorer `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionAzureDataExplorer `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure      *QueueFullBehaviorAzureDataExplorer `default:"block" json:"pqOnBackpressure"`
	PqControls            *PqControlsAzureDataExplorer        `json:"pqControls,omitempty"`
	AdditionalProperties1 map[string]any                      `additionalProperties:"true" json:"-"`
}

func (o OutputAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "clusterUrl", "database", "table", "tenantId", "clientId", "scope"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureDataExplorer) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputAzureDataExplorer) GetType() TypeAzureDataExplorer {
	if o == nil {
		return TypeAzureDataExplorer("")
	}
	return o.Type
}

func (o *OutputAzureDataExplorer) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureDataExplorer) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureDataExplorer) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureDataExplorer) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureDataExplorer) GetClusterURL() string {
	if o == nil {
		return ""
	}
	return o.ClusterURL
}

func (o *OutputAzureDataExplorer) GetDatabase() string {
	if o == nil {
		return ""
	}
	return o.Database
}

func (o *OutputAzureDataExplorer) GetTable() string {
	if o == nil {
		return ""
	}
	return o.Table
}

func (o *OutputAzureDataExplorer) GetValidateDatabaseSettings() *bool {
	if o == nil {
		return nil
	}
	return o.ValidateDatabaseSettings
}

func (o *OutputAzureDataExplorer) GetIngestMode() *IngestionMode {
	if o == nil {
		return nil
	}
	return o.IngestMode
}

func (o *OutputAzureDataExplorer) GetOauthEndpoint() *MicrosoftEntraIDAuthenticationEndpointAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.OauthEndpoint
}

func (o *OutputAzureDataExplorer) GetTenantID() string {
	if o == nil {
		return ""
	}
	return o.TenantID
}

func (o *OutputAzureDataExplorer) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputAzureDataExplorer) GetScope() string {
	if o == nil {
		return ""
	}
	return o.Scope
}

func (o *OutputAzureDataExplorer) GetOauthType() *OauthTypeAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.OauthType
}

func (o *OutputAzureDataExplorer) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureDataExplorer) GetClientSecret() *string {
	if o == nil {
		return nil
	}
	return o.ClientSecret
}

func (o *OutputAzureDataExplorer) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputAzureDataExplorer) GetCertificate() *CertificateAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Certificate
}

func (o *OutputAzureDataExplorer) GetFormat() *DataFormatAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureDataExplorer) GetCompress() *CompressCompressionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureDataExplorer) GetCompressionLevel() *CompressionLevelAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputAzureDataExplorer) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputAzureDataExplorer) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputAzureDataExplorer) GetParquetVersion() *ParquetVersionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputAzureDataExplorer) GetParquetDataPageVersion() *DataPageVersionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputAzureDataExplorer) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputAzureDataExplorer) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputAzureDataExplorer) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputAzureDataExplorer) GetKeyValueMetadata() []KeyValueMetadatumAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputAzureDataExplorer) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputAzureDataExplorer) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputAzureDataExplorer) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputAzureDataExplorer) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputAzureDataExplorer) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputAzureDataExplorer) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputAzureDataExplorer) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputAzureDataExplorer) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputAzureDataExplorer) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputAzureDataExplorer) GetIsMappingObj() *bool {
	if o == nil {
		return nil
	}
	return o.IsMappingObj
}

func (o *OutputAzureDataExplorer) GetMappingObj() *string {
	if o == nil {
		return nil
	}
	return o.MappingObj
}

func (o *OutputAzureDataExplorer) GetMappingRef() *string {
	if o == nil {
		return nil
	}
	return o.MappingRef
}

func (o *OutputAzureDataExplorer) GetIngestURL() *string {
	if o == nil {
		return nil
	}
	return o.IngestURL
}

func (o *OutputAzureDataExplorer) GetOnBackpressure() *BackpressureBehaviorAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureDataExplorer) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputAzureDataExplorer) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputAzureDataExplorer) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputAzureDataExplorer) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputAzureDataExplorer) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputAzureDataExplorer) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputAzureDataExplorer) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputAzureDataExplorer) GetOnDiskFullBackpressure() *DiskSpaceProtectionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputAzureDataExplorer) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputAzureDataExplorer) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputAzureDataExplorer) GetFlushImmediately() *bool {
	if o == nil {
		return nil
	}
	return o.FlushImmediately
}

func (o *OutputAzureDataExplorer) GetRetainBlobOnSuccess() *bool {
	if o == nil {
		return nil
	}
	return o.RetainBlobOnSuccess
}

func (o *OutputAzureDataExplorer) GetExtentTags() []ExtentTag {
	if o == nil {
		return nil
	}
	return o.ExtentTags
}

func (o *OutputAzureDataExplorer) GetIngestIfNotExists() []IngestIfNotExist {
	if o == nil {
		return nil
	}
	return o.IngestIfNotExists
}

func (o *OutputAzureDataExplorer) GetReportLevel() *ReportLevel {
	if o == nil {
		return nil
	}
	return o.ReportLevel
}

func (o *OutputAzureDataExplorer) GetReportMethod() *ReportMethod {
	if o == nil {
		return nil
	}
	return o.ReportMethod
}

func (o *OutputAzureDataExplorer) GetAdditionalProperties() []AdditionalProperty {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

func (o *OutputAzureDataExplorer) GetResponseRetrySettings() []ResponseRetrySettingAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputAzureDataExplorer) GetTimeoutRetrySettings() *TimeoutRetrySettingsAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputAzureDataExplorer) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputAzureDataExplorer) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputAzureDataExplorer) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputAzureDataExplorer) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputAzureDataExplorer) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureDataExplorer) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputAzureDataExplorer) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputAzureDataExplorer) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputAzureDataExplorer) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputAzureDataExplorer) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputAzureDataExplorer) GetPqMode() *ModeAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureDataExplorer) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputAzureDataExplorer) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputAzureDataExplorer) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureDataExplorer) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureDataExplorer) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureDataExplorer) GetPqCompress() *PqCompressCompressionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureDataExplorer) GetPqOnBackpressure() *QueueFullBehaviorAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureDataExplorer) GetPqControls() *PqControlsAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputAzureDataExplorer) GetAdditionalProperties1() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties1
}

type OutputTypeAzureBlob string

const (
	OutputTypeAzureBlobAzureBlob OutputTypeAzureBlob = "azure_blob"
)

func (e OutputTypeAzureBlob) ToPointer() *OutputTypeAzureBlob {
	return &e
}
func (e *OutputTypeAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_blob":
		*e = OutputTypeAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeAzureBlob: %v", v)
	}
}

// DataFormatAzureBlob - Format of the output data
type DataFormatAzureBlob string

const (
	// DataFormatAzureBlobJSON JSON
	DataFormatAzureBlobJSON DataFormatAzureBlob = "json"
	// DataFormatAzureBlobRaw Raw
	DataFormatAzureBlobRaw DataFormatAzureBlob = "raw"
	// DataFormatAzureBlobParquet Parquet
	DataFormatAzureBlobParquet DataFormatAzureBlob = "parquet"
)

func (e DataFormatAzureBlob) ToPointer() *DataFormatAzureBlob {
	return &e
}

// BackpressureBehaviorAzureBlob - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorAzureBlob string

const (
	// BackpressureBehaviorAzureBlobBlock Block
	BackpressureBehaviorAzureBlobBlock BackpressureBehaviorAzureBlob = "block"
	// BackpressureBehaviorAzureBlobDrop Drop
	BackpressureBehaviorAzureBlobDrop BackpressureBehaviorAzureBlob = "drop"
)

func (e BackpressureBehaviorAzureBlob) ToPointer() *BackpressureBehaviorAzureBlob {
	return &e
}

// DiskSpaceProtectionAzureBlob - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionAzureBlob string

const (
	// DiskSpaceProtectionAzureBlobBlock Block
	DiskSpaceProtectionAzureBlobBlock DiskSpaceProtectionAzureBlob = "block"
	// DiskSpaceProtectionAzureBlobDrop Drop
	DiskSpaceProtectionAzureBlobDrop DiskSpaceProtectionAzureBlob = "drop"
)

func (e DiskSpaceProtectionAzureBlob) ToPointer() *DiskSpaceProtectionAzureBlob {
	return &e
}

type OutputAuthenticationMethodAzureBlob string

const (
	OutputAuthenticationMethodAzureBlobManual       OutputAuthenticationMethodAzureBlob = "manual"
	OutputAuthenticationMethodAzureBlobSecret       OutputAuthenticationMethodAzureBlob = "secret"
	OutputAuthenticationMethodAzureBlobClientSecret OutputAuthenticationMethodAzureBlob = "clientSecret"
	OutputAuthenticationMethodAzureBlobClientCert   OutputAuthenticationMethodAzureBlob = "clientCert"
)

func (e OutputAuthenticationMethodAzureBlob) ToPointer() *OutputAuthenticationMethodAzureBlob {
	return &e
}

type BlobAccessTier string

const (
	// BlobAccessTierInferred Default account access tier
	BlobAccessTierInferred BlobAccessTier = "Inferred"
	// BlobAccessTierHot Hot tier
	BlobAccessTierHot BlobAccessTier = "Hot"
	// BlobAccessTierCool Cool tier
	BlobAccessTierCool BlobAccessTier = "Cool"
	// BlobAccessTierCold Cold tier
	BlobAccessTierCold BlobAccessTier = "Cold"
	// BlobAccessTierArchive Archive tier
	BlobAccessTierArchive BlobAccessTier = "Archive"
)

func (e BlobAccessTier) ToPointer() *BlobAccessTier {
	return &e
}

// OutputCompressionAzureBlob - Data compression format to apply to HTTP content before it is delivered
type OutputCompressionAzureBlob string

const (
	OutputCompressionAzureBlobNone OutputCompressionAzureBlob = "none"
	OutputCompressionAzureBlobGzip OutputCompressionAzureBlob = "gzip"
)

func (e OutputCompressionAzureBlob) ToPointer() *OutputCompressionAzureBlob {
	return &e
}

// CompressionLevelAzureBlob - Compression level to apply before moving files to final destination
type CompressionLevelAzureBlob string

const (
	// CompressionLevelAzureBlobBestSpeed Best Speed
	CompressionLevelAzureBlobBestSpeed CompressionLevelAzureBlob = "best_speed"
	// CompressionLevelAzureBlobNormal Normal
	CompressionLevelAzureBlobNormal CompressionLevelAzureBlob = "normal"
	// CompressionLevelAzureBlobBestCompression Best Compression
	CompressionLevelAzureBlobBestCompression CompressionLevelAzureBlob = "best_compression"
)

func (e CompressionLevelAzureBlob) ToPointer() *CompressionLevelAzureBlob {
	return &e
}

// ParquetVersionAzureBlob - Determines which data types are supported and how they are represented
type ParquetVersionAzureBlob string

const (
	// ParquetVersionAzureBlobParquet10 1.0
	ParquetVersionAzureBlobParquet10 ParquetVersionAzureBlob = "PARQUET_1_0"
	// ParquetVersionAzureBlobParquet24 2.4
	ParquetVersionAzureBlobParquet24 ParquetVersionAzureBlob = "PARQUET_2_4"
	// ParquetVersionAzureBlobParquet26 2.6
	ParquetVersionAzureBlobParquet26 ParquetVersionAzureBlob = "PARQUET_2_6"
)

func (e ParquetVersionAzureBlob) ToPointer() *ParquetVersionAzureBlob {
	return &e
}

// DataPageVersionAzureBlob - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionAzureBlob string

const (
	// DataPageVersionAzureBlobDataPageV1 V1
	DataPageVersionAzureBlobDataPageV1 DataPageVersionAzureBlob = "DATA_PAGE_V1"
	// DataPageVersionAzureBlobDataPageV2 V2
	DataPageVersionAzureBlobDataPageV2 DataPageVersionAzureBlob = "DATA_PAGE_V2"
)

func (e DataPageVersionAzureBlob) ToPointer() *DataPageVersionAzureBlob {
	return &e
}

type KeyValueMetadatumAzureBlob struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumAzureBlob) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumAzureBlob) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumAzureBlob) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumAzureBlob) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputCertificateAzureBlob struct {
	// The certificate you registered as credentials for your app in the Azure portal
	CertificateName string `json:"certificateName"`
}

func (o OutputCertificateAzureBlob) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCertificateAzureBlob) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"certificateName"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputCertificateAzureBlob) GetCertificateName() string {
	if o == nil {
		return ""
	}
	return o.CertificateName
}

type OutputAzureBlob struct {
	// Unique ID for this output
	ID   *string             `json:"id,omitempty"`
	Type OutputTypeAzureBlob `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env["CRIBL_WORKER_ID"]}`.
	ContainerName string `json:"containerName"`
	// Create the configured container in Azure Blob Storage if it does not already exist
	CreateContainer *bool `default:"false" json:"createContainer"`
	// Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`.
	DestPath *string `json:"destPath,omitempty"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Maximum number of parts to upload in parallel per file
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatAzureBlob `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureBlob `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionAzureBlob `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool                                `default:"false" json:"forceCloseOnShutdown"`
	AuthType             *OutputAuthenticationMethodAzureBlob `default:"manual" json:"authType"`
	StorageClass         *BlobAccessTier                      `default:"Inferred" json:"storageClass"`
	Description          *string                              `json:"description,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *OutputCompressionAzureBlob `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelAzureBlob `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionAzureBlob `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionAzureBlob `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumAzureBlob `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
	// Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.
	ConnectionString *string `json:"connectionString,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The name of your Azure storage account
	StorageAccountName *string `json:"storageAccountName,omitempty"`
	// The service principal's tenant ID
	TenantID *string `json:"tenantId,omitempty"`
	// The service principal's client ID
	ClientID *string `json:"clientId,omitempty"`
	// The Azure cloud to use. Defaults to Azure Public Cloud.
	AzureCloud *string `json:"azureCloud,omitempty"`
	// Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.
	EndpointSuffix *string `json:"endpointSuffix,omitempty"`
	// Select or create a stored text secret
	ClientTextSecret     *string                     `json:"clientTextSecret,omitempty"`
	Certificate          *OutputCertificateAzureBlob `json:"certificate,omitempty"`
	AdditionalProperties map[string]any              `additionalProperties:"true" json:"-"`
}

func (o OutputAzureBlob) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureBlob) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "containerName"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureBlob) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputAzureBlob) GetType() OutputTypeAzureBlob {
	if o == nil {
		return OutputTypeAzureBlob("")
	}
	return o.Type
}

func (o *OutputAzureBlob) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureBlob) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureBlob) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureBlob) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureBlob) GetContainerName() string {
	if o == nil {
		return ""
	}
	return o.ContainerName
}

func (o *OutputAzureBlob) GetCreateContainer() *bool {
	if o == nil {
		return nil
	}
	return o.CreateContainer
}

func (o *OutputAzureBlob) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputAzureBlob) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputAzureBlob) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputAzureBlob) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputAzureBlob) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputAzureBlob) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputAzureBlob) GetFormat() *DataFormatAzureBlob {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureBlob) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputAzureBlob) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputAzureBlob) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputAzureBlob) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputAzureBlob) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputAzureBlob) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputAzureBlob) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputAzureBlob) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputAzureBlob) GetOnBackpressure() *BackpressureBehaviorAzureBlob {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureBlob) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputAzureBlob) GetOnDiskFullBackpressure() *DiskSpaceProtectionAzureBlob {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputAzureBlob) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputAzureBlob) GetAuthType() *OutputAuthenticationMethodAzureBlob {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputAzureBlob) GetStorageClass() *BlobAccessTier {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputAzureBlob) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureBlob) GetCompress() *OutputCompressionAzureBlob {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureBlob) GetCompressionLevel() *CompressionLevelAzureBlob {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputAzureBlob) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputAzureBlob) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputAzureBlob) GetParquetVersion() *ParquetVersionAzureBlob {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputAzureBlob) GetParquetDataPageVersion() *DataPageVersionAzureBlob {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputAzureBlob) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputAzureBlob) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputAzureBlob) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputAzureBlob) GetKeyValueMetadata() []KeyValueMetadatumAzureBlob {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputAzureBlob) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputAzureBlob) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputAzureBlob) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputAzureBlob) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputAzureBlob) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputAzureBlob) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputAzureBlob) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputAzureBlob) GetConnectionString() *string {
	if o == nil {
		return nil
	}
	return o.ConnectionString
}

func (o *OutputAzureBlob) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputAzureBlob) GetStorageAccountName() *string {
	if o == nil {
		return nil
	}
	return o.StorageAccountName
}

func (o *OutputAzureBlob) GetTenantID() *string {
	if o == nil {
		return nil
	}
	return o.TenantID
}

func (o *OutputAzureBlob) GetClientID() *string {
	if o == nil {
		return nil
	}
	return o.ClientID
}

func (o *OutputAzureBlob) GetAzureCloud() *string {
	if o == nil {
		return nil
	}
	return o.AzureCloud
}

func (o *OutputAzureBlob) GetEndpointSuffix() *string {
	if o == nil {
		return nil
	}
	return o.EndpointSuffix
}

func (o *OutputAzureBlob) GetClientTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.ClientTextSecret
}

func (o *OutputAzureBlob) GetCertificate() *OutputCertificateAzureBlob {
	if o == nil {
		return nil
	}
	return o.Certificate
}

func (o *OutputAzureBlob) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeS3 string

const (
	OutputTypeS3S3 OutputTypeS3 = "s3"
)

func (e OutputTypeS3) ToPointer() *OutputTypeS3 {
	return &e
}
func (e *OutputTypeS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "s3":
		*e = OutputTypeS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeS3: %v", v)
	}
}

// OutputAuthenticationMethodS3 - AWS authentication method. Choose Auto to use IAM roles.
type OutputAuthenticationMethodS3 string

const (
	// OutputAuthenticationMethodS3Auto Auto
	OutputAuthenticationMethodS3Auto OutputAuthenticationMethodS3 = "auto"
	// OutputAuthenticationMethodS3Manual Manual
	OutputAuthenticationMethodS3Manual OutputAuthenticationMethodS3 = "manual"
	// OutputAuthenticationMethodS3Secret Secret Key pair
	OutputAuthenticationMethodS3Secret OutputAuthenticationMethodS3 = "secret"
)

func (e OutputAuthenticationMethodS3) ToPointer() *OutputAuthenticationMethodS3 {
	return &e
}

// OutputSignatureVersionS3 - Signature version to use for signing S3 requests
type OutputSignatureVersionS3 string

const (
	OutputSignatureVersionS3V2 OutputSignatureVersionS3 = "v2"
	OutputSignatureVersionS3V4 OutputSignatureVersionS3 = "v4"
)

func (e OutputSignatureVersionS3) ToPointer() *OutputSignatureVersionS3 {
	return &e
}

// ObjectACLS3 - Object ACL to assign to uploaded objects
type ObjectACLS3 string

const (
	// ObjectACLS3Private Private
	ObjectACLS3Private ObjectACLS3 = "private"
	// ObjectACLS3PublicRead Public Read Only
	ObjectACLS3PublicRead ObjectACLS3 = "public-read"
	// ObjectACLS3PublicReadWrite Public Read/Write
	ObjectACLS3PublicReadWrite ObjectACLS3 = "public-read-write"
	// ObjectACLS3AuthenticatedRead Authenticated Read Only
	ObjectACLS3AuthenticatedRead ObjectACLS3 = "authenticated-read"
	// ObjectACLS3AwsExecRead AWS EC2 AMI Read Only
	ObjectACLS3AwsExecRead ObjectACLS3 = "aws-exec-read"
	// ObjectACLS3BucketOwnerRead Bucket Owner Read Only
	ObjectACLS3BucketOwnerRead ObjectACLS3 = "bucket-owner-read"
	// ObjectACLS3BucketOwnerFullControl Bucket Owner Full Control
	ObjectACLS3BucketOwnerFullControl ObjectACLS3 = "bucket-owner-full-control"
)

func (e ObjectACLS3) ToPointer() *ObjectACLS3 {
	return &e
}

// StorageClassS3 - Storage class to select for uploaded objects
type StorageClassS3 string

const (
	// StorageClassS3Standard Standard
	StorageClassS3Standard StorageClassS3 = "STANDARD"
	// StorageClassS3ReducedRedundancy Reduced Redundancy Storage
	StorageClassS3ReducedRedundancy StorageClassS3 = "REDUCED_REDUNDANCY"
	// StorageClassS3StandardIa Standard, Infrequent Access
	StorageClassS3StandardIa StorageClassS3 = "STANDARD_IA"
	// StorageClassS3OnezoneIa One Zone, Infrequent Access
	StorageClassS3OnezoneIa StorageClassS3 = "ONEZONE_IA"
	// StorageClassS3IntelligentTiering Intelligent Tiering
	StorageClassS3IntelligentTiering StorageClassS3 = "INTELLIGENT_TIERING"
	// StorageClassS3Glacier Glacier Flexible Retrieval
	StorageClassS3Glacier StorageClassS3 = "GLACIER"
	// StorageClassS3GlacierIr Glacier Instant Retrieval
	StorageClassS3GlacierIr StorageClassS3 = "GLACIER_IR"
	// StorageClassS3DeepArchive Glacier Deep Archive
	StorageClassS3DeepArchive StorageClassS3 = "DEEP_ARCHIVE"
)

func (e StorageClassS3) ToPointer() *StorageClassS3 {
	return &e
}

type ServerSideEncryptionForUploadedObjectsS3 string

const (
	// ServerSideEncryptionForUploadedObjectsS3Aes256 Amazon S3 Managed Key
	ServerSideEncryptionForUploadedObjectsS3Aes256 ServerSideEncryptionForUploadedObjectsS3 = "AES256"
	// ServerSideEncryptionForUploadedObjectsS3AwsKms AWS KMS Managed Key
	ServerSideEncryptionForUploadedObjectsS3AwsKms ServerSideEncryptionForUploadedObjectsS3 = "aws:kms"
)

func (e ServerSideEncryptionForUploadedObjectsS3) ToPointer() *ServerSideEncryptionForUploadedObjectsS3 {
	return &e
}

// DataFormatS3 - Format of the output data
type DataFormatS3 string

const (
	// DataFormatS3JSON JSON
	DataFormatS3JSON DataFormatS3 = "json"
	// DataFormatS3Raw Raw
	DataFormatS3Raw DataFormatS3 = "raw"
	// DataFormatS3Parquet Parquet
	DataFormatS3Parquet DataFormatS3 = "parquet"
)

func (e DataFormatS3) ToPointer() *DataFormatS3 {
	return &e
}

// BackpressureBehaviorS3 - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorS3 string

const (
	// BackpressureBehaviorS3Block Block
	BackpressureBehaviorS3Block BackpressureBehaviorS3 = "block"
	// BackpressureBehaviorS3Drop Drop
	BackpressureBehaviorS3Drop BackpressureBehaviorS3 = "drop"
)

func (e BackpressureBehaviorS3) ToPointer() *BackpressureBehaviorS3 {
	return &e
}

// DiskSpaceProtectionS3 - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionS3 string

const (
	// DiskSpaceProtectionS3Block Block
	DiskSpaceProtectionS3Block DiskSpaceProtectionS3 = "block"
	// DiskSpaceProtectionS3Drop Drop
	DiskSpaceProtectionS3Drop DiskSpaceProtectionS3 = "drop"
)

func (e DiskSpaceProtectionS3) ToPointer() *DiskSpaceProtectionS3 {
	return &e
}

// OutputCompressionS3 - Data compression format to apply to HTTP content before it is delivered
type OutputCompressionS3 string

const (
	OutputCompressionS3None OutputCompressionS3 = "none"
	OutputCompressionS3Gzip OutputCompressionS3 = "gzip"
)

func (e OutputCompressionS3) ToPointer() *OutputCompressionS3 {
	return &e
}

// CompressionLevelS3 - Compression level to apply before moving files to final destination
type CompressionLevelS3 string

const (
	// CompressionLevelS3BestSpeed Best Speed
	CompressionLevelS3BestSpeed CompressionLevelS3 = "best_speed"
	// CompressionLevelS3Normal Normal
	CompressionLevelS3Normal CompressionLevelS3 = "normal"
	// CompressionLevelS3BestCompression Best Compression
	CompressionLevelS3BestCompression CompressionLevelS3 = "best_compression"
)

func (e CompressionLevelS3) ToPointer() *CompressionLevelS3 {
	return &e
}

// ParquetVersionS3 - Determines which data types are supported and how they are represented
type ParquetVersionS3 string

const (
	// ParquetVersionS3Parquet10 1.0
	ParquetVersionS3Parquet10 ParquetVersionS3 = "PARQUET_1_0"
	// ParquetVersionS3Parquet24 2.4
	ParquetVersionS3Parquet24 ParquetVersionS3 = "PARQUET_2_4"
	// ParquetVersionS3Parquet26 2.6
	ParquetVersionS3Parquet26 ParquetVersionS3 = "PARQUET_2_6"
)

func (e ParquetVersionS3) ToPointer() *ParquetVersionS3 {
	return &e
}

// DataPageVersionS3 - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionS3 string

const (
	// DataPageVersionS3DataPageV1 V1
	DataPageVersionS3DataPageV1 DataPageVersionS3 = "DATA_PAGE_V1"
	// DataPageVersionS3DataPageV2 V2
	DataPageVersionS3DataPageV2 DataPageVersionS3 = "DATA_PAGE_V2"
)

func (e DataPageVersionS3) ToPointer() *DataPageVersionS3 {
	return &e
}

type KeyValueMetadatumS3 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumS3) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumS3) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputS3 struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type OutputTypeS3 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
	Bucket string `json:"bucket"`
	// Region where the S3 bucket is located
	Region *string `json:"region,omitempty"`
	// Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputAuthenticationMethodS3 `default:"auto" json:"awsAuthenticationMethod"`
	// S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing S3 requests
	SignatureVersion *OutputSignatureVersionS3 `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
	StagePath *string `default:"$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`
	DestPath *string `default:"" json:"destPath"`
	// Object ACL to assign to uploaded objects
	ObjectACL *ObjectACLS3 `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects
	StorageClass         *StorageClassS3                           `json:"storageClass,omitempty"`
	ServerSideEncryption *ServerSideEncryptionForUploadedObjectsS3 `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatS3 `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorS3 `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionS3 `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool `default:"false" json:"forceCloseOnShutdown"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	Description                   *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *OutputCompressionS3 `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelS3 `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionS3 `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionS3 `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumS3 `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum          *float64       `default:"20" json:"maxRetryNum"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "bucket"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputS3) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputS3) GetType() OutputTypeS3 {
	if o == nil {
		return OutputTypeS3("")
	}
	return o.Type
}

func (o *OutputS3) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputS3) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputS3) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputS3) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputS3) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputS3) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputS3) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputS3) GetAwsAuthenticationMethod() *OutputAuthenticationMethodS3 {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputS3) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputS3) GetSignatureVersion() *OutputSignatureVersionS3 {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputS3) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputS3) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputS3) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputS3) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputS3) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputS3) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputS3) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputS3) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputS3) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputS3) GetObjectACL() *ObjectACLS3 {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputS3) GetStorageClass() *StorageClassS3 {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputS3) GetServerSideEncryption() *ServerSideEncryptionForUploadedObjectsS3 {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputS3) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputS3) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputS3) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputS3) GetFormat() *DataFormatS3 {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputS3) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputS3) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputS3) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputS3) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputS3) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputS3) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputS3) GetOnBackpressure() *BackpressureBehaviorS3 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputS3) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputS3) GetOnDiskFullBackpressure() *DiskSpaceProtectionS3 {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputS3) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputS3) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputS3) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputS3) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputS3) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputS3) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputS3) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputS3) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputS3) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputS3) GetCompress() *OutputCompressionS3 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputS3) GetCompressionLevel() *CompressionLevelS3 {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputS3) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputS3) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputS3) GetParquetVersion() *ParquetVersionS3 {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputS3) GetParquetDataPageVersion() *DataPageVersionS3 {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputS3) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputS3) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputS3) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputS3) GetKeyValueMetadata() []KeyValueMetadatumS3 {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputS3) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputS3) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputS3) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputS3) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputS3) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputS3) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputS3) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputS3) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeFilesystem string

const (
	TypeFilesystemFilesystem TypeFilesystem = "filesystem"
)

func (e TypeFilesystem) ToPointer() *TypeFilesystem {
	return &e
}
func (e *TypeFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "filesystem":
		*e = TypeFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeFilesystem: %v", v)
	}
}

// DataFormatFilesystem - Format of the output data
type DataFormatFilesystem string

const (
	// DataFormatFilesystemJSON JSON
	DataFormatFilesystemJSON DataFormatFilesystem = "json"
	// DataFormatFilesystemRaw Raw
	DataFormatFilesystemRaw DataFormatFilesystem = "raw"
	// DataFormatFilesystemParquet Parquet
	DataFormatFilesystemParquet DataFormatFilesystem = "parquet"
)

func (e DataFormatFilesystem) ToPointer() *DataFormatFilesystem {
	return &e
}

// BackpressureBehaviorFilesystem - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorFilesystem string

const (
	// BackpressureBehaviorFilesystemBlock Block
	BackpressureBehaviorFilesystemBlock BackpressureBehaviorFilesystem = "block"
	// BackpressureBehaviorFilesystemDrop Drop
	BackpressureBehaviorFilesystemDrop BackpressureBehaviorFilesystem = "drop"
)

func (e BackpressureBehaviorFilesystem) ToPointer() *BackpressureBehaviorFilesystem {
	return &e
}

// DiskSpaceProtectionFilesystem - How to handle events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionFilesystem string

const (
	// DiskSpaceProtectionFilesystemBlock Block
	DiskSpaceProtectionFilesystemBlock DiskSpaceProtectionFilesystem = "block"
	// DiskSpaceProtectionFilesystemDrop Drop
	DiskSpaceProtectionFilesystemDrop DiskSpaceProtectionFilesystem = "drop"
)

func (e DiskSpaceProtectionFilesystem) ToPointer() *DiskSpaceProtectionFilesystem {
	return &e
}

// CompressionFilesystem - Data compression format to apply to HTTP content before it is delivered
type CompressionFilesystem string

const (
	CompressionFilesystemNone CompressionFilesystem = "none"
	CompressionFilesystemGzip CompressionFilesystem = "gzip"
)

func (e CompressionFilesystem) ToPointer() *CompressionFilesystem {
	return &e
}

// CompressionLevelFilesystem - Compression level to apply before moving files to final destination
type CompressionLevelFilesystem string

const (
	// CompressionLevelFilesystemBestSpeed Best Speed
	CompressionLevelFilesystemBestSpeed CompressionLevelFilesystem = "best_speed"
	// CompressionLevelFilesystemNormal Normal
	CompressionLevelFilesystemNormal CompressionLevelFilesystem = "normal"
	// CompressionLevelFilesystemBestCompression Best Compression
	CompressionLevelFilesystemBestCompression CompressionLevelFilesystem = "best_compression"
)

func (e CompressionLevelFilesystem) ToPointer() *CompressionLevelFilesystem {
	return &e
}

// ParquetVersionFilesystem - Determines which data types are supported and how they are represented
type ParquetVersionFilesystem string

const (
	// ParquetVersionFilesystemParquet10 1.0
	ParquetVersionFilesystemParquet10 ParquetVersionFilesystem = "PARQUET_1_0"
	// ParquetVersionFilesystemParquet24 2.4
	ParquetVersionFilesystemParquet24 ParquetVersionFilesystem = "PARQUET_2_4"
	// ParquetVersionFilesystemParquet26 2.6
	ParquetVersionFilesystemParquet26 ParquetVersionFilesystem = "PARQUET_2_6"
)

func (e ParquetVersionFilesystem) ToPointer() *ParquetVersionFilesystem {
	return &e
}

// DataPageVersionFilesystem - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionFilesystem string

const (
	// DataPageVersionFilesystemDataPageV1 V1
	DataPageVersionFilesystemDataPageV1 DataPageVersionFilesystem = "DATA_PAGE_V1"
	// DataPageVersionFilesystemDataPageV2 V2
	DataPageVersionFilesystemDataPageV2 DataPageVersionFilesystem = "DATA_PAGE_V2"
)

func (e DataPageVersionFilesystem) ToPointer() *DataPageVersionFilesystem {
	return &e
}

type KeyValueMetadatumFilesystem struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumFilesystem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumFilesystem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (k *KeyValueMetadatumFilesystem) GetKey() *string {
	if k == nil {
		return nil
	}
	return k.Key
}

func (k *KeyValueMetadatumFilesystem) GetValue() string {
	if k == nil {
		return ""
	}
	return k.Value
}

type OutputFilesystem struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type TypeFilesystem `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Final destination for the output files
	DestPath string `json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `json:"stagePath,omitempty"`
	// Add the Output ID value to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatFilesystem `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorFilesystem `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// How to handle events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionFilesystem `default:"block" json:"onDiskFullBackpressure"`
	// Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
	ForceCloseOnShutdown *bool   `default:"false" json:"forceCloseOnShutdown"`
	Description          *string `json:"description,omitempty"`
	// Data compression format to apply to HTTP content before it is delivered
	Compress *CompressionFilesystem `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelFilesystem `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionFilesystem `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionFilesystem `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
	KeyValueMetadata []KeyValueMetadatumFilesystem `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
	DirectoryBatchSize *float64 `default:"1000" json:"directoryBatchSize"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum          *float64       `default:"20" json:"maxRetryNum"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputFilesystem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputFilesystem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "destPath"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputFilesystem) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputFilesystem) GetType() TypeFilesystem {
	if o == nil {
		return TypeFilesystem("")
	}
	return o.Type
}

func (o *OutputFilesystem) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputFilesystem) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputFilesystem) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputFilesystem) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputFilesystem) GetDestPath() string {
	if o == nil {
		return ""
	}
	return o.DestPath
}

func (o *OutputFilesystem) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputFilesystem) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputFilesystem) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputFilesystem) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputFilesystem) GetFormat() *DataFormatFilesystem {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputFilesystem) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputFilesystem) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputFilesystem) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputFilesystem) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputFilesystem) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputFilesystem) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputFilesystem) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputFilesystem) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputFilesystem) GetOnBackpressure() *BackpressureBehaviorFilesystem {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputFilesystem) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputFilesystem) GetOnDiskFullBackpressure() *DiskSpaceProtectionFilesystem {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputFilesystem) GetForceCloseOnShutdown() *bool {
	if o == nil {
		return nil
	}
	return o.ForceCloseOnShutdown
}

func (o *OutputFilesystem) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputFilesystem) GetCompress() *CompressionFilesystem {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputFilesystem) GetCompressionLevel() *CompressionLevelFilesystem {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputFilesystem) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputFilesystem) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputFilesystem) GetParquetVersion() *ParquetVersionFilesystem {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputFilesystem) GetParquetDataPageVersion() *DataPageVersionFilesystem {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputFilesystem) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputFilesystem) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputFilesystem) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputFilesystem) GetKeyValueMetadata() []KeyValueMetadatumFilesystem {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputFilesystem) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputFilesystem) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputFilesystem) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputFilesystem) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputFilesystem) GetDirectoryBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.DirectoryBatchSize
}

func (o *OutputFilesystem) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputFilesystem) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputFilesystem) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeSignalfx string

const (
	TypeSignalfxSignalfx TypeSignalfx = "signalfx"
)

func (e TypeSignalfx) ToPointer() *TypeSignalfx {
	return &e
}
func (e *TypeSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "signalfx":
		*e = TypeSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSignalfx: %v", v)
	}
}

// AuthenticationMethodSignalfx - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSignalfx string

const (
	AuthenticationMethodSignalfxManual AuthenticationMethodSignalfx = "manual"
	AuthenticationMethodSignalfxSecret AuthenticationMethodSignalfx = "secret"
)

func (e AuthenticationMethodSignalfx) ToPointer() *AuthenticationMethodSignalfx {
	return &e
}

type ExtraHTTPHeaderSignalfx struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderSignalfx) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderSignalfx) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeSignalfx - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSignalfx string

const (
	// FailedRequestLoggingModeSignalfxPayload Payload
	FailedRequestLoggingModeSignalfxPayload FailedRequestLoggingModeSignalfx = "payload"
	// FailedRequestLoggingModeSignalfxPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeSignalfxPayloadAndHeaders FailedRequestLoggingModeSignalfx = "payloadAndHeaders"
	// FailedRequestLoggingModeSignalfxNone None
	FailedRequestLoggingModeSignalfxNone FailedRequestLoggingModeSignalfx = "none"
)

func (e FailedRequestLoggingModeSignalfx) ToPointer() *FailedRequestLoggingModeSignalfx {
	return &e
}

type ResponseRetrySettingSignalfx struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingSignalfx) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingSignalfx) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingSignalfx) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingSignalfx) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsSignalfx struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsSignalfx) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsSignalfx) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsSignalfx) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsSignalfx) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorSignalfx - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSignalfx string

const (
	// BackpressureBehaviorSignalfxBlock Block
	BackpressureBehaviorSignalfxBlock BackpressureBehaviorSignalfx = "block"
	// BackpressureBehaviorSignalfxDrop Drop
	BackpressureBehaviorSignalfxDrop BackpressureBehaviorSignalfx = "drop"
	// BackpressureBehaviorSignalfxQueue Persistent Queue
	BackpressureBehaviorSignalfxQueue BackpressureBehaviorSignalfx = "queue"
)

func (e BackpressureBehaviorSignalfx) ToPointer() *BackpressureBehaviorSignalfx {
	return &e
}

// ModeSignalfx - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSignalfx string

const (
	// ModeSignalfxError Error
	ModeSignalfxError ModeSignalfx = "error"
	// ModeSignalfxAlways Backpressure
	ModeSignalfxAlways ModeSignalfx = "always"
	// ModeSignalfxBackpressure Always On
	ModeSignalfxBackpressure ModeSignalfx = "backpressure"
)

func (e ModeSignalfx) ToPointer() *ModeSignalfx {
	return &e
}

// CompressionSignalfx - Codec to use to compress the persisted data
type CompressionSignalfx string

const (
	// CompressionSignalfxNone None
	CompressionSignalfxNone CompressionSignalfx = "none"
	// CompressionSignalfxGzip Gzip
	CompressionSignalfxGzip CompressionSignalfx = "gzip"
)

func (e CompressionSignalfx) ToPointer() *CompressionSignalfx {
	return &e
}

// QueueFullBehaviorSignalfx - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSignalfx string

const (
	// QueueFullBehaviorSignalfxBlock Block
	QueueFullBehaviorSignalfxBlock QueueFullBehaviorSignalfx = "block"
	// QueueFullBehaviorSignalfxDrop Drop new data
	QueueFullBehaviorSignalfxDrop QueueFullBehaviorSignalfx = "drop"
)

func (e QueueFullBehaviorSignalfx) ToPointer() *QueueFullBehaviorSignalfx {
	return &e
}

type PqControlsSignalfx struct {
}

func (p PqControlsSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSignalfx struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type TypeSignalfx `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodSignalfx `default:"manual" json:"authType"`
	// SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions).
	Realm *string `default:"us0" json:"realm"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderSignalfx `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSignalfx `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSignalfx `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSignalfx  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSignalfx `default:"block" json:"onBackpressure"`
	Description    *string                       `json:"description,omitempty"`
	// SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSignalfx `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSignalfx `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorSignalfx `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsSignalfx        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any             `additionalProperties:"true" json:"-"`
}

func (o OutputSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSignalfx) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSignalfx) GetType() TypeSignalfx {
	if o == nil {
		return TypeSignalfx("")
	}
	return o.Type
}

func (o *OutputSignalfx) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSignalfx) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSignalfx) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSignalfx) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSignalfx) GetAuthType() *AuthenticationMethodSignalfx {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSignalfx) GetRealm() *string {
	if o == nil {
		return nil
	}
	return o.Realm
}

func (o *OutputSignalfx) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSignalfx) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSignalfx) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSignalfx) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSignalfx) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSignalfx) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSignalfx) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSignalfx) GetExtraHTTPHeaders() []ExtraHTTPHeaderSignalfx {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSignalfx) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSignalfx) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSignalfx {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSignalfx) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSignalfx) GetResponseRetrySettings() []ResponseRetrySettingSignalfx {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSignalfx) GetTimeoutRetrySettings() *TimeoutRetrySettingsSignalfx {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSignalfx) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSignalfx) GetOnBackpressure() *BackpressureBehaviorSignalfx {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSignalfx) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSignalfx) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputSignalfx) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSignalfx) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSignalfx) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSignalfx) GetPqMode() *ModeSignalfx {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSignalfx) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSignalfx) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSignalfx) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSignalfx) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSignalfx) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSignalfx) GetPqCompress() *CompressionSignalfx {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSignalfx) GetPqOnBackpressure() *QueueFullBehaviorSignalfx {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSignalfx) GetPqControls() *PqControlsSignalfx {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSignalfx) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeWavefront string

const (
	TypeWavefrontWavefront TypeWavefront = "wavefront"
)

func (e TypeWavefront) ToPointer() *TypeWavefront {
	return &e
}
func (e *TypeWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "wavefront":
		*e = TypeWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeWavefront: %v", v)
	}
}

// AuthenticationMethodWavefront - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodWavefront string

const (
	AuthenticationMethodWavefrontManual AuthenticationMethodWavefront = "manual"
	AuthenticationMethodWavefrontSecret AuthenticationMethodWavefront = "secret"
)

func (e AuthenticationMethodWavefront) ToPointer() *AuthenticationMethodWavefront {
	return &e
}

type ExtraHTTPHeaderWavefront struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderWavefront) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderWavefront) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeWavefront - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeWavefront string

const (
	// FailedRequestLoggingModeWavefrontPayload Payload
	FailedRequestLoggingModeWavefrontPayload FailedRequestLoggingModeWavefront = "payload"
	// FailedRequestLoggingModeWavefrontPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeWavefrontPayloadAndHeaders FailedRequestLoggingModeWavefront = "payloadAndHeaders"
	// FailedRequestLoggingModeWavefrontNone None
	FailedRequestLoggingModeWavefrontNone FailedRequestLoggingModeWavefront = "none"
)

func (e FailedRequestLoggingModeWavefront) ToPointer() *FailedRequestLoggingModeWavefront {
	return &e
}

type ResponseRetrySettingWavefront struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingWavefront) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingWavefront) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingWavefront) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingWavefront) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsWavefront struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsWavefront) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsWavefront) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsWavefront) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsWavefront) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorWavefront - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorWavefront string

const (
	// BackpressureBehaviorWavefrontBlock Block
	BackpressureBehaviorWavefrontBlock BackpressureBehaviorWavefront = "block"
	// BackpressureBehaviorWavefrontDrop Drop
	BackpressureBehaviorWavefrontDrop BackpressureBehaviorWavefront = "drop"
	// BackpressureBehaviorWavefrontQueue Persistent Queue
	BackpressureBehaviorWavefrontQueue BackpressureBehaviorWavefront = "queue"
)

func (e BackpressureBehaviorWavefront) ToPointer() *BackpressureBehaviorWavefront {
	return &e
}

// ModeWavefront - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeWavefront string

const (
	// ModeWavefrontError Error
	ModeWavefrontError ModeWavefront = "error"
	// ModeWavefrontAlways Backpressure
	ModeWavefrontAlways ModeWavefront = "always"
	// ModeWavefrontBackpressure Always On
	ModeWavefrontBackpressure ModeWavefront = "backpressure"
)

func (e ModeWavefront) ToPointer() *ModeWavefront {
	return &e
}

// CompressionWavefront - Codec to use to compress the persisted data
type CompressionWavefront string

const (
	// CompressionWavefrontNone None
	CompressionWavefrontNone CompressionWavefront = "none"
	// CompressionWavefrontGzip Gzip
	CompressionWavefrontGzip CompressionWavefront = "gzip"
)

func (e CompressionWavefront) ToPointer() *CompressionWavefront {
	return &e
}

// QueueFullBehaviorWavefront - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorWavefront string

const (
	// QueueFullBehaviorWavefrontBlock Block
	QueueFullBehaviorWavefrontBlock QueueFullBehaviorWavefront = "block"
	// QueueFullBehaviorWavefrontDrop Drop new data
	QueueFullBehaviorWavefrontDrop QueueFullBehaviorWavefront = "drop"
)

func (e QueueFullBehaviorWavefront) ToPointer() *QueueFullBehaviorWavefront {
	return &e
}

type PqControlsWavefront struct {
}

func (p PqControlsWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputWavefront struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeWavefront `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodWavefront `default:"manual" json:"authType"`
	// WaveFront domain name, e.g. "longboard"
	Domain *string `default:"longboard" json:"domain"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderWavefront `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeWavefront `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingWavefront `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsWavefront  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorWavefront `default:"block" json:"onBackpressure"`
	Description    *string                        `json:"description,omitempty"`
	// WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeWavefront `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionWavefront `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorWavefront `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsWavefront        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any              `additionalProperties:"true" json:"-"`
}

func (o OutputWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputWavefront) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputWavefront) GetType() TypeWavefront {
	if o == nil {
		return TypeWavefront("")
	}
	return o.Type
}

func (o *OutputWavefront) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputWavefront) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputWavefront) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputWavefront) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputWavefront) GetAuthType() *AuthenticationMethodWavefront {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputWavefront) GetDomain() *string {
	if o == nil {
		return nil
	}
	return o.Domain
}

func (o *OutputWavefront) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputWavefront) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputWavefront) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputWavefront) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputWavefront) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputWavefront) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputWavefront) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputWavefront) GetExtraHTTPHeaders() []ExtraHTTPHeaderWavefront {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputWavefront) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputWavefront) GetFailedRequestLoggingMode() *FailedRequestLoggingModeWavefront {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputWavefront) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputWavefront) GetResponseRetrySettings() []ResponseRetrySettingWavefront {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputWavefront) GetTimeoutRetrySettings() *TimeoutRetrySettingsWavefront {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputWavefront) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputWavefront) GetOnBackpressure() *BackpressureBehaviorWavefront {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputWavefront) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputWavefront) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputWavefront) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputWavefront) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputWavefront) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputWavefront) GetPqMode() *ModeWavefront {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputWavefront) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputWavefront) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputWavefront) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputWavefront) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputWavefront) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputWavefront) GetPqCompress() *CompressionWavefront {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputWavefront) GetPqOnBackpressure() *QueueFullBehaviorWavefront {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputWavefront) GetPqControls() *PqControlsWavefront {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputWavefront) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeTcpjson string

const (
	OutputTypeTcpjsonTcpjson OutputTypeTcpjson = "tcpjson"
)

func (e OutputTypeTcpjson) ToPointer() *OutputTypeTcpjson {
	return &e
}
func (e *OutputTypeTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "tcpjson":
		*e = OutputTypeTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeTcpjson: %v", v)
	}
}

// OutputCompressionTcpjson - Codec to use to compress the data before sending
type OutputCompressionTcpjson string

const (
	// OutputCompressionTcpjsonNone None
	OutputCompressionTcpjsonNone OutputCompressionTcpjson = "none"
	// OutputCompressionTcpjsonGzip Gzip
	OutputCompressionTcpjsonGzip OutputCompressionTcpjson = "gzip"
)

func (e OutputCompressionTcpjson) ToPointer() *OutputCompressionTcpjson {
	return &e
}

type OutputMinimumTLSVersionTcpjson string

const (
	OutputMinimumTLSVersionTcpjsonTlSv1  OutputMinimumTLSVersionTcpjson = "TLSv1"
	OutputMinimumTLSVersionTcpjsonTlSv11 OutputMinimumTLSVersionTcpjson = "TLSv1.1"
	OutputMinimumTLSVersionTcpjsonTlSv12 OutputMinimumTLSVersionTcpjson = "TLSv1.2"
	OutputMinimumTLSVersionTcpjsonTlSv13 OutputMinimumTLSVersionTcpjson = "TLSv1.3"
)

func (e OutputMinimumTLSVersionTcpjson) ToPointer() *OutputMinimumTLSVersionTcpjson {
	return &e
}

type OutputMaximumTLSVersionTcpjson string

const (
	OutputMaximumTLSVersionTcpjsonTlSv1  OutputMaximumTLSVersionTcpjson = "TLSv1"
	OutputMaximumTLSVersionTcpjsonTlSv11 OutputMaximumTLSVersionTcpjson = "TLSv1.1"
	OutputMaximumTLSVersionTcpjsonTlSv12 OutputMaximumTLSVersionTcpjson = "TLSv1.2"
	OutputMaximumTLSVersionTcpjsonTlSv13 OutputMaximumTLSVersionTcpjson = "TLSv1.3"
)

func (e OutputMaximumTLSVersionTcpjson) ToPointer() *OutputMaximumTLSVersionTcpjson {
	return &e
}

type TLSSettingsClientSideTcpjson struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                         `json:"passphrase,omitempty"`
	MinVersion *OutputMinimumTLSVersionTcpjson `json:"minVersion,omitempty"`
	MaxVersion *OutputMaximumTLSVersionTcpjson `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideTcpjson) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideTcpjson) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideTcpjson) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideTcpjson) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideTcpjson) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideTcpjson) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideTcpjson) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideTcpjson) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideTcpjson) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideTcpjson) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideTcpjson) GetMinVersion() *OutputMinimumTLSVersionTcpjson {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideTcpjson) GetMaxVersion() *OutputMaximumTLSVersionTcpjson {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// BackpressureBehaviorTcpjson - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorTcpjson string

const (
	// BackpressureBehaviorTcpjsonBlock Block
	BackpressureBehaviorTcpjsonBlock BackpressureBehaviorTcpjson = "block"
	// BackpressureBehaviorTcpjsonDrop Drop
	BackpressureBehaviorTcpjsonDrop BackpressureBehaviorTcpjson = "drop"
	// BackpressureBehaviorTcpjsonQueue Persistent Queue
	BackpressureBehaviorTcpjsonQueue BackpressureBehaviorTcpjson = "queue"
)

func (e BackpressureBehaviorTcpjson) ToPointer() *BackpressureBehaviorTcpjson {
	return &e
}

// OutputAuthenticationMethodTcpjson - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type OutputAuthenticationMethodTcpjson string

const (
	OutputAuthenticationMethodTcpjsonManual OutputAuthenticationMethodTcpjson = "manual"
	OutputAuthenticationMethodTcpjsonSecret OutputAuthenticationMethodTcpjson = "secret"
)

func (e OutputAuthenticationMethodTcpjson) ToPointer() *OutputAuthenticationMethodTcpjson {
	return &e
}

// TLSTcpjson - Whether to inherit TLS configs from group setting or disable TLS
type TLSTcpjson string

const (
	TLSTcpjsonInherit TLSTcpjson = "inherit"
	TLSTcpjsonOff     TLSTcpjson = "off"
)

func (e TLSTcpjson) ToPointer() *TLSTcpjson {
	return &e
}

type HostTcpjson struct {
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port float64 `json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS
	TLS *TLSTcpjson `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostTcpjson) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostTcpjson) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host", "port"}); err != nil {
		return err
	}
	return nil
}

func (h *HostTcpjson) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostTcpjson) GetPort() float64 {
	if h == nil {
		return 0.0
	}
	return h.Port
}

func (h *HostTcpjson) GetTLS() *TLSTcpjson {
	if h == nil {
		return nil
	}
	return h.TLS
}

func (h *HostTcpjson) GetServername() *string {
	if h == nil {
		return nil
	}
	return h.Servername
}

func (h *HostTcpjson) GetWeight() *float64 {
	if h == nil {
		return nil
	}
	return h.Weight
}

// OutputModeTcpjson - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeTcpjson string

const (
	// OutputModeTcpjsonError Error
	OutputModeTcpjsonError OutputModeTcpjson = "error"
	// OutputModeTcpjsonAlways Backpressure
	OutputModeTcpjsonAlways OutputModeTcpjson = "always"
	// OutputModeTcpjsonBackpressure Always On
	OutputModeTcpjsonBackpressure OutputModeTcpjson = "backpressure"
)

func (e OutputModeTcpjson) ToPointer() *OutputModeTcpjson {
	return &e
}

// PqCompressCompressionTcpjson - Codec to use to compress the persisted data
type PqCompressCompressionTcpjson string

const (
	// PqCompressCompressionTcpjsonNone None
	PqCompressCompressionTcpjsonNone PqCompressCompressionTcpjson = "none"
	// PqCompressCompressionTcpjsonGzip Gzip
	PqCompressCompressionTcpjsonGzip PqCompressCompressionTcpjson = "gzip"
)

func (e PqCompressCompressionTcpjson) ToPointer() *PqCompressCompressionTcpjson {
	return &e
}

// QueueFullBehaviorTcpjson - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorTcpjson string

const (
	// QueueFullBehaviorTcpjsonBlock Block
	QueueFullBehaviorTcpjsonBlock QueueFullBehaviorTcpjson = "block"
	// QueueFullBehaviorTcpjsonDrop Drop new data
	QueueFullBehaviorTcpjsonDrop QueueFullBehaviorTcpjson = "drop"
)

func (e QueueFullBehaviorTcpjson) ToPointer() *QueueFullBehaviorTcpjson {
	return &e
}

type OutputPqControlsTcpjson struct {
}

func (o OutputPqControlsTcpjson) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsTcpjson) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputTcpjson struct {
	// Unique ID for this output
	ID   *string           `json:"id,omitempty"`
	Type OutputTypeTcpjson `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Use load-balanced destinations
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// Codec to use to compress the data before sending
	Compression *OutputCompressionTcpjson `default:"gzip" json:"compression"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string                       `default:"0" json:"throttleRatePerSec"`
	TLS                *TLSSettingsClientSideTcpjson `json:"tls,omitempty"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// The number of minutes before the internally generated authentication token expires, valid values between 1 and 60
	TokenTTLMinutes *float64 `default:"60" json:"tokenTTLMinutes"`
	// Upon connection, send a header-like record containing the auth token and other metadata.This record will not contain an actual event – only subsequent records will.
	SendHeader *bool `default:"true" json:"sendHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorTcpjson `default:"block" json:"onBackpressure"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType    *OutputAuthenticationMethodTcpjson `default:"manual" json:"authType"`
	Description *string                            `json:"description,omitempty"`
	// The hostname of the receiver
	Host *string `json:"host,omitempty"`
	// The port to connect to on the provided host
	Port *float64 `json:"port,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of hosts to load-balance data to
	Hosts []HostTcpjson `json:"hosts,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeTcpjson `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionTcpjson `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorTcpjson `default:"block" json:"pqOnBackpressure"`
	PqControls       *OutputPqControlsTcpjson  `json:"pqControls,omitempty"`
	// Optional authentication token to include as part of the connection header
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret           *string        `json:"textSecret,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputTcpjson) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputTcpjson) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputTcpjson) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputTcpjson) GetType() OutputTypeTcpjson {
	if o == nil {
		return OutputTypeTcpjson("")
	}
	return o.Type
}

func (o *OutputTcpjson) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputTcpjson) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputTcpjson) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputTcpjson) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputTcpjson) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputTcpjson) GetCompression() *OutputCompressionTcpjson {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputTcpjson) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputTcpjson) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputTcpjson) GetTLS() *TLSSettingsClientSideTcpjson {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputTcpjson) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputTcpjson) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputTcpjson) GetTokenTTLMinutes() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTTLMinutes
}

func (o *OutputTcpjson) GetSendHeader() *bool {
	if o == nil {
		return nil
	}
	return o.SendHeader
}

func (o *OutputTcpjson) GetOnBackpressure() *BackpressureBehaviorTcpjson {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputTcpjson) GetAuthType() *OutputAuthenticationMethodTcpjson {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputTcpjson) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputTcpjson) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputTcpjson) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputTcpjson) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputTcpjson) GetHosts() []HostTcpjson {
	if o == nil {
		return nil
	}
	return o.Hosts
}

func (o *OutputTcpjson) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputTcpjson) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputTcpjson) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputTcpjson) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputTcpjson) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputTcpjson) GetPqMode() *OutputModeTcpjson {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputTcpjson) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputTcpjson) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputTcpjson) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputTcpjson) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputTcpjson) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputTcpjson) GetPqCompress() *PqCompressCompressionTcpjson {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputTcpjson) GetPqOnBackpressure() *QueueFullBehaviorTcpjson {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputTcpjson) GetPqControls() *OutputPqControlsTcpjson {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputTcpjson) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *OutputTcpjson) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputTcpjson) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeSplunkHec string

const (
	OutputTypeSplunkHecSplunkHec OutputTypeSplunkHec = "splunk_hec"
)

func (e OutputTypeSplunkHec) ToPointer() *OutputTypeSplunkHec {
	return &e
}
func (e *OutputTypeSplunkHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk_hec":
		*e = OutputTypeSplunkHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSplunkHec: %v", v)
	}
}

type OutputMinimumTLSVersionSplunkHec string

const (
	OutputMinimumTLSVersionSplunkHecTlSv1  OutputMinimumTLSVersionSplunkHec = "TLSv1"
	OutputMinimumTLSVersionSplunkHecTlSv11 OutputMinimumTLSVersionSplunkHec = "TLSv1.1"
	OutputMinimumTLSVersionSplunkHecTlSv12 OutputMinimumTLSVersionSplunkHec = "TLSv1.2"
	OutputMinimumTLSVersionSplunkHecTlSv13 OutputMinimumTLSVersionSplunkHec = "TLSv1.3"
)

func (e OutputMinimumTLSVersionSplunkHec) ToPointer() *OutputMinimumTLSVersionSplunkHec {
	return &e
}

type OutputMaximumTLSVersionSplunkHec string

const (
	OutputMaximumTLSVersionSplunkHecTlSv1  OutputMaximumTLSVersionSplunkHec = "TLSv1"
	OutputMaximumTLSVersionSplunkHecTlSv11 OutputMaximumTLSVersionSplunkHec = "TLSv1.1"
	OutputMaximumTLSVersionSplunkHecTlSv12 OutputMaximumTLSVersionSplunkHec = "TLSv1.2"
	OutputMaximumTLSVersionSplunkHecTlSv13 OutputMaximumTLSVersionSplunkHec = "TLSv1.3"
)

func (e OutputMaximumTLSVersionSplunkHec) ToPointer() *OutputMaximumTLSVersionSplunkHec {
	return &e
}

type TLSSettingsClientSideSplunkHec struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                           `json:"passphrase,omitempty"`
	MinVersion *OutputMinimumTLSVersionSplunkHec `json:"minVersion,omitempty"`
	MaxVersion *OutputMaximumTLSVersionSplunkHec `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideSplunkHec) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideSplunkHec) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideSplunkHec) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideSplunkHec) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideSplunkHec) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideSplunkHec) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideSplunkHec) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideSplunkHec) GetMinVersion() *OutputMinimumTLSVersionSplunkHec {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideSplunkHec) GetMaxVersion() *OutputMaximumTLSVersionSplunkHec {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

type ExtraHTTPHeaderSplunkHec struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderSplunkHec) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderSplunkHec) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeSplunkHec - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSplunkHec string

const (
	// FailedRequestLoggingModeSplunkHecPayload Payload
	FailedRequestLoggingModeSplunkHecPayload FailedRequestLoggingModeSplunkHec = "payload"
	// FailedRequestLoggingModeSplunkHecPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeSplunkHecPayloadAndHeaders FailedRequestLoggingModeSplunkHec = "payloadAndHeaders"
	// FailedRequestLoggingModeSplunkHecNone None
	FailedRequestLoggingModeSplunkHecNone FailedRequestLoggingModeSplunkHec = "none"
)

func (e FailedRequestLoggingModeSplunkHec) ToPointer() *FailedRequestLoggingModeSplunkHec {
	return &e
}

// OutputAuthenticationMethodSplunkHec - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type OutputAuthenticationMethodSplunkHec string

const (
	OutputAuthenticationMethodSplunkHecManual OutputAuthenticationMethodSplunkHec = "manual"
	OutputAuthenticationMethodSplunkHecSecret OutputAuthenticationMethodSplunkHec = "secret"
)

func (e OutputAuthenticationMethodSplunkHec) ToPointer() *OutputAuthenticationMethodSplunkHec {
	return &e
}

type ResponseRetrySettingSplunkHec struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingSplunkHec) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingSplunkHec) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingSplunkHec) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingSplunkHec) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsSplunkHec struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsSplunkHec) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsSplunkHec) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsSplunkHec) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsSplunkHec) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorSplunkHec - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSplunkHec string

const (
	// BackpressureBehaviorSplunkHecBlock Block
	BackpressureBehaviorSplunkHecBlock BackpressureBehaviorSplunkHec = "block"
	// BackpressureBehaviorSplunkHecDrop Drop
	BackpressureBehaviorSplunkHecDrop BackpressureBehaviorSplunkHec = "drop"
	// BackpressureBehaviorSplunkHecQueue Persistent Queue
	BackpressureBehaviorSplunkHecQueue BackpressureBehaviorSplunkHec = "queue"
)

func (e BackpressureBehaviorSplunkHec) ToPointer() *BackpressureBehaviorSplunkHec {
	return &e
}

type URLSplunkHec struct {
	// URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event
	URL *string `default:"http://localhost:8088/services/collector/event" json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (u *URLSplunkHec) GetURL() *string {
	if u == nil {
		return nil
	}
	return u.URL
}

func (u *URLSplunkHec) GetWeight() *float64 {
	if u == nil {
		return nil
	}
	return u.Weight
}

// OutputModeSplunkHec - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeSplunkHec string

const (
	// OutputModeSplunkHecError Error
	OutputModeSplunkHecError OutputModeSplunkHec = "error"
	// OutputModeSplunkHecAlways Backpressure
	OutputModeSplunkHecAlways OutputModeSplunkHec = "always"
	// OutputModeSplunkHecBackpressure Always On
	OutputModeSplunkHecBackpressure OutputModeSplunkHec = "backpressure"
)

func (e OutputModeSplunkHec) ToPointer() *OutputModeSplunkHec {
	return &e
}

// PqCompressCompressionSplunkHec - Codec to use to compress the persisted data
type PqCompressCompressionSplunkHec string

const (
	// PqCompressCompressionSplunkHecNone None
	PqCompressCompressionSplunkHecNone PqCompressCompressionSplunkHec = "none"
	// PqCompressCompressionSplunkHecGzip Gzip
	PqCompressCompressionSplunkHecGzip PqCompressCompressionSplunkHec = "gzip"
)

func (e PqCompressCompressionSplunkHec) ToPointer() *PqCompressCompressionSplunkHec {
	return &e
}

// QueueFullBehaviorSplunkHec - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSplunkHec string

const (
	// QueueFullBehaviorSplunkHecBlock Block
	QueueFullBehaviorSplunkHecBlock QueueFullBehaviorSplunkHec = "block"
	// QueueFullBehaviorSplunkHecDrop Drop new data
	QueueFullBehaviorSplunkHecDrop QueueFullBehaviorSplunkHec = "drop"
)

func (e QueueFullBehaviorSplunkHec) ToPointer() *QueueFullBehaviorSplunkHec {
	return &e
}

type OutputPqControlsSplunkHec struct {
}

func (o OutputPqControlsSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSplunkHec struct {
	// Unique ID for this output
	ID   *string             `json:"id,omitempty"`
	Type OutputTypeSplunkHec `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// In the Splunk app, define which Splunk processing queue to send the events after HEC processing.
	NextQueue *string `default:"indexQueue" json:"nextQueue"`
	// In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set.
	TCPRouting *string                         `default:"nowhere" json:"tcpRouting"`
	TLS        *TLSSettingsClientSideSplunkHec `json:"tls,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events
	ExtraHTTPHeaders []ExtraHTTPHeaderSplunkHec `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSplunkHec `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Output metrics in multiple-metric format, supported in Splunk 8.0 and above to allow multiple metrics in a single event.
	EnableMultiMetrics *bool `default:"false" json:"enableMultiMetrics"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *OutputAuthenticationMethodSplunkHec `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSplunkHec `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSplunkHec  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"true" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSplunkHec `default:"block" json:"onBackpressure"`
	Description    *string                        `json:"description,omitempty"`
	// URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event
	URL *string `default:"http://localhost:8088/services/collector/event" json:"url"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool          `default:"false" json:"excludeSelf"`
	Urls        []URLSplunkHec `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Splunk HEC authentication token
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeSplunkHec `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionSplunkHec `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorSplunkHec `default:"block" json:"pqOnBackpressure"`
	PqControls           *OutputPqControlsSplunkHec  `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any              `additionalProperties:"true" json:"-"`
}

func (o OutputSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSplunkHec) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSplunkHec) GetType() OutputTypeSplunkHec {
	if o == nil {
		return OutputTypeSplunkHec("")
	}
	return o.Type
}

func (o *OutputSplunkHec) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSplunkHec) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSplunkHec) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSplunkHec) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSplunkHec) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputSplunkHec) GetNextQueue() *string {
	if o == nil {
		return nil
	}
	return o.NextQueue
}

func (o *OutputSplunkHec) GetTCPRouting() *string {
	if o == nil {
		return nil
	}
	return o.TCPRouting
}

func (o *OutputSplunkHec) GetTLS() *TLSSettingsClientSideSplunkHec {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSplunkHec) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSplunkHec) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSplunkHec) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSplunkHec) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSplunkHec) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSplunkHec) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSplunkHec) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSplunkHec) GetExtraHTTPHeaders() []ExtraHTTPHeaderSplunkHec {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSplunkHec) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSplunkHec {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSplunkHec) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSplunkHec) GetEnableMultiMetrics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableMultiMetrics
}

func (o *OutputSplunkHec) GetAuthType() *OutputAuthenticationMethodSplunkHec {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSplunkHec) GetResponseRetrySettings() []ResponseRetrySettingSplunkHec {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSplunkHec) GetTimeoutRetrySettings() *TimeoutRetrySettingsSplunkHec {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSplunkHec) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSplunkHec) GetOnBackpressure() *BackpressureBehaviorSplunkHec {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSplunkHec) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSplunkHec) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputSplunkHec) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSplunkHec) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputSplunkHec) GetUrls() []URLSplunkHec {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputSplunkHec) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSplunkHec) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputSplunkHec) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputSplunkHec) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSplunkHec) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSplunkHec) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSplunkHec) GetPqMode() *OutputModeSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSplunkHec) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSplunkHec) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSplunkHec) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSplunkHec) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSplunkHec) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSplunkHec) GetPqCompress() *PqCompressCompressionSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSplunkHec) GetPqOnBackpressure() *QueueFullBehaviorSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSplunkHec) GetPqControls() *OutputPqControlsSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSplunkHec) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeSplunkLb string

const (
	TypeSplunkLbSplunkLb TypeSplunkLb = "splunk_lb"
)

func (e TypeSplunkLb) ToPointer() *TypeSplunkLb {
	return &e
}
func (e *TypeSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk_lb":
		*e = TypeSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSplunkLb: %v", v)
	}
}

// NestedFieldSerializationSplunkLb - How to serialize nested fields into index-time fields
type NestedFieldSerializationSplunkLb string

const (
	// NestedFieldSerializationSplunkLbJSON JSON
	NestedFieldSerializationSplunkLbJSON NestedFieldSerializationSplunkLb = "json"
	// NestedFieldSerializationSplunkLbNone None
	NestedFieldSerializationSplunkLbNone NestedFieldSerializationSplunkLb = "none"
)

func (e NestedFieldSerializationSplunkLb) ToPointer() *NestedFieldSerializationSplunkLb {
	return &e
}

type MinimumTLSVersionSplunkLb string

const (
	MinimumTLSVersionSplunkLbTlSv1  MinimumTLSVersionSplunkLb = "TLSv1"
	MinimumTLSVersionSplunkLbTlSv11 MinimumTLSVersionSplunkLb = "TLSv1.1"
	MinimumTLSVersionSplunkLbTlSv12 MinimumTLSVersionSplunkLb = "TLSv1.2"
	MinimumTLSVersionSplunkLbTlSv13 MinimumTLSVersionSplunkLb = "TLSv1.3"
)

func (e MinimumTLSVersionSplunkLb) ToPointer() *MinimumTLSVersionSplunkLb {
	return &e
}

type MaximumTLSVersionSplunkLb string

const (
	MaximumTLSVersionSplunkLbTlSv1  MaximumTLSVersionSplunkLb = "TLSv1"
	MaximumTLSVersionSplunkLbTlSv11 MaximumTLSVersionSplunkLb = "TLSv1.1"
	MaximumTLSVersionSplunkLbTlSv12 MaximumTLSVersionSplunkLb = "TLSv1.2"
	MaximumTLSVersionSplunkLbTlSv13 MaximumTLSVersionSplunkLb = "TLSv1.3"
)

func (e MaximumTLSVersionSplunkLb) ToPointer() *MaximumTLSVersionSplunkLb {
	return &e
}

type TLSSettingsClientSideSplunkLb struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                    `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionSplunkLb `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionSplunkLb `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideSplunkLb) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideSplunkLb) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideSplunkLb) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideSplunkLb) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideSplunkLb) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideSplunkLb) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideSplunkLb) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideSplunkLb) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideSplunkLb) GetMinVersion() *MinimumTLSVersionSplunkLb {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideSplunkLb) GetMaxVersion() *MaximumTLSVersionSplunkLb {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// MaxS2SVersionSplunkLb - The highest S2S protocol version to advertise during handshake
type MaxS2SVersionSplunkLb string

const (
	MaxS2SVersionSplunkLbV3 MaxS2SVersionSplunkLb = "v3"
	MaxS2SVersionSplunkLbV4 MaxS2SVersionSplunkLb = "v4"
)

func (e MaxS2SVersionSplunkLb) ToPointer() *MaxS2SVersionSplunkLb {
	return &e
}

// BackpressureBehaviorSplunkLb - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSplunkLb string

const (
	// BackpressureBehaviorSplunkLbBlock Block
	BackpressureBehaviorSplunkLbBlock BackpressureBehaviorSplunkLb = "block"
	// BackpressureBehaviorSplunkLbDrop Drop
	BackpressureBehaviorSplunkLbDrop BackpressureBehaviorSplunkLb = "drop"
	// BackpressureBehaviorSplunkLbQueue Persistent Queue
	BackpressureBehaviorSplunkLbQueue BackpressureBehaviorSplunkLb = "queue"
)

func (e BackpressureBehaviorSplunkLb) ToPointer() *BackpressureBehaviorSplunkLb {
	return &e
}

// AuthenticationMethodSplunkLb - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSplunkLb string

const (
	AuthenticationMethodSplunkLbManual AuthenticationMethodSplunkLb = "manual"
	AuthenticationMethodSplunkLbSecret AuthenticationMethodSplunkLb = "secret"
)

func (e AuthenticationMethodSplunkLb) ToPointer() *AuthenticationMethodSplunkLb {
	return &e
}

// CompressCompressionSplunkLb - Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
type CompressCompressionSplunkLb string

const (
	// CompressCompressionSplunkLbDisabled Disabled
	CompressCompressionSplunkLbDisabled CompressCompressionSplunkLb = "disabled"
	// CompressCompressionSplunkLbAuto Automatic
	CompressCompressionSplunkLbAuto CompressCompressionSplunkLb = "auto"
	// CompressCompressionSplunkLbAlways Always
	CompressCompressionSplunkLbAlways CompressCompressionSplunkLb = "always"
)

func (e CompressCompressionSplunkLb) ToPointer() *CompressCompressionSplunkLb {
	return &e
}

// IndexerDiscoveryConfigsAuthTokenAuthenticationMethod - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type IndexerDiscoveryConfigsAuthTokenAuthenticationMethod string

const (
	IndexerDiscoveryConfigsAuthTokenAuthenticationMethodManual IndexerDiscoveryConfigsAuthTokenAuthenticationMethod = "manual"
	IndexerDiscoveryConfigsAuthTokenAuthenticationMethodSecret IndexerDiscoveryConfigsAuthTokenAuthenticationMethod = "secret"
)

func (e IndexerDiscoveryConfigsAuthTokenAuthenticationMethod) ToPointer() *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod {
	return &e
}

type IndexerDiscoveryConfigsAuthToken struct {
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod `default:"manual" json:"authType"`
}

func (i IndexerDiscoveryConfigsAuthToken) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(i, "", false)
}

func (i *IndexerDiscoveryConfigsAuthToken) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &i, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (i *IndexerDiscoveryConfigsAuthToken) GetAuthType() *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod {
	if i == nil {
		return nil
	}
	return i.AuthType
}

// IndexerDiscoveryConfigsAuthenticationMethod - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type IndexerDiscoveryConfigsAuthenticationMethod string

const (
	IndexerDiscoveryConfigsAuthenticationMethodManual IndexerDiscoveryConfigsAuthenticationMethod = "manual"
	IndexerDiscoveryConfigsAuthenticationMethodSecret IndexerDiscoveryConfigsAuthenticationMethod = "secret"
)

func (e IndexerDiscoveryConfigsAuthenticationMethod) ToPointer() *IndexerDiscoveryConfigsAuthenticationMethod {
	return &e
}

// IndexerDiscoveryConfigs - List of configurations to set up indexer discovery in Splunk Indexer clustering environment.
type IndexerDiscoveryConfigs struct {
	// Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site.
	Site *string `default:"default" json:"site"`
	// Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089
	MasterURI string `json:"masterUri"`
	// Time interval, in seconds, between two consecutive indexer list fetches from cluster manager
	RefreshIntervalSec *float64 `default:"300" json:"refreshIntervalSec"`
	// During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates.
	RejectUnauthorized *bool `default:"false" json:"rejectUnauthorized"`
	// Tokens required to authenticate to cluster manager for indexer discovery
	AuthTokens []IndexerDiscoveryConfigsAuthToken `json:"authTokens,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *IndexerDiscoveryConfigsAuthenticationMethod `default:"manual" json:"authType"`
	// Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (i IndexerDiscoveryConfigs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(i, "", false)
}

func (i *IndexerDiscoveryConfigs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &i, "", false, []string{"masterUri"}); err != nil {
		return err
	}
	return nil
}

func (i *IndexerDiscoveryConfigs) GetSite() *string {
	if i == nil {
		return nil
	}
	return i.Site
}

func (i *IndexerDiscoveryConfigs) GetMasterURI() string {
	if i == nil {
		return ""
	}
	return i.MasterURI
}

func (i *IndexerDiscoveryConfigs) GetRefreshIntervalSec() *float64 {
	if i == nil {
		return nil
	}
	return i.RefreshIntervalSec
}

func (i *IndexerDiscoveryConfigs) GetRejectUnauthorized() *bool {
	if i == nil {
		return nil
	}
	return i.RejectUnauthorized
}

func (i *IndexerDiscoveryConfigs) GetAuthTokens() []IndexerDiscoveryConfigsAuthToken {
	if i == nil {
		return nil
	}
	return i.AuthTokens
}

func (i *IndexerDiscoveryConfigs) GetAuthType() *IndexerDiscoveryConfigsAuthenticationMethod {
	if i == nil {
		return nil
	}
	return i.AuthType
}

func (i *IndexerDiscoveryConfigs) GetAuthToken() *string {
	if i == nil {
		return nil
	}
	return i.AuthToken
}

func (i *IndexerDiscoveryConfigs) GetTextSecret() *string {
	if i == nil {
		return nil
	}
	return i.TextSecret
}

// TLSSplunkLb - Whether to inherit TLS configs from group setting or disable TLS
type TLSSplunkLb string

const (
	TLSSplunkLbInherit TLSSplunkLb = "inherit"
	TLSSplunkLbOff     TLSSplunkLb = "off"
)

func (e TLSSplunkLb) ToPointer() *TLSSplunkLb {
	return &e
}

type HostSplunkLb struct {
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port *float64 `default:"9997" json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS
	TLS *TLSSplunkLb `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host"}); err != nil {
		return err
	}
	return nil
}

func (h *HostSplunkLb) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostSplunkLb) GetPort() *float64 {
	if h == nil {
		return nil
	}
	return h.Port
}

func (h *HostSplunkLb) GetTLS() *TLSSplunkLb {
	if h == nil {
		return nil
	}
	return h.TLS
}

func (h *HostSplunkLb) GetServername() *string {
	if h == nil {
		return nil
	}
	return h.Servername
}

func (h *HostSplunkLb) GetWeight() *float64 {
	if h == nil {
		return nil
	}
	return h.Weight
}

// ModeSplunkLb - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSplunkLb string

const (
	// ModeSplunkLbError Error
	ModeSplunkLbError ModeSplunkLb = "error"
	// ModeSplunkLbAlways Backpressure
	ModeSplunkLbAlways ModeSplunkLb = "always"
	// ModeSplunkLbBackpressure Always On
	ModeSplunkLbBackpressure ModeSplunkLb = "backpressure"
)

func (e ModeSplunkLb) ToPointer() *ModeSplunkLb {
	return &e
}

// PqCompressCompressionSplunkLb - Codec to use to compress the persisted data
type PqCompressCompressionSplunkLb string

const (
	// PqCompressCompressionSplunkLbNone None
	PqCompressCompressionSplunkLbNone PqCompressCompressionSplunkLb = "none"
	// PqCompressCompressionSplunkLbGzip Gzip
	PqCompressCompressionSplunkLbGzip PqCompressCompressionSplunkLb = "gzip"
)

func (e PqCompressCompressionSplunkLb) ToPointer() *PqCompressCompressionSplunkLb {
	return &e
}

// QueueFullBehaviorSplunkLb - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSplunkLb string

const (
	// QueueFullBehaviorSplunkLbBlock Block
	QueueFullBehaviorSplunkLbBlock QueueFullBehaviorSplunkLb = "block"
	// QueueFullBehaviorSplunkLbDrop Drop new data
	QueueFullBehaviorSplunkLbDrop QueueFullBehaviorSplunkLb = "drop"
)

func (e QueueFullBehaviorSplunkLb) ToPointer() *QueueFullBehaviorSplunkLb {
	return &e
}

type PqControlsSplunkLb struct {
}

func (p PqControlsSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSplunkLb struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type TypeSplunkLb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// How to serialize nested fields into index-time fields
	NestedFields *NestedFieldSerializationSplunkLb `default:"none" json:"nestedFields"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64                       `default:"60000" json:"writeTimeout"`
	TLS          *TLSSettingsClientSideSplunkLb `json:"tls,omitempty"`
	// Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.
	EnableMultiMetrics *bool `default:"false" json:"enableMultiMetrics"`
	// Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.
	EnableACK *bool `default:"true" json:"enableACK"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// The highest S2S protocol version to advertise during handshake
	MaxS2Sversion *MaxS2SVersionSplunkLb `default:"v3" json:"maxS2Sversion"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSplunkLb `default:"block" json:"onBackpressure"`
	// Automatically discover indexers in indexer clustering environment.
	IndexerDiscovery *bool `default:"false" json:"indexerDiscovery"`
	// How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute.
	SenderUnhealthyTimeAllowance *float64 `default:"100" json:"senderUnhealthyTimeAllowance"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType    *AuthenticationMethodSplunkLb `default:"manual" json:"authType"`
	Description *string                       `json:"description,omitempty"`
	// Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.
	MaxFailedHealthChecks *float64 `default:"1" json:"maxFailedHealthChecks"`
	// Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
	Compress *CompressCompressionSplunkLb `default:"disabled" json:"compress"`
	// List of configurations to set up indexer discovery in Splunk Indexer clustering environment.
	IndexerDiscoveryConfigs *IndexerDiscoveryConfigs `json:"indexerDiscoveryConfigs,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of Splunk indexers to load-balance data to.
	Hosts []HostSplunkLb `json:"hosts"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSplunkLb `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionSplunkLb `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSplunkLb `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsSplunkLb        `json:"pqControls,omitempty"`
	// Shared secret token to use when establishing a connection to a Splunk indexer.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret           *string        `json:"textSecret,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "hosts"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSplunkLb) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSplunkLb) GetType() TypeSplunkLb {
	if o == nil {
		return TypeSplunkLb("")
	}
	return o.Type
}

func (o *OutputSplunkLb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSplunkLb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSplunkLb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSplunkLb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSplunkLb) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSplunkLb) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputSplunkLb) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputSplunkLb) GetNestedFields() *NestedFieldSerializationSplunkLb {
	if o == nil {
		return nil
	}
	return o.NestedFields
}

func (o *OutputSplunkLb) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputSplunkLb) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputSplunkLb) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputSplunkLb) GetTLS() *TLSSettingsClientSideSplunkLb {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSplunkLb) GetEnableMultiMetrics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableMultiMetrics
}

func (o *OutputSplunkLb) GetEnableACK() *bool {
	if o == nil {
		return nil
	}
	return o.EnableACK
}

func (o *OutputSplunkLb) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputSplunkLb) GetMaxS2Sversion() *MaxS2SVersionSplunkLb {
	if o == nil {
		return nil
	}
	return o.MaxS2Sversion
}

func (o *OutputSplunkLb) GetOnBackpressure() *BackpressureBehaviorSplunkLb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSplunkLb) GetIndexerDiscovery() *bool {
	if o == nil {
		return nil
	}
	return o.IndexerDiscovery
}

func (o *OutputSplunkLb) GetSenderUnhealthyTimeAllowance() *float64 {
	if o == nil {
		return nil
	}
	return o.SenderUnhealthyTimeAllowance
}

func (o *OutputSplunkLb) GetAuthType() *AuthenticationMethodSplunkLb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSplunkLb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSplunkLb) GetMaxFailedHealthChecks() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFailedHealthChecks
}

func (o *OutputSplunkLb) GetCompress() *CompressCompressionSplunkLb {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSplunkLb) GetIndexerDiscoveryConfigs() *IndexerDiscoveryConfigs {
	if o == nil {
		return nil
	}
	return o.IndexerDiscoveryConfigs
}

func (o *OutputSplunkLb) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputSplunkLb) GetHosts() []HostSplunkLb {
	if o == nil {
		return []HostSplunkLb{}
	}
	return o.Hosts
}

func (o *OutputSplunkLb) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSplunkLb) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSplunkLb) GetPqMode() *ModeSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSplunkLb) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSplunkLb) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSplunkLb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSplunkLb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSplunkLb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSplunkLb) GetPqCompress() *PqCompressCompressionSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSplunkLb) GetPqOnBackpressure() *QueueFullBehaviorSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSplunkLb) GetPqControls() *PqControlsSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSplunkLb) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *OutputSplunkLb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSplunkLb) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeSplunk string

const (
	OutputTypeSplunkSplunk OutputTypeSplunk = "splunk"
)

func (e OutputTypeSplunk) ToPointer() *OutputTypeSplunk {
	return &e
}
func (e *OutputTypeSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk":
		*e = OutputTypeSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSplunk: %v", v)
	}
}

// NestedFieldSerializationSplunk - How to serialize nested fields into index-time fields
type NestedFieldSerializationSplunk string

const (
	// NestedFieldSerializationSplunkJSON JSON
	NestedFieldSerializationSplunkJSON NestedFieldSerializationSplunk = "json"
	// NestedFieldSerializationSplunkNone None
	NestedFieldSerializationSplunkNone NestedFieldSerializationSplunk = "none"
)

func (e NestedFieldSerializationSplunk) ToPointer() *NestedFieldSerializationSplunk {
	return &e
}

type OutputMinimumTLSVersionSplunk string

const (
	OutputMinimumTLSVersionSplunkTlSv1  OutputMinimumTLSVersionSplunk = "TLSv1"
	OutputMinimumTLSVersionSplunkTlSv11 OutputMinimumTLSVersionSplunk = "TLSv1.1"
	OutputMinimumTLSVersionSplunkTlSv12 OutputMinimumTLSVersionSplunk = "TLSv1.2"
	OutputMinimumTLSVersionSplunkTlSv13 OutputMinimumTLSVersionSplunk = "TLSv1.3"
)

func (e OutputMinimumTLSVersionSplunk) ToPointer() *OutputMinimumTLSVersionSplunk {
	return &e
}

type OutputMaximumTLSVersionSplunk string

const (
	OutputMaximumTLSVersionSplunkTlSv1  OutputMaximumTLSVersionSplunk = "TLSv1"
	OutputMaximumTLSVersionSplunkTlSv11 OutputMaximumTLSVersionSplunk = "TLSv1.1"
	OutputMaximumTLSVersionSplunkTlSv12 OutputMaximumTLSVersionSplunk = "TLSv1.2"
	OutputMaximumTLSVersionSplunkTlSv13 OutputMaximumTLSVersionSplunk = "TLSv1.3"
)

func (e OutputMaximumTLSVersionSplunk) ToPointer() *OutputMaximumTLSVersionSplunk {
	return &e
}

type TLSSettingsClientSideSplunk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                        `json:"passphrase,omitempty"`
	MinVersion *OutputMinimumTLSVersionSplunk `json:"minVersion,omitempty"`
	MaxVersion *OutputMaximumTLSVersionSplunk `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSplunk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSplunk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideSplunk) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideSplunk) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideSplunk) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideSplunk) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideSplunk) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideSplunk) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideSplunk) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideSplunk) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideSplunk) GetMinVersion() *OutputMinimumTLSVersionSplunk {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideSplunk) GetMaxVersion() *OutputMaximumTLSVersionSplunk {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// OutputMaxS2SVersionSplunk - The highest S2S protocol version to advertise during handshake
type OutputMaxS2SVersionSplunk string

const (
	OutputMaxS2SVersionSplunkV3 OutputMaxS2SVersionSplunk = "v3"
	OutputMaxS2SVersionSplunkV4 OutputMaxS2SVersionSplunk = "v4"
)

func (e OutputMaxS2SVersionSplunk) ToPointer() *OutputMaxS2SVersionSplunk {
	return &e
}

// BackpressureBehaviorSplunk - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSplunk string

const (
	// BackpressureBehaviorSplunkBlock Block
	BackpressureBehaviorSplunkBlock BackpressureBehaviorSplunk = "block"
	// BackpressureBehaviorSplunkDrop Drop
	BackpressureBehaviorSplunkDrop BackpressureBehaviorSplunk = "drop"
	// BackpressureBehaviorSplunkQueue Persistent Queue
	BackpressureBehaviorSplunkQueue BackpressureBehaviorSplunk = "queue"
)

func (e BackpressureBehaviorSplunk) ToPointer() *BackpressureBehaviorSplunk {
	return &e
}

// AuthenticationMethodSplunk - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSplunk string

const (
	AuthenticationMethodSplunkManual AuthenticationMethodSplunk = "manual"
	AuthenticationMethodSplunkSecret AuthenticationMethodSplunk = "secret"
)

func (e AuthenticationMethodSplunk) ToPointer() *AuthenticationMethodSplunk {
	return &e
}

// OutputCompressCompressionSplunk - Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
type OutputCompressCompressionSplunk string

const (
	// OutputCompressCompressionSplunkDisabled Disabled
	OutputCompressCompressionSplunkDisabled OutputCompressCompressionSplunk = "disabled"
	// OutputCompressCompressionSplunkAuto Automatic
	OutputCompressCompressionSplunkAuto OutputCompressCompressionSplunk = "auto"
	// OutputCompressCompressionSplunkAlways Always
	OutputCompressCompressionSplunkAlways OutputCompressCompressionSplunk = "always"
)

func (e OutputCompressCompressionSplunk) ToPointer() *OutputCompressCompressionSplunk {
	return &e
}

// OutputModeSplunk - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type OutputModeSplunk string

const (
	// OutputModeSplunkError Error
	OutputModeSplunkError OutputModeSplunk = "error"
	// OutputModeSplunkAlways Backpressure
	OutputModeSplunkAlways OutputModeSplunk = "always"
	// OutputModeSplunkBackpressure Always On
	OutputModeSplunkBackpressure OutputModeSplunk = "backpressure"
)

func (e OutputModeSplunk) ToPointer() *OutputModeSplunk {
	return &e
}

// PqCompressCompressionSplunk - Codec to use to compress the persisted data
type PqCompressCompressionSplunk string

const (
	// PqCompressCompressionSplunkNone None
	PqCompressCompressionSplunkNone PqCompressCompressionSplunk = "none"
	// PqCompressCompressionSplunkGzip Gzip
	PqCompressCompressionSplunkGzip PqCompressCompressionSplunk = "gzip"
)

func (e PqCompressCompressionSplunk) ToPointer() *PqCompressCompressionSplunk {
	return &e
}

// QueueFullBehaviorSplunk - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSplunk string

const (
	// QueueFullBehaviorSplunkBlock Block
	QueueFullBehaviorSplunkBlock QueueFullBehaviorSplunk = "block"
	// QueueFullBehaviorSplunkDrop Drop new data
	QueueFullBehaviorSplunkDrop QueueFullBehaviorSplunk = "drop"
)

func (e QueueFullBehaviorSplunk) ToPointer() *QueueFullBehaviorSplunk {
	return &e
}

type OutputPqControlsSplunk struct {
}

func (o OutputPqControlsSplunk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPqControlsSplunk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSplunk struct {
	// Unique ID for this output
	ID   *string          `json:"id,omitempty"`
	Type OutputTypeSplunk `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port *float64 `default:"9997" json:"port"`
	// How to serialize nested fields into index-time fields
	NestedFields *NestedFieldSerializationSplunk `default:"none" json:"nestedFields"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64                     `default:"60000" json:"writeTimeout"`
	TLS          *TLSSettingsClientSideSplunk `json:"tls,omitempty"`
	// Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.
	EnableMultiMetrics *bool `default:"false" json:"enableMultiMetrics"`
	// Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.
	EnableACK *bool `default:"true" json:"enableACK"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// The highest S2S protocol version to advertise during handshake
	MaxS2Sversion *OutputMaxS2SVersionSplunk `default:"v3" json:"maxS2Sversion"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSplunk `default:"block" json:"onBackpressure"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType    *AuthenticationMethodSplunk `default:"manual" json:"authType"`
	Description *string                     `json:"description,omitempty"`
	// Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.
	MaxFailedHealthChecks *float64 `default:"1" json:"maxFailedHealthChecks"`
	// Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
	Compress *OutputCompressCompressionSplunk `default:"disabled" json:"compress"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *OutputModeSplunk `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *PqCompressCompressionSplunk `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSplunk `default:"block" json:"pqOnBackpressure"`
	PqControls       *OutputPqControlsSplunk  `json:"pqControls,omitempty"`
	// Shared secret token to use when establishing a connection to a Splunk indexer.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret           *string        `json:"textSecret,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputSplunk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSplunk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "host"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSplunk) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSplunk) GetType() OutputTypeSplunk {
	if o == nil {
		return OutputTypeSplunk("")
	}
	return o.Type
}

func (o *OutputSplunk) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSplunk) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSplunk) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSplunk) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSplunk) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputSplunk) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputSplunk) GetNestedFields() *NestedFieldSerializationSplunk {
	if o == nil {
		return nil
	}
	return o.NestedFields
}

func (o *OutputSplunk) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputSplunk) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputSplunk) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputSplunk) GetTLS() *TLSSettingsClientSideSplunk {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSplunk) GetEnableMultiMetrics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableMultiMetrics
}

func (o *OutputSplunk) GetEnableACK() *bool {
	if o == nil {
		return nil
	}
	return o.EnableACK
}

func (o *OutputSplunk) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputSplunk) GetMaxS2Sversion() *OutputMaxS2SVersionSplunk {
	if o == nil {
		return nil
	}
	return o.MaxS2Sversion
}

func (o *OutputSplunk) GetOnBackpressure() *BackpressureBehaviorSplunk {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSplunk) GetAuthType() *AuthenticationMethodSplunk {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSplunk) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSplunk) GetMaxFailedHealthChecks() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFailedHealthChecks
}

func (o *OutputSplunk) GetCompress() *OutputCompressCompressionSplunk {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSplunk) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSplunk) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSplunk) GetPqMode() *OutputModeSplunk {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSplunk) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSplunk) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSplunk) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSplunk) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSplunk) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSplunk) GetPqCompress() *PqCompressCompressionSplunk {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSplunk) GetPqOnBackpressure() *QueueFullBehaviorSplunk {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSplunk) GetPqControls() *OutputPqControlsSplunk {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSplunk) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *OutputSplunk) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSplunk) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeSyslog string

const (
	OutputTypeSyslogSyslog OutputTypeSyslog = "syslog"
)

func (e OutputTypeSyslog) ToPointer() *OutputTypeSyslog {
	return &e
}
func (e *OutputTypeSyslog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "syslog":
		*e = OutputTypeSyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSyslog: %v", v)
	}
}

// ProtocolSyslog - The network protocol to use for sending out syslog messages
type ProtocolSyslog string

const (
	// ProtocolSyslogTCP TCP
	ProtocolSyslogTCP ProtocolSyslog = "tcp"
	// ProtocolSyslogUDP UDP
	ProtocolSyslogUDP ProtocolSyslog = "udp"
)

func (e ProtocolSyslog) ToPointer() *ProtocolSyslog {
	return &e
}

// Facility - Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.
type Facility int64

const (
	FacilityZero      Facility = 0
	FacilityOne       Facility = 1
	FacilityTwo       Facility = 2
	FacilityThree     Facility = 3
	FacilityFour      Facility = 4
	FacilityFive      Facility = 5
	FacilitySix       Facility = 6
	FacilitySeven     Facility = 7
	FacilityEight     Facility = 8
	FacilityNine      Facility = 9
	FacilityTen       Facility = 10
	FacilityEleven    Facility = 11
	FacilityTwelve    Facility = 12
	FacilityThirteen  Facility = 13
	FacilityFourteen  Facility = 14
	FacilityFifteen   Facility = 15
	FacilitySixteen   Facility = 16
	FacilitySeventeen Facility = 17
	FacilityEighteen  Facility = 18
	FacilityNineteen  Facility = 19
	FacilityTwenty    Facility = 20
	FacilityTwentyOne Facility = 21
)

func (e Facility) ToPointer() *Facility {
	return &e
}

// SeveritySyslog - Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.
type SeveritySyslog int64

const (
	// SeveritySyslogZero emergency
	SeveritySyslogZero SeveritySyslog = 0
	// SeveritySyslogOne alert
	SeveritySyslogOne SeveritySyslog = 1
	// SeveritySyslogTwo critical
	SeveritySyslogTwo SeveritySyslog = 2
	// SeveritySyslogThree error
	SeveritySyslogThree SeveritySyslog = 3
	// SeveritySyslogFour warning
	SeveritySyslogFour SeveritySyslog = 4
	// SeveritySyslogFive notice
	SeveritySyslogFive SeveritySyslog = 5
	// SeveritySyslogSix info
	SeveritySyslogSix SeveritySyslog = 6
	// SeveritySyslogSeven debug
	SeveritySyslogSeven SeveritySyslog = 7
)

func (e SeveritySyslog) ToPointer() *SeveritySyslog {
	return &e
}

// MessageFormatSyslog - The syslog message format depending on the receiver's support
type MessageFormatSyslog string

const (
	// MessageFormatSyslogRfc3164 RFC3164
	MessageFormatSyslogRfc3164 MessageFormatSyslog = "rfc3164"
	// MessageFormatSyslogRfc5424 RFC5424
	MessageFormatSyslogRfc5424 MessageFormatSyslog = "rfc5424"
)

func (e MessageFormatSyslog) ToPointer() *MessageFormatSyslog {
	return &e
}

// TimestampFormat - Timestamp format to use when serializing event's time field
type TimestampFormat string

const (
	// TimestampFormatSyslog Syslog
	TimestampFormatSyslog TimestampFormat = "syslog"
	// TimestampFormatIso8601 ISO8601
	TimestampFormatIso8601 TimestampFormat = "iso8601"
)

func (e TimestampFormat) ToPointer() *TimestampFormat {
	return &e
}

// TLSSyslog - Whether to inherit TLS configs from group setting or disable TLS
type TLSSyslog string

const (
	TLSSyslogInherit TLSSyslog = "inherit"
	TLSSyslogOff     TLSSyslog = "off"
)

func (e TLSSyslog) ToPointer() *TLSSyslog {
	return &e
}

type HostSyslog struct {
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port float64 `json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS
	TLS *TLSSyslog `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostSyslog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostSyslog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, []string{"host", "port"}); err != nil {
		return err
	}
	return nil
}

func (h *HostSyslog) GetHost() string {
	if h == nil {
		return ""
	}
	return h.Host
}

func (h *HostSyslog) GetPort() float64 {
	if h == nil {
		return 0.0
	}
	return h.Port
}

func (h *HostSyslog) GetTLS() *TLSSyslog {
	if h == nil {
		return nil
	}
	return h.TLS
}

func (h *HostSyslog) GetServername() *string {
	if h == nil {
		return nil
	}
	return h.Servername
}

func (h *HostSyslog) GetWeight() *float64 {
	if h == nil {
		return nil
	}
	return h.Weight
}

type MinimumTLSVersionSyslog string

const (
	MinimumTLSVersionSyslogTlSv1  MinimumTLSVersionSyslog = "TLSv1"
	MinimumTLSVersionSyslogTlSv11 MinimumTLSVersionSyslog = "TLSv1.1"
	MinimumTLSVersionSyslogTlSv12 MinimumTLSVersionSyslog = "TLSv1.2"
	MinimumTLSVersionSyslogTlSv13 MinimumTLSVersionSyslog = "TLSv1.3"
)

func (e MinimumTLSVersionSyslog) ToPointer() *MinimumTLSVersionSyslog {
	return &e
}

type MaximumTLSVersionSyslog string

const (
	MaximumTLSVersionSyslogTlSv1  MaximumTLSVersionSyslog = "TLSv1"
	MaximumTLSVersionSyslogTlSv11 MaximumTLSVersionSyslog = "TLSv1.1"
	MaximumTLSVersionSyslogTlSv12 MaximumTLSVersionSyslog = "TLSv1.2"
	MaximumTLSVersionSyslogTlSv13 MaximumTLSVersionSyslog = "TLSv1.3"
)

func (e MaximumTLSVersionSyslog) ToPointer() *MaximumTLSVersionSyslog {
	return &e
}

type TLSSettingsClientSideSyslog struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certificates that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                  `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionSyslog `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionSyslog `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSyslog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSyslog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideSyslog) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideSyslog) GetRejectUnauthorized() *bool {
	if t == nil {
		return nil
	}
	return t.RejectUnauthorized
}

func (t *TLSSettingsClientSideSyslog) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideSyslog) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideSyslog) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideSyslog) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideSyslog) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideSyslog) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideSyslog) GetMinVersion() *MinimumTLSVersionSyslog {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideSyslog) GetMaxVersion() *MaximumTLSVersionSyslog {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// BackpressureBehaviorSyslog - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSyslog string

const (
	// BackpressureBehaviorSyslogBlock Block
	BackpressureBehaviorSyslogBlock BackpressureBehaviorSyslog = "block"
	// BackpressureBehaviorSyslogDrop Drop
	BackpressureBehaviorSyslogDrop BackpressureBehaviorSyslog = "drop"
	// BackpressureBehaviorSyslogQueue Persistent Queue
	BackpressureBehaviorSyslogQueue BackpressureBehaviorSyslog = "queue"
)

func (e BackpressureBehaviorSyslog) ToPointer() *BackpressureBehaviorSyslog {
	return &e
}

// ModeSyslog - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSyslog string

const (
	// ModeSyslogError Error
	ModeSyslogError ModeSyslog = "error"
	// ModeSyslogAlways Backpressure
	ModeSyslogAlways ModeSyslog = "always"
	// ModeSyslogBackpressure Always On
	ModeSyslogBackpressure ModeSyslog = "backpressure"
)

func (e ModeSyslog) ToPointer() *ModeSyslog {
	return &e
}

// CompressionSyslog - Codec to use to compress the persisted data
type CompressionSyslog string

const (
	// CompressionSyslogNone None
	CompressionSyslogNone CompressionSyslog = "none"
	// CompressionSyslogGzip Gzip
	CompressionSyslogGzip CompressionSyslog = "gzip"
)

func (e CompressionSyslog) ToPointer() *CompressionSyslog {
	return &e
}

// QueueFullBehaviorSyslog - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSyslog string

const (
	// QueueFullBehaviorSyslogBlock Block
	QueueFullBehaviorSyslogBlock QueueFullBehaviorSyslog = "block"
	// QueueFullBehaviorSyslogDrop Drop new data
	QueueFullBehaviorSyslogDrop QueueFullBehaviorSyslog = "drop"
)

func (e QueueFullBehaviorSyslog) ToPointer() *QueueFullBehaviorSyslog {
	return &e
}

type PqControlsSyslog struct {
}

func (p PqControlsSyslog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSyslog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSyslog struct {
	// Unique ID for this output
	ID   *string          `json:"id,omitempty"`
	Type OutputTypeSyslog `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The network protocol to use for sending out syslog messages
	Protocol *ProtocolSyslog `default:"tcp" json:"protocol"`
	// Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.
	Facility *Facility `default:"1" json:"facility"`
	// Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.
	Severity *SeveritySyslog `default:"5" json:"severity"`
	// Default name for device or application that originated the message. Defaults to Cribl, but will be overwritten by value of __appname if set.
	AppName *string `default:"Cribl" json:"appName"`
	// The syslog message format depending on the receiver's support
	MessageFormat *MessageFormatSyslog `default:"rfc3164" json:"messageFormat"`
	// Timestamp format to use when serializing event's time field
	TimestampFormat *TimestampFormat `default:"syslog" json:"timestampFormat"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Prefix messages with the byte count of the message. If disabled, no prefix will be set, and the message will be appended with a \n.
	OctetCountFraming *bool `json:"octetCountFraming,omitempty"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool   `default:"false" json:"logFailedRequests"`
	Description       *string `json:"description,omitempty"`
	// For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs.  If this setting is disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// The hostname of the receiver
	Host *string `json:"host,omitempty"`
	// The port to connect to on the provided host
	Port *float64 `json:"port,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of hosts to load-balance data to
	Hosts []HostSyslog `json:"hosts,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64                     `default:"60000" json:"writeTimeout"`
	TLS          *TLSSettingsClientSideSyslog `json:"tls,omitempty"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSyslog `default:"block" json:"onBackpressure"`
	// Maximum size of syslog messages. Make sure this value is less than or equal to the MTU to avoid UDP packet fragmentation.
	MaxRecordSize *float64 `default:"1500" json:"maxRecordSize"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every message sent will incur a DNS lookup.
	UDPDNSResolvePeriodSec *float64 `default:"0" json:"udpDnsResolvePeriodSec"`
	// Send Syslog traffic using the original event's Source IP and port. To enable this, you must install the external `udp-sender` helper binary at `/usr/bin/udp-sender` on all Worker Nodes and grant it the `CAP_NET_RAW` capability.
	EnableIPSpoofing *bool `default:"false" json:"enableIpSpoofing"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSyslog `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSyslog `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure     *QueueFullBehaviorSyslog `default:"block" json:"pqOnBackpressure"`
	PqControls           *PqControlsSyslog        `json:"pqControls,omitempty"`
	AdditionalProperties map[string]any           `additionalProperties:"true" json:"-"`
}

func (o OutputSyslog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSyslog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSyslog) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSyslog) GetType() OutputTypeSyslog {
	if o == nil {
		return OutputTypeSyslog("")
	}
	return o.Type
}

func (o *OutputSyslog) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSyslog) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSyslog) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSyslog) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSyslog) GetProtocol() *ProtocolSyslog {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputSyslog) GetFacility() *Facility {
	if o == nil {
		return nil
	}
	return o.Facility
}

func (o *OutputSyslog) GetSeverity() *SeveritySyslog {
	if o == nil {
		return nil
	}
	return o.Severity
}

func (o *OutputSyslog) GetAppName() *string {
	if o == nil {
		return nil
	}
	return o.AppName
}

func (o *OutputSyslog) GetMessageFormat() *MessageFormatSyslog {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputSyslog) GetTimestampFormat() *TimestampFormat {
	if o == nil {
		return nil
	}
	return o.TimestampFormat
}

func (o *OutputSyslog) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputSyslog) GetOctetCountFraming() *bool {
	if o == nil {
		return nil
	}
	return o.OctetCountFraming
}

func (o *OutputSyslog) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputSyslog) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSyslog) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputSyslog) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputSyslog) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputSyslog) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputSyslog) GetHosts() []HostSyslog {
	if o == nil {
		return nil
	}
	return o.Hosts
}

func (o *OutputSyslog) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSyslog) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputSyslog) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputSyslog) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputSyslog) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputSyslog) GetTLS() *TLSSettingsClientSideSyslog {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSyslog) GetOnBackpressure() *BackpressureBehaviorSyslog {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSyslog) GetMaxRecordSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSize
}

func (o *OutputSyslog) GetUDPDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.UDPDNSResolvePeriodSec
}

func (o *OutputSyslog) GetEnableIPSpoofing() *bool {
	if o == nil {
		return nil
	}
	return o.EnableIPSpoofing
}

func (o *OutputSyslog) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSyslog) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSyslog) GetPqMode() *ModeSyslog {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSyslog) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSyslog) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSyslog) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSyslog) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSyslog) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSyslog) GetPqCompress() *CompressionSyslog {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSyslog) GetPqOnBackpressure() *QueueFullBehaviorSyslog {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSyslog) GetPqControls() *PqControlsSyslog {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSyslog) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeDevnull string

const (
	TypeDevnullDevnull TypeDevnull = "devnull"
)

func (e TypeDevnull) ToPointer() *TypeDevnull {
	return &e
}
func (e *TypeDevnull) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "devnull":
		*e = TypeDevnull(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDevnull: %v", v)
	}
}

type OutputDevnull struct {
	// Unique ID for this output
	ID   *string     `json:"id,omitempty"`
	Type TypeDevnull `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags           []string       `json:"streamtags,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputDevnull) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDevnull) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDevnull) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDevnull) GetType() TypeDevnull {
	if o == nil {
		return TypeDevnull("")
	}
	return o.Type
}

func (o *OutputDevnull) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDevnull) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDevnull) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDevnull) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDevnull) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeSentinel string

const (
	TypeSentinelSentinel TypeSentinel = "sentinel"
)

func (e TypeSentinel) ToPointer() *TypeSentinel {
	return &e
}
func (e *TypeSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sentinel":
		*e = TypeSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSentinel: %v", v)
	}
}

type ExtraHTTPHeaderSentinel struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderSentinel) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderSentinel) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeSentinel - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSentinel string

const (
	// FailedRequestLoggingModeSentinelPayload Payload
	FailedRequestLoggingModeSentinelPayload FailedRequestLoggingModeSentinel = "payload"
	// FailedRequestLoggingModeSentinelPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeSentinelPayloadAndHeaders FailedRequestLoggingModeSentinel = "payloadAndHeaders"
	// FailedRequestLoggingModeSentinelNone None
	FailedRequestLoggingModeSentinelNone FailedRequestLoggingModeSentinel = "none"
)

func (e FailedRequestLoggingModeSentinel) ToPointer() *FailedRequestLoggingModeSentinel {
	return &e
}

type ResponseRetrySettingSentinel struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingSentinel) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingSentinel) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingSentinel) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingSentinel) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsSentinel struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsSentinel) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsSentinel) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsSentinel) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsSentinel) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorSentinel - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorSentinel string

const (
	// BackpressureBehaviorSentinelBlock Block
	BackpressureBehaviorSentinelBlock BackpressureBehaviorSentinel = "block"
	// BackpressureBehaviorSentinelDrop Drop
	BackpressureBehaviorSentinelDrop BackpressureBehaviorSentinel = "drop"
	// BackpressureBehaviorSentinelQueue Persistent Queue
	BackpressureBehaviorSentinelQueue BackpressureBehaviorSentinel = "queue"
)

func (e BackpressureBehaviorSentinel) ToPointer() *BackpressureBehaviorSentinel {
	return &e
}

type AuthType string

const (
	AuthTypeOauth AuthType = "oauth"
)

func (e AuthType) ToPointer() *AuthType {
	return &e
}

// EndpointConfiguration - Enter the data collection endpoint URL or the individual ID
type EndpointConfiguration string

const (
	// EndpointConfigurationURL URL
	EndpointConfigurationURL EndpointConfiguration = "url"
	// EndpointConfigurationID ID
	EndpointConfigurationID EndpointConfiguration = "ID"
)

func (e EndpointConfiguration) ToPointer() *EndpointConfiguration {
	return &e
}

type FormatSentinel string

const (
	FormatSentinelNdjson    FormatSentinel = "ndjson"
	FormatSentinelJSONArray FormatSentinel = "json_array"
	FormatSentinelCustom    FormatSentinel = "custom"
	FormatSentinelAdvanced  FormatSentinel = "advanced"
)

func (e FormatSentinel) ToPointer() *FormatSentinel {
	return &e
}

// ModeSentinel - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeSentinel string

const (
	// ModeSentinelError Error
	ModeSentinelError ModeSentinel = "error"
	// ModeSentinelAlways Backpressure
	ModeSentinelAlways ModeSentinel = "always"
	// ModeSentinelBackpressure Always On
	ModeSentinelBackpressure ModeSentinel = "backpressure"
)

func (e ModeSentinel) ToPointer() *ModeSentinel {
	return &e
}

// CompressionSentinel - Codec to use to compress the persisted data
type CompressionSentinel string

const (
	// CompressionSentinelNone None
	CompressionSentinelNone CompressionSentinel = "none"
	// CompressionSentinelGzip Gzip
	CompressionSentinelGzip CompressionSentinel = "gzip"
)

func (e CompressionSentinel) ToPointer() *CompressionSentinel {
	return &e
}

// QueueFullBehaviorSentinel - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSentinel string

const (
	// QueueFullBehaviorSentinelBlock Block
	QueueFullBehaviorSentinelBlock QueueFullBehaviorSentinel = "block"
	// QueueFullBehaviorSentinelDrop Drop new data
	QueueFullBehaviorSentinelDrop QueueFullBehaviorSentinel = "drop"
)

func (e QueueFullBehaviorSentinel) ToPointer() *QueueFullBehaviorSentinel {
	return &e
}

type PqControlsSentinel struct {
}

func (p PqControlsSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OutputSentinel struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type TypeSentinel `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB)
	MaxPayloadSizeKB *float64 `default:"1000" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
	ExtraHTTPHeaders []ExtraHTTPHeaderSentinel `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSentinel `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingSentinel `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSentinel  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSentinel `default:"block" json:"onBackpressure"`
	AuthType       *AuthType                     `json:"authType,omitempty"`
	// URL for OAuth
	LoginURL string `json:"loginUrl"`
	// Secret parameter value to pass in request body
	Secret string `json:"secret"`
	// JavaScript expression to compute the Client ID for the Azure application. Can be a constant.
	ClientID string `json:"client_id"`
	// Scope to pass in the OAuth request
	Scope *string `default:"https://monitor.azure.com/.default" json:"scope"`
	// Enter the data collection endpoint URL or the individual ID
	EndpointURLConfiguration *EndpointConfiguration `default:"url" json:"endpointURLConfiguration"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64        `json:"totalMemoryLimitKB,omitempty"`
	Description        *string         `json:"description,omitempty"`
	Format             *FormatSentinel `json:"format,omitempty"`
	// Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.
	CustomSourceExpression *string `default:"__httpOut" json:"customSourceExpression"`
	// Whether to drop events when the source expression evaluates to null
	CustomDropWhenNull *bool `default:"false" json:"customDropWhenNull"`
	// Delimiter string to insert between individual events. Defaults to newline character.
	CustomEventDelimiter *string `default:"\\n" json:"customEventDelimiter"`
	// Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.
	CustomContentType *string `default:"application/x-ndjson" json:"customContentType"`
	// Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.
	CustomPayloadExpression *string `default:"\\${events}" json:"customPayloadExpression"`
	// HTTP content-type header value
	AdvancedContentType *string `default:"application/json" json:"advancedContentType"`
	// Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatEventCode *string `json:"formatEventCode,omitempty"`
	// Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatPayloadCode *string `json:"formatPayloadCode,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeSentinel `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionSentinel `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSentinel `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsSentinel        `json:"pqControls,omitempty"`
	// URL to send events to. Can be overwritten by an event's __url field.
	URL *string `json:"url,omitempty"`
	// Immutable ID for the Data Collection Rule (DCR)
	DcrID *string `json:"dcrID,omitempty"`
	// Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`
	DceEndpoint *string `json:"dceEndpoint,omitempty"`
	// The name of the stream (Sentinel table) in which to store the events
	StreamName           *string        `json:"streamName,omitempty"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "loginUrl", "secret", "client_id"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputSentinel) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSentinel) GetType() TypeSentinel {
	if o == nil {
		return TypeSentinel("")
	}
	return o.Type
}

func (o *OutputSentinel) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSentinel) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSentinel) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSentinel) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSentinel) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputSentinel) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSentinel) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSentinel) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSentinel) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSentinel) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSentinel) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSentinel) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSentinel) GetExtraHTTPHeaders() []ExtraHTTPHeaderSentinel {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSentinel) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSentinel) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSentinel {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSentinel) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSentinel) GetResponseRetrySettings() []ResponseRetrySettingSentinel {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSentinel) GetTimeoutRetrySettings() *TimeoutRetrySettingsSentinel {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSentinel) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSentinel) GetOnBackpressure() *BackpressureBehaviorSentinel {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSentinel) GetAuthType() *AuthType {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSentinel) GetLoginURL() string {
	if o == nil {
		return ""
	}
	return o.LoginURL
}

func (o *OutputSentinel) GetSecret() string {
	if o == nil {
		return ""
	}
	return o.Secret
}

func (o *OutputSentinel) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputSentinel) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputSentinel) GetEndpointURLConfiguration() *EndpointConfiguration {
	if o == nil {
		return nil
	}
	return o.EndpointURLConfiguration
}

func (o *OutputSentinel) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputSentinel) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSentinel) GetFormat() *FormatSentinel {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputSentinel) GetCustomSourceExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomSourceExpression
}

func (o *OutputSentinel) GetCustomDropWhenNull() *bool {
	if o == nil {
		return nil
	}
	return o.CustomDropWhenNull
}

func (o *OutputSentinel) GetCustomEventDelimiter() *string {
	if o == nil {
		return nil
	}
	return o.CustomEventDelimiter
}

func (o *OutputSentinel) GetCustomContentType() *string {
	if o == nil {
		return nil
	}
	return o.CustomContentType
}

func (o *OutputSentinel) GetCustomPayloadExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomPayloadExpression
}

func (o *OutputSentinel) GetAdvancedContentType() *string {
	if o == nil {
		return nil
	}
	return o.AdvancedContentType
}

func (o *OutputSentinel) GetFormatEventCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatEventCode
}

func (o *OutputSentinel) GetFormatPayloadCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatPayloadCode
}

func (o *OutputSentinel) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputSentinel) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputSentinel) GetPqMode() *ModeSentinel {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSentinel) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputSentinel) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputSentinel) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSentinel) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSentinel) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSentinel) GetPqCompress() *CompressionSentinel {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSentinel) GetPqOnBackpressure() *QueueFullBehaviorSentinel {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSentinel) GetPqControls() *PqControlsSentinel {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSentinel) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputSentinel) GetDcrID() *string {
	if o == nil {
		return nil
	}
	return o.DcrID
}

func (o *OutputSentinel) GetDceEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.DceEndpoint
}

func (o *OutputSentinel) GetStreamName() *string {
	if o == nil {
		return nil
	}
	return o.StreamName
}

func (o *OutputSentinel) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeWebhook string

const (
	TypeWebhookWebhook TypeWebhook = "webhook"
)

func (e TypeWebhook) ToPointer() *TypeWebhook {
	return &e
}
func (e *TypeWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "webhook":
		*e = TypeWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeWebhook: %v", v)
	}
}

// MethodWebhook - The method to use when sending events
type MethodWebhook string

const (
	MethodWebhookPost  MethodWebhook = "POST"
	MethodWebhookPut   MethodWebhook = "PUT"
	MethodWebhookPatch MethodWebhook = "PATCH"
)

func (e MethodWebhook) ToPointer() *MethodWebhook {
	return &e
}

// FormatWebhook - How to format events before sending out
type FormatWebhook string

const (
	// FormatWebhookNdjson NDJSON (Newline Delimited JSON)
	FormatWebhookNdjson FormatWebhook = "ndjson"
	// FormatWebhookJSONArray JSON Array
	FormatWebhookJSONArray FormatWebhook = "json_array"
	// FormatWebhookCustom Custom
	FormatWebhookCustom FormatWebhook = "custom"
	// FormatWebhookAdvanced Advanced
	FormatWebhookAdvanced FormatWebhook = "advanced"
)

func (e FormatWebhook) ToPointer() *FormatWebhook {
	return &e
}

type ExtraHTTPHeaderWebhook struct {
	Name  *string `json:"name,omitempty"`
	Value string  `json:"value"`
}

func (e ExtraHTTPHeaderWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(e, "", false)
}

func (e *ExtraHTTPHeaderWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &e, "", false, []string{"value"}); err != nil {
		return err
	}
	return nil
}

func (e *ExtraHTTPHeaderWebhook) GetName() *string {
	if e == nil {
		return nil
	}
	return e.Name
}

func (e *ExtraHTTPHeaderWebhook) GetValue() string {
	if e == nil {
		return ""
	}
	return e.Value
}

// FailedRequestLoggingModeWebhook - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeWebhook string

const (
	// FailedRequestLoggingModeWebhookPayload Payload
	FailedRequestLoggingModeWebhookPayload FailedRequestLoggingModeWebhook = "payload"
	// FailedRequestLoggingModeWebhookPayloadAndHeaders Payload + Headers
	FailedRequestLoggingModeWebhookPayloadAndHeaders FailedRequestLoggingModeWebhook = "payloadAndHeaders"
	// FailedRequestLoggingModeWebhookNone None
	FailedRequestLoggingModeWebhookNone FailedRequestLoggingModeWebhook = "none"
)

func (e FailedRequestLoggingModeWebhook) ToPointer() *FailedRequestLoggingModeWebhook {
	return &e
}

type ResponseRetrySettingWebhook struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, []string{"httpStatus"}); err != nil {
		return err
	}
	return nil
}

func (r *ResponseRetrySettingWebhook) GetHTTPStatus() float64 {
	if r == nil {
		return 0.0
	}
	return r.HTTPStatus
}

func (r *ResponseRetrySettingWebhook) GetInitialBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.InitialBackoff
}

func (r *ResponseRetrySettingWebhook) GetBackoffRate() *float64 {
	if r == nil {
		return nil
	}
	return r.BackoffRate
}

func (r *ResponseRetrySettingWebhook) GetMaxBackoff() *float64 {
	if r == nil {
		return nil
	}
	return r.MaxBackoff
}

type TimeoutRetrySettingsWebhook struct {
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TimeoutRetrySettingsWebhook) GetTimeoutRetry() *bool {
	if t == nil {
		return nil
	}
	return t.TimeoutRetry
}

func (t *TimeoutRetrySettingsWebhook) GetInitialBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.InitialBackoff
}

func (t *TimeoutRetrySettingsWebhook) GetBackoffRate() *float64 {
	if t == nil {
		return nil
	}
	return t.BackoffRate
}

func (t *TimeoutRetrySettingsWebhook) GetMaxBackoff() *float64 {
	if t == nil {
		return nil
	}
	return t.MaxBackoff
}

// BackpressureBehaviorWebhook - How to handle events when all receivers are exerting backpressure
type BackpressureBehaviorWebhook string

const (
	// BackpressureBehaviorWebhookBlock Block
	BackpressureBehaviorWebhookBlock BackpressureBehaviorWebhook = "block"
	// BackpressureBehaviorWebhookDrop Drop
	BackpressureBehaviorWebhookDrop BackpressureBehaviorWebhook = "drop"
	// BackpressureBehaviorWebhookQueue Persistent Queue
	BackpressureBehaviorWebhookQueue BackpressureBehaviorWebhook = "queue"
)

func (e BackpressureBehaviorWebhook) ToPointer() *BackpressureBehaviorWebhook {
	return &e
}

// AuthenticationTypeWebhook - Authentication method to use for the HTTP request
type AuthenticationTypeWebhook string

const (
	// AuthenticationTypeWebhookNone None
	AuthenticationTypeWebhookNone AuthenticationTypeWebhook = "none"
	// AuthenticationTypeWebhookBasic Basic
	AuthenticationTypeWebhookBasic AuthenticationTypeWebhook = "basic"
	// AuthenticationTypeWebhookCredentialsSecret Basic (credentials secret)
	AuthenticationTypeWebhookCredentialsSecret AuthenticationTypeWebhook = "credentialsSecret"
	// AuthenticationTypeWebhookToken Token
	AuthenticationTypeWebhookToken AuthenticationTypeWebhook = "token"
	// AuthenticationTypeWebhookTextSecret Token (text secret)
	AuthenticationTypeWebhookTextSecret AuthenticationTypeWebhook = "textSecret"
	// AuthenticationTypeWebhookOauth OAuth
	AuthenticationTypeWebhookOauth AuthenticationTypeWebhook = "oauth"
)

func (e AuthenticationTypeWebhook) ToPointer() *AuthenticationTypeWebhook {
	return &e
}

type MinimumTLSVersionWebhook string

const (
	MinimumTLSVersionWebhookTlSv1  MinimumTLSVersionWebhook = "TLSv1"
	MinimumTLSVersionWebhookTlSv11 MinimumTLSVersionWebhook = "TLSv1.1"
	MinimumTLSVersionWebhookTlSv12 MinimumTLSVersionWebhook = "TLSv1.2"
	MinimumTLSVersionWebhookTlSv13 MinimumTLSVersionWebhook = "TLSv1.3"
)

func (e MinimumTLSVersionWebhook) ToPointer() *MinimumTLSVersionWebhook {
	return &e
}

type MaximumTLSVersionWebhook string

const (
	MaximumTLSVersionWebhookTlSv1  MaximumTLSVersionWebhook = "TLSv1"
	MaximumTLSVersionWebhookTlSv11 MaximumTLSVersionWebhook = "TLSv1.1"
	MaximumTLSVersionWebhookTlSv12 MaximumTLSVersionWebhook = "TLSv1.2"
	MaximumTLSVersionWebhookTlSv13 MaximumTLSVersionWebhook = "TLSv1.3"
)

func (e MaximumTLSVersionWebhook) ToPointer() *MaximumTLSVersionWebhook {
	return &e
}

type TLSSettingsClientSideWebhook struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key
	Passphrase *string                   `json:"passphrase,omitempty"`
	MinVersion *MinimumTLSVersionWebhook `json:"minVersion,omitempty"`
	MaxVersion *MaximumTLSVersionWebhook `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (t *TLSSettingsClientSideWebhook) GetDisabled() *bool {
	if t == nil {
		return nil
	}
	return t.Disabled
}

func (t *TLSSettingsClientSideWebhook) GetServername() *string {
	if t == nil {
		return nil
	}
	return t.Servername
}

func (t *TLSSettingsClientSideWebhook) GetCertificateName() *string {
	if t == nil {
		return nil
	}
	return t.CertificateName
}

func (t *TLSSettingsClientSideWebhook) GetCaPath() *string {
	if t == nil {
		return nil
	}
	return t.CaPath
}

func (t *TLSSettingsClientSideWebhook) GetPrivKeyPath() *string {
	if t == nil {
		return nil
	}
	return t.PrivKeyPath
}

func (t *TLSSettingsClientSideWebhook) GetCertPath() *string {
	if t == nil {
		return nil
	}
	return t.CertPath
}

func (t *TLSSettingsClientSideWebhook) GetPassphrase() *string {
	if t == nil {
		return nil
	}
	return t.Passphrase
}

func (t *TLSSettingsClientSideWebhook) GetMinVersion() *MinimumTLSVersionWebhook {
	if t == nil {
		return nil
	}
	return t.MinVersion
}

func (t *TLSSettingsClientSideWebhook) GetMaxVersion() *MaximumTLSVersionWebhook {
	if t == nil {
		return nil
	}
	return t.MaxVersion
}

// ModeWebhook - In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
type ModeWebhook string

const (
	// ModeWebhookError Error
	ModeWebhookError ModeWebhook = "error"
	// ModeWebhookAlways Backpressure
	ModeWebhookAlways ModeWebhook = "always"
	// ModeWebhookBackpressure Always On
	ModeWebhookBackpressure ModeWebhook = "backpressure"
)

func (e ModeWebhook) ToPointer() *ModeWebhook {
	return &e
}

// CompressionWebhook - Codec to use to compress the persisted data
type CompressionWebhook string

const (
	// CompressionWebhookNone None
	CompressionWebhookNone CompressionWebhook = "none"
	// CompressionWebhookGzip Gzip
	CompressionWebhookGzip CompressionWebhook = "gzip"
)

func (e CompressionWebhook) ToPointer() *CompressionWebhook {
	return &e
}

// QueueFullBehaviorWebhook - How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorWebhook string

const (
	// QueueFullBehaviorWebhookBlock Block
	QueueFullBehaviorWebhookBlock QueueFullBehaviorWebhook = "block"
	// QueueFullBehaviorWebhookDrop Drop new data
	QueueFullBehaviorWebhookDrop QueueFullBehaviorWebhook = "drop"
)

func (e QueueFullBehaviorWebhook) ToPointer() *QueueFullBehaviorWebhook {
	return &e
}

type PqControlsWebhook struct {
}

func (p PqControlsWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(p, "", false)
}

func (p *PqControlsWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &p, "", false, nil); err != nil {
		return err
	}
	return nil
}

type OauthParamWebhook struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o OauthParamWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthParamWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthParamWebhook) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamWebhook) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderWebhook struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o OauthHeaderWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OauthHeaderWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"name", "value"}); err != nil {
		return err
	}
	return nil
}

func (o *OauthHeaderWebhook) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderWebhook) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type URLWebhook struct {
	// URL of a webhook endpoint to send events to, such as http://localhost:10200
	URL string `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, []string{"url"}); err != nil {
		return err
	}
	return nil
}

func (u *URLWebhook) GetURL() string {
	if u == nil {
		return ""
	}
	return u.URL
}

func (u *URLWebhook) GetWeight() *float64 {
	if u == nil {
		return nil
	}
	return u.Weight
}

type OutputWebhook struct {
	// Unique ID for this output
	ID   *string     `json:"id,omitempty"`
	Type TypeWebhook `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The method to use when sending events
	Method *MethodWebhook `default:"POST" json:"method"`
	// How to format events before sending out
	Format *FormatWebhook `default:"ndjson" json:"format"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Enabled by default. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
	ExtraHTTPHeaders []ExtraHTTPHeaderWebhook `json:"extraHttpHeaders,omitempty"`
	// Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeWebhook `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
	ResponseRetrySettings []ResponseRetrySettingWebhook `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsWebhook  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// How to handle events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorWebhook `default:"block" json:"onBackpressure"`
	// Authentication method to use for the HTTP request
	AuthType *AuthenticationTypeWebhook    `default:"none" json:"authType"`
	TLS      *TLSSettingsClientSideWebhook `json:"tls,omitempty"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool   `default:"false" json:"loadBalanced"`
	Description  *string `json:"description,omitempty"`
	// Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.
	CustomSourceExpression *string `default:"__httpOut" json:"customSourceExpression"`
	// Whether to drop events when the source expression evaluates to null
	CustomDropWhenNull *bool `default:"false" json:"customDropWhenNull"`
	// Delimiter string to insert between individual events. Defaults to newline character.
	CustomEventDelimiter *string `default:"\\n" json:"customEventDelimiter"`
	// Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.
	CustomContentType *string `default:"application/x-ndjson" json:"customContentType"`
	// Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.
	CustomPayloadExpression *string `default:"\\${events}" json:"customPayloadExpression"`
	// HTTP content-type header value
	AdvancedContentType *string `default:"application/json" json:"advancedContentType"`
	// Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatEventCode *string `json:"formatEventCode,omitempty"`
	// Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatPayloadCode *string `json:"formatPayloadCode,omitempty"`
	// Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
	PqStrictOrdering *bool `default:"true" json:"pqStrictOrdering"`
	// Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
	PqRatePerSec *float64 `default:"0" json:"pqRatePerSec"`
	// In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
	PqMode *ModeWebhook `default:"error" json:"pqMode"`
	// The maximum number of events to hold in memory before writing the events to disk
	PqMaxBufferSize *float64 `default:"42" json:"pqMaxBufferSize"`
	// How long (in seconds) to wait for backpressure to resolve before engaging the queue
	PqMaxBackpressureSec *float64 `default:"30" json:"pqMaxBackpressureSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data
	PqCompress *CompressionWebhook `default:"none" json:"pqCompress"`
	// How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorWebhook `default:"block" json:"pqOnBackpressure"`
	PqControls       *PqControlsWebhook        `json:"pqControls,omitempty"`
	Username         *string                   `json:"username,omitempty"`
	Password         *string                   `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamWebhook `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderWebhook `json:"oauthHeaders,omitempty"`
	// URL of a webhook endpoint to send events to, such as http://localhost:10200
	URL *string `json:"url,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames
	ExcludeSelf *bool        `default:"false" json:"excludeSelf"`
	Urls        []URLWebhook `json:"urls,omitempty"`
	// The interval in which to re-resolve any hostnames and pick up destinations from A records
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes
	LoadBalanceStatsPeriodSec *float64       `default:"300" json:"loadBalanceStatsPeriodSec"`
	AdditionalProperties      map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputWebhook) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputWebhook) GetType() TypeWebhook {
	if o == nil {
		return TypeWebhook("")
	}
	return o.Type
}

func (o *OutputWebhook) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputWebhook) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputWebhook) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputWebhook) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputWebhook) GetMethod() *MethodWebhook {
	if o == nil {
		return nil
	}
	return o.Method
}

func (o *OutputWebhook) GetFormat() *FormatWebhook {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputWebhook) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputWebhook) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputWebhook) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputWebhook) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputWebhook) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputWebhook) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputWebhook) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputWebhook) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputWebhook) GetExtraHTTPHeaders() []ExtraHTTPHeaderWebhook {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputWebhook) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputWebhook) GetFailedRequestLoggingMode() *FailedRequestLoggingModeWebhook {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputWebhook) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputWebhook) GetResponseRetrySettings() []ResponseRetrySettingWebhook {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputWebhook) GetTimeoutRetrySettings() *TimeoutRetrySettingsWebhook {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputWebhook) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputWebhook) GetOnBackpressure() *BackpressureBehaviorWebhook {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputWebhook) GetAuthType() *AuthenticationTypeWebhook {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputWebhook) GetTLS() *TLSSettingsClientSideWebhook {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputWebhook) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputWebhook) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputWebhook) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputWebhook) GetCustomSourceExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomSourceExpression
}

func (o *OutputWebhook) GetCustomDropWhenNull() *bool {
	if o == nil {
		return nil
	}
	return o.CustomDropWhenNull
}

func (o *OutputWebhook) GetCustomEventDelimiter() *string {
	if o == nil {
		return nil
	}
	return o.CustomEventDelimiter
}

func (o *OutputWebhook) GetCustomContentType() *string {
	if o == nil {
		return nil
	}
	return o.CustomContentType
}

func (o *OutputWebhook) GetCustomPayloadExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomPayloadExpression
}

func (o *OutputWebhook) GetAdvancedContentType() *string {
	if o == nil {
		return nil
	}
	return o.AdvancedContentType
}

func (o *OutputWebhook) GetFormatEventCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatEventCode
}

func (o *OutputWebhook) GetFormatPayloadCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatPayloadCode
}

func (o *OutputWebhook) GetPqStrictOrdering() *bool {
	if o == nil {
		return nil
	}
	return o.PqStrictOrdering
}

func (o *OutputWebhook) GetPqRatePerSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqRatePerSec
}

func (o *OutputWebhook) GetPqMode() *ModeWebhook {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputWebhook) GetPqMaxBufferSize() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBufferSize
}

func (o *OutputWebhook) GetPqMaxBackpressureSec() *float64 {
	if o == nil {
		return nil
	}
	return o.PqMaxBackpressureSec
}

func (o *OutputWebhook) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputWebhook) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputWebhook) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputWebhook) GetPqCompress() *CompressionWebhook {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputWebhook) GetPqOnBackpressure() *QueueFullBehaviorWebhook {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputWebhook) GetPqControls() *PqControlsWebhook {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputWebhook) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputWebhook) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputWebhook) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputWebhook) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputWebhook) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputWebhook) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputWebhook) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputWebhook) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputWebhook) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputWebhook) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputWebhook) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputWebhook) GetOauthParams() []OauthParamWebhook {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputWebhook) GetOauthHeaders() []OauthHeaderWebhook {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputWebhook) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputWebhook) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputWebhook) GetUrls() []URLWebhook {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputWebhook) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputWebhook) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputWebhook) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type TypeDefault string

const (
	TypeDefaultDefault TypeDefault = "default"
)

func (e TypeDefault) ToPointer() *TypeDefault {
	return &e
}
func (e *TypeDefault) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "default":
		*e = TypeDefault(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDefault: %v", v)
	}
}

type OutputDefault struct {
	// Unique ID for this output
	ID   *string     `json:"id,omitempty"`
	Type TypeDefault `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.
	DefaultID            string         `json:"defaultId"`
	AdditionalProperties map[string]any `additionalProperties:"true" json:"-"`
}

func (o OutputDefault) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDefault) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, []string{"type", "defaultId"}); err != nil {
		return err
	}
	return nil
}

func (o *OutputDefault) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDefault) GetType() TypeDefault {
	if o == nil {
		return TypeDefault("")
	}
	return o.Type
}

func (o *OutputDefault) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDefault) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDefault) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDefault) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDefault) GetDefaultID() string {
	if o == nil {
		return ""
	}
	return o.DefaultID
}

func (o *OutputDefault) GetAdditionalProperties() map[string]any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputType string

const (
	OutputTypeDefault                OutputType = "default"
	OutputTypeWebhook                OutputType = "webhook"
	OutputTypeSentinel               OutputType = "sentinel"
	OutputTypeDevnull                OutputType = "devnull"
	OutputTypeSyslogValue            OutputType = "syslog"
	OutputTypeSplunkValue            OutputType = "splunk"
	OutputTypeSplunkLb               OutputType = "splunk_lb"
	OutputTypeSplunkHecValue         OutputType = "splunk_hec"
	OutputTypeTcpjsonValue           OutputType = "tcpjson"
	OutputTypeWavefront              OutputType = "wavefront"
	OutputTypeSignalfx               OutputType = "signalfx"
	OutputTypeFilesystem             OutputType = "filesystem"
	OutputTypeS3Value                OutputType = "s3"
	OutputTypeAzureBlobValue         OutputType = "azure_blob"
	OutputTypeAzureDataExplorer      OutputType = "azure_data_explorer"
	OutputTypeAzureLogs              OutputType = "azure_logs"
	OutputTypeKinesisValue           OutputType = "kinesis"
	OutputTypeHoneycomb              OutputType = "honeycomb"
	OutputTypeAzureEventhub          OutputType = "azure_eventhub"
	OutputTypeGoogleChronicle        OutputType = "google_chronicle"
	OutputTypeGoogleCloudStorage     OutputType = "google_cloud_storage"
	OutputTypeGoogleCloudLogging     OutputType = "google_cloud_logging"
	OutputTypeGooglePubsubValue      OutputType = "google_pubsub"
	OutputTypeExabeam                OutputType = "exabeam"
	OutputTypeKafkaValue             OutputType = "kafka"
	OutputTypeConfluentCloudValue    OutputType = "confluent_cloud"
	OutputTypeMskValue               OutputType = "msk"
	OutputTypeElasticValue           OutputType = "elastic"
	OutputTypeElasticCloud           OutputType = "elastic_cloud"
	OutputTypeNewrelic               OutputType = "newrelic"
	OutputTypeNewrelicEvents         OutputType = "newrelic_events"
	OutputTypeInfluxdb               OutputType = "influxdb"
	OutputTypeCloudwatch             OutputType = "cloudwatch"
	OutputTypeMinio                  OutputType = "minio"
	OutputTypeStatsd                 OutputType = "statsd"
	OutputTypeStatsdExt              OutputType = "statsd_ext"
	OutputTypeGraphite               OutputType = "graphite"
	OutputTypeRouter                 OutputType = "router"
	OutputTypeSns                    OutputType = "sns"
	OutputTypeSqsValue               OutputType = "sqs"
	OutputTypeSnmpValue              OutputType = "snmp"
	OutputTypeSumoLogic              OutputType = "sumo_logic"
	OutputTypeDatadog                OutputType = "datadog"
	OutputTypeGrafanaCloud           OutputType = "grafana_cloud"
	OutputTypeLokiValue              OutputType = "loki"
	OutputTypePrometheusValue        OutputType = "prometheus"
	OutputTypeRing                   OutputType = "ring"
	OutputTypeOpenTelemetryValue     OutputType = "open_telemetry"
	OutputTypeServiceNow             OutputType = "service_now"
	OutputTypeDataset                OutputType = "dataset"
	OutputTypeCriblTCPValue          OutputType = "cribl_tcp"
	OutputTypeCriblHTTPValue         OutputType = "cribl_http"
	OutputTypeHumioHec               OutputType = "humio_hec"
	OutputTypeCrowdstrikeNextGenSiem OutputType = "crowdstrike_next_gen_siem"
	OutputTypeDlS3                   OutputType = "dl_s3"
	OutputTypeSecurityLakeValue      OutputType = "security_lake"
	OutputTypeCriblLake              OutputType = "cribl_lake"
	OutputTypeDiskSpool              OutputType = "disk_spool"
	OutputTypeClickHouse             OutputType = "click_house"
	OutputTypeXsiam                  OutputType = "xsiam"
	OutputTypeNetflowValue           OutputType = "netflow"
	OutputTypeDynatraceHTTP          OutputType = "dynatrace_http"
	OutputTypeDynatraceOtlp          OutputType = "dynatrace_otlp"
	OutputTypeSentinelOneAiSiem      OutputType = "sentinel_one_ai_siem"
	OutputTypeChronicle              OutputType = "chronicle"
	OutputTypeDatabricks             OutputType = "databricks"
	OutputTypeMicrosoftFabric        OutputType = "microsoft_fabric"
	OutputTypeCloudflareR2           OutputType = "cloudflare_r2"
)

type Output struct {
	OutputDefault                *OutputDefault                `queryParam:"inline,name=Output"`
	OutputWebhook                *OutputWebhook                `queryParam:"inline,name=Output"`
	OutputSentinel               *OutputSentinel               `queryParam:"inline,name=Output"`
	OutputDevnull                *OutputDevnull                `queryParam:"inline,name=Output"`
	OutputSyslog                 *OutputSyslog                 `queryParam:"inline,name=Output"`
	OutputSplunk                 *OutputSplunk                 `queryParam:"inline,name=Output"`
	OutputSplunkLb               *OutputSplunkLb               `queryParam:"inline,name=Output"`
	OutputSplunkHec              *OutputSplunkHec              `queryParam:"inline,name=Output"`
	OutputTcpjson                *OutputTcpjson                `queryParam:"inline,name=Output"`
	OutputWavefront              *OutputWavefront              `queryParam:"inline,name=Output"`
	OutputSignalfx               *OutputSignalfx               `queryParam:"inline,name=Output"`
	OutputFilesystem             *OutputFilesystem             `queryParam:"inline,name=Output"`
	OutputS3                     *OutputS3                     `queryParam:"inline,name=Output"`
	OutputAzureBlob              *OutputAzureBlob              `queryParam:"inline,name=Output"`
	OutputAzureDataExplorer      *OutputAzureDataExplorer      `queryParam:"inline,name=Output"`
	OutputAzureLogs              *OutputAzureLogs              `queryParam:"inline,name=Output"`
	OutputKinesis                *OutputKinesis                `queryParam:"inline,name=Output"`
	OutputHoneycomb              *OutputHoneycomb              `queryParam:"inline,name=Output"`
	OutputAzureEventhub          *OutputAzureEventhub          `queryParam:"inline,name=Output"`
	OutputGoogleChronicle        *OutputGoogleChronicle        `queryParam:"inline,name=Output"`
	OutputGoogleCloudStorage     *OutputGoogleCloudStorage     `queryParam:"inline,name=Output"`
	OutputGoogleCloudLogging     *OutputGoogleCloudLogging     `queryParam:"inline,name=Output"`
	OutputGooglePubsub           *OutputGooglePubsub           `queryParam:"inline,name=Output"`
	OutputExabeam                *OutputExabeam                `queryParam:"inline,name=Output"`
	OutputKafka                  *OutputKafka                  `queryParam:"inline,name=Output"`
	OutputConfluentCloud         *OutputConfluentCloud         `queryParam:"inline,name=Output"`
	OutputMsk                    *OutputMsk                    `queryParam:"inline,name=Output"`
	OutputElastic                *OutputElastic                `queryParam:"inline,name=Output"`
	OutputElasticCloud           *OutputElasticCloud           `queryParam:"inline,name=Output"`
	OutputNewrelic               *OutputNewrelic               `queryParam:"inline,name=Output"`
	OutputNewrelicEvents         *OutputNewrelicEvents         `queryParam:"inline,name=Output"`
	OutputInfluxdb               *OutputInfluxdb               `queryParam:"inline,name=Output"`
	OutputCloudwatch             *OutputCloudwatch             `queryParam:"inline,name=Output"`
	OutputMinio                  *OutputMinio                  `queryParam:"inline,name=Output"`
	OutputStatsd                 *OutputStatsd                 `queryParam:"inline,name=Output"`
	OutputStatsdExt              *OutputStatsdExt              `queryParam:"inline,name=Output"`
	OutputGraphite               *OutputGraphite               `queryParam:"inline,name=Output"`
	OutputRouter                 *OutputRouter                 `queryParam:"inline,name=Output"`
	OutputSns                    *OutputSns                    `queryParam:"inline,name=Output"`
	OutputSqs                    *OutputSqs                    `queryParam:"inline,name=Output"`
	OutputSnmp                   *OutputSnmp                   `queryParam:"inline,name=Output"`
	OutputSumoLogic              *OutputSumoLogic              `queryParam:"inline,name=Output"`
	OutputDatadog                *OutputDatadog                `queryParam:"inline,name=Output"`
	OutputGrafanaCloud           *OutputGrafanaCloud           `queryParam:"inline,name=Output"`
	OutputLoki                   *OutputLoki                   `queryParam:"inline,name=Output"`
	OutputPrometheus             *OutputPrometheus             `queryParam:"inline,name=Output"`
	OutputRing                   *OutputRing                   `queryParam:"inline,name=Output"`
	OutputOpenTelemetry          *OutputOpenTelemetry          `queryParam:"inline,name=Output"`
	OutputServiceNow             *OutputServiceNow             `queryParam:"inline,name=Output"`
	OutputDataset                *OutputDataset                `queryParam:"inline,name=Output"`
	OutputCriblTCP               *OutputCriblTCP               `queryParam:"inline,name=Output"`
	OutputCriblHTTP              *OutputCriblHTTP              `queryParam:"inline,name=Output"`
	OutputHumioHec               *OutputHumioHec               `queryParam:"inline,name=Output"`
	OutputCrowdstrikeNextGenSiem *OutputCrowdstrikeNextGenSiem `queryParam:"inline,name=Output"`
	OutputDlS3                   *OutputDlS3                   `queryParam:"inline,name=Output"`
	OutputSecurityLake           *OutputSecurityLake           `queryParam:"inline,name=Output"`
	OutputCriblLake              *OutputCriblLake              `queryParam:"inline,name=Output"`
	OutputDiskSpool              *OutputDiskSpool              `queryParam:"inline,name=Output"`
	OutputClickHouse             *OutputClickHouse             `queryParam:"inline,name=Output"`
	OutputXsiam                  *OutputXsiam                  `queryParam:"inline,name=Output"`
	OutputNetflow                *OutputNetflow                `queryParam:"inline,name=Output"`
	OutputDynatraceHTTP          *OutputDynatraceHTTP          `queryParam:"inline,name=Output"`
	OutputDynatraceOtlp          *OutputDynatraceOtlp          `queryParam:"inline,name=Output"`
	OutputSentinelOneAiSiem      *OutputSentinelOneAiSiem      `queryParam:"inline,name=Output"`
	OutputChronicle              *OutputChronicle              `queryParam:"inline,name=Output"`
	OutputDatabricks             *OutputDatabricks             `queryParam:"inline,name=Output"`
	OutputMicrosoftFabric        *OutputMicrosoftFabric        `queryParam:"inline,name=Output"`
	OutputCloudflareR2           *OutputCloudflareR2           `queryParam:"inline,name=Output"`

	Type OutputType
}

func CreateOutputDefault(defaultT OutputDefault) Output {
	typ := OutputTypeDefault

	typStr := TypeDefault(typ)
	defaultT.Type = typStr

	return Output{
		OutputDefault: &defaultT,
		Type:          typ,
	}
}

func CreateOutputWebhook(webhook OutputWebhook) Output {
	typ := OutputTypeWebhook

	typStr := TypeWebhook(typ)
	webhook.Type = typStr

	return Output{
		OutputWebhook: &webhook,
		Type:          typ,
	}
}

func CreateOutputSentinel(sentinel OutputSentinel) Output {
	typ := OutputTypeSentinel

	typStr := TypeSentinel(typ)
	sentinel.Type = typStr

	return Output{
		OutputSentinel: &sentinel,
		Type:           typ,
	}
}

func CreateOutputDevnull(devnull OutputDevnull) Output {
	typ := OutputTypeDevnull

	typStr := TypeDevnull(typ)
	devnull.Type = typStr

	return Output{
		OutputDevnull: &devnull,
		Type:          typ,
	}
}

func CreateOutputSyslog(syslog OutputSyslog) Output {
	typ := OutputTypeSyslogValue

	typStr := OutputTypeSyslog(typ)
	syslog.Type = typStr

	return Output{
		OutputSyslog: &syslog,
		Type:         typ,
	}
}

func CreateOutputSplunk(splunk OutputSplunk) Output {
	typ := OutputTypeSplunkValue

	typStr := OutputTypeSplunk(typ)
	splunk.Type = typStr

	return Output{
		OutputSplunk: &splunk,
		Type:         typ,
	}
}

func CreateOutputSplunkLb(splunkLb OutputSplunkLb) Output {
	typ := OutputTypeSplunkLb

	typStr := TypeSplunkLb(typ)
	splunkLb.Type = typStr

	return Output{
		OutputSplunkLb: &splunkLb,
		Type:           typ,
	}
}

func CreateOutputSplunkHec(splunkHec OutputSplunkHec) Output {
	typ := OutputTypeSplunkHecValue

	typStr := OutputTypeSplunkHec(typ)
	splunkHec.Type = typStr

	return Output{
		OutputSplunkHec: &splunkHec,
		Type:            typ,
	}
}

func CreateOutputTcpjson(tcpjson OutputTcpjson) Output {
	typ := OutputTypeTcpjsonValue

	typStr := OutputTypeTcpjson(typ)
	tcpjson.Type = typStr

	return Output{
		OutputTcpjson: &tcpjson,
		Type:          typ,
	}
}

func CreateOutputWavefront(wavefront OutputWavefront) Output {
	typ := OutputTypeWavefront

	typStr := TypeWavefront(typ)
	wavefront.Type = typStr

	return Output{
		OutputWavefront: &wavefront,
		Type:            typ,
	}
}

func CreateOutputSignalfx(signalfx OutputSignalfx) Output {
	typ := OutputTypeSignalfx

	typStr := TypeSignalfx(typ)
	signalfx.Type = typStr

	return Output{
		OutputSignalfx: &signalfx,
		Type:           typ,
	}
}

func CreateOutputFilesystem(filesystem OutputFilesystem) Output {
	typ := OutputTypeFilesystem

	typStr := TypeFilesystem(typ)
	filesystem.Type = typStr

	return Output{
		OutputFilesystem: &filesystem,
		Type:             typ,
	}
}

func CreateOutputS3(s3 OutputS3) Output {
	typ := OutputTypeS3Value

	typStr := OutputTypeS3(typ)
	s3.Type = typStr

	return Output{
		OutputS3: &s3,
		Type:     typ,
	}
}

func CreateOutputAzureBlob(azureBlob OutputAzureBlob) Output {
	typ := OutputTypeAzureBlobValue

	typStr := OutputTypeAzureBlob(typ)
	azureBlob.Type = typStr

	return Output{
		OutputAzureBlob: &azureBlob,
		Type:            typ,
	}
}

func CreateOutputAzureDataExplorer(azureDataExplorer OutputAzureDataExplorer) Output {
	typ := OutputTypeAzureDataExplorer

	typStr := TypeAzureDataExplorer(typ)
	azureDataExplorer.Type = typStr

	return Output{
		OutputAzureDataExplorer: &azureDataExplorer,
		Type:                    typ,
	}
}

func CreateOutputAzureLogs(azureLogs OutputAzureLogs) Output {
	typ := OutputTypeAzureLogs

	typStr := TypeAzureLogs(typ)
	azureLogs.Type = typStr

	return Output{
		OutputAzureLogs: &azureLogs,
		Type:            typ,
	}
}

func CreateOutputKinesis(kinesis OutputKinesis) Output {
	typ := OutputTypeKinesisValue

	typStr := OutputTypeKinesis(typ)
	kinesis.Type = typStr

	return Output{
		OutputKinesis: &kinesis,
		Type:          typ,
	}
}

func CreateOutputHoneycomb(honeycomb OutputHoneycomb) Output {
	typ := OutputTypeHoneycomb

	typStr := TypeHoneycomb(typ)
	honeycomb.Type = typStr

	return Output{
		OutputHoneycomb: &honeycomb,
		Type:            typ,
	}
}

func CreateOutputAzureEventhub(azureEventhub OutputAzureEventhub) Output {
	typ := OutputTypeAzureEventhub

	typStr := TypeAzureEventhub(typ)
	azureEventhub.Type = typStr

	return Output{
		OutputAzureEventhub: &azureEventhub,
		Type:                typ,
	}
}

func CreateOutputGoogleChronicle(googleChronicle OutputGoogleChronicle) Output {
	typ := OutputTypeGoogleChronicle

	typStr := TypeGoogleChronicle(typ)
	googleChronicle.Type = typStr

	return Output{
		OutputGoogleChronicle: &googleChronicle,
		Type:                  typ,
	}
}

func CreateOutputGoogleCloudStorage(googleCloudStorage OutputGoogleCloudStorage) Output {
	typ := OutputTypeGoogleCloudStorage

	typStr := TypeGoogleCloudStorage(typ)
	googleCloudStorage.Type = typStr

	return Output{
		OutputGoogleCloudStorage: &googleCloudStorage,
		Type:                     typ,
	}
}

func CreateOutputGoogleCloudLogging(googleCloudLogging OutputGoogleCloudLogging) Output {
	typ := OutputTypeGoogleCloudLogging

	typStr := TypeGoogleCloudLogging(typ)
	googleCloudLogging.Type = typStr

	return Output{
		OutputGoogleCloudLogging: &googleCloudLogging,
		Type:                     typ,
	}
}

func CreateOutputGooglePubsub(googlePubsub OutputGooglePubsub) Output {
	typ := OutputTypeGooglePubsubValue

	typStr := OutputTypeGooglePubsub(typ)
	googlePubsub.Type = typStr

	return Output{
		OutputGooglePubsub: &googlePubsub,
		Type:               typ,
	}
}

func CreateOutputExabeam(exabeam OutputExabeam) Output {
	typ := OutputTypeExabeam

	typStr := TypeExabeam(typ)
	exabeam.Type = typStr

	return Output{
		OutputExabeam: &exabeam,
		Type:          typ,
	}
}

func CreateOutputKafka(kafka OutputKafka) Output {
	typ := OutputTypeKafkaValue

	typStr := OutputTypeKafka(typ)
	kafka.Type = typStr

	return Output{
		OutputKafka: &kafka,
		Type:        typ,
	}
}

func CreateOutputConfluentCloud(confluentCloud OutputConfluentCloud) Output {
	typ := OutputTypeConfluentCloudValue

	typStr := OutputTypeConfluentCloud(typ)
	confluentCloud.Type = typStr

	return Output{
		OutputConfluentCloud: &confluentCloud,
		Type:                 typ,
	}
}

func CreateOutputMsk(msk OutputMsk) Output {
	typ := OutputTypeMskValue

	typStr := OutputTypeMsk(typ)
	msk.Type = typStr

	return Output{
		OutputMsk: &msk,
		Type:      typ,
	}
}

func CreateOutputElastic(elastic OutputElastic) Output {
	typ := OutputTypeElasticValue

	typStr := OutputTypeElastic(typ)
	elastic.Type = typStr

	return Output{
		OutputElastic: &elastic,
		Type:          typ,
	}
}

func CreateOutputElasticCloud(elasticCloud OutputElasticCloud) Output {
	typ := OutputTypeElasticCloud

	typStr := TypeElasticCloud(typ)
	elasticCloud.Type = typStr

	return Output{
		OutputElasticCloud: &elasticCloud,
		Type:               typ,
	}
}

func CreateOutputNewrelic(newrelic OutputNewrelic) Output {
	typ := OutputTypeNewrelic

	typStr := TypeNewrelic(typ)
	newrelic.Type = typStr

	return Output{
		OutputNewrelic: &newrelic,
		Type:           typ,
	}
}

func CreateOutputNewrelicEvents(newrelicEvents OutputNewrelicEvents) Output {
	typ := OutputTypeNewrelicEvents

	typStr := TypeNewrelicEvents(typ)
	newrelicEvents.Type = typStr

	return Output{
		OutputNewrelicEvents: &newrelicEvents,
		Type:                 typ,
	}
}

func CreateOutputInfluxdb(influxdb OutputInfluxdb) Output {
	typ := OutputTypeInfluxdb

	typStr := TypeInfluxdb(typ)
	influxdb.Type = typStr

	return Output{
		OutputInfluxdb: &influxdb,
		Type:           typ,
	}
}

func CreateOutputCloudwatch(cloudwatch OutputCloudwatch) Output {
	typ := OutputTypeCloudwatch

	typStr := TypeCloudwatch(typ)
	cloudwatch.Type = typStr

	return Output{
		OutputCloudwatch: &cloudwatch,
		Type:             typ,
	}
}

func CreateOutputMinio(minio OutputMinio) Output {
	typ := OutputTypeMinio

	typStr := TypeMinio(typ)
	minio.Type = typStr

	return Output{
		OutputMinio: &minio,
		Type:        typ,
	}
}

func CreateOutputStatsd(statsd OutputStatsd) Output {
	typ := OutputTypeStatsd

	typStr := TypeStatsd(typ)
	statsd.Type = typStr

	return Output{
		OutputStatsd: &statsd,
		Type:         typ,
	}
}

func CreateOutputStatsdExt(statsdExt OutputStatsdExt) Output {
	typ := OutputTypeStatsdExt

	typStr := TypeStatsdExt(typ)
	statsdExt.Type = typStr

	return Output{
		OutputStatsdExt: &statsdExt,
		Type:            typ,
	}
}

func CreateOutputGraphite(graphite OutputGraphite) Output {
	typ := OutputTypeGraphite

	typStr := TypeGraphite(typ)
	graphite.Type = typStr

	return Output{
		OutputGraphite: &graphite,
		Type:           typ,
	}
}

func CreateOutputRouter(router OutputRouter) Output {
	typ := OutputTypeRouter

	typStr := TypeRouter(typ)
	router.Type = typStr

	return Output{
		OutputRouter: &router,
		Type:         typ,
	}
}

func CreateOutputSns(sns OutputSns) Output {
	typ := OutputTypeSns

	typStr := TypeSns(typ)
	sns.Type = typStr

	return Output{
		OutputSns: &sns,
		Type:      typ,
	}
}

func CreateOutputSqs(sqs OutputSqs) Output {
	typ := OutputTypeSqsValue

	typStr := OutputTypeSqs(typ)
	sqs.Type = typStr

	return Output{
		OutputSqs: &sqs,
		Type:      typ,
	}
}

func CreateOutputSnmp(snmp OutputSnmp) Output {
	typ := OutputTypeSnmpValue

	typStr := OutputTypeSnmp(typ)
	snmp.Type = typStr

	return Output{
		OutputSnmp: &snmp,
		Type:       typ,
	}
}

func CreateOutputSumoLogic(sumoLogic OutputSumoLogic) Output {
	typ := OutputTypeSumoLogic

	typStr := TypeSumoLogic(typ)
	sumoLogic.Type = typStr

	return Output{
		OutputSumoLogic: &sumoLogic,
		Type:            typ,
	}
}

func CreateOutputDatadog(datadog OutputDatadog) Output {
	typ := OutputTypeDatadog

	typStr := TypeDatadog(typ)
	datadog.Type = typStr

	return Output{
		OutputDatadog: &datadog,
		Type:          typ,
	}
}

func CreateOutputGrafanaCloud(grafanaCloud OutputGrafanaCloud) Output {
	typ := OutputTypeGrafanaCloud

	return Output{
		OutputGrafanaCloud: &grafanaCloud,
		Type:               typ,
	}
}

func CreateOutputLoki(loki OutputLoki) Output {
	typ := OutputTypeLokiValue

	typStr := OutputTypeLoki(typ)
	loki.Type = typStr

	return Output{
		OutputLoki: &loki,
		Type:       typ,
	}
}

func CreateOutputPrometheus(prometheus OutputPrometheus) Output {
	typ := OutputTypePrometheusValue

	typStr := OutputTypePrometheus(typ)
	prometheus.Type = typStr

	return Output{
		OutputPrometheus: &prometheus,
		Type:             typ,
	}
}

func CreateOutputRing(ring OutputRing) Output {
	typ := OutputTypeRing

	typStr := TypeRing(typ)
	ring.Type = typStr

	return Output{
		OutputRing: &ring,
		Type:       typ,
	}
}

func CreateOutputOpenTelemetry(openTelemetry OutputOpenTelemetry) Output {
	typ := OutputTypeOpenTelemetryValue

	typStr := OutputTypeOpenTelemetry(typ)
	openTelemetry.Type = typStr

	return Output{
		OutputOpenTelemetry: &openTelemetry,
		Type:                typ,
	}
}

func CreateOutputServiceNow(serviceNow OutputServiceNow) Output {
	typ := OutputTypeServiceNow

	typStr := TypeServiceNow(typ)
	serviceNow.Type = typStr

	return Output{
		OutputServiceNow: &serviceNow,
		Type:             typ,
	}
}

func CreateOutputDataset(dataset OutputDataset) Output {
	typ := OutputTypeDataset

	typStr := TypeDataset(typ)
	dataset.Type = typStr

	return Output{
		OutputDataset: &dataset,
		Type:          typ,
	}
}

func CreateOutputCriblTCP(criblTCP OutputCriblTCP) Output {
	typ := OutputTypeCriblTCPValue

	typStr := OutputTypeCriblTCP(typ)
	criblTCP.Type = typStr

	return Output{
		OutputCriblTCP: &criblTCP,
		Type:           typ,
	}
}

func CreateOutputCriblHTTP(criblHTTP OutputCriblHTTP) Output {
	typ := OutputTypeCriblHTTPValue

	typStr := OutputTypeCriblHTTP(typ)
	criblHTTP.Type = typStr

	return Output{
		OutputCriblHTTP: &criblHTTP,
		Type:            typ,
	}
}

func CreateOutputHumioHec(humioHec OutputHumioHec) Output {
	typ := OutputTypeHumioHec

	typStr := TypeHumioHec(typ)
	humioHec.Type = typStr

	return Output{
		OutputHumioHec: &humioHec,
		Type:           typ,
	}
}

func CreateOutputCrowdstrikeNextGenSiem(crowdstrikeNextGenSiem OutputCrowdstrikeNextGenSiem) Output {
	typ := OutputTypeCrowdstrikeNextGenSiem

	typStr := TypeCrowdstrikeNextGenSiem(typ)
	crowdstrikeNextGenSiem.Type = typStr

	return Output{
		OutputCrowdstrikeNextGenSiem: &crowdstrikeNextGenSiem,
		Type:                         typ,
	}
}

func CreateOutputDlS3(dlS3 OutputDlS3) Output {
	typ := OutputTypeDlS3

	typStr := TypeDlS3(typ)
	dlS3.Type = typStr

	return Output{
		OutputDlS3: &dlS3,
		Type:       typ,
	}
}

func CreateOutputSecurityLake(securityLake OutputSecurityLake) Output {
	typ := OutputTypeSecurityLakeValue

	typStr := OutputTypeSecurityLake(typ)
	securityLake.Type = typStr

	return Output{
		OutputSecurityLake: &securityLake,
		Type:               typ,
	}
}

func CreateOutputCriblLake(criblLake OutputCriblLake) Output {
	typ := OutputTypeCriblLake

	typStr := TypeCriblLake(typ)
	criblLake.Type = typStr

	return Output{
		OutputCriblLake: &criblLake,
		Type:            typ,
	}
}

func CreateOutputDiskSpool(diskSpool OutputDiskSpool) Output {
	typ := OutputTypeDiskSpool

	typStr := TypeDiskSpool(typ)
	diskSpool.Type = typStr

	return Output{
		OutputDiskSpool: &diskSpool,
		Type:            typ,
	}
}

func CreateOutputClickHouse(clickHouse OutputClickHouse) Output {
	typ := OutputTypeClickHouse

	typStr := TypeClickHouse(typ)
	clickHouse.Type = typStr

	return Output{
		OutputClickHouse: &clickHouse,
		Type:             typ,
	}
}

func CreateOutputXsiam(xsiam OutputXsiam) Output {
	typ := OutputTypeXsiam

	typStr := TypeXsiam(typ)
	xsiam.Type = typStr

	return Output{
		OutputXsiam: &xsiam,
		Type:        typ,
	}
}

func CreateOutputNetflow(netflow OutputNetflow) Output {
	typ := OutputTypeNetflowValue

	typStr := OutputTypeNetflow(typ)
	netflow.Type = typStr

	return Output{
		OutputNetflow: &netflow,
		Type:          typ,
	}
}

func CreateOutputDynatraceHTTP(dynatraceHTTP OutputDynatraceHTTP) Output {
	typ := OutputTypeDynatraceHTTP

	typStr := TypeDynatraceHTTP(typ)
	dynatraceHTTP.Type = typStr

	return Output{
		OutputDynatraceHTTP: &dynatraceHTTP,
		Type:                typ,
	}
}

func CreateOutputDynatraceOtlp(dynatraceOtlp OutputDynatraceOtlp) Output {
	typ := OutputTypeDynatraceOtlp

	typStr := TypeDynatraceOtlp(typ)
	dynatraceOtlp.Type = typStr

	return Output{
		OutputDynatraceOtlp: &dynatraceOtlp,
		Type:                typ,
	}
}

func CreateOutputSentinelOneAiSiem(sentinelOneAiSiem OutputSentinelOneAiSiem) Output {
	typ := OutputTypeSentinelOneAiSiem

	typStr := TypeSentinelOneAiSiem(typ)
	sentinelOneAiSiem.Type = typStr

	return Output{
		OutputSentinelOneAiSiem: &sentinelOneAiSiem,
		Type:                    typ,
	}
}

func CreateOutputChronicle(chronicle OutputChronicle) Output {
	typ := OutputTypeChronicle

	typStr := TypeChronicle(typ)
	chronicle.Type = typStr

	return Output{
		OutputChronicle: &chronicle,
		Type:            typ,
	}
}

func CreateOutputDatabricks(databricks OutputDatabricks) Output {
	typ := OutputTypeDatabricks

	typStr := TypeDatabricks(typ)
	databricks.Type = typStr

	return Output{
		OutputDatabricks: &databricks,
		Type:             typ,
	}
}

func CreateOutputMicrosoftFabric(microsoftFabric OutputMicrosoftFabric) Output {
	typ := OutputTypeMicrosoftFabric

	typStr := TypeMicrosoftFabric(typ)
	microsoftFabric.Type = typStr

	return Output{
		OutputMicrosoftFabric: &microsoftFabric,
		Type:                  typ,
	}
}

func CreateOutputCloudflareR2(cloudflareR2 OutputCloudflareR2) Output {
	typ := OutputTypeCloudflareR2

	typStr := TypeCloudflareR2(typ)
	cloudflareR2.Type = typStr

	return Output{
		OutputCloudflareR2: &cloudflareR2,
		Type:               typ,
	}
}

func (u *Output) UnmarshalJSON(data []byte) error {

	type discriminator struct {
		Type string `json:"type"`
	}

	dis := new(discriminator)
	if err := json.Unmarshal(data, &dis); err != nil {
		return fmt.Errorf("could not unmarshal discriminator: %w", err)
	}

	switch dis.Type {
	case "default":
		outputDefault := new(OutputDefault)
		if err := utils.UnmarshalJSON(data, &outputDefault, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == default) type OutputDefault within Output: %w", string(data), err)
		}

		u.OutputDefault = outputDefault
		u.Type = OutputTypeDefault
		return nil
	case "webhook":
		outputWebhook := new(OutputWebhook)
		if err := utils.UnmarshalJSON(data, &outputWebhook, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == webhook) type OutputWebhook within Output: %w", string(data), err)
		}

		u.OutputWebhook = outputWebhook
		u.Type = OutputTypeWebhook
		return nil
	case "sentinel":
		outputSentinel := new(OutputSentinel)
		if err := utils.UnmarshalJSON(data, &outputSentinel, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == sentinel) type OutputSentinel within Output: %w", string(data), err)
		}

		u.OutputSentinel = outputSentinel
		u.Type = OutputTypeSentinel
		return nil
	case "devnull":
		outputDevnull := new(OutputDevnull)
		if err := utils.UnmarshalJSON(data, &outputDevnull, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == devnull) type OutputDevnull within Output: %w", string(data), err)
		}

		u.OutputDevnull = outputDevnull
		u.Type = OutputTypeDevnull
		return nil
	case "syslog":
		outputSyslog := new(OutputSyslog)
		if err := utils.UnmarshalJSON(data, &outputSyslog, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == syslog) type OutputSyslog within Output: %w", string(data), err)
		}

		u.OutputSyslog = outputSyslog
		u.Type = OutputTypeSyslogValue
		return nil
	case "splunk":
		outputSplunk := new(OutputSplunk)
		if err := utils.UnmarshalJSON(data, &outputSplunk, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == splunk) type OutputSplunk within Output: %w", string(data), err)
		}

		u.OutputSplunk = outputSplunk
		u.Type = OutputTypeSplunkValue
		return nil
	case "splunk_lb":
		outputSplunkLb := new(OutputSplunkLb)
		if err := utils.UnmarshalJSON(data, &outputSplunkLb, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == splunk_lb) type OutputSplunkLb within Output: %w", string(data), err)
		}

		u.OutputSplunkLb = outputSplunkLb
		u.Type = OutputTypeSplunkLb
		return nil
	case "splunk_hec":
		outputSplunkHec := new(OutputSplunkHec)
		if err := utils.UnmarshalJSON(data, &outputSplunkHec, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == splunk_hec) type OutputSplunkHec within Output: %w", string(data), err)
		}

		u.OutputSplunkHec = outputSplunkHec
		u.Type = OutputTypeSplunkHecValue
		return nil
	case "tcpjson":
		outputTcpjson := new(OutputTcpjson)
		if err := utils.UnmarshalJSON(data, &outputTcpjson, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == tcpjson) type OutputTcpjson within Output: %w", string(data), err)
		}

		u.OutputTcpjson = outputTcpjson
		u.Type = OutputTypeTcpjsonValue
		return nil
	case "wavefront":
		outputWavefront := new(OutputWavefront)
		if err := utils.UnmarshalJSON(data, &outputWavefront, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == wavefront) type OutputWavefront within Output: %w", string(data), err)
		}

		u.OutputWavefront = outputWavefront
		u.Type = OutputTypeWavefront
		return nil
	case "signalfx":
		outputSignalfx := new(OutputSignalfx)
		if err := utils.UnmarshalJSON(data, &outputSignalfx, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == signalfx) type OutputSignalfx within Output: %w", string(data), err)
		}

		u.OutputSignalfx = outputSignalfx
		u.Type = OutputTypeSignalfx
		return nil
	case "filesystem":
		outputFilesystem := new(OutputFilesystem)
		if err := utils.UnmarshalJSON(data, &outputFilesystem, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == filesystem) type OutputFilesystem within Output: %w", string(data), err)
		}

		u.OutputFilesystem = outputFilesystem
		u.Type = OutputTypeFilesystem
		return nil
	case "s3":
		outputS3 := new(OutputS3)
		if err := utils.UnmarshalJSON(data, &outputS3, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == s3) type OutputS3 within Output: %w", string(data), err)
		}

		u.OutputS3 = outputS3
		u.Type = OutputTypeS3Value
		return nil
	case "azure_blob":
		outputAzureBlob := new(OutputAzureBlob)
		if err := utils.UnmarshalJSON(data, &outputAzureBlob, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == azure_blob) type OutputAzureBlob within Output: %w", string(data), err)
		}

		u.OutputAzureBlob = outputAzureBlob
		u.Type = OutputTypeAzureBlobValue
		return nil
	case "azure_data_explorer":
		outputAzureDataExplorer := new(OutputAzureDataExplorer)
		if err := utils.UnmarshalJSON(data, &outputAzureDataExplorer, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == azure_data_explorer) type OutputAzureDataExplorer within Output: %w", string(data), err)
		}

		u.OutputAzureDataExplorer = outputAzureDataExplorer
		u.Type = OutputTypeAzureDataExplorer
		return nil
	case "azure_logs":
		outputAzureLogs := new(OutputAzureLogs)
		if err := utils.UnmarshalJSON(data, &outputAzureLogs, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == azure_logs) type OutputAzureLogs within Output: %w", string(data), err)
		}

		u.OutputAzureLogs = outputAzureLogs
		u.Type = OutputTypeAzureLogs
		return nil
	case "kinesis":
		outputKinesis := new(OutputKinesis)
		if err := utils.UnmarshalJSON(data, &outputKinesis, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == kinesis) type OutputKinesis within Output: %w", string(data), err)
		}

		u.OutputKinesis = outputKinesis
		u.Type = OutputTypeKinesisValue
		return nil
	case "honeycomb":
		outputHoneycomb := new(OutputHoneycomb)
		if err := utils.UnmarshalJSON(data, &outputHoneycomb, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == honeycomb) type OutputHoneycomb within Output: %w", string(data), err)
		}

		u.OutputHoneycomb = outputHoneycomb
		u.Type = OutputTypeHoneycomb
		return nil
	case "azure_eventhub":
		outputAzureEventhub := new(OutputAzureEventhub)
		if err := utils.UnmarshalJSON(data, &outputAzureEventhub, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == azure_eventhub) type OutputAzureEventhub within Output: %w", string(data), err)
		}

		u.OutputAzureEventhub = outputAzureEventhub
		u.Type = OutputTypeAzureEventhub
		return nil
	case "google_chronicle":
		outputGoogleChronicle := new(OutputGoogleChronicle)
		if err := utils.UnmarshalJSON(data, &outputGoogleChronicle, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == google_chronicle) type OutputGoogleChronicle within Output: %w", string(data), err)
		}

		u.OutputGoogleChronicle = outputGoogleChronicle
		u.Type = OutputTypeGoogleChronicle
		return nil
	case "google_cloud_storage":
		outputGoogleCloudStorage := new(OutputGoogleCloudStorage)
		if err := utils.UnmarshalJSON(data, &outputGoogleCloudStorage, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == google_cloud_storage) type OutputGoogleCloudStorage within Output: %w", string(data), err)
		}

		u.OutputGoogleCloudStorage = outputGoogleCloudStorage
		u.Type = OutputTypeGoogleCloudStorage
		return nil
	case "google_cloud_logging":
		outputGoogleCloudLogging := new(OutputGoogleCloudLogging)
		if err := utils.UnmarshalJSON(data, &outputGoogleCloudLogging, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == google_cloud_logging) type OutputGoogleCloudLogging within Output: %w", string(data), err)
		}

		u.OutputGoogleCloudLogging = outputGoogleCloudLogging
		u.Type = OutputTypeGoogleCloudLogging
		return nil
	case "google_pubsub":
		outputGooglePubsub := new(OutputGooglePubsub)
		if err := utils.UnmarshalJSON(data, &outputGooglePubsub, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == google_pubsub) type OutputGooglePubsub within Output: %w", string(data), err)
		}

		u.OutputGooglePubsub = outputGooglePubsub
		u.Type = OutputTypeGooglePubsubValue
		return nil
	case "exabeam":
		outputExabeam := new(OutputExabeam)
		if err := utils.UnmarshalJSON(data, &outputExabeam, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == exabeam) type OutputExabeam within Output: %w", string(data), err)
		}

		u.OutputExabeam = outputExabeam
		u.Type = OutputTypeExabeam
		return nil
	case "kafka":
		outputKafka := new(OutputKafka)
		if err := utils.UnmarshalJSON(data, &outputKafka, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == kafka) type OutputKafka within Output: %w", string(data), err)
		}

		u.OutputKafka = outputKafka
		u.Type = OutputTypeKafkaValue
		return nil
	case "confluent_cloud":
		outputConfluentCloud := new(OutputConfluentCloud)
		if err := utils.UnmarshalJSON(data, &outputConfluentCloud, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == confluent_cloud) type OutputConfluentCloud within Output: %w", string(data), err)
		}

		u.OutputConfluentCloud = outputConfluentCloud
		u.Type = OutputTypeConfluentCloudValue
		return nil
	case "msk":
		outputMsk := new(OutputMsk)
		if err := utils.UnmarshalJSON(data, &outputMsk, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == msk) type OutputMsk within Output: %w", string(data), err)
		}

		u.OutputMsk = outputMsk
		u.Type = OutputTypeMskValue
		return nil
	case "elastic":
		outputElastic := new(OutputElastic)
		if err := utils.UnmarshalJSON(data, &outputElastic, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == elastic) type OutputElastic within Output: %w", string(data), err)
		}

		u.OutputElastic = outputElastic
		u.Type = OutputTypeElasticValue
		return nil
	case "elastic_cloud":
		outputElasticCloud := new(OutputElasticCloud)
		if err := utils.UnmarshalJSON(data, &outputElasticCloud, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == elastic_cloud) type OutputElasticCloud within Output: %w", string(data), err)
		}

		u.OutputElasticCloud = outputElasticCloud
		u.Type = OutputTypeElasticCloud
		return nil
	case "newrelic":
		outputNewrelic := new(OutputNewrelic)
		if err := utils.UnmarshalJSON(data, &outputNewrelic, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == newrelic) type OutputNewrelic within Output: %w", string(data), err)
		}

		u.OutputNewrelic = outputNewrelic
		u.Type = OutputTypeNewrelic
		return nil
	case "newrelic_events":
		outputNewrelicEvents := new(OutputNewrelicEvents)
		if err := utils.UnmarshalJSON(data, &outputNewrelicEvents, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == newrelic_events) type OutputNewrelicEvents within Output: %w", string(data), err)
		}

		u.OutputNewrelicEvents = outputNewrelicEvents
		u.Type = OutputTypeNewrelicEvents
		return nil
	case "influxdb":
		outputInfluxdb := new(OutputInfluxdb)
		if err := utils.UnmarshalJSON(data, &outputInfluxdb, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == influxdb) type OutputInfluxdb within Output: %w", string(data), err)
		}

		u.OutputInfluxdb = outputInfluxdb
		u.Type = OutputTypeInfluxdb
		return nil
	case "cloudwatch":
		outputCloudwatch := new(OutputCloudwatch)
		if err := utils.UnmarshalJSON(data, &outputCloudwatch, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == cloudwatch) type OutputCloudwatch within Output: %w", string(data), err)
		}

		u.OutputCloudwatch = outputCloudwatch
		u.Type = OutputTypeCloudwatch
		return nil
	case "minio":
		outputMinio := new(OutputMinio)
		if err := utils.UnmarshalJSON(data, &outputMinio, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == minio) type OutputMinio within Output: %w", string(data), err)
		}

		u.OutputMinio = outputMinio
		u.Type = OutputTypeMinio
		return nil
	case "statsd":
		outputStatsd := new(OutputStatsd)
		if err := utils.UnmarshalJSON(data, &outputStatsd, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == statsd) type OutputStatsd within Output: %w", string(data), err)
		}

		u.OutputStatsd = outputStatsd
		u.Type = OutputTypeStatsd
		return nil
	case "statsd_ext":
		outputStatsdExt := new(OutputStatsdExt)
		if err := utils.UnmarshalJSON(data, &outputStatsdExt, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == statsd_ext) type OutputStatsdExt within Output: %w", string(data), err)
		}

		u.OutputStatsdExt = outputStatsdExt
		u.Type = OutputTypeStatsdExt
		return nil
	case "graphite":
		outputGraphite := new(OutputGraphite)
		if err := utils.UnmarshalJSON(data, &outputGraphite, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == graphite) type OutputGraphite within Output: %w", string(data), err)
		}

		u.OutputGraphite = outputGraphite
		u.Type = OutputTypeGraphite
		return nil
	case "router":
		outputRouter := new(OutputRouter)
		if err := utils.UnmarshalJSON(data, &outputRouter, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == router) type OutputRouter within Output: %w", string(data), err)
		}

		u.OutputRouter = outputRouter
		u.Type = OutputTypeRouter
		return nil
	case "sns":
		outputSns := new(OutputSns)
		if err := utils.UnmarshalJSON(data, &outputSns, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == sns) type OutputSns within Output: %w", string(data), err)
		}

		u.OutputSns = outputSns
		u.Type = OutputTypeSns
		return nil
	case "sqs":
		outputSqs := new(OutputSqs)
		if err := utils.UnmarshalJSON(data, &outputSqs, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == sqs) type OutputSqs within Output: %w", string(data), err)
		}

		u.OutputSqs = outputSqs
		u.Type = OutputTypeSqsValue
		return nil
	case "snmp":
		outputSnmp := new(OutputSnmp)
		if err := utils.UnmarshalJSON(data, &outputSnmp, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == snmp) type OutputSnmp within Output: %w", string(data), err)
		}

		u.OutputSnmp = outputSnmp
		u.Type = OutputTypeSnmpValue
		return nil
	case "sumo_logic":
		outputSumoLogic := new(OutputSumoLogic)
		if err := utils.UnmarshalJSON(data, &outputSumoLogic, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == sumo_logic) type OutputSumoLogic within Output: %w", string(data), err)
		}

		u.OutputSumoLogic = outputSumoLogic
		u.Type = OutputTypeSumoLogic
		return nil
	case "datadog":
		outputDatadog := new(OutputDatadog)
		if err := utils.UnmarshalJSON(data, &outputDatadog, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == datadog) type OutputDatadog within Output: %w", string(data), err)
		}

		u.OutputDatadog = outputDatadog
		u.Type = OutputTypeDatadog
		return nil
	case "grafana_cloud":
		outputGrafanaCloud := new(OutputGrafanaCloud)
		if err := utils.UnmarshalJSON(data, &outputGrafanaCloud, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == grafana_cloud) type OutputGrafanaCloud within Output: %w", string(data), err)
		}

		u.OutputGrafanaCloud = outputGrafanaCloud
		u.Type = OutputTypeGrafanaCloud
		return nil
	case "loki":
		outputLoki := new(OutputLoki)
		if err := utils.UnmarshalJSON(data, &outputLoki, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == loki) type OutputLoki within Output: %w", string(data), err)
		}

		u.OutputLoki = outputLoki
		u.Type = OutputTypeLokiValue
		return nil
	case "prometheus":
		outputPrometheus := new(OutputPrometheus)
		if err := utils.UnmarshalJSON(data, &outputPrometheus, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == prometheus) type OutputPrometheus within Output: %w", string(data), err)
		}

		u.OutputPrometheus = outputPrometheus
		u.Type = OutputTypePrometheusValue
		return nil
	case "ring":
		outputRing := new(OutputRing)
		if err := utils.UnmarshalJSON(data, &outputRing, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == ring) type OutputRing within Output: %w", string(data), err)
		}

		u.OutputRing = outputRing
		u.Type = OutputTypeRing
		return nil
	case "open_telemetry":
		outputOpenTelemetry := new(OutputOpenTelemetry)
		if err := utils.UnmarshalJSON(data, &outputOpenTelemetry, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == open_telemetry) type OutputOpenTelemetry within Output: %w", string(data), err)
		}

		u.OutputOpenTelemetry = outputOpenTelemetry
		u.Type = OutputTypeOpenTelemetryValue
		return nil
	case "service_now":
		outputServiceNow := new(OutputServiceNow)
		if err := utils.UnmarshalJSON(data, &outputServiceNow, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == service_now) type OutputServiceNow within Output: %w", string(data), err)
		}

		u.OutputServiceNow = outputServiceNow
		u.Type = OutputTypeServiceNow
		return nil
	case "dataset":
		outputDataset := new(OutputDataset)
		if err := utils.UnmarshalJSON(data, &outputDataset, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == dataset) type OutputDataset within Output: %w", string(data), err)
		}

		u.OutputDataset = outputDataset
		u.Type = OutputTypeDataset
		return nil
	case "cribl_tcp":
		outputCriblTCP := new(OutputCriblTCP)
		if err := utils.UnmarshalJSON(data, &outputCriblTCP, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == cribl_tcp) type OutputCriblTCP within Output: %w", string(data), err)
		}

		u.OutputCriblTCP = outputCriblTCP
		u.Type = OutputTypeCriblTCPValue
		return nil
	case "cribl_http":
		outputCriblHTTP := new(OutputCriblHTTP)
		if err := utils.UnmarshalJSON(data, &outputCriblHTTP, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == cribl_http) type OutputCriblHTTP within Output: %w", string(data), err)
		}

		u.OutputCriblHTTP = outputCriblHTTP
		u.Type = OutputTypeCriblHTTPValue
		return nil
	case "humio_hec":
		outputHumioHec := new(OutputHumioHec)
		if err := utils.UnmarshalJSON(data, &outputHumioHec, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == humio_hec) type OutputHumioHec within Output: %w", string(data), err)
		}

		u.OutputHumioHec = outputHumioHec
		u.Type = OutputTypeHumioHec
		return nil
	case "crowdstrike_next_gen_siem":
		outputCrowdstrikeNextGenSiem := new(OutputCrowdstrikeNextGenSiem)
		if err := utils.UnmarshalJSON(data, &outputCrowdstrikeNextGenSiem, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == crowdstrike_next_gen_siem) type OutputCrowdstrikeNextGenSiem within Output: %w", string(data), err)
		}

		u.OutputCrowdstrikeNextGenSiem = outputCrowdstrikeNextGenSiem
		u.Type = OutputTypeCrowdstrikeNextGenSiem
		return nil
	case "dl_s3":
		outputDlS3 := new(OutputDlS3)
		if err := utils.UnmarshalJSON(data, &outputDlS3, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == dl_s3) type OutputDlS3 within Output: %w", string(data), err)
		}

		u.OutputDlS3 = outputDlS3
		u.Type = OutputTypeDlS3
		return nil
	case "security_lake":
		outputSecurityLake := new(OutputSecurityLake)
		if err := utils.UnmarshalJSON(data, &outputSecurityLake, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == security_lake) type OutputSecurityLake within Output: %w", string(data), err)
		}

		u.OutputSecurityLake = outputSecurityLake
		u.Type = OutputTypeSecurityLakeValue
		return nil
	case "cribl_lake":
		outputCriblLake := new(OutputCriblLake)
		if err := utils.UnmarshalJSON(data, &outputCriblLake, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == cribl_lake) type OutputCriblLake within Output: %w", string(data), err)
		}

		u.OutputCriblLake = outputCriblLake
		u.Type = OutputTypeCriblLake
		return nil
	case "disk_spool":
		outputDiskSpool := new(OutputDiskSpool)
		if err := utils.UnmarshalJSON(data, &outputDiskSpool, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == disk_spool) type OutputDiskSpool within Output: %w", string(data), err)
		}

		u.OutputDiskSpool = outputDiskSpool
		u.Type = OutputTypeDiskSpool
		return nil
	case "click_house":
		outputClickHouse := new(OutputClickHouse)
		if err := utils.UnmarshalJSON(data, &outputClickHouse, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == click_house) type OutputClickHouse within Output: %w", string(data), err)
		}

		u.OutputClickHouse = outputClickHouse
		u.Type = OutputTypeClickHouse
		return nil
	case "xsiam":
		outputXsiam := new(OutputXsiam)
		if err := utils.UnmarshalJSON(data, &outputXsiam, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == xsiam) type OutputXsiam within Output: %w", string(data), err)
		}

		u.OutputXsiam = outputXsiam
		u.Type = OutputTypeXsiam
		return nil
	case "netflow":
		outputNetflow := new(OutputNetflow)
		if err := utils.UnmarshalJSON(data, &outputNetflow, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == netflow) type OutputNetflow within Output: %w", string(data), err)
		}

		u.OutputNetflow = outputNetflow
		u.Type = OutputTypeNetflowValue
		return nil
	case "dynatrace_http":
		outputDynatraceHTTP := new(OutputDynatraceHTTP)
		if err := utils.UnmarshalJSON(data, &outputDynatraceHTTP, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == dynatrace_http) type OutputDynatraceHTTP within Output: %w", string(data), err)
		}

		u.OutputDynatraceHTTP = outputDynatraceHTTP
		u.Type = OutputTypeDynatraceHTTP
		return nil
	case "dynatrace_otlp":
		outputDynatraceOtlp := new(OutputDynatraceOtlp)
		if err := utils.UnmarshalJSON(data, &outputDynatraceOtlp, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == dynatrace_otlp) type OutputDynatraceOtlp within Output: %w", string(data), err)
		}

		u.OutputDynatraceOtlp = outputDynatraceOtlp
		u.Type = OutputTypeDynatraceOtlp
		return nil
	case "sentinel_one_ai_siem":
		outputSentinelOneAiSiem := new(OutputSentinelOneAiSiem)
		if err := utils.UnmarshalJSON(data, &outputSentinelOneAiSiem, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == sentinel_one_ai_siem) type OutputSentinelOneAiSiem within Output: %w", string(data), err)
		}

		u.OutputSentinelOneAiSiem = outputSentinelOneAiSiem
		u.Type = OutputTypeSentinelOneAiSiem
		return nil
	case "chronicle":
		outputChronicle := new(OutputChronicle)
		if err := utils.UnmarshalJSON(data, &outputChronicle, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == chronicle) type OutputChronicle within Output: %w", string(data), err)
		}

		u.OutputChronicle = outputChronicle
		u.Type = OutputTypeChronicle
		return nil
	case "databricks":
		outputDatabricks := new(OutputDatabricks)
		if err := utils.UnmarshalJSON(data, &outputDatabricks, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == databricks) type OutputDatabricks within Output: %w", string(data), err)
		}

		u.OutputDatabricks = outputDatabricks
		u.Type = OutputTypeDatabricks
		return nil
	case "microsoft_fabric":
		outputMicrosoftFabric := new(OutputMicrosoftFabric)
		if err := utils.UnmarshalJSON(data, &outputMicrosoftFabric, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == microsoft_fabric) type OutputMicrosoftFabric within Output: %w", string(data), err)
		}

		u.OutputMicrosoftFabric = outputMicrosoftFabric
		u.Type = OutputTypeMicrosoftFabric
		return nil
	case "cloudflare_r2":
		outputCloudflareR2 := new(OutputCloudflareR2)
		if err := utils.UnmarshalJSON(data, &outputCloudflareR2, "", true, nil); err != nil {
			return fmt.Errorf("could not unmarshal `%s` into expected (Type == cloudflare_r2) type OutputCloudflareR2 within Output: %w", string(data), err)
		}

		u.OutputCloudflareR2 = outputCloudflareR2
		u.Type = OutputTypeCloudflareR2
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for Output", string(data))
}

func (u Output) MarshalJSON() ([]byte, error) {
	if u.OutputDefault != nil {
		return utils.MarshalJSON(u.OutputDefault, "", true)
	}

	if u.OutputWebhook != nil {
		return utils.MarshalJSON(u.OutputWebhook, "", true)
	}

	if u.OutputSentinel != nil {
		return utils.MarshalJSON(u.OutputSentinel, "", true)
	}

	if u.OutputDevnull != nil {
		return utils.MarshalJSON(u.OutputDevnull, "", true)
	}

	if u.OutputSyslog != nil {
		return utils.MarshalJSON(u.OutputSyslog, "", true)
	}

	if u.OutputSplunk != nil {
		return utils.MarshalJSON(u.OutputSplunk, "", true)
	}

	if u.OutputSplunkLb != nil {
		return utils.MarshalJSON(u.OutputSplunkLb, "", true)
	}

	if u.OutputSplunkHec != nil {
		return utils.MarshalJSON(u.OutputSplunkHec, "", true)
	}

	if u.OutputTcpjson != nil {
		return utils.MarshalJSON(u.OutputTcpjson, "", true)
	}

	if u.OutputWavefront != nil {
		return utils.MarshalJSON(u.OutputWavefront, "", true)
	}

	if u.OutputSignalfx != nil {
		return utils.MarshalJSON(u.OutputSignalfx, "", true)
	}

	if u.OutputFilesystem != nil {
		return utils.MarshalJSON(u.OutputFilesystem, "", true)
	}

	if u.OutputS3 != nil {
		return utils.MarshalJSON(u.OutputS3, "", true)
	}

	if u.OutputAzureBlob != nil {
		return utils.MarshalJSON(u.OutputAzureBlob, "", true)
	}

	if u.OutputAzureDataExplorer != nil {
		return utils.MarshalJSON(u.OutputAzureDataExplorer, "", true)
	}

	if u.OutputAzureLogs != nil {
		return utils.MarshalJSON(u.OutputAzureLogs, "", true)
	}

	if u.OutputKinesis != nil {
		return utils.MarshalJSON(u.OutputKinesis, "", true)
	}

	if u.OutputHoneycomb != nil {
		return utils.MarshalJSON(u.OutputHoneycomb, "", true)
	}

	if u.OutputAzureEventhub != nil {
		return utils.MarshalJSON(u.OutputAzureEventhub, "", true)
	}

	if u.OutputGoogleChronicle != nil {
		return utils.MarshalJSON(u.OutputGoogleChronicle, "", true)
	}

	if u.OutputGoogleCloudStorage != nil {
		return utils.MarshalJSON(u.OutputGoogleCloudStorage, "", true)
	}

	if u.OutputGoogleCloudLogging != nil {
		return utils.MarshalJSON(u.OutputGoogleCloudLogging, "", true)
	}

	if u.OutputGooglePubsub != nil {
		return utils.MarshalJSON(u.OutputGooglePubsub, "", true)
	}

	if u.OutputExabeam != nil {
		return utils.MarshalJSON(u.OutputExabeam, "", true)
	}

	if u.OutputKafka != nil {
		return utils.MarshalJSON(u.OutputKafka, "", true)
	}

	if u.OutputConfluentCloud != nil {
		return utils.MarshalJSON(u.OutputConfluentCloud, "", true)
	}

	if u.OutputMsk != nil {
		return utils.MarshalJSON(u.OutputMsk, "", true)
	}

	if u.OutputElastic != nil {
		return utils.MarshalJSON(u.OutputElastic, "", true)
	}

	if u.OutputElasticCloud != nil {
		return utils.MarshalJSON(u.OutputElasticCloud, "", true)
	}

	if u.OutputNewrelic != nil {
		return utils.MarshalJSON(u.OutputNewrelic, "", true)
	}

	if u.OutputNewrelicEvents != nil {
		return utils.MarshalJSON(u.OutputNewrelicEvents, "", true)
	}

	if u.OutputInfluxdb != nil {
		return utils.MarshalJSON(u.OutputInfluxdb, "", true)
	}

	if u.OutputCloudwatch != nil {
		return utils.MarshalJSON(u.OutputCloudwatch, "", true)
	}

	if u.OutputMinio != nil {
		return utils.MarshalJSON(u.OutputMinio, "", true)
	}

	if u.OutputStatsd != nil {
		return utils.MarshalJSON(u.OutputStatsd, "", true)
	}

	if u.OutputStatsdExt != nil {
		return utils.MarshalJSON(u.OutputStatsdExt, "", true)
	}

	if u.OutputGraphite != nil {
		return utils.MarshalJSON(u.OutputGraphite, "", true)
	}

	if u.OutputRouter != nil {
		return utils.MarshalJSON(u.OutputRouter, "", true)
	}

	if u.OutputSns != nil {
		return utils.MarshalJSON(u.OutputSns, "", true)
	}

	if u.OutputSqs != nil {
		return utils.MarshalJSON(u.OutputSqs, "", true)
	}

	if u.OutputSnmp != nil {
		return utils.MarshalJSON(u.OutputSnmp, "", true)
	}

	if u.OutputSumoLogic != nil {
		return utils.MarshalJSON(u.OutputSumoLogic, "", true)
	}

	if u.OutputDatadog != nil {
		return utils.MarshalJSON(u.OutputDatadog, "", true)
	}

	if u.OutputGrafanaCloud != nil {
		return utils.MarshalJSON(u.OutputGrafanaCloud, "", true)
	}

	if u.OutputLoki != nil {
		return utils.MarshalJSON(u.OutputLoki, "", true)
	}

	if u.OutputPrometheus != nil {
		return utils.MarshalJSON(u.OutputPrometheus, "", true)
	}

	if u.OutputRing != nil {
		return utils.MarshalJSON(u.OutputRing, "", true)
	}

	if u.OutputOpenTelemetry != nil {
		return utils.MarshalJSON(u.OutputOpenTelemetry, "", true)
	}

	if u.OutputServiceNow != nil {
		return utils.MarshalJSON(u.OutputServiceNow, "", true)
	}

	if u.OutputDataset != nil {
		return utils.MarshalJSON(u.OutputDataset, "", true)
	}

	if u.OutputCriblTCP != nil {
		return utils.MarshalJSON(u.OutputCriblTCP, "", true)
	}

	if u.OutputCriblHTTP != nil {
		return utils.MarshalJSON(u.OutputCriblHTTP, "", true)
	}

	if u.OutputHumioHec != nil {
		return utils.MarshalJSON(u.OutputHumioHec, "", true)
	}

	if u.OutputCrowdstrikeNextGenSiem != nil {
		return utils.MarshalJSON(u.OutputCrowdstrikeNextGenSiem, "", true)
	}

	if u.OutputDlS3 != nil {
		return utils.MarshalJSON(u.OutputDlS3, "", true)
	}

	if u.OutputSecurityLake != nil {
		return utils.MarshalJSON(u.OutputSecurityLake, "", true)
	}

	if u.OutputCriblLake != nil {
		return utils.MarshalJSON(u.OutputCriblLake, "", true)
	}

	if u.OutputDiskSpool != nil {
		return utils.MarshalJSON(u.OutputDiskSpool, "", true)
	}

	if u.OutputClickHouse != nil {
		return utils.MarshalJSON(u.OutputClickHouse, "", true)
	}

	if u.OutputXsiam != nil {
		return utils.MarshalJSON(u.OutputXsiam, "", true)
	}

	if u.OutputNetflow != nil {
		return utils.MarshalJSON(u.OutputNetflow, "", true)
	}

	if u.OutputDynatraceHTTP != nil {
		return utils.MarshalJSON(u.OutputDynatraceHTTP, "", true)
	}

	if u.OutputDynatraceOtlp != nil {
		return utils.MarshalJSON(u.OutputDynatraceOtlp, "", true)
	}

	if u.OutputSentinelOneAiSiem != nil {
		return utils.MarshalJSON(u.OutputSentinelOneAiSiem, "", true)
	}

	if u.OutputChronicle != nil {
		return utils.MarshalJSON(u.OutputChronicle, "", true)
	}

	if u.OutputDatabricks != nil {
		return utils.MarshalJSON(u.OutputDatabricks, "", true)
	}

	if u.OutputMicrosoftFabric != nil {
		return utils.MarshalJSON(u.OutputMicrosoftFabric, "", true)
	}

	if u.OutputCloudflareR2 != nil {
		return utils.MarshalJSON(u.OutputCloudflareR2, "", true)
	}

	return nil, errors.New("could not marshal union type Output: all fields are null")
}
